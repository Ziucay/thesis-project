uses_actions
"['pip install PyGithub', 'python .github/workflows/check-news.py', ""pip install -e '.[doc]'"", 'cd docs && make html', 'pip install build', 'python -m build . --sdist', 'python -m build . --wheel', 'python -m pip --version\npython -m pip install -e "".[test]""\n', 'python -m xonsh run-tests.xsh validate-news-items', 'python -m pip install . --no-deps\npython -m xonsh run-tests.xsh test -- --timeout=240\n', 'python -m pip install -e . --no-deps\npython -m xonsh run-tests.xsh test --report-coverage -- --timeout=240\n']"
"['sudo apt update\nsudo apt install -y graphviz\npython -m pip install --upgrade pip\npython -m pip install .[docs]\n', 'sphinx-build -M html ./doc ./build/sphinx -T -W --jobs=auto -n -vvv --keep-going\n', 'python --version', 'sudo apt-get install graphviz', 'python -m pip install --upgrade pip\npython -m pip install .[test] pytest-cov\n', 'python -m pytest -vv --cov . --cov-append --cov-config pyproject.toml', 'ln -s /usr/bin/python3 /usr/bin/python', 'python --version', 'python -m pip install --upgrade pip\npython -m pip install .[test]\n', 'python -X dev -X warn_default_encoding -m pytest -vv --color yes --durations 25\n', 'python -m pip install --upgrade pip', 'python -m pip install ruff==0.0.261', 'ruff . --diff --format github', 'python -m pip install --upgrade ruff', 'ruff . --diff --format github', 'python -m pip install --upgrade pip\npython -m pip install --upgrade ""flake8>=3.5.0"" ""flake8-simplify""\n', 'flake8 .', 'python -m pip install --upgrade pip\npython -m pip install --upgrade isort\n', 'isort --check-only --diff .', 'python -m pip install --upgrade pip\npython -m pip install --upgrade ""mypy>=0.990"" docutils-stubs types-requests\n', 'mypy sphinx/', 'python -m pip install --upgrade pip\npython -m pip install --upgrade sphinx-lint\n', 'sphinx-lint --enable line-too-long --max-line-length 85  CHANGES CONTRIBUTING.rst README.rst doc/\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade twine build\n', 'python -m build .\ntwine check dist/*\n', 'python --version', 'sudo apt-get install graphviz', 'python -m pip install --upgrade pip\npython -m pip install .[test]\n', 'python -m pip install --upgrade ""docutils~=${{ matrix.docutils }}.0""', 'python -m pytest -vv --durations 25', 'python -m pip install --upgrade pip\npython -m pip install .[test]\n', 'python -m pytest -vv --durations 25', 'python --version', 'sudo apt-get install graphviz', 'python -m pip install --upgrade pip\npython -m pip install .[test]\n', 'python -m pip install git+https://repo.or.cz/docutils.git\\#subdirectory=docutils', 'python -m pytest -vv', 'npm install', 'xvfb-run -a npm test', 'mkdir -p /tmp/tx_cli && cd $_\ncurl -o- https://raw.githubusercontent.com/transifex/cli/master/install.sh | bash\n', 'pip install --upgrade babel jinja2', 'python utils/babel_runner.py extract', 'cd sphinx/locale \n/tmp/tx_cli/tx push --source --use-git-timestamps --workers 10\n', 'mkdir -p /tmp/tx_cli && cd $_\ncurl -o- https://raw.githubusercontent.com/transifex/cli/master/install.sh | bash\n', 'pip install --upgrade babel jinja2', 'python utils/babel_runner.py extract', 'cd sphinx/locale \n/tmp/tx_cli/tx pull --translations --all --force --use-git-timestamps --workers 10\n', 'python utils/babel_runner.py compile']"
"['pip install tox', 'tox -e docset', 'python -Im pip install --upgrade wheel tox', 'V=${{ matrix.python-version }}\n\nif [[ ""$V"" = pypy-* ]]; then\n  V=pypy3\nelse\n  V=py$(echo $V | tr -d .)\nfi\n\necho TOX_PYTHON=$V >>$GITHUB_ENV\n', 'python -Im tox run -f ${{ env.TOX_PYTHON }}', 'python -Im pip install --upgrade coverage[toml]', 'python -Im coverage combine\npython -Im coverage html --skip-covered --skip-empty\npython -Im coverage report --fail-under=100\n', 'python -Im pip install --upgrade wheel tox', 'python -Im tox -e docs,changelog', 'python -Im pip install --upgrade wheel tox', 'python -Im tox run -e mypy', 'python -Im pip install --upgrade wheel tox', 'python -Im tox run -e pyright', 'python -Im pip install -e .[dev]', ""python -Ic 'import attr; print(attr.__version__)'"", ""python -Ic 'import attrs; print(attrs.__version__)'""]"
"['python -m pip install --upgrade pip\npython -m pip install -r development.txt\nsudo yarn install --non-interactive\nsudo yarn global add prettier@1.17.0\n', 'make lint', 'python -m pip install docker-compose==1.26.0\ndocker-compose -f docker-compose.yml config\n', 'python -m pip install --upgrade pip\npython -m pip install -r development.txt\nsudo yarn install --non-interactive\n', 'sudo rm -f /etc/boto.cfg\nmake test\n', 'python -m pip install --upgrade pip\npython -m pip install -r development.txt\nsudo yarn install --non-interactive\n', 'sudo rm -f /etc/boto.cfg\nmake test\n', 'python -m pip install --upgrade pip\npython -m pip install -r development.txt\nsudo yarn install --non-interactive\nsudo yarn global add prettier@1.17.0\n', 'sudo rm -f /etc/boto.cfg\nmake test\n']"
"['pip install poetry && poetry install', 'poetry build', 'poetry run twine upload dist/*', 'python scripts/insert_demo.py', 'cd site && ~/.gem/ruby/${{ matrix.ruby }}/jekyll build', 's3_website push', 'cd ..\nwget --spider -e robots=off -w 1 -r -p https://www.proselint.com\n', 'bundle exec danger', 'pip install poetry && poetry install', './utils ci lint', 'pip install poetry && poetry install', './utils ci test --coverage']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'python -m black --check markovify test', 'python -m flake8 markovify test', 'python -m site\npython -m pip install --upgrade pip setuptools wheel\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'make tests', 'python setup.py build sdist']"
"['pip3 install -r requirements-windows.txt\npip3 install pyinstaller\n', ""pyinstaller -Fw --add-data './assets;assets' KeymouseGo.py\n"", 'mkdir artifact && mv dist/KeymouseGo.exe ./artifact/KeymouseGo${{ env.keymousego-version }}-win.exe\n', 'pip3 install -r requirements-universal.txt\npip3 install pyinstaller\n', ""pyinstaller -Fw --add-data './assets:assets' KeymouseGo.py\n"", 'mkdir artifact && mv dist/KeymouseGo ./artifact/KeymouseGo${{ env.keymousego-version }}-linux\n', 'pip3 install -r requirements-universal.txt\npip3 install pyinstaller\n', ""pyinstaller -Fw --add-data './assets:assets' KeymouseGo.py\n"", 'mkdir artifact && mv dist/KeymouseGo.app ./artifact/KeymouseGo${{ env.keymousego-version }}-mac\n']"
"['python -m pip install -U pip', 'python -m pip install flake8', 'flake8 --show-source smart_open', 'python -m pip install -U pip', 'python -m pip install pyOpenSSL --upgrade', 'pip install -e .[test]', 'pytest smart_open -v -rfxECs --durations=20', 'python -m pip install -U pip', 'python -m pip install pyOpenSSL --upgrade', 'pip install -e .[test]', 'python ci_helpers/doctest.py', 'python -m pip install -U pip', 'python -m pip install pyOpenSSL --upgrade', 'python -m pip install numpy', 'pip install -e .[test]', 'bash ci_helpers/helpers.sh enable_moto_server', 'sudo apt-get install vsftpd\nsudo bash ci_helpers/helpers.sh create_ftp_ftps_servers\n', 'python ci_helpers/run_integration_tests.py', 'bash ci_helpers/helpers.sh disable_moto_server', 'sudo bash ci_helpers/helpers.sh delete_ftp_ftps_servers', 'python -m pip install -U pip', 'python -m pip install pyOpenSSL --upgrade', 'pip install -e .[test]', 'pip install awscli pytest_benchmark', 'python ci_helpers/run_benchmarks.py', 'echo ::set-output name=V::$(python smart_open/version.py)\n', 'python -m pip install --upgrade pip\npython -m venv venv\n. venv/bin/activate\npip install twine wheel\n', '. venv/bin/activate\npython setup.py sdist\ntwine upload dist/smart_open-${{ steps.get_version.outputs.V }}.tar.gz -u ${{ env.PYPI_USERNAME }} -p ${{ env.PYPI_PASSWORD }}\n', '. venv/bin/activate\npython setup.py bdist_wheel\ntwine upload dist/smart_open-${{ steps.get_version.outputs.V }}-py3-none-any.whl -u ${{ env.PYPI_USERNAME }} -p ${{ env.PYPI_PASSWORD }}\n']"
"['pip install --upgrade pip setuptools wheel', 'pip install black codespell mypy pytest ruff safety', 'ruff --format=github --ignore=E501,E701,E713,E722,F401,F403,F405,F841 --line-length=263 .', 'black --check . || true', 'codespell --ignore-words-list=""datas"" --skip=""./.git/*""', 'pip install -r requirements.txt', 'mypy --install-types --non-interactive . || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'python ./loki.py --noprocs --noindicator --dontwait --debug -p ./test', 'python ./loki.py --noprocs --noindicator --dontwait --debug --intense -p ./test', 'python ./loki.py --noprocs --noindicator --dontwait --debug --csv -p ./test']"
"['pip install tox', 'tox', 'pip install tox', 'tox', 'python setup.py sdist', 'pip install -r requirements-doc.txt', 'pip install awscli', 'pip install -e .', '(cd docs && make clean html)', 'aws s3 sync docs/_build/html s3://python-dependency-injector-docs --delete\naws cloudfront create-invalidation --distribution-id ${{ secrets.AWS_CLOUDFRONT_DISTRIBUTION_ID }} --path ""/*"" > /dev/null\necho ""Cache invalidation triggered""\n', 'pip install tox', 'tox', 'pip install tox', 'tox', 'pip install tox cython', 'make cythonize', 'tox', 'pip install tox', 'tox']"
"['pip install ""flake8<6.0.0""', 'pip install mypy\n', 'mypy git_savvy.py --platform=${{ matrix.platform }}\n', 'git config --global user.name GitSavvy\ngit config --global user.email gitsavvy@gitsavvy.com\n']"
"['sudo apt-get update\nsudo apt-get install gir1.2-pango-1.0\nsudo apt-get install gir1.2-gtk-3.0\nsudo apt-get install xdg-utils\nsudo apt-get install librsvg2-common\nsudo apt-get install libglib2.0-dev\nsudo apt-get install intltool\nsudo apt-get install python3-gi\nsudo apt-get install python3-cairo\nsudo apt-get install python3-gi-cairo\nsudo apt-get install python3-bsddb3\nsudo apt-get install python3-dev\nsudo apt-get install python3-nose\nsudo apt-get install python3-mock\nsudo apt-get install python3-icu\nsudo apt-get install python3-coverage\nsudo apt-get install python3-jsonschema\nsudo apt-get install libxml2-utils\nsudo apt-get install python3-lxml\nsudo apt-get install python-libxml2\nsudo apt-get install zlib1g-dev\nsudo apt-get install python3-setuptools\n', 'mkdir -p ~/.gramps/gramps52/plugins/\nwget https://github.com/gramps-project/addons/raw/master/gramps52/download/CliMerge.addon.tgz\ntar -C ~/.gramps/gramps52/plugins -xzf CliMerge.addon.tgz\nwget https://github.com/gramps-project/addons/raw/master/gramps52/download/ExportRaw.addon.tgz\ntar -C ~/.gramps/gramps52/plugins -xzf ExportRaw.addon.tgz\n', 'python3 setup.py build\n', 'python3 setup.py test\n', 'if git --no-pager grep --color -n --full-name \'[    ]$\' -- \\*.py; then\n  echo ""ERROR - Trailing whitespace found in source file(s)"";\n  exit 1;\nfi\n']"
"[""print('::set-output name=is-untagged-devel::true')"", ""print('::set-output name=release-requested::true')"", ""from hashlib import sha512\nfrom sys import version\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'print(\n  ""::set-output name=files-hash-key::${{\n      hashFiles(\n        \'requirements-dev.txt\',\n        \'setup.cfg\',\n        \'pyproject.toml\'\n      )\n  }}"",\n)\n', 'echo ""::set-output name=dir::$(python -m pip cache dir)""', 'git tag --points-at HEAD | xargs git tag --delete', 'python -m pip install --user --upgrade setuptools-scm', 'import setuptools_scm\nver = setuptools_scm.get_version(\n  ${{\n      steps.untagged-check.outputs.is-untagged-devel == \'true\'\n      && \'local_scheme=""no-local-version""\' || \'\'\n  }}\n)\nprint(\'::set-output name=dist-version::{ver}\'.format(ver=ver))\n', ""print('::set-output name=tag::v${{\n    steps.request-check.outputs.release-requested == 'true'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}')"", ""print('::set-output name=sdist::aiomysql-${{\n    steps.request-check.outputs.release-requested == 'true'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}.tar.gz')\nprint('::set-output name=wheel::aiomysql-${{\n    steps.request-check.outputs.release-requested == 'true'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}-py3-none-any.whl')\n"", ""from hashlib import sha512\nfrom sys import version\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'echo ""::set-output name=dir::$(python -m pip cache dir)""', 'python -m pip install --user --upgrade build', ""git tag -m '${{ needs.pre-setup.outputs.git-tag }}' '${{ needs.pre-setup.outputs.git-tag }}' -- ${{ github.event.inputs.release-commitish }}"", 'python -m build', ""ls -1 'dist/${{ needs.pre-setup.outputs.sdist-artifact-name }}' 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}'"", ""from hashlib import sha512\nfrom sys import version\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'echo ""::set-output name=dir::$(python -m pip cache dir)""', 'python -m pip install --user --requirement requirements-dev.txt', 'python -m twine check --strict dist/*\n', ""from sys import version_info\nis_stable_abi = version_info.releaselevel == 'final'\nprint(\n    '::set-output name=is-stable-abi::{is_stable_abi}'.\n    format(is_stable_abi=str(is_stable_abi).lower())\n)\n"", ""from hashlib import sha512\nfrom sys import version\nhash = sha512(version.encode()).hexdigest()\nprint('::set-output name=py-hash-key::{hash}'.format(hash=hash))\n"", 'echo ""::set-output name=dir::$(python -m pip cache dir)""', 'python -m pip install --user --upgrade pip', 'rm -rf aiomysql', 'python -m pip install --user --requirement requirements-dev.txt', ""python -m pip install --user 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}'"", 'python -m platform', 'python -c ""import platform; print(platform.version())""', 'python -c ""import platform; print(platform.uname())""', 'python -c ""import platform; print(platform.release())""', 'python -c ""import ssl; print(\'\\nOPENSSL_VERSION: \' + ssl.OPENSSL_VERSION + \'\\nOPENSSL_VERSION_INFO: \' + repr(ssl.OPENSSL_VERSION_INFO) + \'\\nOPENSSL_VERSION_NUMBER: \' + repr(ssl.OPENSSL_VERSION_NUMBER))""', '# ensure server is started up\nwhile :\ndo\n    sleep 1\n    mysql -h127.0.0.1 -uroot ""-p$MYSQL_ROOT_PASSWORD"" -e \'select version()\' && break\ndone\n\n# inject tls configuration\ndocker container stop mysqld\ndocker container cp ""${{ github.workspace }}/tests/ssl_resources/ssl"" mysqld:/etc/mysql/ssl\ndocker container cp ""${{ github.workspace }}/tests/ssl_resources/tls.cnf"" mysqld:/etc/mysql/conf.d/aiomysql-tls.cnf\n\n# use custom socket path\n# we need to ensure that the socket path is writable for the user running the DB process in the container\nsudo chmod 0777 /tmp/run-${{ join(matrix.db, \'-\') }}\n\ndocker container cp ""${{ github.workspace }}/tests/ssl_resources/socket.cnf"" mysqld:/etc/mysql/conf.d/aiomysql-socket.cnf\n\ndocker container start mysqld\n\n# ensure server is started up\nwhile :\ndo\n    sleep 1\n    mysql -h127.0.0.1 -uroot ""-p$MYSQL_ROOT_PASSWORD"" -e \'select version()\' && break\ndone\n\nmysql -h127.0.0.1 -uroot ""-p$MYSQL_ROOT_PASSWORD"" -e ""SET GLOBAL local_infile=on""\n', '# timeout ensures a more or less clean stop by sending a KeyboardInterrupt which will still provide useful logs\ntimeout --preserve-status --signal=INT --verbose 570s \\\n  pytest --capture=no --verbosity 2 --cov-report term --cov-report xml --cov aiomysql --cov tests ./tests --mysql-unix-socket ""unix-${{ join(matrix.db, \'\') }}=/tmp/run-${{ join(matrix.db, \'-\') }}/mysql.sock"" --mysql-address ""tcp-${{ join(matrix.db, \'\') }}=127.0.0.1:3306""\n', ""git tag -m '${{ needs.pre-setup.outputs.git-tag }}' '${{ needs.pre-setup.outputs.git-tag }}' -- ${{ github.event.inputs.release-commitish }}"", ""git push --atomic origin '${{ needs.pre-setup.outputs.git-tag }}'""]"
"['brew install ninja autoconf automake libtool cmake', 'wget https://github.com/llvm/llvm-project/releases/download/llvmorg-$LLVM_VERSION/openmp-$LLVM_VERSION.src.tar.xz\ntar xf openmp-$LLVM_VERSION.src.tar.xz\ncd openmp-$LLVM_VERSION.src\nmkdir build && cd build\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/local -DLIBOMP_ENABLE_SHARED=OFF -DCMAKE_OSX_ARCHITECTURES=${{ matrix.cmake_arch }}\ncmake --build .\ncmake --install .\n', 'pip install pyodide-build==0.21.0\npyodide build\n', 'choco install winflexbison3 ninja', '%VCPKG_INSTALLATION_ROOT%\\vcpkg.exe integrate install\n%VCPKG_INSTALLATION_ROOT%\\vcpkg.exe install libxml2:${{ matrix.vcpkg_arch }}-windows-static-md\n', 'sudo apt install ninja-build cmake flex bison', 'python setup.py build_c_core\npython setup.py sdist\npython setup.py install\n', 'pip install --prefer-binary numpy scipy pandas networkx pytest pytest-timeout\npython -m pytest -v tests\n', 'sudo apt install ninja-build cmake flex bison', 'python setup.py build_c_core\n', '# NOTE: install calls ""build"" first\npython setup.py install\n', 'pip install --prefer-binary pytest pytest-timeout\n', 'python -m pytest --capture=sys tests\n']"
"['python -m pip install -U pip\npip install -U setuptools wheel build twine\npip install -r requirements.txt\n', 'python setup.py build_ext --inplace\npython -m build\npython -m twine upload dist/*.tar.gz\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH\necho ""LD_LIBRARY_PATH=$HOME/.local/lib"" >> $GITHUB_ENV\n', 'pip install -U pip codecov\npip install -r requirements.txt -r requirements-test.txt\npython setup.py build_ext --inplace\npython setup.py install\n', 'pytest -vs tests/ --cov kaggler/\ncodecov\n']"
"['make develop-javascript\nmake develop-python\n', 'pre-commit run --all-files\nyarn eslint static/\n', 'make test']"
""
"['echo ""ðŸŽ‰ The job was automatically triggered by a ${{ github.event_name }} event.""', 'echo ""ðŸ”Ž The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.""', 'echo ""ðŸ’¡ The ${{ github.repository }} repository has been cloned to the runner.""', 'poetry install', 'poetry run bash -x verify.sh', 'echo ""ðŸ This job\'s status is ${{ job.status }}.""']"
"['echo ""::error Add \'changelog/*\' label"";\nexit 1;', 'echo ""Thank you!""', 'python -m pip install datadog_checks_dev[cli]\n', 'ddev config set pypi.user __token__\nddev config set pypi.pass ${{ secrets.PYPI_TOKEN }}\n', 'ddev release upload . --sdist\n', 'python -m pip install pre-commit\npython -m pip install tox\n', 'tox -e mypy', 'tox -e flake8', 'pip install tox', 'pip install tox', 'tox', 'tox -e integration -- --vcr-record=none', 'tox -e integration-admin -- --vcr-record=none', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e integration -- --vcr-record=all', 'tox -e integration-admin -- --vcr-record=all']"
"['python -m pip install --upgrade pip tox codecov\n', 'python --version\ntox -e coverage\n', 'codecov', 'python -m pip install --upgrade pip tox\n', 'python --version\ntox -e flake8less\n', 'python -m pip install --upgrade pip tox\n', 'python --version\ntox -e docs\n']"
"['pip install --upgrade pip setuptools wheel', 'pip install bandit black codespell flake8 flake8-2020 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101,B105,B106,B108,B110,B112,B303,B311,B314,B318,B324,B404,B405,B408,B413,B602,B603,B605,B607,B608 .', 'black --check . || true', 'codespell || true', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest . || pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
"['python build_conda_pkgs.py --build $pkg', 'tar -a -C $CONDA_PREFIX -cf $PWD/${artifact_name}.tar.bz2 conda-bld', 'if [[ $BUILD_MAC == ""true"" ]]; then\n    target_platform=""\'osx-64\'""\n    include=""{\'os\': \'macos-11\', \'target-platform\': \'osx-64\'}""\nfi\nif [[ $BUILD_LNX == ""true"" ]]; then\n    target_platform=${target_platform:+""$target_platform, ""}""\'linux-64\'""\n    include=${include:+""$include, ""}""{\'os\': \'ubuntu-latest\', \'target-platform\': \'linux-64\'}""\nfi\nif [[ $BUILD_WIN == ""true"" ]]; then\n    target_platform=${target_platform:+""$target_platform, ""}""\'win-64\'""\n    include=${include:+""$include, ""}""{\'os\': \'windows-latest\', \'target-platform\': \'win-64\'}""\nfi\n\npython_version=""\'3.9\'""\n\necho ""target_platform=[$target_platform]"" >> $GITHUB_OUTPUT\necho ""include=[$include]"" >> $GITHUB_OUTPUT\necho ""python_version=[$python_version]"" >> $GITHUB_OUTPUT\n', 'ARTIFACTS_PATH=$RUNNER_TEMP/artifacts\nCONDA_BLD_PATH=$RUNNER_TEMP/conda-bld\n\necho ""ARTIFACTS_PATH=$ARTIFACTS_PATH"" >> $GITHUB_ENV\necho ""CONDA_BLD_PATH=$CONDA_BLD_PATH"" >> $GITHUB_ENV\n\n[[ -d $ARTIFACTS_PATH ]] || mkdir $ARTIFACTS_PATH\n[[ -d $CONDA_BLD_PATH ]] || mkdir $CONDA_BLD_PATH\n\nenv | sort\n', 'files=($(find $ARTIFACTS_PATH -name *.tar.bz2))\necho ${files[@]}\ncd $(dirname $CONDA_BLD_PATH)\n\n[[ $RUNNER_OS == ""Windows"" ]] && opts=(""--force-local"") || opts=()\nfor file in ${files[@]}; do\n    tar -xf $file ${opts[@]}\ndone\n\nmamba index $CONDA_BLD_PATH\n\nmamba search -c $CONDA_BLD_PATH --override-channels || true\n', 'pkgs=(""spyder"")\nif [[ $IS_STANDARD_PR == ""true"" ]]; then\n    pkgs+=(""spyder-kernels"")\nfi\npython build_conda_pkgs.py --build ${pkgs[@]}\n', './certkeychain.sh ""${MACOS_CERTIFICATE_PWD}"" ""${MACOS_CERTIFICATE}"" ""${MACOS_INSTALLER_CERTIFICATE}""\nCNAME=$(security find-identity -p codesigning -v | pcre2grep -o1 ""\\(([0-9A-Z]+)\\)"")\necho ""CNAME=$CNAME"" >> $GITHUB_ENV\n\n_codesign=$(which codesign)\nif [[ $_codesign =~ ${CONDA_PREFIX}.* ]]; then\n    # Find correct codesign\n    echo ""Moving $_codesign...""\n    mv $_codesign ${_codesign}.bak\nfi\n', '[[ -n $CNAME ]] && args=(""--cert-id"" ""$CNAME"") || args=()\npython build_installers.py ${args[@]}\nPKG_PATH=$(python build_installers.py --artifact-name)\nPKG_NAME=$(basename $PKG_PATH)\nPKG_GLOB=${PKG_PATH%.*}\nPKG_BASE_NAME=${PKG_NAME%.*}\necho ""PKG_PATH=$PKG_PATH"" >> $GITHUB_ENV\necho ""PKG_NAME=$PKG_NAME"" >> $GITHUB_ENV\necho ""PKG_GLOB=$PKG_GLOB"" >> $GITHUB_ENV\necho ""PKG_BASE_NAME=$PKG_BASE_NAME"" >> $GITHUB_ENV\n', '# Stream install.log to stdout to view all log messages.\ntail -F /var/log/install.log & tail_id=$!\ntrap ""kill -s TERM $tail_id"" EXIT\n\ninstaller -pkg $PKG_PATH -target CurrentUserHomeDirectory >/dev/null\n\napp_path=$HOME/Applications/Spyder.app\nif [[ -e ""$app_path"" ]]; then\n    echo ""\\nContents of $app_path/Contents/MacOS:""\n    ls -al $app_path/Contents/MacOS\n    echo -e ""\\nContents of $app_path/Contents/Info.plist:""\n    cat $app_path/Contents/Info.plist\n    echo -e ""\\nContents of $app_path/Contents/MacOS/spyder-script:""\n    cat $app_path/Contents/MacOS/spyder-script\n    echo -e ""\\n\\nContents of"" $HOME/Library/spyder-*/uninstall-spyder.sh :\n    cat $HOME/Library/spyder-*/uninstall-spyder.sh\nelse\n    echo ""$app_path does not exist""\n    exit 1\nfi\n', '$PKG_PATH -b\n\nshortcut_path=$HOME/.local/share/applications/spyder_spyder.desktop\nif [[ -e $shortcut_path ]]; then\n    echo ""\\nContents of"" $HOME/.local/spyder-* :\n    ls -al $HOME/.local/spyder-*\n    echo -e ""\\nContents of ${shortcut_path}:""\n    cat $shortcut_path\n    echo -e ""\\nContents of"" $HOME/.local/spyder-*/uninstall-spyder.sh :\n    cat $HOME/.local/spyder-*/uninstall-spyder.sh\nelse\n    echo ""$shortcut_path does not exist""\n    exit 1\nfi\n', 'start /wait %PKG_PATH% /InstallationType=JustMe /NoRegistry=1 /S\n\nset ""shortcut_path=%USERPROFILE%\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\spyder\\Spyder.lnk""\nif exist ""%shortcut_path%"" (\n    echo ""Spyder installed successfully""\n) else (\n    echo ""Spyder NOT installed successfully""\n    EXIT /B 1\n)\nEXIT /B %ERRORLEVEL%\n', 'if [[ $RUNNER_OS == ""macOS"" ]]; then\n    ./notarize.sh -p $APPLICATION_PWD $PKG_PATH\nelse\n    cd $(dirname $PKG_PATH)\n    echo $(sha256sum $PKG_NAME) > ""${PKG_GLOB}-sha256sum.txt""\nfi\n', 'git fetch --prune --unshallow', 'sudo apt-get update --fix-missing\nsudo apt-get install -qq pyqt5-dev-tools libxcb-xinerama0 xterm --fix-missing\n', 'bash -l .github/scripts/install.sh', 'conda info\nconda list\n', 'xvfb-run --auto-servernum .github/scripts/modules_test.sh || xvfb-run --auto-servernum .github/scripts/modules_test.sh', 'git fetch --prune --unshallow', 'sudo apt-get update --fix-missing\nsudo apt-get install -qq pyqt5-dev-tools libxcb-xinerama0 xterm --fix-missing\n', 'bash -l .github/scripts/install.sh', 'micromamba info\nmicromamba list\n', 'micromamba info\nmicromamba list\npip list\n', 'check-manifest', 'xvfb-run --auto-servernum gdb -return-child-result -batch -ex r -ex py-bt --args python runtests.py -s', 'rm -f pytest_log.txt  # Must remove any log file from a previous run\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh\n', 'git fetch --prune --unshallow', 'bash -l .github/scripts/install.sh', 'micromamba info\nmicromamba list\n', 'check-manifest', 'rm -f pytest_log.txt  # Must remove any log file from a previous run\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh\n', 'git fetch --prune --unshallow', 'bash -l .github/scripts/install.sh', 'micromamba info\nmicromamba list\n', 'micromamba info\nmicromamba list\npip list\n', 'check-manifest', 'rm -f pytest_log.txt  # Must remove any log file from a previous run\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh || \\\nbash -l .github/scripts/run_tests.sh\n']"
"['if [[ $GITHUB_REF == refs/tags/* ]]; then\n  echo ""tag=${GITHUB_REF#refs/tags/}"" >> $GITHUB_OUTPUT\nelif [[ $GITHUB_REF == refs/heads/master ]]; then\n  echo ""tag=latest"" >> $GITHUB_OUTPUT\nelse\n  echo ""tag=${GITHUB_REF#refs/heads/}"" >> $GITHUB_OUTPUT\nfi\nif [[ $GITHUB_REF == refs/tags/*-beta ]]; then\n  echo ""branch=beta"" >> $GITHUB_OUTPUT\nelif [[ $GITHUB_REF == refs/tags/* ]]; then\n  echo ""branch=master"" >> $GITHUB_OUTPUT\nelse\n  echo ""branch=${GITHUB_REF#refs/heads/}"" >> $GITHUB_OUTPUT\nfi\necho ""commit=${GITHUB_SHA}"" >> $GITHUB_OUTPUT\necho ""build_date=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')"" >> $GITHUB_OUTPUT\necho ""docker_platforms=linux/amd64,linux/arm64/v8,linux/arm/v7,linux/arm/v6"" >> $GITHUB_OUTPUT\necho ""docker_image=${{ secrets.DOCKER_REPO }}/tautulli"" >> $GITHUB_OUTPUT\n', 'failures=(neutral, skipped, timed_out, action_required)\nif [[ ${array[@]} =~ $WORKFLOW_CONCLUSION ]]; then\n  echo ""status=failure"" >> $GITHUB_OUTPUT\nelse\n  echo ""status=$WORKFLOW_CONCLUSION"" >> $GITHUB_OUTPUT\nfi\n', 'if [[ $GITHUB_REF == refs/tags/* ]]; then\n  echo ""VERSION=${GITHUB_REF#refs/tags/v}"" >> $GITHUB_ENV\n  VERSION_NSIS=${GITHUB_REF#refs/tags/v}.1\n  echo ""VERSION_NSIS=${VERSION_NSIS/%-beta.1/.0}"" >> $GITHUB_OUTPUT\n  echo ""VERSION=${GITHUB_REF#refs/tags/v}"" >> $GITHUB_OUTPUT\n  echo ""RELEASE_VERSION=${GITHUB_REF#refs/tags/}"" >> $GITHUB_OUTPUT\nelse\n  echo ""VERSION=0.0.0"" >> $GITHUB_ENV\n  echo ""VERSION_NSIS=0.0.0.0"" >> $GITHUB_OUTPUT\n  echo ""VERSION=0.0.0"" >> $GITHUB_OUTPUT\n  echo ""RELEASE_VERSION=${GITHUB_SHA::7}"" >> $GITHUB_OUTPUT\nfi\nif [[ $GITHUB_REF == refs/tags/*-beta ]]; then\n  echo ""beta"" > branch.txt\nelif [[ $GITHUB_REF == refs/tags/* ]]; then\n  echo ""master"" > branch.txt\nelse\n  echo ${GITHUB_REF#refs/heads/} > branch.txt\nfi\necho $GITHUB_SHA > version.txt\n', 'python -m pip install --upgrade pip\npip install -r package/requirements-package.txt\n', 'pyinstaller -y ./package/Tautulli-${{ matrix.os }}.spec\n', 'sudo pkgbuild \\\n  --install-location /Applications \\\n  --version ${{ steps.get_version.outputs.VERSION }} \\\n  --component ./dist/Tautulli.app \\\n  --scripts ./package/macos-scripts \\\n  Tautulli-macos-${{ steps.get_version.outputs.RELEASE_VERSION }}-x64.pkg\n', 'echo ""RELEASE_VERSION=${GITHUB_REF#refs/tags/}"" >> $GITHUB_OUTPUT\n', 'CHANGELOG=""$( sed -n \'/^## /{p; :loop n; p; /^## /q; b loop}\' CHANGELOG.md \\\n  | sed \'$d\' | sed \'$d\' | sed \'$d\' )""\nEOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)\necho ""CHANGELOG<<$EOF"" >> $GITHUB_OUTPUT\necho ""$CHANGELOG"" >> $GITHUB_OUTPUT\necho ""$EOF"" >> $GITHUB_OUTPUT\n', 'failures=(neutral, skipped, timed_out, action_required)\nif [[ ${array[@]} =~ $WORKFLOW_CONCLUSION ]]; then\n  echo ""status=failure"" >> $GITHUB_OUTPUT\nelse\n  echo ""status=$WORKFLOW_CONCLUSION"" >> $GITHUB_OUTPUT\nfi\n', 'git fetch --prune --unshallow --tags\nif [[ $GITHUB_REF == refs/tags/*-beta || $GITHUB_REF == refs/heads/beta ]]; then\n  echo ""RELEASE=beta"" >> $GITHUB_OUTPUT\nelif [[ $GITHUB_REF == refs/tags/* || $GITHUB_REF == refs/heads/master ]]; then\n  echo ""RELEASE=stable"" >> $GITHUB_OUTPUT\nelse\n  echo ""RELEASE=edge"" >> $GITHUB_OUTPUT\nfi\n', 'failures=(neutral, skipped, timed_out, action_required)\nif [[ ${array[@]} =~ $WORKFLOW_CONCLUSION ]]; then\n  echo ""status=failure"" >> $GITHUB_OUTPUT\nelse\n  echo ""status=$WORKFLOW_CONCLUSION"" >> $GITHUB_OUTPUT\nfi\n', 'echo Base: ""$GITHUB_BASE_REF""\necho Head: ""$GITHUB_HEAD_REF""\nexit 1\n', '$wingetPackage = ""Tautulli.Tautulli""\n$gitToken = ""${{ secrets.WINGET_TOKEN }}""\n\n$github = Invoke-RestMethod -uri ""https://api.github.com/repos/Tautulli/Tautulli/releases/latest""\n$installerUrl = $github | Select -ExpandProperty assets -First 1 | Where-Object -Property name -match ""Tautulli-windows-.*-x64.exe"" | Select -ExpandProperty browser_download_url\n$version = ""$($github.tag_name.Trim(\'v\')).1""\n\n# getting latest wingetcreate file\niwr https://aka.ms/wingetcreate/latest -OutFile wingetcreate.exe\n.\\wingetcreate.exe update $wingetPackage -s -v $version -u $installerUrl -t $gitToken\n']"
"['pip install virtualenv\nvirtualenv --python=python3 .venv\n', 'source .venv/bin/activate\npython --version\npip --version\nnode --version\nnpm --version\n', 'source .venv/bin/activate\nmake build-kinto-admin\n', 'echo ""geckodriver/firefox""\nwhich geckodriver\ngeckodriver --version\nwhich firefox\nfirefox --version\n', 'source .venv/bin/activate\nmake install-dev\n', 'source .venv/bin/activate\nkinto start --ini tests/browser.ini & sleep 5\n', 'source .venv/bin/activate\nmake browser-test\n', 'pip install virtualenv\nvirtualenv --python=python3 .venv\n', 'source .venv/bin/activate\npython --version\npip --version\n', 'source .venv/bin/activate\nmake docs\n', 'source .venv/bin/activate\nmake test-description\n', 'pip install virtualenv\nvirtualenv --python=python3 .venv\n', 'source .venv/bin/activate\npython --version\npip --version\n', 'source .venv/bin/activate\nmake install-dev\nmake install-postgres\n', 'psql -c ""CREATE DATABASE testdb ENCODING \'UTF8\' TEMPLATE template0;"" -U postgres -h localhost\n', 'source .venv/bin/activate\nmake runkinto & sleep 5\n', 'source .venv/bin/activate\nmake functional\n', 'pip install virtualenv\nvirtualenv --python=python3 .venv\n', 'source .venv/bin/activate\npython --version\npip --version\n', 'source .venv/bin/activate\nmake lint\n', 'pip install virtualenv\nvirtualenv --python=python3 .venv\n', 'source .venv/bin/activate\npython --version\npip --version\n', 'source .venv/bin/activate\nmake install-dev\nmake install-postgres\npip install tox coveralls\n', 'psql -c ""CREATE DATABASE testdb ENCODING \'UTF8\' TEMPLATE template0;"" -U postgres -h localhost\n', 'source .venv/bin/activate\ntox -e ${{ matrix.toxenv }}\n', 'source .venv/bin/activate\ncoveralls --service=github\n', 'pip install coveralls', 'coveralls --service=github --finish']"
"['sudo gem install apt-spy2\nsudo apt-spy2 fix --commit --launchpad --country=US\n', 'sudo bash mycodo/scripts/upgrade_commands.sh update-apt\nsudo apt install -y libatlas-base-dev libboost-python-dev gawk git libffi-dev libi2c-dev nginx python3-dev sqlite3 swig\nsudo bash mycodo/scripts/upgrade_commands.sh update-packages\ngit clone --recursive https://github.com/WiringPi/WiringPi-Python.git && cd WiringPi-Python && git submodule update --init && cd WiringPi && ./build && cd ../..\nsudo bash mycodo/scripts/upgrade_commands.sh build-pigpiod\nsudo bash mycodo/scripts/upgrade_commands.sh update-influxdb-1\nsudo service influxdb start && sleep 3\nsudo bash mycodo/scripts/upgrade_commands.sh update-influxdb-1-db-user\nsudo useradd -M mycodo\nexport PATH=/usr/bin:$PATH\n', 'sudo bash mycodo/scripts/upgrade_commands.sh setup-virtualenv\nsudo bash mycodo/scripts/upgrade_commands.sh update-pip3\nsudo bash mycodo/scripts/upgrade_commands.sh update-pip3-packages\nsudo bash mycodo/scripts/upgrade_commands.sh ssl-certs-generate\nsudo bash mycodo/scripts/upgrade_commands.sh compile-translations\nsudo bash mycodo/scripts/upgrade_commands.sh generate-widget-html\nsudo bash mycodo/scripts/generate_translations_pybabel.sh\nprintf ""\\n#### Generating Docs\\n""\nenv/bin/python mycodo/scripts/generate_manual_actions.py\nenv/bin/python mycodo/scripts/generate_manual_functions.py\nenv/bin/python mycodo/scripts/generate_manual_inputs_by_measure.py\nenv/bin/python mycodo/scripts/generate_manual_inputs.py\nenv/bin/python mycodo/scripts/generate_manual_outputs.py\nenv/bin/python mycodo/scripts/generate_manual_widgets.py\nprintf ""\\n#### Generating API Manual\\n""\nsudo env/bin/python mycodo/start_flask_ui.py &\nsleep 10\nwget --no-check-certificate -p https://127.0.0.1 -O /dev/null\nsudo apt install npm\nsudo npm install -g redoc-cli\nsudo bash mycodo/scripts/generate_manual_api.sh\n', 'cd mycodo && ../env/bin/pytest -W ignore::DeprecationWarning -s tests/software_tests\n']"
"['python -m pip install --upgrade pip poetry\npoetry install\nsudo apt-get install pandoc\n', 'poetry run pylint functional', 'poetry run black --check functional', 'poetry run pytest --cov=functional --cov-report=xml\n']"
""
"['echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\necho ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh --coverage --skip_flakey_fails -api lib/galaxy_test/api -- --num-shards=2 --shard-id=${{ matrix.chunk }}', 'echo ""sha_short=$(git rev-parse --short HEAD)"" >> $GITHUB_OUTPUT', 'if [[ ""$GITHUB_REF"" == ""refs/tags/""* ]]; then\n  echo ""name=${GITHUB_REF#refs/tags/v}"" >> $GITHUB_OUTPUT\nelif [[ ""$GITHUB_REF"" == ""refs/heads/dev"" ]]; then\n  echo ""name=dev"" >> $GITHUB_OUTPUT\nelif [[ ""$GITHUB_REF"" == ""refs/heads/release_""* ]]; then\n  echo ""name=${GITHUB_REF#refs/heads/release_}-auto"" >> $GITHUB_OUTPUT\nfi\n', ""docker build . --build-arg GIT_COMMIT=$(git rev-parse HEAD) --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') --build-arg IMAGE_TAG=${{ steps.branch.outputs.name }} -t galaxy/galaxy-min:${{ steps.branch.outputs.name }} -t quay.io/galaxyproject/galaxy-min:${{ steps.branch.outputs.name }} -f .k8s_ci.Dockerfile"", 'echo ""FROM galaxy/galaxy-min:${{ steps.branch.outputs.name }}"" | docker build --label ""quay.expires-after""=""90d"" -t ""quay.io/galaxyproject/galaxy-k8s-auto:${{ steps.commit.outputs.sha_short }}"" -', 'pip install -r requirements.txt -r lib/galaxy/dependencies/dev-requirements.txt', '.ci/check_test_class_names.sh', 'echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\necho ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', ""rsync -av --remove-source-files --exclude .git galaxy-test-data/ 'galaxy root/test-data/'"", 'pip install planemo', 'ls \'galaxy root\'/lib/galaxy/datatypes/converters/*xml | grep -v -f \'galaxy root\'/lib/galaxy/datatypes/converters/.tt_skip > tool_list.txt\necho ""Skipping checks for the following converters:""\nls \'galaxy root\'/lib/galaxy/datatypes/converters/*xml | grep -f \'galaxy root\'/lib/galaxy/datatypes/converters/.tt_skip\necho ""Checking only the following converters:""\ncat tool_list.txt\n', 'mapfile -t TOOL_ARRAY < tool_list.txt\nplanemo lint --skip citations,stdio,help --report_level warn ""${TOOL_ARRAY[@]}""\n', 'mapfile -t TOOL_ARRAY < tool_list.txt\nplanemo test --galaxy_python_version ${{ matrix.python-version }} --galaxy_root \'galaxy root\' ""${TOOL_ARRAY[@]}""\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh --coverage --skip_flakey_fails -cwl lib/galaxy_test/api/cwl -- -m ""${{ matrix.marker }} and ${{ matrix.conformance-version }}""', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'pip install tox', ""echo 'GALAXY_CONFIG_OVERRIDE_DATABASE_CONNECTION=postgresql://postgres:postgres@localhost:5432/galaxy?client_encoding=utf8' >> $GITHUB_ENV"", 'tox -e check_indexes', 'python -m venv .venv\nmake update-dependencies\n', 'echo ""TARGET_BRANCH=${GITHUB_REF##*/}"" >> $GITHUB_ENV', 'echo ""TARGET_BRANCH=$GITHUB_BASE_REF"" >> $GITHUB_ENV', 'echo $TARGET_BRANCH', 'pip install -r requirements.txt -r lib/galaxy/dependencies/dev-requirements.txt sphinxcontrib-simpleversioning', 'sed -i -e ""/html_theme_options = {/a\\\n\\    \'analytics_id\': \'UA-45719423-17\',"" -e ""s#https://docs.galaxyproject.org/en/[^/]*/#https://docs.galaxyproject.org/en/$TARGET_BRANCH/#"" doc/source/conf.py\n', '# We cannot just download the latest version from dev, because it may be newer in this branch/PR\ngit fetch origin dev:dev\nif [ ! -f doc/source/conf.versioning.py ] || [ ""$(git log -1 --pretty=""format:%ct"" dev -- doc/source/conf.versioning.py)"" -gt ""$(git log -1 --pretty=""format:%ct"" -- doc/source/conf.versioning.py)"" ]; then\n    git checkout dev -- doc/source/conf.versioning.py\nfi\n', 'cat doc/source/conf.versioning.py >> doc/source/conf.py', 'make docs', 'case ""$TARGET_BRANCH"" in\n    release_[[:digit:]][[:digit:]].[[:digit:]][[:digit:]]|master)\n        UPLOAD_DIR=$TARGET_BRANCH\n        ;;\n    dev)\n        UPLOAD_DIR=latest\n        ;;\n    *)\n        echo ""Not deploying documentation for branch $TARGET_BRANCH""\n        exit 0\n        ;;\nesac\npip install awscli\naws s3 sync doc/build/html/ ""s3://galaxy-docs/en/$UPLOAD_DIR"" --region us-east-2 --size-only --delete\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'pip install tox', 'tox -e first_startup', 'echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\necho ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh --coverage --framework', 'echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\n# Skip outputs_to_working_directory: true in integration tests, doesn\'t work with pulsar\n# echo ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', 'docker system prune -a -f', 'rm -Rf /usr/share/dotnet', 'sudo apt-get update && sudo apt-get -y install conntrack ffmpeg', 'eval ${{ steps.minikube.outputs.launcher }}', 'kubectl get pods\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', '. .ci/minikube-test-setup/start_services.sh\n./run_tests.sh --coverage -integration test/integration -- --num-shards=4 --shard-id=${{ matrix.chunk }}\n', 'echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\necho ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', 'docker system prune -a -f', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh --coverage -integration test/integration_selenium', 'yarn install --frozen-lockfile', 'yarn run gulp client', 'yarn run qunit', 'yarn jest', 'yarn install --frozen-lockfile', 'yarn run eslint', 'yarn run format-check', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'pip install tox', 'tox -e lint', 'tox -e lint_docstring_include_list', 'tox -e mypy', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './scripts/common_startup.sh --skip-client-build', 'make lint-api-schema', 'make update-client-api-schema', 'if [[ `git status --porcelain` ]]; then\n  echo ""Rebuilding client/src/schema/schema.ts resulted in changes, run \'make update-client-api-schema\' and commit results""\n  exit 1\nfi\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'pip install tox', 'tox -e mulled', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'pip install tox', 'tox -e first_startup', 'echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\necho ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh --ci_test_metrics --structured_data_html --structured_data_report_file ""test.json"" --skip_flakey_fails -api lib/galaxy_test/performance', 'pip install build click docutils packaging PyGithub requests twine', 'python scripts/release.py build-and-upload --no-confirm\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'pip install tox', 'tox -e reports_startup', 'echo ""GALAXY_CONFIG_OVERRIDE_METADATA_STRATEGY=extended"" >> $GITHUB_ENV\necho ""GALAXY_CONFIG_OVERRIDE_OUTPUTS_TO_WORKING_DIRECTORY=true"" >> $GITHUB_ENV\n', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh --coverage -selenium lib/galaxy_test/selenium -- --num-shards=3 --shard-id=${{ matrix.chunk }}', 'sudo apt-get update && sudo apt-get -y install ffmpeg', 'pip install tox', 'tox -e test_galaxy_packages', './test/release.sh', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh -toolshed', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', './run_tests.sh -unit test/unit/data/model/migrations/test_migrations.py', 'echo ""version=$(python -c \'import sys; print(""-"".join(str(v) for v in sys.version_info))\')"" >> $GITHUB_OUTPUT', 'sudo apt-get update && sudo apt-get -y install ffmpeg', 'pip install tox', 'tox -e unit-coverage']"
"['python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox -e release']"
""
""
"['sudo apt-get update\nsudo apt-get install --yes --no-install-recommends \\\n  python3-configobj \\\n  python3-cheetah \\\n  python3-ephem \\\n  python3-mock \\\n  python3-mysqldb \\\n  python3-pil \\\n  python3-pillow \\\n  python3-serial \\\n  python3-usb \\\n  python3-pip \\\n  mariadb-client \\\n  mariadb-server \\\n  rsync \\\n  sqlite\n', 'sudo make test-setup-ci\n', 'make test\n']"
"['pip install asv==0.4.2', 'cd benchmarks\nasv machine --yes\nasv run HEAD^! --quick --dry-run --show-stderr\n', 'python -m pip install --upgrade pip\npython -m pip install build\n', 'python -m build', 'conda list', 'pytest pvlib/tests/iotools pvlib/tests/test_forecast.py --cov=./ --cov-report=xml --remote-data', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'micromamba list', 'python -m pip install --no-deps .', 'pip install .[test]\npip freeze\n', '# ignore iotools & forecast; those tests are run in a separate workflow\npytest pvlib --cov=./ --cov-report=xml --ignore=pvlib/tests/iotools --ignore=pvlib/tests/test_forecast.py\n']"
"['pip install psutil\npython setup.py test\n', 'pip install coverage\npip install psutil\npython setup.py develop\ncoverage run -m unittest discover tests\n', 'pip install flake8\npython -m flake8 ./janome\n', 'pip install mypy\npython -m mypy ./janome/*.py\n']"
"['pipx install poetry', 'poetry config pypi-token.pypi ${{ secrets.pypy }}', 'poetry publish --build', 'pipx install poetry', 'poetry install --with dev', 'poetry run ruff muffin', 'poetry run mypy', 'poetry run pytest tests']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\n', 'python -m pip install sphinx\npython -m pip install sphinx_rtd_theme\npython -m pip install docutils==0.12\npython -m pip install mock\ncd docs; make clean; make html; cd ..;\n', 'python -m pip install coverage\ncoverage run --source=axelrod -m unittest discover\n', 'coverage report -m --fail-under=100\n', 'python doctests.py\n', 'python -m pip install mypy\npython run_mypy.py\n', 'python -m pip install ""isort==4.3.21""\npython -m isort --check-only --recursive axelrod/.\n', 'python run_strategy_indexer.py\n', 'python -m pip install pylint\npython -m pylint --disable=all --enable=unused-import axelrod/strategies/_strategies.py\n', 'python setup.py install\ncd ..\npython -c ""import axelrod""\n']"
""
"['python -m pip install ""tox>=3,<4""', 'sudo apt-get update\nsudo apt-get install -y apt-transport-https curl\ncurl -s https://brave-browser-apt-release.s3.brave.com/brave-core.asc | sudo apt-key add -\necho ""deb [arch=amd64] https://brave-browser-apt-release.s3.brave.com/ stable main"" | sudo tee /etc/apt/sources.list.d/brave-browser-release.list\nsudo apt-get update\nsudo apt-get install --yes brave-browser\n', 'xvfb-run -- python -m tox -e py', 'xvfb-run -- python -m tox -e py', 'python -m tox -e lint', 'python -m pip install --upgrade pip\npython -m pip install build setuptools wheel twine\n', 'python -m build\npython -m twine upload dist/*\n']"
"['set -x\nsudo apt-get update\nsudo apt-get install --no-install-recommends --yes -V \\\n    debhelper \\\n    dh-python \\\n    file \\\n    gettext \\\n    gir1.2-gdkpixbuf-2.0 \\\n    gir1.2-glib-2.0 \\\n    gir1.2-gstreamer-1.0 \\\n    gir1.2-gtk-3.0 \\\n    gir1.2-gtksource-3.0 \\\n    gir1.2-pango-1.0 \\\n    gir1.2-rsvg-2.0 \\\n    gstreamer1.0-plugins-base \\\n    gstreamer1.0-plugins-good \\\n    libgirepository1.0-dev \\\n    python3-all \\\n    python3-stdeb\n', ""set -x\npython3 --version\npython3 -m venv venv\nsource venv/bin/activate\n\npip3 install -U pip setuptools wheel\npip3 install -r requirements.txt\ndiff -u0 <(sed -e 's,#.*,,' -e '/^$/d' < requirements.txt | sort -f) <(pip3 freeze | sort -f)  # enforces complete pinning\n\ntime PYTHONPATH=lib python3 pgn2ecodb.py\ntime PYTHONPATH=lib python3 create_theme_preview.py\n\npip3 install -e .\n\npychess --help  # smoke test\n"", 'set -x\nbash -x create_deb.sh\nrm -Rf deb_dist/pychess-*/\n', 'summarize_deb_file() {\n    local filename=""$1""\n    ls -lh ""${filename}""\n    file ""${filename}""\n}\n\nrecompress_deb_file() {\n    # NOTE: The idea is to get rid of zstd compression\n    #       because e.g. Debian bullseye cannot yet handle it\n    #       https://github.com/pychess/pychess/issues/2049\n    local filename=""$1""\n    local tempdir=""$(mktemp -d)""\n    sudo dpkg-deb --raw-extract ""${filename}"" ""${tempdir}""\n    sudo dpkg-deb --build -Zxz ""${tempdir}"" ""${filename}""\n    sudo rm -Rf ""${tempdir}""\n}\n\nset -x\nsummarize_deb_file deb_dist/*.deb\nrecompress_deb_file deb_dist/*.deb\nsummarize_deb_file deb_dist/*.deb\n', 'set -x -u\nuse_deadsnakes=""$(python3 -c \'import os, sys; print(""false"" if ""."".join(str(e) for e in sys.version_info[:2]) == os.environ[""PYVER""] else ""true"")\')""\n\nif ${use_deadsnakes}; then\n    # NOTE: We are avoiding actions/setup-python only because\n    #       we need Python interpreters at /usr/bin/python3.XX\n    #       rather than /opt/hostedtoolcache/[..] for bdist_rpm.\n    sudo add-apt-repository ppa:deadsnakes/ppa\nfi\n\nsudo apt-get update\n\nsudo apt-get install --no-install-recommends --yes -V \\\n    gettext \\\n    gir1.2-gdkpixbuf-2.0 \\\n    gir1.2-glib-2.0 \\\n    gir1.2-gstreamer-1.0 \\\n    gir1.2-gtk-3.0 \\\n    gir1.2-gtksource-3.0 \\\n    gir1.2-pango-1.0 \\\n    gir1.2-rsvg-2.0 \\\n    gstreamer1.0-plugins-base \\\n    gstreamer1.0-plugins-good \\\n    libgirepository1.0-dev\n\nif ${use_deadsnakes}; then\n    sudo apt-get install --no-install-recommends --yes -V \\\n        python${PYVER} \\\n        python${PYVER}-dev \\\n        python${PYVER}-venv\nfi\n\napt-cache policy python${PYVER}\n\nls -l /usr/bin/python*\n\nif ${use_deadsnakes}; then\n    sudo rm /usr/bin/python\n    sudo rm /usr/bin/python3\n    sudo rm /usr/bin/python3-config\n    sudo ln -s python${PYVER} /usr/bin/python\n    sudo ln -s python${PYVER} /usr/bin/python3\n    sudo ln -s python${PYVER}-config /usr/bin/python3-config\nfi\n\nsed \'s,:,\\n,g\' <<<""${PATH}""\n\nwhich python\nwhich python3\n\npython --version\npython3 --version\n\n# At least for non-deadsnakes Python, we would end up with files\n# in /usr/local rather than /usr without this adjustment:\ncat <<PYDISTUTILS_CFG_EOF | tee ~/.pydistutils.cfg\n[install]\nprefix = /usr\nPYDISTUTILS_CFG_EOF\n', ""set -x -u\npython3 --version\npython3 -m venv venv\nsource venv/bin/activate\n\npip3 install -U pip setuptools wheel\npip3 install -r requirements.txt\ndiff -u0 <(sed -e 's,#.*,,' -e '/^$/d' < requirements.txt | sort -f) <(pip3 freeze | sort -f)  # enforces complete pinning\n\ntime PYTHONPATH=lib python3 pgn2ecodb.py\ntime PYTHONPATH=lib python3 create_theme_preview.py\n\npip3 install -e .\n\npychess --help  # smoke test\n"", 'set -x -u\nbash -x ""create_rpm_py${PYVER/./}.sh""\n', 'set -x\nsudo apt-get update\nsudo apt-get install --no-install-recommends --yes -V \\\n    gettext \\\n    gir1.2-gdkpixbuf-2.0 \\\n    gir1.2-glib-2.0 \\\n    gir1.2-gstreamer-1.0 \\\n    gir1.2-gtk-3.0 \\\n    gir1.2-gtksource-3.0 \\\n    gir1.2-pango-1.0 \\\n    gir1.2-rsvg-2.0 \\\n    gstreamer1.0-plugins-base \\\n    gstreamer1.0-plugins-good \\\n    libgirepository1.0-dev \\\n    stockfish \\\n    xvfb\n', ""set -x\npython3 --version\npython3 -m venv venv\nsource venv/bin/activate\n\npip3 install -U pip setuptools wheel\npip3 install -r requirements.txt\ndiff -u0 <(sed -e 's,#.*,,' -e '/^$/d' < requirements.txt | sort -f) <(pip3 freeze | sort -f)  # enforces complete pinning\n\ntime PYTHONPATH=lib python3 pgn2ecodb.py\ntime PYTHONPATH=lib python3 create_theme_preview.py\n\npip3 install -e .\n\npychess --help  # smoke test\n"", 'set -x\nsource venv/bin/activate\n\nexport DISPLAY=:1\nXvfb ""${DISPLAY}"" &\nxvfb_pid=$!\n\ncd testing\n./run3 run_tests.py\n\nkill -s SIGTERM ""${xvfb_pid}""\n', 'pip3 install websockets\n', 'set -x\nPYTHONPATH=lib python3 pgn2ecodb.py\nPYTHONPATH=lib python3 create_theme_preview.py\npython3 setup.py bdist_msi --help\npython3 setup.py bdist_msi']"
"[""python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade 'tox>=4.0.0rc3'\n"", 'sudo apt-get update\nsudo apt-get install -y percona-toolkit\n', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine\npython -m coverage html --skip-covered --skip-empty\npython -m coverage report --fail-under=100\n']"
"['python -m pip install --upgrade pip setuptools\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install black==22.3.0 isort==5.10.1 flake8==4.0.1\n', 'sh shell/format.sh', 'python -m pip install --upgrade pip setuptools\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install black==22.3.0 isort==5.10.1 flake8==4.0.1\n', 'sh shell/lint.sh']"
"['pip3 install --upgrade pip\nsudo apt install python3-setuptools libpango1.0-dev\npip3 install -r docs/requirements.txt\npip3 install -r requirements.txt\n', 'cd docs\nexport PATH=""$PATH:/home/runner/.local/bin""\nexport SPHINXBUILD=""python3 -m sphinx""\nmake html\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine build\n', 'python setup.py bdist_wheel --python-tag ${{ matrix.python }}', 'twine upload dist/*']"
"['python -m pip install --upgrade pip\npip install ta-lib==0.4.24 --index=https://pypi.vnpy.com\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['python -m pip install --upgrade pip\npython -m pip install autopep8 pycodestyle\n', ""export HASH_SHORT=$(git rev-parse --short HEAD)\ngit checkout -b format--${HASH_SHORT}\ngit config --global user.email $GIT_EMAIL\ngit config --global user.name  $GIT_ACTOR\npython -m autopep8 --in-place --aggressive --aggressive --experimental -r ./\ngit add -A\ngit commit -m 'Format by autopep8' -m From: -m $(git rev-parse HEAD)\ngit push --set-upstream origin format--${HASH_SHORT}\n"", 'python -m pip install --upgrade pip\npip install flake8\n', 'python -m flake8 . --builtins=_,I --ignore=E501 --count --benchmark --show-source --statistics\n']"
"['python -m pip install --upgrade --disable-pip-version-check pip', 'pip install . pytest pytest-cov', 'pytest', 'pip install \\\n  --disable-pip-version-check \\\n  --no-warn-script-location \\\n  --user \\\n  .\necho ""PATH=${HOME}/.local/bin:${PATH}"" >> ""${GITHUB_ENV}""']"
""
"['pip install flake8\nflake8 --ignore=E126,E203,E401,E701,E711,E712,E722,E731,E741,F401,F841,W291,W503,W605 \\\n       --count --max-complexity=62 --max-line-length=3266 --show-source --statistics\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo apt update\nsudo apt upgrade -y\n', 'sudo apt install -y libgmp3-dev\n', 'sudo apt install -y libmpc-dev\n', 'sudo apt install sagemath\n', 'pip install wheel\npip install -r ""requirements.txt""\n', './RsaCtfTool.py --tests\n']"
"['DOCKER_IMAGE=""quay.io/pypa/${{ matrix.platform }}""\necho ""DOCKER_IMAGE=$DOCKER_IMAGE"" >> $GITHUB_ENV\ndocker image pull ""$DOCKER_IMAGE""\n', 'docker container run \\\n  --rm \\\n  -e PLAT=${{ matrix.platform }} \\\n  -e PYTHON_TAGS=${{ matrix.pyver }} \\\n  -v ""$(pwd):/io"" \\\n  ""$DOCKER_IMAGE"" \\\n  /io/build_scripts/wheels/manylinux.sh\n', './build_scripts/wheels/macos.sh', 'Invoke-WebRequest -Uri ${{ matrix.python-url }} -OutFile python-installer.exe', '.\\python-installer /quiet /passive Include_debug=1 PrependPath=1 TargetDir=C:\\Python | more', 'set LIB=C:\\Python\\Libs;%LIB%\nset TYPESHED_HOME=%cd%\\typeshed\nset PATH=C:\\Python;C:\\Python\\Scripts;%PATH%\npip install -r requirements.txt\npython build_scripts/ci_script.py\n', 'sudo apt-get install g++\nsudo apt-get install cmake\nsudo apt-get install bison\nsudo apt-get install flex\n', 'pip install -r requirements.txt', 'python build_scripts/ci_script.py']"
"['python -c ""import commix""', 'python commix.py --smoke-test']"
""
"['pip install -r requirements-tests.txt', 'python tests/stubtest_stdlib.py', 'pip install -r requirements-tests.txt', 'PACKAGES=$(python tests/get_stubtest_system_requirements.py)\n\nif [ ""${{ runner.os }}"" = ""Linux"" ]; then\n  if [ -n ""$PACKAGES"" ]; then\n    sudo apt update && sudo apt install -y $PACKAGES\n  fi\n\n  PYTHON_EXECUTABLE=""xvfb-run python""\nelse\n  if [ ""${{ runner.os }}"" = ""macOS"" ] && [ -n ""$PACKAGES"" ]; then\n    brew install $PACKAGES\n  fi\n\n  if [ ""${{ runner.os }}"" = ""Windows"" ] && [ -n ""$PACKAGES"" ]; then\n    choco install -y $PACKAGES\n  fi\n\n  PYTHON_EXECUTABLE=""python""\nfi\n\n$PYTHON_EXECUTABLE tests/stubtest_third_party.py --specified-platforms-only --num-shards 4 --shard-index ${{ matrix.shard-index }}\n', 'cd stub_uploader\npip install -r requirements.txt\npython -m pytest tests\n', 'pip install git+https://github.com/hauntsaninja/mypy_primer.git', 'cd typeshed_to_test\nMYPY_VERSION=$(grep mypy== requirements-tests.txt | cut -d = -f 3)\necho ""new commit""\ngit rev-list --format=%s --max-count=1 $GITHUB_SHA\ngit checkout -b upstream_main origin/main\necho ""base commit""\ngit rev-list --format=%s --max-count=1 upstream_main\necho \'\'\ncd ..\n# fail action if exit code isn\'t zero or one\n(\n  mypy_primer \\\n  --new v${MYPY_VERSION} --old v${MYPY_VERSION} \\\n  --custom-typeshed-repo typeshed_to_test \\\n  --new-typeshed $GITHUB_SHA --old-typeshed upstream_main \\\n  --num-shards 4 --shard-index ${{ matrix.shard-index }} \\\n  --debug \\\n  --output concise \\\n  | tee diff_${{ matrix.shard-index }}.txt\n) || [ $? -eq 1 ]\n', 'echo ${{ github.event.pull_request.number }} | tee pr_number.txt\n', 'unzip diff.zip', 'cat diff_*.txt | tee fulldiff.txt\n', ""git config --global user.name stubsabot\ngit config --global user.email '<>'\n"", 'pip install -r requirements-tests.txt', 'GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }} python scripts/stubsabot.py --action-level everything', 'pip install -r requirements-tests.txt', 'python tests/stubtest_stdlib.py', 'pip install -r requirements-tests.txt', '# This only runs stubtest on changed stubs, because it is much faster.\n# Use the daily.yml workflow to run stubtest on all third party stubs.\nfunction find_stubs {\n  git diff --name-only origin/${{ github.base_ref }} HEAD | \\\n  egrep ^stubs/ | cut -d ""/"" -f 2 | sort -u | \\\n  (while read stub; do [ -d ""stubs/$stub"" ] && echo ""$stub"" || true; done)\n}\nSTUBS=$(find_stubs || echo \'\')\n\nif [ -n ""$STUBS"" ]; then\n  echo ""Testing $STUBS...""\n  PACKAGES=$(python tests/get_stubtest_system_requirements.py $STUBS)\n\n  if [ ""${{ runner.os }}"" = ""Linux"" ]; then\n    if [ -n ""$PACKAGES"" ]; then\n      echo ""Installing apt packages: $PACKAGES""\n      sudo apt update && sudo apt install -y $PACKAGES\n    fi\n\n    PYTHON_EXECUTABLE=""xvfb-run python""\n  else\n    if [ ""${{ runner.os }}"" = ""macOS"" ] && [ -n ""$PACKAGES"" ]; then\n      echo ""Installing Homebrew packages: $PACKAGES""\n      brew install $PACKAGES\n    fi\n\n    if [ ""${{ runner.os }}"" = ""Windows"" ] && [ -n ""$PACKAGES"" ]; then\n      echo ""Installing Chocolatey packages: $PACKAGES""\n      choco install -y $PACKAGES\n    fi\n\n    PYTHON_EXECUTABLE=""python""\n  fi\n\n  $PYTHON_EXECUTABLE tests/stubtest_third_party.py --specified-platforms-only $STUBS\nelse\n  echo ""Nothing to test""\nfi\n', 'pip install -r requirements-tests.txt', 'python ./tests/check_consistent.py', './tests/check_new_syntax.py', 'pip install -r requirements-tests.txt', 'flake8 --color always', 'pip install -r requirements-tests.txt', 'DEPENDENCIES=$(python tests/get_external_stub_requirements.py)\nif [ -n ""$DEPENDENCIES"" ]; then\n  echo ""Installing packages: $DEPENDENCIES""\n  pip install $DEPENDENCIES\nfi\n', 'pip freeze --all', './tests/pytype_test.py --print-stderr', 'pip install -r requirements-tests.txt', 'python ./tests/mypy_test.py --platform=${{ matrix.platform }} --python-version=${{ matrix.python-version }}', 'pip install -r requirements-tests.txt', 'python ./tests/mypy_test.py --platform=${{ matrix.platform }} --python-version=3.12', 'pip install -r requirements-tests.txt', 'python ./tests/regr_test.py --all --verbosity QUIET', 'pip install -r requirements-tests.txt', 'python -m venv .venv', 'DEPENDENCIES=$(python tests/get_external_stub_requirements.py)\nif [ -n ""$DEPENDENCIES"" ]; then\n  source .venv/bin/activate\n  echo ""Installing packages: $DEPENDENCIES""\n  pip install $DEPENDENCIES\nfi\n', 'source .venv/bin/activate\npip freeze --all\n', 'cd stub_uploader\npip install -r requirements.txt\npython -m pytest tests\n', 'pip install -r requirements-tests.txt', 'python ./tests/typecheck_typeshed.py --platform=${{ matrix.platform }}', 'pip install -r requirements-tests.txt']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get update -qq', 'sudo apt-get install -qq -y --no-install-recommends graphviz', 'python --version', 'python -m pip install --upgrade pip\npip install coverage\n', 'python tests/test.py --coverage', 'python -m pip install --upgrade pip\npip install build twine\n', 'python -m build\ntwine check --strict dist/*\ntwine upload dist/*\n', 'sudo apt-get update -qq', 'sudo apt-get install -qq -y --no-install-recommends graphviz', 'python --version', 'python -m pip install --upgrade pip setuptools wheel', 'python tests/test.py --max-acceptable=0']"
"['set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip wheel poetry==1.2.0\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox', 'python -m pip install poetry twine check-wheel-contents', 'poetry build', 'ls -l dist', 'check-wheel-contents dist/*.whl', 'python -m twine check dist/*']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox -e packaging\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e lint\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.toxenv }}\n']"
"['python -m pip install --upgrade pip\npython -m pip install -e .\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'black --check .\n', 'pylama .\n', 'mypy -p napalm --config-file mypy.ini\n', 'py.test --cov=napalm --cov-report term-missing -vs --pylama\n', 'python -m pip install --upgrade pip\npython -m pip install -e .\npython -m pip install -r docs/requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements.txt\n', 'make doctest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel\n', 'python setup.py sdist bdist_wheel\n']"
"['echo ""DV is ${{ matrix.django-version }}""\npython -m pip install --upgrade pip\npython -m pip install ""Django~=${{ matrix.django-version }}""\nmake testenv\n', 'make test\n', 'python -m pip install pre-commit\n', 'pre-commit run -a\n', 'docker build -f docker/Dockerfile -t wooey/wooey:latest .\ndocker tag wooey/wooey:latest wooey/wooey:${GITHUB_REF#refs/tags/}\n', 'docker login --username wooeyservice -p ${{ secrets.DOCKERHUB_ACCESS_TOKEN }}\ndocker push wooey/wooey:latest\ndocker push wooey/wooey:${GITHUB_REF#refs/tags/}\n', 'pip install -U setuptools twine pip\n', 'python setup.py sdist\ntwine upload -u __token__ -p $PYPI_API_TOKEN dist/*\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions codecov\n', 'tox', 'codecov', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions codecov\n', 'tox', 'codecov']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\n', '# Disable Cassandra optional extensions (faster testing).\nexport CASS_DRIVER_NO_EXTENSIONS=1\npip install -e .\n', 'python -m pip install sphinx sphinx-rtd-theme\ncd docs\nmake html\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# Disable Cassandra optional extensions (faster testing).\nexport CASS_DRIVER_NO_EXTENSIONS=1\npip install -e .[test,experimental_aio]\n', 'export DO_TEST_CASSANDRA=true\npytest\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\n', '# Disable Cassandra optional extensions (faster testing).\nexport CASS_DRIVER_NO_EXTENSIONS=1\npip install -e .[test,experimental_aio]\n', 'export DO_TEST_MONGO=true\npytest\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# Disable Cassandra optional extensions (faster testing).\nexport CASS_DRIVER_NO_EXTENSIONS=1\npip install -e .[test,experimental_aio]\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\n', 'cd doc && make html && cd ..', 'git clone https://github.com/coin-or/pulp.git --branch gh-pages --single-branch gh-pages\nrm -rf gh-pages/*\ncp -r doc/build/html/* gh-pages/\ncd gh-pages\ntouch .nojekyll\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""Update documentation"" -a || true\n# The above command will fail if no changes were present, so we ignore\n# that.\n', 'pip install -U wheel build', 'python -m build', 'sudo apt-get update -qq\nsudo apt-get install -qq glpk-utils\n', 'python -m pip install --upgrade pip\npip install .\n', 'pip install highspy\n', 'pulptest', 'pip install black\nblack pulp/ --check\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are code styling problems. The GitHub editor is 127 chars wide.\nflake8 . --count --max-line-length=127 --show-source --statistics']"
"['python -m pip install --upgrade pip\npython -m pip install pytest pytest-cov coveralls\npython -m pip install pandas\n', 'python -m pip install scikit-learn==1.0.2 joblib==1.0.0', 'python -m pip install .', 'python --version\npython -c ""import sklearn; print(\'sklearn %s\' % sklearn.__version__)""\npython -c ""import joblib; print(\'joblib %s\' % joblib.__version__)""\npython -c ""import numpy; print(\'numpy %s\' % numpy.__version__)""\npython -c ""import scipy; print(\'scipy %s\' % scipy.__version__)""\n', 'pytest -v --cov', 'coveralls --service=github']"
""
"['sudo apt update\nsudo apt install calibre default-jre git python3-venv\n', 'pipx install .\npipx inject standardebooks pylint==2.17.3 pytest==7.3.1 mypy==1.2.0 types-requests==2.28.11.17 types-setuptools==67.7.0.0\n', '$PIPX_HOME/venvs/standardebooks/bin/mypy', '$PIPX_HOME/venvs/standardebooks/bin/pylint tests/*.py se', '$PIPX_HOME/venvs/standardebooks/bin/pytest']"
"['sudo apt-get update -y && sudo apt-get install -y git python3-sphinx graphviz libenchant-2-2 && sudo apt-get install -y gcc libcups2-dev python3-dev python3-setuptools && sudo pip install tox pycups', 'tox -e docs', 'python -m pip install --upgrade pip\npip install flake8 pytest tox tox-gh-actions\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'tox\n']"
[]
"[""python -m pip install --upgrade pip\npip install -r requirements.txt\npip install 'django==${{ matrix.dj-version }}' 'djangorestframework==${{ matrix.drf-version }}'\n"", './runtests.py --fast --coverage -rw\n', 'python -m pip install --upgrade pip\npip install flake8==5.0.4\n', 'flake8 dynamic_rest tests\n']"
"['python setup.py install\n', 'pushd autotest\n./svwar.sh\npopd\n', 'pushd autotest\n./svcrack.sh\npopd\n', 'pushd autotest\n./svmap.sh\npopd\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -mpip install --progress-bar=off cibuildwheel -r ci/requirements.txt\nvirtualenv --version\npip --version\ntox --version\npip list --format=freeze\n', 'sudo apt-get install gdb\n', 'cibuildwheel', 'tox -e ${{ matrix.tox_env }} -v\n', 'twine check wheelhouse/*.whl']"
"['python -m pip install --upgrade pip\npip install -r src/requirements-dev.txt\npip install -r src/requirements.txt\npip install -r src/requirements-linux.txt\n', 'python -m pip install --upgrade pip\npip install -r src/requirements-dev.txt\npip install -r src/requirements.txt\npip install -r src/requirements-linux.txt\n', 'mkdir -p ""$HOME/bin""\ncurl -sfL \\\n  https://github.com/reviewdog/reviewdog/raw/master/install.sh | \\\n    sh -s -- -b ""$HOME/bin""\necho ""$HOME/bin"" >> $GITHUB_PATH\n', 'set -o pipefail\npylint --disable=C --disable=R --extension-pkg-allow-list=PyQt6 --load-plugins=pylint.extensions.no_self_use src/plus | reviewdog -efm=""%f:%l:%c: %m"" -reporter=github-check\npylint --disable=C --disable=R --extension-pkg-allow-list=PyQt6 --load-plugins=pylint.extensions.no_self_use src/artisanlib | reviewdog -efm=""%f:%l:%c: %m"" -reporter=github-check\n']"
"['pip install -U setuptools wheel\npython setup.py sdist bdist_wheel\n', 'sudo apt-get update\nsudo apt-get install libxml2-dev libxslt-dev\n', 'pip install -U tox\ntox\n', 'bash <(curl -s https://codecov.io/bash)', 'pip install -U check-manifest setuptools\ncheck-manifest\n']"
"['sudo apt update', 'sudo apt install -y ${{ env.apt-dependencies }}', 'python -m pip install --upgrade pip\npip install ${{ env.pip-dependencies }}\n', 'sudo apt install -y libmnl-dev libgmp-dev libreadline-dev \\\nlibjansson-dev libedit-dev\n', 'cd /tmp\ngit clone --depth=1 --branch ${{ matrix.libnftnl-version }} git://git.netfilter.org/libnftnl\ncd libnftnl\n./autogen.sh\n./configure\nmake\nsudo make install\nsudo ldconfig\n', 'cd /tmp\ngit clone --depth=1 --branch ${{ matrix.nftables-version }} git://git.netfilter.org/nftables\ncd nftables\n./autogen.sh\n./configure --disable-man-doc --with-json --disable-python\nmake\nsudo make install\ncd py\npip install .\nsudo ldconfig\n', './autogen.sh\n./configure --disable-docs ${{ matrix.configure_args }}\nmake -j $(nproc)\n', 'sudo systemctl stop NetworkManager\n', 'install -m 0644 config/FirewallD.conf /usr/share/dbus-1/system.d/FirewallD.conf\ninstall -m 0644 config/org.fedoraproject.FirewallD1.server.policy  /usr/share/polkit-1/actions/org.fedoraproject.FirewallD1.server.policy.choice\ninstall -m 0644 config/org.fedoraproject.FirewallD1.desktop.policy /usr/share/polkit-1/actions/org.fedoraproject.FirewallD1.desktop.policy.choice\nln -s /usr/share/polkit-1/actions/org.fedoraproject.FirewallD1.server.policy.choice /usr/share/polkit-1/actions/org.fedoraproject.FirewallD1.policy\n', 'sudo pkill -HUP dbus-daemon\n', 'sudo systemctl restart polkit\n', 'sudo make -C src/tests check-integration \\\n|| sudo make -C src/tests check-integration TESTSUITEFLAGS=""--recheck --errexit --verbose""\n', 'sudo apt update', 'sudo apt install -y ${{ env.apt-dependencies }}', './autogen.sh\n./configure\nmake dist\n', 'set -- $(grep ""^Version:"" firewalld.spec |cut -f2 -d \' \' |tr \'.\' \' \')\ntest ${{ github.ref_name }} = v${1}.${2}.${3} || { echo ""pushed tag doesn\'t match rpm spec file""; exit 1; }\nif test 0 -eq ${2} && test 0 -eq ${3}; then\n  echo ""This is a major release. It includes breaking changes.""\n  echo\n  git shortlog --format=""%s (%h)"" --no-merges --grep ""BREAKING"" v$(expr ${1} - 1).0.0..HEAD\nelif test 0 -eq ${3}; then\n  echo ""This is a feature release. It also includes all bug fixes since v${1}.$(expr ${2} - 1).0.""\n  echo\n  git shortlog --format=""%s (%h)"" --no-merges --grep ""^feat"" v${1}.$(expr ${2} - 1).0..HEAD\nelse\n  echo ""This is a bug fix only release.""\n  echo\n  git shortlog --format=""%s (%h)"" --grep ""^\\(fix\\|doc\\)"" v${1}.${2}.$(expr ${3} - 1)..HEAD\nfi | sed -e \'s/^[ ]\\+/- /\' > body.md\n', 'sudo apt update', 'sudo apt install -y ${{ env.apt-dependencies }}', 'python -m pip install --upgrade pip\npip install ${{ env.pip-dependencies }}\n', './autogen.sh\n./configure\nmake -j $(nproc)\n', 'make -C src check-local\n', 'make -C config check-local\n', 'sudo apt update', 'sudo apt install -y ${{ env.apt-dependencies }}', 'python -m pip install --upgrade pip\npip install ${{ env.pip-dependencies }}\n', 'sudo apt install -y libmnl-dev libgmp-dev libreadline-dev \\\nlibjansson-dev libedit-dev\n', 'cd /tmp\ngit clone --depth=1 --branch ${{ matrix.libnftnl-version }} git://git.netfilter.org/libnftnl\ncd libnftnl\n./autogen.sh\n./configure\nmake\nsudo make install\nsudo ldconfig\n', 'cd /tmp\ngit clone --depth=1 --branch ${{ matrix.nftables-version }} git://git.netfilter.org/nftables\ncd nftables\n./autogen.sh\n./configure --disable-man-doc --with-json --disable-python\nmake\nsudo make install\ncd py\npip install .\nsudo ldconfig\n', 'sudo apt-get -y remove iptables ebtables ipset\n', './autogen.sh\n./configure --disable-docs IP6TABLES=/bin/false IP6TABLES_RESTORE=/bin/false\nmake -j $(nproc)\n', './autogen.sh\n./configure --disable-docs\nmake -j $(nproc)\n', 'sudo make -C src/tests check-local TESTSUITEFLAGS=""-k ${{ matrix.keyword }} -j$(nproc)"" \\\n|| sudo make -C src/tests check-local TESTSUITEFLAGS=""--recheck --errexit --verbose""\n']"
"['sudo apt-get update --fix-missing\nsudo apt-get install -qq pyqt5-dev-tools libxcb-xinerama0 xterm --fix-missing\n', 'conda env update --file requirements/environment_tests_${{ matrix.QT_BINDING }}.yml', 'pip install -e .', 'conda info\nconda list\n', 'xvfb-run --auto-servernum python example.py\nxvfb-run --auto-servernum pytest -x -vv --cov-report xml --cov=qtawesome qtawesome\n', 'conda env update --file requirements/environment_tests_${{ matrix.QT_BINDING }}.yml', 'pip install -e .', 'conda info\nconda list\n', 'python example.py\npytest -x -vv --cov-report xml --cov=qtawesome qtawesome\n', 'conda env update --file requirements/environment_tests_${{ matrix.QT_BINDING }}.yml', 'pip install -e .', 'conda info\nconda list\n', 'python example.py\npytest -x -vv --cov-report xml --cov=qtawesome qtawesome\n']"
""
"['docker build -t pyfakefs -f $GITHUB_WORKSPACE/.github/workflows/dockerfiles/Dockerfile_${{ matrix.docker-image }} . --build-arg github_repo=$GITHUB_REPOSITORY --build-arg github_branch=$GITHUB_REF_NAME\n', 'docker run -t pyfakefs', 'python -m pip install --upgrade pip\npython -m pip install build\npython -m build\n', 'pip install setuptools pytype pytest scandir pathlib2 pandas xlrd django', 'pytype pyfakefs --keep-going --exclude pyfakefs/tests/* --exclude pyfakefs/pytest_tests/*\n', 'python -m pip install --upgrade pip\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install setuptools wheel\npip install -r requirements.txt\n', 'export TEST_REAL_FS=1\npython -bb -m pyfakefs.tests.all_tests_without_extra_packages\n', 'python setup.py test\n', 'if [[ \'${{ matrix.os  }}\' != \'windows-latest\' ]]; then\n  # provide the same path as non-root to get the correct virtualenv\n  sudo env ""PATH=$PATH"" python -m pyfakefs.tests.all_tests_without_extra_packages\nfi\n', 'pip install -r extra_requirements.txt\n', 'python -m pyfakefs.tests.all_tests\n', ""if [[ '${{ matrix.os  }}' != 'macOS-latest' ]]; then\n  export TEST_PERFORMANCE=1\n  python -m pyfakefs.tests.performance_test\nfi\n"", ""pip install -r requirements.txt\npip install -U pytest==${{ matrix.pytest-version }}\nif [[ '${{ matrix.pytest-version }}' == '4.0.2' ]]; then\n   pip install -U attrs==19.1.0\nfi\n"", 'echo ""$(python -m pytest pyfakefs/pytest_tests/pytest_plugin_failing_helper.py)"" > ./testresult.txt\npython -m pytest pyfakefs/pytest_tests\nif [[ \'${{ matrix.pytest-version }}\' > \'3.0.0\' ]]; then\n  cd pyfakefs/pytest_tests/ns_package\n  python -m pytest --log-cli-level=INFO test\nfi\n', 'pip install -r requirements.txt\npip install -r extra_requirements.txt\npip install pytest-find-dependencies\n', 'python -m pytest --find-dependencies pyfakefs/tests']"
"['pip install -Ur requirements.txt coveralls\npython setup.py develop\n', 'flake8', 'coverage run --source=thefuck,tests -m pytest -v --capture=sys tests', 'coverage run --source=thefuck,tests -m pytest -v --capture=sys tests --enable-functional', 'coveralls --service=github']"
"['TARGET_COMMIT_SHA=""$(gh api \'${{ github.event.pull_request.url }}\' --jq .merge_commit_sha)""\necho ""TARGET_COMMIT_SHA=$TARGET_COMMIT_SHA"" >> ${GITHUB_ENV}\necho ""target-commit-sha=${TARGET_COMMIT_SHA}"" >> ${GITHUB_OUTPUT}\n', 'echo -n ""pull-request-labels="" >> ${GITHUB_OUTPUT}\ngh api graphql --paginate -F node_id=${{github.event.pull_request.node_id}} -f query=\'\n  query($node_id: ID!, $endCursor: String) {\n    node(id:$node_id) {\n      ... on PullRequest {\n        labels(first: 100, after: $endCursor) {\n          nodes { name }\n          pageInfo { hasNextPage endCursor }\n        }\n      }\n    }\n  }\' --jq \'.data.node.labels.nodes[]\' | jq --slurp -c \'[.[].name]\' >> ${GITHUB_OUTPUT}\n', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'python - <<EOF >> ${GITHUB_ENV}\nfrom pathlib import Path\nimport re\nimport sys\n\nDEFAULTS_CONTENT = Path(\'dev/breeze/src/airflow_breeze/branch_defaults.py\').read_text()\nBRANCH_PATTERN = r\'^AIRFLOW_BRANCH = ""(.*)""$\'\nCONSTRAINTS_BRANCH_PATTERN = r\'^DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH = ""(.*)""$\'\n\nbranch = re.search(BRANCH_PATTERN, DEFAULTS_CONTENT, re.MULTILINE).group(1)\nconstraints_branch = re.search(CONSTRAINTS_BRANCH_PATTERN, DEFAULTS_CONTENT, re.MULTILINE).group(1)\n\noutput = f""""""\nDEFAULT_BRANCH={branch}\nDEFAULT_CONSTRAINTS_BRANCH={constraints_branch}\n"""""".strip()\n\nprint(output)\n# Stdout is redirected to GITHUB_ENV but we also print it to stderr to see it in ci log\nprint(output, file=sys.stderr)\nEOF\n', 'breeze ci selective-check 2>> ${GITHUB_OUTPUT}', 'printenv', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', './scripts/ci/images/ci_start_arm_instance_and_connect_to_docker.sh', 'breeze ci-image build --run-in-parallel --builder airflow_cache --platform ""linux/arm64""\n', './scripts/ci/images/ci_stop_arm_instance.sh', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'python - <<EOF >> ${GITHUB_ENV}\nfrom pathlib import Path\nimport re\nimport sys\n\nDEFAULTS_CONTENT = Path(\'dev/breeze/src/airflow_breeze/branch_defaults.py\').read_text()\nBRANCH_PATTERN = r\'^AIRFLOW_BRANCH = ""(.*)""$\'\nCONSTRAINTS_BRANCH_PATTERN = r\'^DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH = ""(.*)""$\'\n\nbranch = re.search(BRANCH_PATTERN, DEFAULTS_CONTENT, re.MULTILINE).group(1)\nconstraints_branch = re.search(CONSTRAINTS_BRANCH_PATTERN, DEFAULTS_CONTENT, re.MULTILINE).group(1)\n\noutput = f""""""\nDEFAULT_BRANCH={branch}\nDEFAULT_CONSTRAINTS_BRANCH={constraints_branch}\n"""""".strip()\n\nprint(output)\n# Stdout is redirected to GITHUB_ENV but we also print it to stderr to see it in ci log\nprint(output, file=sys.stderr)\nEOF\n', 'breeze ci get-workflow-info 2>> ${GITHUB_OUTPUT}', 'breeze ci selective-check 2>> ${GITHUB_OUTPUT}', 'printenv', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', './scripts/ci/images/ci_start_arm_instance_and_connect_to_docker.sh', 'breeze ci-image build --builder airflow_cache --prepare-buildx-cache --run-in-parallel --force-build --platform ${{ matrix.platform }}\n', 'breeze ci-image build --tag-as-latest --push --run-in-parallel --platform ${{ matrix.platform }}\n', './scripts/ci/images/ci_stop_arm_instance.sh', 'docker system prune --all --force', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze shell --max-time 120', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'python -m pip install --editable ./dev/breeze/', 'breeze setup version', 'python -m pytest ./dev/breeze/ -n auto --color=yes', 'breeze setup check-all-params-in-groups', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'yarn --cwd airflow/www/ install --frozen-lockfile --non-interactive', 'yarn --cwd airflow/www/ run test', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', './scripts/ci/openapi/client_codegen_diff.sh', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'cd ./docker_tests && python -m pip install -r requirements.txt && python -m pytest test_examples_of_prod_image_building.py -n auto --color=yes\n', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze ci-image pull --run-in-parallel --verify --wait-for-image --tag-as-latest', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze static-checks --all-files --show-diff-on-failure --color always --initialize-environment', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze static-checks --show-diff-on-failure --color always --initialize-environment --commit-ref ""${{ github.sha }}""\n', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze build-docs ${{ needs.build-info.outputs.docs-filter-list-as-string }}', 'aws s3 sync --delete ./files/documentation s3://apache-airflow-docs', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'rm -fv ./dist/*', 'breeze release-management prepare-provider-documentation ${{ needs.build-info.outputs.affected-providers-list-as-string }}\n', 'breeze release-management prepare-provider-packages --version-suffix-for-pypi dev0 --package-format wheel ${{ needs.build-info.outputs.affected-providers-list-as-string }}\n', 'breeze release-management prepare-airflow-package --version-suffix-for-pypi dev0', 'pipx install twine --force && twine check dist/*.whl', 'breeze release-management generate-issue-content-providers --only-available-in-dist --disable-progress\n', 'breeze release-management verify-provider-packages --use-packages-from-dist --package-format wheel --use-airflow-version wheel\n', 'breeze release-management install-provider-packages --package-format wheel --use-airflow-version wheel --run-in-parallel\n', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'rm -fv ./dist/*', 'breeze release-management prepare-provider-packages --version-suffix-for-pypi dev0 --package-format wheel ${{ needs.build-info.outputs.affected-providers-list-as-string }}\n', '# This step should remove the provider packages that are not compatible with 2.4\n# or replace them with 2.4 compatible versions. Sometimes we have good reasons to bump\n# the min airflow versions for some providers and then we need to add exclusions here.\n#\n# The Removal can be done with:\n#\n# rm -vf dist/apache_airflow_providers_<PROVIDER>*.whl\n#\n# Then it can be followed by downloading a compatible version from PyPI in case other\n# providers depend on it and fail with import errors (you need to download compatible version):\n#\n# pip download --no-deps --dest dist apache-airflow-providers-<PROVIDER>==3.1.0\n#\nrm -vf dist/apache_airflow_providers_openlineage*.whl\n', 'python -c \'from pathlib import Path; import json\nproviders = json.loads(Path(""generated/provider_dependencies.json"").read_text())\nprovider_keys = "","".join(providers.keys())\nprint(""AIRFLOW_EXTRAS={}"".format(provider_keys))\' >> $GITHUB_ENV\n', 'breeze release-management verify-provider-packages --use-airflow-version 2.4.0 --use-packages-from-dist --airflow-constraints-reference constraints-2.4.0\n', 'breeze release-management install-provider-packages --use-airflow-version 2.4.0 --airflow-constraints-reference constraints-2.4.0 --run-in-parallel\n', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'rm -fv ./dist/*', 'breeze release-management prepare-provider-packages --version-suffix-for-pypi dev0 --package-format sdist ${{ needs.build-info.outputs.affected-providers-list-as-string }}\n', 'breeze release-management prepare-airflow-package --version-suffix-for-pypi dev0 --package-format sdist\n', 'pipx install twine --force && twine check dist/*.tar.gz', 'breeze release-management install-provider-packages --package-format sdist --use-airflow-version sdist --run-in-parallel\n', 'breeze release-management install-provider-packages --package-format sdist --use-airflow-version sdist --run-in-parallel\n', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'rm -fv ./dist/*', 'python -m pip install --editable ./dev/breeze/', 'breeze release-management create-minor-branch --version-branch 2-4 -a y', 'breeze release-management start-rc-process --version 2.4.3rc1 --previous-version 2.4.2  -a y\n', 'breeze release-management start-release --release-candidate 2.4.3rc1 --previous-release 2.4.2 -a y\n', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing helm-tests --helm-test-package ""${{ matrix.helm-test-package }}""', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel', 'breeze testing tests --run-in-parallel --collect-only --remove-arm-packages', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing integration-tests --integration cassandra\nbreeze down\n', 'breeze testing integration-tests --integration mongo\nbreeze down\n', 'breeze testing integration-tests --integration pinot\nbreeze down\n', 'breeze testing integration-tests --integration celery\nbreeze down\n', 'breeze testing integration-tests --integration trino --integration kerberos\nbreeze down\n', 'breeze testing integration-tests --integration kafka\nbreeze down\n', 'breeze testing integration-tests --integration all-testable', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing integration-tests --integration all-testable', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing tests --run-in-parallel || true', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'ls -R ./artifacts/\ncat ./artifacts/test-warnings*/* | sort | uniq\necho\necho Total number of unique warnings $(cat ./artifacts/test-warnings*/* | sort | uniq | wc -l)\n', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze prod-image pull --verify --wait-for-image --run-in-parallel', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze testing docker-compose-tests', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze prod-image pull --run-in-parallel --tag-as-latest', 'breeze k8s run-complete-tests --run-in-parallel --upgrade', 'breeze k8s delete-cluster --all', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze ci-image pull --run-in-parallel --tag-as-latest', 'breeze release-management generate-constraints --run-in-parallel \\\n    --airflow-constraints-mode constraints-source-providers\nbreeze release-management generate-constraints \\\n    --run-in-parallel --airflow-constraints-mode constraints-no-providers\nbreeze release-management generate-constraints \\\n    --run-in-parallel --airflow-constraints-mode constraints\n', './scripts/ci/constraints/ci_branch_constraints.sh >> ${GITHUB_OUTPUT}', './scripts/ci/constraints/ci_commit_constraints.sh', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'rm -fv ./dist/* ./docker-context-files/*', 'breeze release-management prepare-airflow-package --package-format wheel', 'breeze release-management prepare-provider-packages --package-list-file ./scripts/ci/installed_providers.txt --package-format wheel\n', './scripts/ci/images/ci_start_arm_instance_and_connect_to_docker.sh', 'breeze ci-image build --builder airflow_cache --prepare-buildx-cache --run-in-parallel --force-build --platform ${{ matrix.platform }}\n', 'breeze ci-image build --tag-as-latest --push --run-in-parallel --platform ${{ matrix.platform }}\n', 'mv -v ./dist/*.whl ./docker-context-files', 'breeze prod-image build --builder airflow_cache --install-packages-from-context --run-in-parallel --prepare-buildx-cache --platform ${{ matrix.platform }}\n', 'breeze prod-image build --tag-as-latest --install-packages-from-context --push --run-in-parallel --platform ${{ matrix.platform }}\n', './scripts/ci/images/ci_stop_arm_instance.sh', 'breeze ci fix-ownership', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', './scripts/ci/images/ci_start_arm_instance_and_connect_to_docker.sh', 'breeze ci-image build --run-in-parallel --builder airflow_cache --platform ""linux/arm64""\n', './scripts/ci/images/ci_stop_arm_instance.sh', 'breeze ci fix-ownership', 'breeze ci selective-check 2>> ${GITHUB_OUTPUT}', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze ci selective-check 2>> ${GITHUB_OUTPUT}', 'docker run -v ""${GITHUB_WORKSPACE}:/workspace"" -u 0:0 bash -c ""rm -rf /workspace/*""', 'breeze ci-image build', 'rm -fv ./dist/* ./docker-context-files/*', './scripts/ci/images/ci_start_arm_instance_and_connect_to_docker.sh', 'echo ${{ secrets.DOCKERHUB_TOKEN }} | docker login --password-stdin --username ${{ secrets.DOCKERHUB_USER }}\n', 'breeze release-management release-prod-images --dockerhub-repo ${{ github.repository }} --airflow-version  ${{ github.event.inputs.airflowVersion }} ${{ needs.build-info.outputs.skipLatest }} ${{ needs.build-info.outputs.limitPlatform }} --limit-python ${{ matrix.python-version }}\n', 'breeze release-management release-prod-images --dockerhub-repo ${{ github.repository }} --airflow-version  ${{ github.event.inputs.airflowVersion }} ${{ needs.build-info.outputs.skipLatest }} ${{ needs.build-info.outputs.limitPlatform }} --limit-python ${{ matrix.python-version }} --slim-images\n', './scripts/ci/images/ci_stop_arm_instance.sh', 'breeze prod-image verify --pull --image-name ${{github.repository}}:${{github.event.inputs.airflowVersion}}-python${{matrix.python-version}}\n', 'breeze prod-image verify --pull --slim-image --image-name ${{github.repository}}:slim-${{github.event.inputs.airflowVersion}}-python${{matrix.python-version}}\n', 'docker logout', 'breeze ci fix-ownership', 'pip install requests toml\n', 'python scripts/ci/runners/sync_authors.py\ngit config user.name ""GitHub Actions""\ngit config user.email ""actions@users.noreply.github.com""\nif [ -n ""$(git status --porcelain)"" ]; then\n  branch=update-$(date +%s)\n  git add -A\n  git checkout -b $branch\n  git commit --message ""Authors list automatic update""\n  git push origin $branch\n  gh pr create --title ""Authors list automatic update"" --body \'\'\nfi\n']"
"['sudo /etc/init.d/mysql start\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\npip install --no-cache-dir -e .\n', 'while ! mysqladmin ping --host=localhost --port=3306 --user=root --password=root --silent; do\n  sleep 5\ndone\n', './setup.py test --pytest-args=""--cov-report= --cov=mycli""\n', './setup.py lint --branch=HEAD\n', 'coverage combine\ncoverage report\n']"
"['python -m pip install flake8\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'flake8 . --count --ignore=E1,E2,E3,E501,W291,W293 --exit-zero --max-complexity=65 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip wheel\npython -m pip install tox tox-gh-actions -r requirements.txt -r requirements-test.txt\n', ""tox -- -m 'not remote'\n"", 'python setup.py bdist_wheel\n', 'docker build -t ${{ env.DOCKER_TAG }} .\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt -r dev_requirements.txt --timeout 45\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest tests/\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'python -m pip install --upgrade pip\npip install coverage docopt yarg requests\n', 'coverage run --source=pipreqs -m unittest discover', 'coverage xml']"
""
"['sudo apt-get update -y -q', 'sudo apt-get install -y -q build-essential graphviz libgeos-dev', ""pip install 'poetry<1.4'"", 'poetry lock --check', 'set -euo pipefail\n\njs=""$(mktemp --suffix=.pyproject.json)""\ndocker run --rm -i sclevine/yj -tj < pyproject.toml > ""$js""\n\nold=""$(mktemp --suffix=.old)""\njq -rM \'.tool.poetry.extras.all | unique | sort\' < ""$js"" > ""$old""\n\nnew=""$(mktemp --suffix=.new)""\njq -rM \'.tool.poetry.extras | with_entries(select(.key != ""all"")) | [.[]] | add | unique | sort\' < ""$js"" > ""$new""\n\ndiff --unified ""$old"" ""$new""\n', 'poetry export --extras all --with dev --with test --with docs --without-hashes --no-ansi > requirements.txt', 'git diff --exit-code requirements.txt', 'pip install -r requirements.txt', 'pip install -e .', 'pytest -m core --cov-report=xml:coverage.xml --cov=ibis', 'set -euo pipefail\n\necho ""yesterday=$(date --date=yesterday +%Y-%m-%d)"" >> ""$GITHUB_OUTPUT""\n', 'set -euo pipefail\n\nyesterday=""${{ steps.set_date.outputs.yesterday }}""\n\n[ -n ""$yesterday"" ] || exit 1\n\nseconds_per_hour=3600\nrequests_per_hour=1000\nrate_limit_sleep=""$(bc <<< ""scale = 1; $seconds_per_hour / $requests_per_hour"" | xargs printf ""%.0f"")""\n\ngh api --paginate ""repos/ibis-project/ibis/actions/runs?per_page=100&created=${yesterday}"" --jq \'.workflow_runs[]\' > workflows.json\n\njq -rcM \'.id\' < workflows.json | while read -r run_id; do\n  gh api --paginate ""repos/ibis-project/ibis/actions/runs/${run_id}/jobs?per_page=100"" --jq \'.jobs[]\' >> jobs.json\n  # sleep to avoid rate limiting\n  sleep ""$rate_limit_sleep""\ndone\n', 'gcloud info', 'set -euo pipefail\n\nyesterday=""${{ steps.set_date.outputs.yesterday }}""\n\nbucket=""gs://ibis-workflow-data/${yesterday}""\ngsutil cp -Z workflows.json jobs.json ""${bucket}""\n', 'conda install -n base conda-libmamba-solver', 'conda config --set solver libmamba', 'conda install conda-lock=1.4', './ci/conda-lock/generate.sh ""${{ matrix.python-version }}""', 'conda create --name ibis${{ matrix.python-version }} --file conda-lock/linux-64-${{ matrix.python-version }}.lock', ""set -euo pipefail\n\ngit config --global user.name 'ibis-squawk-bot[bot]'\ngit config --global user.email 'ibis-squawk-bot[bot]@users.noreply.github.com'\n"", ""set -euo pipefail\n\ngit add conda-lock/*.lock\n\nif git commit -m 'chore(conda-lock): relock'; then\n  # pull in case another commit happened in the meantime\n  #\n  # `ours` is actually the *other* changeset, not the current branch, per\n  # https://stackoverflow.com/a/3443225/564538\n  git pull --rebase -s recursive -X ours\n  git push\nfi\n"", ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry install --without dev --without docs --extras ${{ matrix.backend.name }}', 'just download-data', 'pyversion=""${{ matrix.python-version }}""\n{\n  echo ""SNOWFLAKE_USER=${SNOWFLAKE_USER}""\n  echo ""SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}""\n  echo ""SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}""\n  echo ""SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}""\n  echo ""SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}_python${pyversion//./}""\n  echo ""SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}""\n} >> ""$GITHUB_ENV""\n', 'just ci-check -m ${{ matrix.backend.name }} --numprocesses auto --dist=loadgroup', 'echo ""No build required""', 'echo ""No build required""', 'exit 0', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry remove snowflake-sqlalchemy snowflake-connector-python', 'poetry update numpy pandas pyarrow', ""set -euo pipefail\n\nsudo apt-get update -qq -y\nsudo apt-get install -qq -y build-essential ${{ join(matrix.backend.sys-deps, ' ') }}\n"", 'choco install sqlite', 'just download-data', ""docker compose up --wait ${{ join(matrix.backend.services, ' ') }}"", 'set -euo pipefail\n\nmv -f deps/* .\nrm -r deps\n', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry install --without dev --without docs --extras ""${{ join(matrix.backend.extras, \' \') }}""', 'poetry run pip install ""${{ join(matrix.backend.additional_deps, \' \') }}""', 'poetry run pip list', 'just ci-check -m ${{ matrix.backend.name }} --numprocesses auto --dist=loadgroup', 'just ci-check -m ${{ matrix.backend.name }}', 'docker compose logs', 'sudo apt-get install -qq -y build-essential libgeos-dev', 'just download-data', ""docker compose up --wait ${{ join(matrix.backend.services, ' ') }}"", ""python -m pip install --upgrade pip 'poetry<1.4'"", ""poetry add --lock --optional ${{ join(matrix.backend.deps, ' ') }}"", 'git checkout poetry.lock', 'poetry lock --no-update', 'poetry install --without dev --without docs --extras ""${{ join(matrix.backend.extras, \' \') }}""', 'just ci-check -m ${{ matrix.backend.name }} --numprocesses auto --dist=loadgroup', 'docker compose logs', 'just download-data', ""python -m pip install --upgrade pip 'poetry<1.4'"", ""poetry remove ${{ join(matrix.pandas.conflicts, ' ') }}"", ""poetry add --lock 'pandas@${{ matrix.pandas.version }}' 'numpy@1.23.*'"", 'git checkout poetry.lock', 'poetry lock --no-update', 'poetry install --without dev --without docs --extras pyspark', 'just ci-check -m pyspark', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry remove snowflake-sqlalchemy', ""poetry add --lock --optional 'sqlalchemy>=2,<3'"", 'git checkout poetry.lock', 'poetry lock --no-update', ""poetry show sqlalchemy --no-ansi | grep version | cut -d ':' -f2- | sed 's/ //g' | grep -P '^2\\.'"", 'sudo apt-get install -qq -y build-essential libgeos-dev', 'just download-data', ""docker compose up --wait ${{ join(matrix.backend.services, ' ') }}"", 'set -euo pipefail\n\nmv -f deps/* .\nrm -r deps\n', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry install --without dev --without docs --extras ""${{ join(matrix.backend.extras, \' \') }}""', 'just ci-check -m ${{ matrix.backend.name }} --numprocesses auto --dist=loadgroup', 'docker compose logs', 'exit 0', ""nix run 'nixpkgs#commitlint' -- --from=${{ github.event.pull_request.base.sha }} --to=${{ github.sha }} --verbose"", ""nix develop '.#preCommit' --ignore-environment --keep-going -c pre-commit run --all-files --show-diff-on-failure --color=always"", 'sudo apt-get install -qq -y build-essential libgeos-dev', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry install --without dev --without docs --all-extras', 'mkdir .benchmarks', 'poetry run pytest --benchmark-enable --benchmark-json .benchmarks/output.json ibis/tests/benchmarks', 'git checkout gh-pages', 'nix develop --ignore-environment -c mkdocs build --strict', ""nix develop --ignore-environment '.#links' -c just checklinks --offline --no-progress"", ""set -euo pipefail\n\ngit config user.name 'ibis-docs-bot[bot]'\ngit config user.email 'ibis-docs-bot[bot]@users.noreply.github.com'\n"", ""nix develop --ignore-environment -c \\\n  mkdocs gh-deploy --message 'docs: ibis@${{ github.sha }}'\n"", ""set -euo pipefail\n\n# not incredibly important what user we use here\n#\n# we're making a commit in a temporary worktree that is thrown away\n# if the process exits successfully\n#\n# git requires user information to make commits\ngit config user.name 'ibis-squawk-bot[bot]'\ngit config user.email 'ibis-squawk-bot[bot]@users.noreply.github.com'\n"", './ci/release/dry_run.sh', 'echo ""No build required""', 'echo ""No build required""', 'echo ""No build required""', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'set -euo pipefail\n\nsudo apt-get update -y -q\nsudo apt-get install -y -q build-essential graphviz libgeos-dev\n', 'choco install graphviz', 'poetry install --without dev --without docs --extras visualization', 'just ci-check -m ""\'core or benchmark\'"" -n auto', 'just ci-check -m ""\'core or benchmark\'""', 'set -euo pipefail\n\nsudo apt-get update -y -q\nsudo apt-get install -y -q build-essential libgeos-dev\n', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry install --without dev --without docs --without test --extras duckdb --extras geospatial', ""poetry run python -c 'import shapely.geometry, duckdb'"", 'set -euo pipefail\n\nsudo apt-get update -y -q\nsudo apt-get install -y -q build-essential graphviz libgeos-dev libkrb5-dev\n', ""python -m pip install --upgrade pip 'poetry<1.4'"", 'poetry install --without dev --without docs --extras all', 'just doctest --junitxml=junit.xml --cov=ibis --cov-report=xml:coverage.xml', 'echo ""No build required""', ""python -m pip install -r requirements.txt\npython -m pip install -U 'duckdb>=0.4' 'duckdb-engine>=0.6'\n"", 'python -m pip install "".[sqlite,duckdb]""', 'python -c ""import duckdb; duckdb.connect(\'tpch.ddb\').execute(\'CALL dbgen(sf=0.1)\')""', ""./runtpc -i ibis -i duckdb -d 'tpch.ddb' -b 'duckdb'"", 'echo ""No build required""', 'nix run \'.#check-poetry-version\' -- ""1.3""', 'set -euo pipefail\n\nversion=\'${{ matrix.python-version }}\'\nnix build "".#ibis${version//./}"" --fallback --keep-going --print-build-logs\n', 'set -euo pipefail\n\nversion=\'${{ matrix.python-version }}\'\nhost_system=""$(nix eval --raw \'nixpkgs#stdenv.hostPlatform.system\')""\nflake="".#devShells.${host_system}.ibis${version//./}""\nnix build ""$flake"" --fallback --keep-going --print-build-logs\n', './ci/release/run.sh', 'set -euo pipefail\n\nflakes=""$(nix flake metadata --json | jq -rcM \'.locks.nodes.root.inputs | {flake: keys}\')""\necho ""matrix=${flakes}"" >> ""$GITHUB_OUTPUT""\n', 'nix flake lock --update-input ${{ matrix.flake }}', 'echo ""did_change=${{ steps.get_current_commit.outputs.rev != steps.get_new_commit.outputs.rev }}"" >> ""$GITHUB_OUTPUT""']"
""
"['python -m pip install --upgrade pip\npip install flake8 tox tox-gh-actions\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m tox\n', 'python -m pip install tox tox-gh-actions\n', 'python -m tox -e coverage\nexport TOTAL=$(python -c ""import json;print(json.load(open(\'coverage.json\'))[\'totals\'][\'percent_covered_display\'])"")\necho ""total=$TOTAL"" >> $GITHUB_ENV\necho ""### Total coverage: ${TOTAL}%"" >> $GITHUB_STEP_SUMMARY\n']"
"['sudo apt-get update\nsudo apt-get install texlive-plain-generic inkscape texlive-xetex latexmk enchant-2\n\n# pandoc is not up to date in the ubuntu repos, so we install directly\nwget https://github.com/jgm/pandoc/releases/download/2.14.2/pandoc-2.14.2-1-amd64.deb && sudo dpkg -i pandoc-2.14.2-1-amd64.deb\n', 'pip install -v "".[all]""', 'pip freeze\npip check\n', 'cd docs\nmake html SPHINXOPTS=""-W""\n', 'cd docs\nmake latexpdf\n', 'cd docs\nmake linkcheck\n', 'echo ""Optional): Review Draft Release: ${{ steps.prep-release.outputs.release_url }}""\n', 'echo ""Verify the final release""\necho ${{ steps.finalize-release.outputs.release_url }}\n', 'echo ""Failed to Publish the Draft Release Url:""\necho ${{ steps.populate-release.outputs.release_url }}\n', 'sudo apt-get update\nsudo apt-get install texlive-plain-generic inkscape texlive-xetex latexmk\nsudo apt-get install xvfb x11-utils libxkbcommon-x11-0  libxcb-xinerama0 python3-pyqt5\n\n# pandoc is not up to date in the ubuntu repos, so we install directly\nwget https://github.com/jgm/pandoc/releases/download/3.1.2/pandoc-3.1.2-1-amd64.deb && sudo dpkg -i pandoc-3.1.2-1-amd64.deb\n', 'xvfb-run --auto-servernum hatch run cov:test\n', 'hatch run cov:test\n', 'hatch run typing:test\nhatch run lint:style\npipx run interrogate -v .\npipx run doc8 --max-line-length=200  --ignore-path=docs/source/other/full-config.rst\n', 'pip install -e .', 'sudo apt-get update\nsudo apt-get install texlive-plain-generic inkscape texlive-xetex latexmk\nsudo apt-get install xvfb x11-utils libxkbcommon-x11-0  libxcb-xinerama0 python3-pyqt5\n\n# pandoc is not up to date in the ubuntu repos, so we install directly\nwget https://github.com/jgm/pandoc/releases/download/2.14.2/pandoc-2.14.2-1-amd64.deb && sudo dpkg -i pandoc-2.14.2-1-amd64.deb\n', 'xvfb-run --auto-servernum hatch run test:nowarn || xvfb-run --auto-servernum hatch run test:nowarn --lf\n', 'export NBFORMAT_VALIDATOR=jsonschema\nhatch run test:nowarn || hatch run test:nowarn --lf\n']"
"['python -m pip install --upgrade pip\npip install nala\npip install -e .\n', 'env PYTHONPATH=. python examples/hello_world.py\nenv PYTHONPATH=. python examples/diagnostics/did.py\nmake test-c-clean\nmake test-c\nmake test-c-clean\nCC=gcc make test-c\nmake test-c-clean\nCC=clang make test-c-src\n', 'python -m pip install --upgrade pip\npip install tox\n', 'python -m pip install -e "".[windows-all]""\n', 'tox -e gh\n', 'python -m pip install --upgrade pip\npip install mypy ruff\npip install -e .[plot]\n', 'mypy --python-version 3.8 .\nmypy --python-version 3.9 .\nmypy --python-version 3.10 .\nmypy --python-version 3.11 .\n', 'ruff check ./cantools\n', 'python -m pip install --upgrade pip\npip install -r docs/requirements.txt\npip install -e .\n', 'python -m sphinx -Ean docs build\n', 'pipx run build', 'pipx run twine check --strict dist/*', 'python -m pip install build --user\n', 'git clean -dfx\npython -m build --sdist --wheel --outdir dist/ .\n']"
"['sudo apt update\nsudo apt install tesseract-ocr poppler-utils imagemagick ghostscript\npip install -U ocrmypdf\n', 'pip install -U wheel pip\npip install -U "".[test]""\n', 'flake8', 'pytest']"
"['python -V\npython -m pip install ""flake8<6.0.0""\n', 'bash ./.github/scripts/flake8_diff.sh\n', 'python -m pip install --upgrade pip\npython -m pip install -e .\npython -m pip install -r dev-requirements.txt\npython ci/install_coverage_subprocess_pth.py\nexport\n', 'python -c ""import sys; print(sys.version)""', 'python -m flake8 . --count --verbose --select=E901,E999,F821,F822,F823 \\\n  --show-source --statistics\n', ""COVERAGE_PROCESS_START=$GITHUB_WORKSPACE/.coveragerc \\\n  PYTHONPATH='.:tests' python -m pytest -r s\ncoverage combine --append\ncoverage xml -i\n"", 'bash ./.github/scripts/install_downstream_project.sh\n', 'python -m pip install -e .\n', '# FIXME ipv6-related failures on Ubuntu github actions CI\n# https://github.com/dask/distributed/issues/4514\nexport DISABLE_IPV6=1\nexport PYTEST_ADDOPTS=(""-m"" ""not avoid_ci"")\nsource ./.github/scripts/test_downstream_project.sh\n', 'bash ./.github/scripts/install_downstream_project.sh\n', 'pushd ../joblib/joblib/externals\nsource vendor_cloudpickle.sh ../../../cloudpickle\npopd\n', 'bash ./.github/scripts/test_downstream_project.sh\n', 'bash ./.github/scripts/install_downstream_project.sh\n', 'python -m pip install -e .\n', 'bash ./.github/scripts/test_downstream_project.sh\n', 'python -m pip install --upgrade -r dev-requirements.txt\npython -m pip install setproctitle psutil\n# from https://docs.ray.io/en/master/development.html#building-ray\npip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-2.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl\npushd ..\ngit clone https://github.com/ray-project/ray.git\npushd ray\npython python/ray/setup-dev.py --yes\npopd\npopd\nPROJECT_DIR=$(python -c ""import os, ray; print(os.path.dirname(ray.__file__), flush=True)"")\nrm $PROJECT_DIR/cloudpickle/cloudpickle.py\ncp cloudpickle/cloudpickle.py $PROJECT_DIR/cloudpickle/cloudpickle.py\ncp cloudpickle/compat.py $PROJECT_DIR/cloudpickle/compat.py\ncp cloudpickle/cloudpickle_fast.py $PROJECT_DIR/cloudpickle/cloudpickle_fast.py\n', 'PROJECT_DIR=""$(python -c ""import os, ray; print(os.path.dirname(ray.__file__), flush=True)"")""\nCOVERAGE_PROCESS_START=""$TRAVIS_BUILD_DIR/.coveragerc"" PYTHONPATH=\'.:tests\' pytest -r s\npytest -vl $PROJECT_DIR/tests/test_serialization.py::test_simple_serialization\npytest -vl $PROJECT_DIR/tests/test_serialization.py::test_complex_serialization\npytest -vl $PROJECT_DIR/tests/test_basic.py::test_ray_recursive_objects\npytest -vl $PROJECT_DIR/tests/test_serialization.py::test_serialization_final_fallback\npytest -vl $PROJECT_DIR/tests/test_basic.py::test_nested_functions\n']"
"['pip install -U ""pip>=23.1.2""\npip install -U ""tox-gh-actions==3.1.0"" coverage\n', 'python --version\npip --version\n', 'tox', 'coverage xml', 'pip install -U setuptools \npip install -U ""tox>=4.5.1,<5""\n', 'tox -e lint', 'python -m pip install build twine check-wheel-contents', 'python -m build --sdist --wheel .', 'ls -l dist', 'check-wheel-contents dist/*.whl', 'python -m twine check dist/*']"
"['pipx run build --sdist', 'python -m pip install --upgrade pip\n', 'pip install .[dev,collision]\npip install pytest-timeout\npytest --ignore=roboticstoolbox/blocks --timeout=50 --timeout_method thread -s\n', 'python -m pip install --upgrade pip\n', 'pip install -e .[dev,collision,vpython]\npytest --ignore=roboticstoolbox/blocks --cov=roboticstoolbox --cov-report xml:coverage.xml\ncoverage report\n', 'python -m pip install --upgrade pip\npip install .[docs]\npip install git+https://github.com/petercorke/sphinx-autorun.git\npip install sympy\nsudo apt-get install graphviz\n', 'cd docs\nmake html\n# Tell GitHub not to use jekyll to compile the docs\ntouch build/html/.nojekyll\ncd ../\n', 'git clone https://github.com/petercorke/robotics-toolbox-python.git --branch gh-pages --single-branch gh-pages\ncp -r docs/build/html/* gh-pages/\ncd gh-pages\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""Update documentation"" -a || true\n# The above command will fail if no changes were present, so we ignore\n# that. \n', 'echo ""Update pip""\npython -m pip install --upgrade pip\npip install -U build\ncd sm\necho ""Install sm""\npip install .\ncd ../sg\necho ""Install sg""\npip install .\ncd ../swift\necho ""Install swift""\npip install .\ncd ../robotics-toolbox-python/rtb-data\npip install .\n', 'cd robotics-toolbox-python\npip install -e .[dev,collision]\npip install pytest-timeout\npython -c ""import spatialgeometry""\npython -c ""import roboticstoolbox""\npytest --ignore=roboticstoolbox/blocks --timeout=50 --timeout_method thread -s\n']"
"['sudo apt-get update\nsudo apt-get install binutils libproj-dev gdal-bin libmemcached-dev\n', 'pip install -U ""pip>=23.1.1""\npip install -U ""tox-gh-actions==3.1.0"" coverage\n', 'python --version\npip --version\npsql -V\nmysql -V\n', ""psql -U postgres -c 'CREATE DATABASE postgis'\npsql -U postgres postgis -c 'CREATE EXTENSION IF NOT EXISTS postgis;'\nmysql --protocol=TCP --user=root -e 'create database django_prometheus_1;'\n"", 'tox', 'coverage combine .coverage django_prometheus/tests/end2end/.coverage\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install wheel setuptools packaging twine build --upgrade\n', 'python update_version_from_git.py', 'python -m build', 'true', 'RELEASE_TAG=${GITHUB_REF#refs/tags/}\necho ""Release tag: ${RELEASE_TAG}""\nif [[ ""${RELEASE_TAG}"" =~ ^v[0-9]+.[0-9]+.[0-9]+$ ]]; then\n  echo ""release-tag=${RELEASE_TAG}"" >> $GITHUB_OUTPUT\n  echo ""release-version=${RELEASE_TAG#v}"" >> $GITHUB_OUTPUT\nelse\n  echo ""::error::Release tag \'${RELEASE_TAG}\' must match \'v\\d+.\\d+.\\d+\'.""\n  exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install wheel setuptools packaging twine build --upgrade\n', 'python -m build']"
"['echo ""::add-matcher::.github/actionlint-matcher.json""\n', 'python -m pip install pre-commit', 'pre-commit run --all-files --show-diff-on-failure', 'python -m pip install build check-wheel-contents tox twine', 'python -m build', 'ls -l dist', 'check-wheel-contents dist/*.whl', 'python -m twine check dist/*', 'pip install .', 'python ./tools/get-version.py >> $GITHUB_OUTPUT', 'set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox', 'set -xe\npython -m pip install --upgrade coverage[toml]\n', 'coverage combine\ncoverage xml\n', 'sudo apt-get install -y pandoc\n', 'tree dist\n', 'pandoc -s -o README.md README.rst\n']"
"['kubectl apply -f kubernetes/spilo_kubernetes.yaml', 'python -m pip install flake8 docker-compose==1.17.1', 'PGVERSION=$(sed -n \'s/^ARG PGVERSION=\\([1-9][0-9]*\\).*$/\\1/p\' Dockerfile)\nIMAGE=""${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-$PGVERSION:${GITHUB_REF/refs\\/tags\\//}""\necho ""NAME=$IMAGE"" >> $GITHUB_OUTPUT\n', 'bash postgres-appliance/tests/test_spilo.sh \n', 'sudo apt-get install -y shellcheck', 'python -m pip install flake8 docker-compose==1.17.1', ""find postgres-appliance -name '*.sh' -print0 | xargs -0 shellcheck"", ""find postgres-appliance -name '*.py' -print0 | xargs -0 python -m flake8"", 'cd postgres-appliance && docker build -t spilo .', 'bash postgres-appliance/tests/test_spilo.sh', 'bash -x postgres-appliance/tests/locales_test/test_locales.sh']"
"['# stop the build if there are Python syntax errors or undefined names\nsudo apt-get install postgresql postgresql-contrib\n', 'sudo pg_ctlcluster 12 main start\nsudo apt-get -qq install graphviz python-dev libgraphviz-dev pkg-config\nsudo -u postgres psql -p 5433 -U postgres -c ""CREATE DATABASE test ENCODING \'UTF8\';""\nsudo -u postgres psql -p 5433 -U postgres test -c ""CREATE SCHEMA test;"" \n', 'python -m pip install --upgrade pip\npip install flake8 pytest tox\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox\n']"
[]
"['# Set OS specific vars and compiler flags\necho ""OS_NAME=osx"" >> $GITHUB_ENV\n# TODO: work out why this is necessary (from CI helpers)\necho ""MACOSX_DEPLOYMENT_TARGET=10.9"" >> $GITHUB_ENV\nulimit -S -n 2048\nclang -v\necho ""CC=clang"" >> $GITHUB_ENV\nclang++ -v\necho ""CXX=clang++"" >> $GITHUB_ENV\ngfortran-9 -v\necho ""FC=gfortran-9"" >> $GITHUB_ENV\necho ""numprocs=3"" >> $GITHUB_ENV\n', '# Set OS specific vars and compiler flags\necho ""OS_NAME=linux"" >> $GITHUB_ENV\ngcc -v\necho ""CC=gcc"" >> $GITHUB_ENV\ng++ -v\necho ""CXX=g++"" >> $GITHUB_ENV\ngfortran -v\necho ""FC=gfortran"" >> $GITHUB_ENV\necho ""numprocs=2"" >> $GITHUB_ENV\n', '# NOTE: vars need to be re-assigned\n# NOTE: set matrix.numpy to pin to a specific numpy version\nconda_deps=""${{ matrix.numpy }} ${MDA_CONDA_${{ matrix.run_type }}_DEPS}""\npip_deps=${MDA_PIP_${{ matrix.run_type }}_DEPS}\nmamba install ${conda_deps}\npip install ${pip_deps}\n\n# also install asv if required\nif [ ${{ matrix.name }} = ""asv_check"" ]; then\n  pip install asv\nfi\n', '# Check OS and python setup\necho ""OS: ${OS_NAME}""\nwhich python\nwhich pip\npip list\nconda info\nconda list\n', '# We manually build hole2 to avoid OS incompatibilities\ngit clone https://github.com/MDAnalysis/hole2.git\ncd hole2/src\nsource ../source.apache\n(make FC=${FC}) && (make PREFIX=${HOME}/hole2 FC=${FC} install)\nsource ../source.unset\necho ""HOLE_BINDIR=${HOME}/hole2/bin"" >> $GITHUB_ENV\necho ""${HOME}/hole2/bin"" >> $GITHUB_PATH\n', '# TODO: using install instead of develop here causes coverage to drop\n# for .pyx file. If possible a solution for this should be found.\n(cd package/ && python setup.py develop) && (cd testsuite/ && python setup.py install)\n', 'PYTEST_FLAGS=""--disable-pytest-warnings --durations=50""\nif [ ${{ matrix.codecov }} = ""true"" ]; then\n  PYTEST_FLAGS=""${PYTEST_FLAGS} --cov=MDAnalysis --cov-report=xml""\nfi\necho $PYTEST_FLAGS\npytest -n $numprocs testsuite/MDAnalysisTests $PYTEST_FLAGS\n', 'cd benchmarks\ntime python -m asv check -E existing\n', 'conda_deps=""${{ env.MDA_CONDA_MIN_DEPS }} ${{ env.MDA_CONDA_EXTRA_DEPS}}""\npip_deps=""${{ env.MDA_PIP_MIN_DEPS}} ${{ env.MDA_PIP_EXTRA_DEPS }} sphinx==1.8.5 sphinx-sitemap sphinx_rtd_theme msmb_theme==1.2.0""\nmamba install ${conda_deps}\npip install ${pip_deps}\n', 'cd package && python setup.py develop\n', 'cd package && python setup.py build_sphinx -E\n', '# set up environment variables\n# cannot execute bash to make variables in env section\n# export URL for the Python script $UPDATE_JSON\nexport URL\nexport VERSION=$(cd package/MDAnalysis; python -c \'import version; print(version.__version__)\')\nUPDATE_JSON=$(pwd)/maintainer/update_json_stubs_sitemap.py\nBRANCH=""${GITHUB_REF#refs/heads/}""\n\n# the below turns off non-blocking as it causes large writes to stdout to fail\n# (see https://github.com/travis-ci/travis-ci/issues/4704)\n# commented out as this is not a problem with gh-actions\n# python -c \'import os,sys,fcntl; flags = fcntl.fcntl(sys.stdout, fcntl.F_GETFL); fcntl.fcntl(sys.stdout, fcntl.F_SETFL, flags&~os.O_NONBLOCK);\'\ncd package/doc/html/html\n\n# move docs into version subfolder\nmkdir ../${VERSION} && mv * ../${VERSION} && mv ../${VERSION} $VERSION\n\n# set up git\nREV=$(git rev-parse --short HEAD)\ngit init\ngit config user.name $GH_USER\ngit config user.email $GH_EMAIL\ngit remote add upstream ""https://${GH_USER}:${GH_TOKEN}@${GH_REPOSITORY}""\ngit fetch --depth 50 upstream $BRANCH gh-pages\ngit reset upstream/gh-pages\n\n# redirects and copies\nmkdir latest\npython $UPDATE_JSON\ntouch .\ntouch .nojekyll\n\ngit add -A ${VERSION}/\ngit add .nojekyll versions.json *.xml *.html index.html latest\n\nfor dirname in dev stable documentation_pages ; do\n  if [ -d $dirname ]; then git add $dirname; fi\ndone\n\n# check for anything to commit\n# https://stackoverflow.com/questions/3878624/how-do-i-programmatically-determine-if-there-are-uncommited-changes\ngit diff-index --quiet HEAD -- || git commit -m ""rebuilt html docs for version ${VERSION} from branch ${BRANCH} with sphinx at ${REV}""\ngit push -q upstream HEAD:gh-pages\n', 'which pip\nwhich python\npip install pylint\n', 'pylint --py3k package/MDAnalysis && pylint --py3k testsuite/MDAnalysisTests\n', 'conda install setuptools cython numpy twine\n', 'cd package && python setup.py sdist\n', 'DISTRIBUTION=$(ls -t1 package/dist/MDAnalysis-*.tar.gz | head -n 1)\ntest -n ""${DISTRIBUTION}"" || { echo ""no distribution package/dist/MDAnalysis-*.tar.gz found""; exit 1; }\necho ""twine check $DISTRIBUTION""\ntwine check $DISTRIBUTION\n']"
"['python -m pip install --upgrade pip setuptools\npython -m pip install tox coverage\n', 'tox -e ${{ matrix.tox_env }}', 'tox -e ${{ matrix.tox_env }}-coverage', 'bash scripts/upload-coverage.sh -F GHA,${{ runner.os }}', 'python -m pip install --upgrade pip\npip install --upgrade wheel setuptools setuptools_scm\n', 'python setup.py sdist bdist_wheel']"
"['pip install -U tox\ntox\n', 'if [[ ${{ github.event.ref }} =~ ^refs/tags/v[0-9]+[.][0-9]+[.][0-9]+(rc[0-9]+|[.]dev[0-9]+)?$ ]]; then\n  echo ::set-output name=release_tag::true\nfi\n', 'pip install --upgrade setuptools wheel twine\npython setup.py sdist bdist_wheel\nexport TWINE_USERNAME=__token__\nexport TWINE_PASSWORD=${{ secrets.PYPI_TOKEN }}\ntwine upload dist/*\n', 'sudo apt-get update\nsudo apt-get install libxml2-dev libxslt-dev\n', 'pip install -U tox\ntox\n', 'bash <(curl -s https://codecov.io/bash)']"
[]
"['python -m pip install --upgrade pip\npip install -r test_requirements.txt\n', 'make lint\n', 'make typing\n', 'make test\n']"
"['docker network create data', 'python -m pip install -r render/requirements.txt', 'python render/render.py', 'echo ""::set-output name=date::$(date +\'%Y-%m-%d\')""', 'git config --global user.name \'Github Actions\'\ngit config --global user.email \'41898282+github-actions[bot]@users.noreply.github.com\'\ngit add .\ngit commit -am ""Update results""\ngit tag ${{ steps.date.outputs.date }}\ngit push -f --tags\ngit push\n', 'find frameworks | grep requirements.txt | xargs -n 1 pip install -r\npip install pytest pytest-aio\n', 'pytest frameworks\n']"
[]
[]
"['git checkout HEAD^2', 'pip install .\npip install -r requirements-test.txt\npip freeze\n', 'make test\n', 'pip install -U coveralls\ncoveralls\n', 'pip install -U coveralls\ncoveralls --finish\n']"
"['pip install poetry', 'poetry install --no-root --only docs', 'poetry run mkdocs build', 'pip install black\n.run/lint_black.sh\n', 'pip install pycodestyle\n.run/lint_pycodestyle.sh\n', 'pip install pylint\n.run/lint_pylint.sh\n', 'pip install pylint\ntouch __init__.py\npylint $(pwd) --disable="""" --ignore-patterns=.venv --exit-zero\nrm __init__.py\n', 'pip install mypy\n.run/lint_mypy.sh', 'version=${{ github.event.release.tag_name }}\npoetry version $version\nsed -i ""s/__version__ = .*/__version__ = \\\'${version}\\\'/g"" ./selene/__init__.py\n', 'python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'sudo apt-get update\nsudo apt-get install xvfb\nsudo Xvfb -ac :99 -screen 0 1280x1024x24 > /dev/null 2>&1 &\n', 'poetry run pytest -sv --cov-config .coveragerc --cov-report html:skip-covered --cov-report term:skip-covered --cov=selene  --cov-report xml:coverage.xml --tb=short tests/ --headless=True\nmkdir -p Artifacts/skip-covered\ncp -r skip-covered Artifacts/skip-covered\n', 'echo ${{ steps.traffic.outputs.traffic_branch }}\necho ${{ steps.traffic.outputs.traffic_path }}\ncd ${{ steps.traffic.outputs.traffic_path }}\nls -a\n']"
"['python -m pip install --upgrade pip', 'python -m pip install --upgrade tox', 'tox -e docs', 'npm ci', 'npm run lint', 'npm run unit', 'git config --global core.autocrlf false', 'python -m pip install --upgrade pip', 'python -m pip install --upgrade tox', 'tox -e py${{ matrix.python-version }}-cov -- testing/test_unit.py', 'tox -e py${{ matrix.python-version }} -- testing/test_unit.py', 'tox -e ${{ matrix.tox-env }} -- testing/test_unit.py', 'git config --global core.autocrlf false', './start', 'python -m pip install --upgrade pip', 'python -m pip install --upgrade tox', 'tox -e ${{ matrix.python-version }}-cov -- testing/test_integration.py', 'tox -e ${{ matrix.python-version }} -- testing/test_integration.py', 'tox -e ${{ matrix.tox-env }} -- testing/test_integration.py']"
"['hatch run cov:test --cov-fail-under 50 || hatch run test:test --lf\n', 'hatch run test:nowarn || hatch run test:nowarn --lf\n', 'hatch run cov:nowarn || hatch run test:nowarn --lf\n', 'pip install .\ncd $HOME\npython -m ipykernel_launcher --help\n', 'hatch run typing:test\nhatch run lint:style\npipx run interrogate -vv .\npipx run doc8 --max-line-length=200\n', 'hatch run docs:api\n# If this fails run `hatch run docs:api` locally\n# and commit.\ngit status --porcelain\ngit status -s | grep ""A"" && exit 1\ngit status -s | grep ""M"" && exit 1\necho ""API docs done""\n', 'hatch run docs:build', 'pip install .[test]\npip uninstall --yes debugpy\n', 'pip freeze\n', 'pytest -W default -vv || pytest --vv -W default --lf', 'hatch run test:list\n', 'hatch run test:nowarn || hatch run test:nowarn --lf\n', 'hatch run test:nowarn || hatch run test:nowarn --lf\n', 'git clone https://github.com/jupyter/jupyter_kernel_test.git\ncd jupyter_kernel_test\npip install -e "".[test]""\npython test_ipykernel.py\n', ""sudo apt-get update\nsudo apt-get install -y --no-install-recommends '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\n"", 'cd ${GITHUB_WORKSPACE}/..\ngit clone https://github.com/jupyter/qtconsole.git\ncd qtconsole\n${pythonLocation}/bin/python -m pip install -e "".[test]""\n${pythonLocation}/bin/python -m pip install pyqt5\n', '${pythonLocation}/bin/python -m pip install -e .', 'cd ${GITHUB_WORKSPACE}/../qtconsole\nxvfb-run --auto-servernum ${pythonLocation}/bin/python -m pytest -x -vv -s --full-trace --color=yes qtconsole\n', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends libegl1-mesa\n', 'cd ${GITHUB_WORKSPACE}/..\ngit clone https://github.com/spyder-ide/spyder-kernels.git\ncd spyder-kernels\n${pythonLocation}/bin/python -m pip install -e "".[test]""\n', '${pythonLocation}/bin/python -m pip install -e .', 'cd ${GITHUB_WORKSPACE}/../spyder-kernels\nxvfb-run --auto-servernum ${pythonLocation}/bin/python -m pytest -x -vv -s --full-trace --color=yes spyder_kernels\n', 'echo ""Optional): Review Draft Release: ${{ steps.prep-release.outputs.release_url }}""\n', 'echo ""Verify the final release""\necho ${{ steps.finalize-release.outputs.release_url }}\n', 'echo ""Failed to Publish the Draft Release Url:""\necho ${{ steps.populate-release.outputs.release_url }}\n']"
"['nox -s lint', 'nox -s tests-${{ matrix.python-version }} -- --run-slow', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['pip install flake8\nflake8 src scripts conftest.py\n', 'sudo apt-get update\nsudo apt-get install -y pkg-config build-essential\nsudo apt-get install -y libssl-dev libacl1-dev libxxhash-dev liblz4-dev libzstd-dev\nsudo apt-get install -y libfuse-dev fuse || true  # Required for Python llfuse module\nsudo apt-get install -y libfuse3-dev fuse3 || true  # Required for Python pyfuse3 module\n', 'brew install pkg-config || brew upgrade pkg-config\nbrew install zstd || brew upgrade zstd\nbrew install lz4 || brew upgrade lz4\nbrew install xxhash || brew upgrade xxhash\nbrew install openssl@1.1 || brew upgrade openssl@1.1\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -r requirements.d/development.txt\n', '# pip install -e .\npython setup.py -v develop\n', '# do not use fakeroot, but run as root. avoids the dreaded EISDIR sporadic failures. see #2482.\n#sudo -E bash -c ""tox -e py""\ntox --skip-missing-interpreters\n', './scripts/msys2-install-deps development', 'pip install -e .\npyinstaller -y scripts/borg.exe.spec\n', './dist/borg.exe -V\npytest -n4 --benchmark-skip -vv -rs -k ""not remote""\n', 'sudo apt-get update\nsudo apt-get install -y pkg-config build-essential\nsudo apt-get install -y libssl-dev libacl1-dev libxxhash-dev liblz4-dev libzstd-dev\n', 'python3 -m venv ../borg-env\nsource ../borg-env/bin/activate\npip3 install -r requirements.d/development.txt\npip3 install -e .\n']"
"['poetry install', 'poetry run mypy src', 'poetry run flake8 src', 'poetry run pylint src', 'poetry run pytest src/tests', 'poetry run vulture src']"
"['python -m pip install --upgrade pip\npip install flake8 pytest -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['pip install --upgrade pip\npip install ""poetry<2"" ""tox<4"" ""tox-gh-actions<3"" ""coveralls<4""\n', 'tox', 'coveralls --service github', 'pip install coveralls', 'coveralls --service github --finish', 'sed -i ""s/^version = .*/version = \'${{github.ref_name}}\'/"" pyproject.toml', 'pip install poetry\npoetry build\n']"
"['echo ""ðŸŽ‰ The job was automatically triggered by a ${{ github.event_name }} event.""', 'echo ""ðŸ§ This job is now running on a ${{ runner.os }} server hosted by GitHub!""', 'echo ""ðŸ”Ž The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.""', 'python -m pip install --upgrade pip setuptools wheel pur\n', 'pur -r requirements.txt\npur -r requirements-dev.txt\npur --skip csirtgsdk -r requirements-output.txt\npur -r docs/requirements.txt\n', 'pip install --upgrade -r requirements.txt\npip install --upgrade -r requirements-dev.txt\npip install --upgrade -r requirements-output.txt\npip install --upgrade -r docs/requirements.txt\n', 'pip check\n', 'git diff\n', 'echo ""Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}""\necho ""Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}""\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npip install tox-gh-actions\n', 'tox']"
""
"['pip install wheel\n', 'python setup.py build_ext -j $(getconf _NPROCESSORS_ONLN) --inplace\n', 'pip install --upgrade pip\npip install -r requirements_tests.txt\npython -m pytest tests/ -n2 -v\n', 'echo ""Building wheel and uploading""\npip install wheel twine\npython setup.py sdist bdist_wheel\npython -m twine upload --non-interactive -u \'__token__\' -p ${{ secrets.PYPI_API_TOKEN }} dist/*.whl\necho ""Uploading source code""\npython -m twine upload --non-interactive --skip-existing -u \'__token__\' -p ${{ secrets.PYPI_API_TOKEN }} dist/*.tar.gz\n', '# Fix cryptography build: https://github.com/pyca/cryptography/issues/3489\nbrew install openssl\necho ""CPPFLAGS=-I/usr/local/opt/openssl/include"" >> $GITHUB_ENV\necho ""LDFLAGS=-L/usr/local/opt/openssl/lib"" >> $GITHUB_ENV\n', 'pypy3 -m ensurepip\npypy3 -m pip install wheel\n', '# Fix issue with NumPy and PolyFit: https://github.com/numpy/numpy/issues/15947\npip cache remove numpy\n# brew install openblas\nOPENBLAS=""$(brew --prefix openblas)"" pypy3 -m pip install numpy\n', 'pip install --upgrade pip\npypy3 -m pip install -r requirements_tests.txt\npypy3 -m pytest tests/ -n2 -v\n', 'echo ""Uploading wheel""\npip install twine\npython -m twine upload --non-interactive -u \'__token__\' -p ${{ secrets.PYPI_API_TOKEN }} dist/*.whl\n']"
"['sudo apt-get update && sudo apt-get install -y libhdf5-dev python3-numpy python3-scipy python3-matplotlib python3-sklearn', 'pip3 install --quiet -r requirements.txt', 'pytest', 'sudo apt-get update && sudo apt-get install -y libhdf5-dev python3-numpy python3-scipy python3-matplotlib python3-sklearn', 'pip3 install --quiet -r requirements.txt', 'python3 install.py', 'python3 run.py --docker-tag ann-benchmarks-${LIBRARY} --max-n-algorithms 3 --runs 2 --dataset $DATASET --run-disabled --timeout 300\npython3 run.py --docker-tag ann-benchmarks-${LIBRARY} --max-n-algorithms 3 --runs 2 --dataset $DATASET --run-disabled --batch --timeout 300\nsudo chmod -R 777 results/\npython3 plot.py --dataset $DATASET --output plot.png\npython3 plot.py --dataset $DATASET --output plot-batch.png --batch\npython3 data_export.py --out test.csv\npython3 create_website.py --outputdir . --scatter --latex\n']"
""
['echo ${{ steps.docker_build.outputs.digest }}']
"['poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run pytest --cov-report=xml']"
"['# glibc-langpacks-en needed to work around python locale issues\ndnf install -y \\\n  rpm-build \\\n  dnf-plugins-core \\\n  glibc-langpack-en \\\n  python3-pytest python3-pytest-cov\n\ndnf builddep -y ./virt-manager.spec\n', './setup.py rpm\ndnf install -y \\\n  noarch/virt-install*.rpm \\\n  noarch/virt-manager-common*.rpm\n', 'pytest --cov --cov-report=xml\n', 'dnf install -y python3-setuptools gettext git diffutils\n', 'git config --global user.email ""actions@github.com""\ngit config --global user.name ""Github Actions""\n\ngit fetch --all\ngit rebase remotes/origin/translations\n\ncp po/virt-manager.pot old.pot\n./setup.py extract_messages\nec=0\ndiff -q -I \'POT-Creation-Date\' old.pot po/virt-manager.pot || ec=$?\ncase ""${ec}"" in\n  0) ;;\n  1)\n    git commit po/virt-manager.pot \\\n      --message ""Refresh translation .pot template""\n\n    remote_repo=""https://${GITHUB_ACTOR}:${{ secrets.GITHUB_TOKEN }}@github.com/${GITHUB_REPOSITORY}.git""\n    git push ""${remote_repo}"" HEAD:translations\n    ;;\n  *)\n    echo ""diff failed with exit status ${ec}"" >&2\n    exit 1\n    ;;\nesac\n']"
"['Invoke-WebRequest http://jaist.dl.sourceforge.net/project/libusb-win32/libusb-win32-releases/1.2.6.0/libusb-win32-bin-1.2.6.0.zip -OutFile libusb-win32-bin-1.2.6.0.zip\nExpand-Archive libusb-win32-bin-1.2.6.0.zip .\nMove-Item libusb-win32-bin-1.2.6.0\\bin\\amd64\\libusb0.dll .\necho $env:GITHUB_WORKSPACE | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\n', 'brew install libusb\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -OO -m PyInstaller pmca-console.spec\npython -OO -m PyInstaller pmca-gui.spec\n', 'dist/pmca-console* -h\n', 'tag=""${GITHUB_REF##*/}""\nassets=""$(find dist/* -exec echo -a {} \\;)""\nhub release create $assets -m ""$tag"" ""$tag"" || hub release edit $assets -m """" ""$tag""\n']"
"['sudo systemctl start postgresql.service', 'sudo -u postgres createuser --superuser $(whoami)', 'createdb liberapay_tests', 'DATABASE_URL=liberapay_tests ./recreate-schema.sh test', 'rm -rfv tests/py/fixtures\necho LIBERAPAY_I18N_TEST=yes >> $GITHUB_ENV\n', 'pip install tox', 'tox']"
"['pip install -c requirements.txt -r requirements-lint.txt\nlintlizard --ci\n', 'pip install -r requirements.txt -r requirements-test.txt\n', 'pytest']"
"['poetry install --no-interaction --no-root', 'poetry build\npip install dist/hvac-*.whl\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run black --check .', 'poetry run flake8 . --count --statistics', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\necho ""deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"" \\\n  | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\nsudo apt update \\\n  -o Dir::Etc::sourceparts=""-"" \\\n  -o APT::Get::List-Cleanup=""0"" \\\n  -o Dir::Etc::sourcelist=""sources.list.d/hashicorp.list""\n\nsudo apt install \\\n  vault-enterprise=1.7.2+ent \\\n  ;\n\n# We disble cap_ipc_lock here as its generally incompatabile with GitHub\n# Actions\' runtime environments.\nsudo setcap cap_ipc_lock= /usr/bin/vault\n', 'poetry run make doctest\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run pytest \\\n  --cov=hvac \\\n  --cov-report=xml \\\n  tests/unit_tests\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\necho ""deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"" \\\n  | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\nsudo apt update \\\n  -o Dir::Etc::sourceparts=""-"" \\\n  -o APT::Get::List-Cleanup=""0"" \\\n  -o Dir::Etc::sourcelist=""sources.list.d/hashicorp.list""\n\nsudo apt install \\\n  consul \\\n  ${{ matrix.vault-version }} \\\n  ;\n\n# We disble cap_ipc_lock here as its generally incompatabile with GitHub\n# Actions\' runtime environments.\nsudo setcap cap_ipc_lock= /usr/bin/vault\n', 'poetry run pytest \\\n  --cov=hvac \\\n  --cov-report=xml \\\n  tests/integration_tests\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry publish --build --username ""${PYPI_USERNAME}"" --password ""${PYPI_PASSWORD}""\n', 'cat <<MKDOWN >> $GITHUB_STEP_SUMMARY\n### Release Drafter :rocket:\n\n* **Draft Release URL**:\n[${{ steps.release-drafter.outputs.name }} (${{ steps.release-drafter.outputs.id }})](${{ steps.release-drafter.outputs.html_url }})\n* **Tag Name**: ${{ steps.release-drafter.outputs.tag_name }}\nMKDOWN\n']"
"['pip install tox\ntox -e docs\n', 'pip install -r requirements.txt\npip install -r test-requirements.txt\nnose2 -v\n']"
"['pip3 install pre-commit', 'pre-commit run --all-files', 'python -c ""import sys; print(f\'Python version: {sys.version}\')""\npython -m pip install -U setuptools pip pipenv codecov wheel\npipenv lock --python ${{ matrix.python-version }}\npipenv sync --dev\npipenv check || true\npipenv graph\n', 'make flake\n', 'make mototest\n', 'echo ""Predeploy step""\n', 'python -c ""import sys; print(f\'Python version: {sys.version}\')""\npython -m pip install -U setuptools pip wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install twine\n', 'twine upload dist/*\n']"
"['pip install black', 'black --check .', 'black .\ngit config --global user.name github-actions\ngit config --global user.email \'${GITHUB_ACTOR}@users.noreply.github.com\'          \ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit commit -am ""fixup! Format Python code with psf/black push""\ngit push --force origin HEAD:$GITHUB_REF\n', 'conda install pip pyyaml fiona shapely nomkl\n', 'python -m pip install --upgrade setuptools pip black codecov\npython -m pip install .[dev]\n', 'black --check .\n', 'coverage run --source=sentinelsat -m pytest -v\n', 'python -m pip install --upgrade setuptools pip\npython -m pip install .[dev]\n', './tests/fixtures/clean_cassettes.sh\n', 'sentinelsat --info | grep ""DHuS version""\n', 'pytest --vcr-record new_episodes -vvv\n', 'pytest -vvv\n', 'python -m pip install -U build wheel setuptools', 'python -m build', 'ls dist/']"
"['sudo apt update\nsudo apt install libpulse-dev libegl1-mesa libopengl0 gstreamer1.0-gl\n', 'mamba info\nmamba list\n', './.github/workflows/test.sh pyqt5', './.github/workflows/test.sh pyqt6', './.github/workflows/test.sh pyside2', './.github/workflows/test.sh pyside6', 'cd temp_test_dir  # Switch to test working dir per non-src-layout hack\ncat qtpy_basedir.txt\npipx run coveralls --service=github --rcfile=""../.coveragerc"" --basedir=""$(cat qtpy_basedir.txt)""\n']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload --repository testpypi dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo systemctl start mysql\nwhile ! mysqladmin ping -h""127.0.0.1"" -P3306 --silent; do\n    sleep 1\ndone\nmysql -uroot -proot -e ""create database flask_blogging;""\nsudo systemctl start postgresql.service\npg_isready\nsudo -u postgres psql -c ""create database flask_blogging;""\nsudo -u postgres psql -c ""alter user postgres password \'postgres\';""\n', 'sudo apt-get install -y python3-dev default-libmysqlclient-dev build-essential\npython -m pip install --upgrade pip wheel\npython -m pip install flake8 nose2 mysqlclient psycopg2 flask_sqlalchemy moto\nif [ -f Requirements.txt ]; then pip install -r Requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'nose2 --with-coverage --coverage flask_blogging -v\n']"
"['python -m pip install ""Django~=${{ matrix.django-version }}.0""\n', 'python -m pip install pytz ""djangorestframework~=${{ matrix.drf-version }}.0"" ""Django~=${{ matrix.django-version }}.0""\n', 'python -m pip install -e "".[test]""\n', 'pytest .\n', 'python -m pip install -e "".""\npython -m pip install -e "".[test]""\n', 'flake8 . --ignore=E501,E402\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'cd testing\npython -m unittest']"
"['pip install -U tox', 'tox', 'pip install -U wheel\npip install -U virtualenv asv\ngit checkout master && git checkout -\nasv machine --machine github-actions --yes\n', 'asv run -j 8 --interleave-processes --skip-existing v3.2.0..HEAD\n', 'git config --global user.email ""$GIT_AUTHOR_EMAIL""\ngit config --global user.name ""$GIT_AUTHOR_NAME""\nasv gh-pages --no-push\ngit push -f origin gh-pages:gh-pages\n', 'pip install -U wheel\npip install -U virtualenv asv\ngit checkout master && git checkout -\nasv machine --machine github-actions --yes\n', 'asv continuous --interleave-processes --only-changed -f 1.25 master HEAD\nCHANGES=""$(asv compare --only-changed -f 1.25 master HEAD)""\necho ""$CHANGES""\n[ -z ""$CHANGES"" ] || exit 1\n', 'git clone https://${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY} repo\ngit -C repo tag $(echo ""$BODY"" | awk \'{print $2"" ""$3}\')\ngit -C repo push --tags\nrm -rf repo\n', 'pip install -U wheel\npip install -U -r ./docs/requirements.txt\ngit config --global user.email ""$GIT_AUTHOR_EMAIL""\ngit config --global user.name ""$GIT_AUTHOR_NAME""\n', 'pushd wiki\nmake\ngit commit -a -m ""update release notes to ${GITHUB_REF#refs/tags/}"" || :\ngit push\npopd\n', 'make -C docs build', 'echo ""PYSHA=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'pip install -U pre-commit', 'if [[ $EVENT == pull_request ]]; then\n  REPORTER=github-pr-review\nelse\n  REPORTER=github-check\nfi\npre-commit run -a todo | reviewdog -efm=""%f:%l: %m"" -name=TODO -tee -reporter=$REPORTER -filter-mode nofilter\npre-commit run -a flake8 | reviewdog -f=pep8 -name=flake8 -tee -reporter=$REPORTER -filter-mode nofilter\n', 'pre-commit run -a --show-diff-on-failure', 'pip install -U tox tox-gh-actions\nmkdir -p ""$HOME/bin""\ncurl -sfL https://coverage.codacy.com/get.sh > ""$HOME/bin/codacy""\nchmod +x ""$HOME/bin/codacy""\necho ""$HOME/bin"" >> $GITHUB_PATH\n', 'TIMEOUT=10m\nif [[ ""${{ matrix.python }}"" = ""2.7"" ]]; then\n  TIMEOUT=15m\nelif [[ ""py${{ matrix.python }}-${{ matrix.os }}"" = ""py3.8-ubuntu"" ]]; then\n  export TOXENV=""py38-tf,py38-tf-keras""  # full\nfi\nif [[ ""${{ matrix.os }}"" != ""ubuntu"" ]]; then\n  tox -e py${PYVER/./}                   # basic\nelse\n  timeout $TIMEOUT tox || timeout $TIMEOUT tox || timeout $TIMEOUT tox\nfi\n', 'pip install -U coveralls\ncoveralls --finish || :\n', 'curl -sfL https://coverage.codacy.com/get.sh > codacy\nbash codacy final || :\n', 'sudo apt-get install -yqq pandoc\npip install -r .meta/requirements-build.txt\nmake build .dockerignore Dockerfile snapcraft.yaml\n', 'if [[ $GITHUB_REF == refs/tags/v* ]]; then\n  echo docker_tags=latest,${GITHUB_REF/refs\\/tags\\/v/} >> $GITHUB_OUTPUT\n  echo snap_channel=stable,candidate,edge >> $GITHUB_OUTPUT\nelif [[ $GITHUB_REF == refs/heads/master ]]; then\n  echo docker_tags=master >> $GITHUB_OUTPUT\n  echo snap_channel=candidate,edge >> $GITHUB_OUTPUT\nelif [[ $GITHUB_REF == refs/heads/devel ]]; then\n  echo docker_tags=devel >> $GITHUB_OUTPUT\n  echo snap_channel=edge >> $GITHUB_OUTPUT\nfi\n', 'changelog=$(git log --pretty=\'format:%d%n- %s%n%b---\' $(git tag --sort=v:refname | tail -n2 | head -n1)..HEAD)\ntag=""${GITHUB_REF#refs/tags/}""\ngh release create --title ""tqdm $tag stable"" --draft --notes ""$changelog"" ""$tag"" dist/${{ steps.dist.outputs.whl }} dist/${{ steps.dist.outputs.whl_asc }}\n']"
"['echo ""all=@networkException"" >> $GITHUB_OUTPUT\necho ""linux=@rany2 @clickot"" >> $GITHUB_OUTPUT\necho ""windows="" >> $GITHUB_OUTPUT\necho ""macos="" >> $GITHUB_OUTPUT\n', 'echo ""linux_version=$( curl -s \'https://versionhistory.googleapis.com/v1/chrome/platforms/linux/channels/stable/versions/all/releases?filter=endtime=none&order_by=version%20desc\'  | jq -rc \'.releases | first | .version\' )"" >> $GITHUB_OUTPUT\necho ""win_version=$( curl -s \'https://versionhistory.googleapis.com/v1/chrome/platforms/win/channels/stable/versions/all/releases?filter=endtime=none&order_by=version%20desc\'  | jq -rc \'.releases | first | .version\' )"" >> $GITHUB_OUTPUT\necho ""mac_version=$( curl -s \'https://versionhistory.googleapis.com/v1/chrome/platforms/mac/channels/stable/versions/all/releases?filter=endtime=none&order_by=version%20desc\'  | jq -rc \'.releases | first | .version\' )"" >> $GITHUB_OUTPUT\n']"
"['pip install --upgrade pip\npip install build\npip freeze\n', 'python -m build --sdist --wheel .\nls -l dist\n']"
"['set -eux\n\ngit config --global user.name ""pytest bot""\ngit config --global user.email ""pytestbot@gmail.com""\n\nlabel=\'${{ github.event.label.name }}\'\ntarget_branch=""${label#backport }""\nbackport_branch=backport-${{ github.event.number }}-to-""${target_branch}""\nsubject=""[$target_branch] $(gh pr view --json title -q .title ${{ github.event.number }})""\n\ngit checkout origin/""${target_branch}"" -b ""${backport_branch}""\ngit cherry-pick -x --mainline 1 ${{ github.event.pull_request.merge_commit_sha }}\ngit commit --amend --message ""$subject""\ngit push --set-upstream origin --force-with-lease ""${backport_branch}""\ngh pr create \\\n  --base ""${target_branch}"" \\\n  --title ""${subject}"" \\\n  --body ""Backport of PR #${{ github.event.number }} to $target_branch branch. PR created by backport workflow.""\n', 'python -m pip install --upgrade pip\npip install --upgrade tox\n', 'sudo apt-get install pandoc\ntox -e publish-gh-release-notes\n', 'python -m pip install --upgrade pip\npip install --upgrade setuptools tox\n', ""tox -e prepare-release-pr -- ${{ github.event.inputs.branch }} ${{ github.token }} --prerelease='${{ github.event.inputs.prerelease }}'\n"", ""tox -e prepare-release-pr -- ${{ github.event.inputs.branch }} ${{ github.token }} --major --prerelease='${{ github.event.inputs.prerelease }}'\n"", 'python -m pip install --upgrade pip\npip install tox coverage\n', 'tox -e ${{ matrix.tox_env }}', 'tox -e ${{ matrix.tox_env }}-coverage', 'python -m coverage xml', 'python -m pip install --upgrade pip\npip install packaging requests tabulate[widechars] tqdm\n', 'python scripts/update-plugin-list.py']"
['pip3 install -r requirements.txt\npython setup.py install\ncd tests && py.test\n']
"['sudo apt-get update\nsudo apt-get install -y libcurl4-openssl-dev libpython3-dev libssl-dev\n', 'sudo systemctl start mysql.service', 'sudo systemctl start postgresql.service\nsudo -u postgres createuser -s runner\n', 'sudo apt-get update\nsudo apt-get install -y libcurl4-openssl-dev libpython3-dev libssl-dev\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install apprise braintree coverage coveralls minio mysqlclient\n', 'coverage run --omit=*/tests/*,*/migrations/* --source=hc manage.py test\n', 'coveralls --service=github', '# Workaround for https://github.com/rust-lang/cargo/issues/8719\nsudo mkdir -p /var/lib/docker\nsudo mount -t tmpfs -o size=10G none /var/lib/docker\nsudo systemctl restart docker\n']"
"['python -m pip install --upgrade pip wheel setuptools\npython -m pip install --upgrade tox\n', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -m pip install --upgrade pip tox\n', 'tox -e qa']"
"['sudo apt-get update\nsudo apt-get -y install libmpv1\n', 'brew install mpv\n', 'choco install make curl\ncurl -L https://github.com/feeluown/FeelUOwn/releases/download/v3.6a0/mpv-1.dll -o C:Windows\\system32\\mpv-1.dll\n', 'pip install --upgrade pip\npip install pyqt5\npip install ""pytest<7.2""\npip install -e .[dev]\n', 'pip install -e .[macos]\n', 'pip install -e .[win32]\n', 'feeluown -h', 'make test\n', 'make integration_test\n', 'pip install coveralls\ncoveralls --service=github\n', 'python -m pip install --upgrade pip\npip install pyqt5\npip install pyinstaller\npip install -e .[macos,battery]\n', 'brew install mpv\n', 'make bundle\n', '# List dist to help double check if bundle is ok.\nls dist/\ncd dist/ && zip FeelUOwnX.zip -r FeelUOwnX.app/\n', 'python setup.py sdist', 'python -m pip install --upgrade pip\npip install pyqt5\npip install pyinstaller\npip install -e .[win32,battery]\n', 'choco install curl\ncurl -L https://github.com/feeluown/FeelUOwn/releases/download/v3.8/mpv-1.dll -o mpv-1.dll\n', '# Add current working directory to path so that mpv-1.dll can be found.\n$env:path += "";.""\nmake bundle\n', '# List dist to help double check if bundle is ok.\nls dist/\npowershell Compress-Archive dist/FeelUOwn dist/FeelUOwn-windows.zip\n']"
"['pip install -r requirements.txt', 'pytest']"
"['sudo apt-get install --no-install-recommends --assume-yes gettext\npip install --upgrade --disable-pip-version-check pip\npip install --upgrade setuptools wheel\npip install --upgrade shuup\n', 'rm -rf build/\nrm -rf dist/\npython setup.py bdist_wheel\n', 'pip install -r requirements-dev.txt', '_misc/check_sanity.py', '_misc/ensure_license_headers.py -s shuup', 'flake8 .', 'isort --check --diff .', 'black --check --diff .', 'sudo apt-get -y install gettext', 'pip install -r requirements-tests.txt', 'mkdir .unit_tests', 'python -m shuup_workbench shuup_makemessages -l en', 'py.test --nomigrations shuup_tests --cov shuup --cov-config=.coveragerc', 'python -m shuup_workbench compilemessages', 'codecov', 'geckodriver --version', 'sudo apt-get -y install gettext', 'python setup.py build_resources', 'pip install -r requirements-tests.txt', 'python -m shuup_workbench compilemessages', 'mkdir .unit_tests', 'py.test -v --nomigrations shuup_tests/browser/front shuup_tests/browser/admin --splinter-headless --splinter-screenshot-dir=.unit_tests/']"
"['git lfs pull', 'python -m venv pygraphistry\nsource pygraphistry/bin/activate\npython -m pip install --upgrade pip\npython -m pip install -e .[test]\n', 'source pygraphistry/bin/activate\n./bin/lint.sh\n', 'source pygraphistry/bin/activate\n./bin/typecheck.sh\n', 'source pygraphistry/bin/activate\n./bin/test-minimal.sh\n', 'git lfs pull', 'python -m venv pygraphistry\nsource pygraphistry/bin/activate\npython -m pip install --upgrade pip\npython -m pip install -e .[docs,test,build,bolt,igraph,networkx,gremlin,nodexl,jupyter]\n', 'source pygraphistry/bin/activate\n./bin/lint.sh\n', 'source pygraphistry/bin/activate\n./bin/typecheck.sh\n', 'source pygraphistry/bin/activate\n./bin/test.sh\n', 'git lfs pull', 'python -m venv pygraphistry\nsource pygraphistry/bin/activate\npython -m pip install --upgrade pip\npython -m pip install -e .[test,umap-learn]\n', 'source pygraphistry/bin/activate\n./bin/typecheck.sh\n', 'source pygraphistry/bin/activate\n./bin/test-features.sh\n', 'source pygraphistry/bin/activate\n./bin/test-umap-learn-core.sh\n', 'git lfs pull', 'python -m venv pygraphistry\nsource pygraphistry/bin/activate\npython -m pip install --upgrade pip\npython -m pip install -e .[test,ai]\necho ""dirty-cat: `pip show dirty-cat | grep Version`""\necho ""pandas: `pip show pandas | grep Version`""\necho ""numpy: `pip show numpy | grep Version`""\necho ""scikit-learn: `pip show scikit-learn | grep Version`""\necho ""scipy: `pip show scipy | grep Version`""\necho ""umap-learn: `pip show umap-learn | grep Version`""\n', 'source pygraphistry/bin/activate\n./bin/typecheck.sh\n', 'source pygraphistry/bin/activate\n./bin/test-dbscan.sh\n', 'source pygraphistry/bin/activate\n./bin/test-features.sh\n', 'source pygraphistry/bin/activate\n./bin/test-text.sh\n', 'source pygraphistry/bin/activate\n./bin/test-umap-learn-core.sh\n', 'source pygraphistry/bin/activate\n./bin/test-embed.sh\n', 'git lfs pull', 'cd docker && WITH_SUDO="" "" ./test-cpu-local-neo4j-only.sh\n', 'git lfs pull', 'python -m pip install --upgrade pip\npython -m pip install -e .[build]\n', './bin/build.sh\n', 'cd docs && ./docker.sh\n', 'docker run --rm -v ""$(pwd)/README.md:/README.md:ro"" avtodev/markdown-lint:v1 README.md\n', 'git lfs pull', 'python -m pip install -e .[build]', './bin/build.sh']"
"['# Some packages, e.g. build-essential and curl are available\n# implicitly in GitHub Actions-specific images.\nsudo apt-get -qy update\nsudo apt-get -y --no-install-recommends install \\\n  gcc-multilib    \\\n  python3-dev     \\\n  python3-venv\n', 'set +e # Do not hard exit on an erroring call!\npushd cppcheck\n\n# Note: Cppcheck\'s compilation would require CMake, but a new enough\n# version is automatically present in the GitHub Actions-specific\n# image.\n\necho ""::group::Building Cppcheck""\nmkdir Build\npushd Build\n\ncmake .. -DCMAKE_BUILD_TYPE=Release\ncmake --build . -- -j $(nproc)\n\nif [[ $? -eq 0 ]]\nthen\n  export CPPCHECK_BUILD_SUCCESSFUL=YES\nfi\n\npopd # Build\n\nif [[ x""$CPPCHECK_BUILD_SUCCESSFUL""y == ""xYESy"" ]]\nthen\n  sudo update-alternatives --install \\\n    /usr/bin/cppcheck cppcheck ""$(pwd)/Build/bin/cppcheck"" 10000\n  echo ""::endgroup::""\n\n  echo ""Installed Cppcheck:""\n  update-alternatives --query cppcheck\nelse\n  echo ""::endgroup::""\n  echo ""::notice title=Cppcheck failed to build locally::Coverage will check using Ubuntu official Cppcheck release instead.""\n\n  sudo apt-get install -y --no-install-recommends \\\n    cppcheck\nfi\n\npopd # cppcheck\n', 'export DISTRO_FANCYNAME=""$(lsb_release -c | awk \'{ print $2 }\')""\n\necho ""::group::Setup LLVM PPA""\ncurl -sL http://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -\nsudo add-apt-repository -y ""deb http://apt.llvm.org/$DISTRO_FANCYNAME/ llvm-toolchain-$DISTRO_FANCYNAME main""\necho ""::endgroup::""\n\n# Get the largest Clang package number available.\nexport LLVM_VER=""$(apt-cache search --full \'clang-[[:digit:]]*$\' | grep \'^Package: clang\' | cut -d \' \' -f 2 | sort -V | tail -n 1 | sed \'s/clang-//\')""\necho ""::group::Install Clang and Clang-Tidy version ${LLVM_VER}""\nsudo apt-get -y --no-install-recommends install \\\n  clang-$LLVM_VER \\\n  clang-tidy-$LLVM_VER\nsudo update-alternatives --install \\\n  /usr/bin/clang clang /usr/bin/clang-$LLVM_VER 10000\nsudo update-alternatives --install \\\n  /usr/bin/clang-tidy clang-tidy /usr/bin/clang-tidy-$LLVM_VER 10000\necho ""::endgroup::""\n\necho ""Installed Clang:""\nupdate-alternatives --query clang\nupdate-alternatives --query clang-tidy\n', 'pushd analyzer\n\necho ""::group::venv""\nmake venv\nsource venv/bin/activate\necho ""::endgroup::""\n\necho ""::group::CodeChecker package""\nmake standalone_package\ndeactivate\necho ""::endgroup::""\n\necho ""CODECHECKER_PATH=$(readlink -f ./build/CodeChecker/bin)"" >> ""$GITHUB_OUTPUT""\npopd # analyzer\n', 'export PATH=""${{ steps.codechecker.outputs.CODECHECKER_PATH }}:$PATH""\nCodeChecker analyzers --details\nCodeChecker checkers \\\n  --analyzer clangsa clang-tidy cppcheck \\\n  --output rows \\\n    > checker_list_normal.txt\nCodeChecker checkers \\\n  --analyzer clangsa clang-tidy cppcheck \\\n  --warnings \\\n  --output rows \\\n    > checker_list_diagnostics.txt\n', 'set +e # Do not hard exit on an erroring call!\n.github/workflows/config_label_check.py \\\n  ""checker_list_diagnostics.txt"" \\\n  ""config/labels/analyzers/clangsa.json"" \\\n  ""config/labels/analyzers/clang-tidy.json"" \\\n  ""config/labels/analyzers/cppcheck.json"" \\\n  --existing-filter ""clang-diagnostic-"" \\\n  --new-filter ""clang-diagnostic-""\nEXIT_STATUS=$?\necho ""Coverage check returned: $EXIT_STATUS.""\n# Explicitly check if the bit for ""8"" is set in the result,\n# indicating new checkers without severity set.\nif [[ $(($EXIT_STATUS & 8)) -eq 8 ]]\nthen\n  echo ""::warning title=New unconfigured diagnostics::The checker label config files lack some new diagnostic report (\\""warning\\"") kinds.""\n  exit 0\nelif [[ $EXIT_STATUS -eq 1 || $EXIT_STATUS -eq 2 ]]\nthen\n  # Script execution error.\n  exit $EXIT_STATUS\nelse\n  # We do not wish to fail if only removed checkers are reported.\n  exit 0\nfi\n', 'set +e # Do not hard exit on an erroring call!\n.github/workflows/config_label_check.py \\\n  ""checker_list_normal.txt"" \\\n  ""config/labels/analyzers/clangsa.json"" \\\n  ""config/labels/analyzers/clang-tidy.json"" \\\n  ""config/labels/analyzers/cppcheck.json"" \\\n  --existing-ignore ""clang-diagnostic-"" \\\n  --new-ignore \\\n    ""clang-diagnostic-"" \\\n    ""alpha."" \\\n    ""apiModeling."" \\\n    ""debug."" \\\n    ""optin.osx."" \\\n    ""osx."" \\\n    ""darwin-"" \\\n    ""objc-""\nEXIT_STATUS=$?\necho ""Coverage check returned: $EXIT_STATUS.""\n# Explicitly check if the bit for ""8"" is set in the result,\n# indicating new checkers without severity set.\nif [[ $(($EXIT_STATUS & 8)) -eq 8 ]]\nthen\n  exit 8\nelif [[ $EXIT_STATUS -eq 1 || $EXIT_STATUS -eq 2 ]]\nthen\n  # Script execution error.\n  exit $EXIT_STATUS\nelse\n  # We do not wish to fail if only removed checkers are reported.\n  exit 0\nfi\n', 'echo ::set-output name=VERSION::${GITHUB_REF#refs/tags/v}', 'sudo apt-get update -q\nsudo apt-get install g++ gcc-multilib\n', ""echo ::set-output name=VERSION::$(echo ${GITHUB_REF#refs/tags/v} | sed 's/-//')\n"", 'sed -i ""s/version=\\"".*\\""/version=\\""${{ steps.get_version.outputs.VERSION }}\\""/"" setup.py\n', 'make dist\n', 'sudo apt-get update && sudo apt-get install g++ clang clang-tidy cppcheck', 'brew install llvm cppcheck', 'choco install llvm;\nchoco install --ignore-package-exit-codes cppcheck;\necho ""C:\\Program Files\\LLVM\\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\n', 'pip install wheel nose\npip install dist/codechecker-*.tar.gz\n', 'nosetests tests/functional/binary_package/\n', 'pip install $(grep -iE ""pylint|pycodestyle"" analyzer/requirements_py/dev/requirements.txt)\n', 'make pylint pycodestyle', 'sudo apt-get update -q\nsudo apt-get install gcc-multilib\n', 'pip install -r requirements_py/dev/requirements.txt\nmake all\nmake test\n', 'pip install -r requirements_py/dev/requirements.txt\nmake test\n', 'pip install -r requirements_py/dev/requirements.txt\nmake test\n', 'pip install -r requirements_py/dev/requirements.txt\nmake package\nmake test\n', 'pip install -r requirements_py/dev/requirements.txt\nmake test\n', 'pip install -r requirements_py/dev/requirements.txt\nmake test\n', 'sh .github/workflows/install-deps.sh', 'make pip_dev_deps\nBUILD_UI_DIST=NO make package\n', 'make test_unit test_functional', 'make test_unit_cov', 'pip install -r requirements_py/dev/requirements.txt\n', 'make mypy', 'sh .github/workflows/install-deps.sh', ""echo '*:*:*:*:postgres' > $HOME/.pgpass\nchmod 0600 $HOME/.pgpass\n"", 'export PGPASSFILE=$HOME/.pgpass\n\nmake pip_dev_deps\npip3 install -r web/requirements_py/auth/requirements.txt\nBUILD_UI_DIST=NO make package\n\nmake -C web test_matrix_${{ matrix.database }}\n', 'make test_unit_cov', 'sh .github/workflows/install-deps.sh', 'make pip_dev_deps\nmake package\n', 'export PATH=""${{ github.workspace }}/build/CodeChecker/bin:$PATH""\n\nnpm run test:lint\nnpm run test:unit\nnpm run test:e2e.${{ matrix.browser }}\n']"
"['python -m pip install --upgrade pip\npip install poetry\npoetry add ""django==${{ matrix.django }}""\npoetry install -E testing\n', 'poetry run pytest --cov=./django_q --cov-report=xml\n', 'poetry run sphinx-build -b html -d docs/_build/doctrees  -nW docs docs/_build/html\n']"
"['python -m venv ~/env\nsource ~/env/bin/activate\npython -m pip install --upgrade pip\npython -m pip install --upgrade ""setuptools<=v65.7.0"" # https://github.com/Netflix/lemur/issues/4297\npip install mypy\npip install bandit\nsudo apt-get update\nsudo apt-get install libsasl2-dev libldap2-dev xvfb\n', 'python -m venv ~/env\nsource ~/env/bin/activate\nmake test\n', 'python -m venv ~/env\nsource ~/env/bin/activate\nmypy --install-types --non-interactive .\n', 'python -m venv ~/env\nsource ~/env/bin/activate\nbandit -r . -ll -ii -x lemur/tests/,docs\nxvfb-run make test-js\n', 'sudo apt-get update && sudo apt-get install libldap2-dev libsasl2-dev', '# from refs/tags/v0.8.1 get 0.8.1\nVERSION=$(echo $GITHUB_REF | sed \'s#.*/v##\')\nPLACEHOLDER=\'^__version__ =.*\'\nVERSION_FILE=\'lemur/__about__.py\'\n# in case placeholder is missing, exists with code 1 and github actions aborts the build\ngrep ""$PLACEHOLDER"" ""$VERSION_FILE""\nsed -i ""s/$PLACEHOLDER/__version__ = \\""${VERSION}\\""/g"" ""$VERSION_FILE""\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install poetry', 'poetry install', 'make test', './inspect_fixtures.sh', 'pip install poetry', 'poetry install', 'make pypi', 'bin/wait_for_pip.sh $(poetry run ./current_version.py)', 'make push']"
"['python -m pip install --upgrade pip \npip install pyqt5==5.15.7 lxml pytest pytest-faulthandler markdown language_tool_python symspellpy pyspellchecker pyenchant\npip install pyinstaller\n', 'pyinstaller ./manuskript.spec\n', 'powershell Remove-Item ./dist/manuskript/PyQt5/Qt5/bin/Qt5Bluetooth.dll;\npowershell Remove-Item ./dist/manuskript/ucrtbase.dll;\npowershell Remove-Item ./dist/manuskript/api-ms-win-*;\n', 'python -m pip install --upgrade pip\npip install pyqt5==5.15.7 lxml pytest pytest-faulthandler\nsudo apt-get -qq update\nsudo apt-get -qq install python3-pip python3-dev build-essential qt5-default libxml2-dev libxslt1-dev mesa-utils libgl1-mesa-glx libgl1-mesa-dev libxcb-xinerama0-dev\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', ""xvfb-run -s '-screen 0 640x480x24 +extension GLX' pytest -vs""]"
"['python -m pip install --upgrade pip\npip install --user cookiecutter\ncookiecutter gh:agconti/cookiecutter-django-rest --no-input\n', 'docker-compose up -d', 'docker-compose run --rm web bash -c ""flake8 . && python wait_for_postgres.py && python -Wall ./manage.py test""']"
"['pip install -r requirements.txt\npip install -r requirements.dev.txt -c constraints.txt\n', 'make lint', 'make mypy', 'make fmt\nif [ $(git diff --name-only --diff-filter=ACMR | wc -l ) != 0 ]; then\n  echo ""Reformatting failed! Please run make fmt on your commits and resubmit!"" 1>&2;\n  git diff;\n  exit 1;\nfi\n', '# Setup the Postgres repositories\nsudo sh -c \'echo ""deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main 14"" > /etc/apt/sources.list.d/pgdg.list\'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\n# Setup build deps\nsudo apt-get install -y libsnappy-dev postgresql-10 postgresql-11 postgresql-12 postgresql-13 postgresql-14 postgresql-15\n# Setup common python dependencies\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements.dev.txt -c constraints.txt\npip install -e .\n', 'make coverage', 'pip install sphinx==4.5.0 sphinx-rtd-theme==1.0.0', 'sphinx-build -b html main/docs gh-pages -E -d $GITHUB_WORKSPACE/.doctree', 'cd gh-pages\ngit add -A\ngit config --global user.email ""$(git show --format=%ae -s)""\ngit config --global user.name ""$(git show --format=%an -s)""\ngit commit -m ""From $GITHUB_REF $(echo ${GITHUB_SHA} | cut -c 1-8)""\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}\ngit push\n', 'git push origin main:master']"
"['# Django 4.1 is the last one to support Postgres 11\npip install Django==4.1.*\n', '# always use latest Django b/c the point here is to test PostgreSQL compatibility\npip install Django\n', 'pip install psycopg2 coverage\npython setup.py develop\n', '# maps the container port to localhost\ndocker run --name db -p 5432:5432 -d -e POSTGRES_PASSWORD=testing postgres:${{ matrix.postgres-version }}\n\nsleep 10 # wait for server to initialize\nPGPASSWORD=""testing"" psql -c \'create database dts_test_project;\' -U postgres -h localhost\n', 'export DATABASE_PASSWORD=""testing""\n./run_tests.sh\n', 'pip install psycopg2 coverage\npip install Django${{ matrix.django-version }}\npython setup.py develop\n', '# maps the container port to localhost\ndocker run --name db -p 5432:5432 -d -e POSTGRES_PASSWORD=testing postgres:${{ matrix.postgres-version }}\n\nsleep 10 # wait for server to initialize\nPGPASSWORD=""testing"" psql -c \'create database dts_test_project;\' -U postgres -h localhost\n', 'export DATABASE_PASSWORD=""testing""\n./run_tests.sh\n', 'mv dts_test_project/.coverage* .\ncoverage report -m\n']"
""
"['cat $GITHUB_EVENT_PATH', 'echo ""BASE_REPO_URL: ${BASE_REPO_URL}""\necho ""BASE_REPO_OWNER: ${BASE_REPO_OWNER}""\n# Add the \'base\' repo as a new remote\ngit remote add ${BASE_REPO_OWNER} ${BASE_REPO_URL}\n# And then fetch its references\ngit fetch ${BASE_REPO_OWNER}\n', 'python -m pip install --upgrade pip\npip install wheel flake8\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; elif [ -f requirements.txt ]; then pip install -r requirements.txt ; fi\n', 'echo ""BASE_REPO_URL: ${BASE_REPO_URL}""\necho ""BASE_REPO_OWNER: ${BASE_REPO_OWNER}""\necho ""BASE_REF: ${BASE_REF}""\n# Explicitly check for some errors\n# E9 - Runtime (syntax and the like)\n# F63 - \'tests\' checking\n# F7 - syntax errors\n# F82 - undefined checking\ngit diff --name-only --diff-filter=d -z ""refs/remotes/${BASE_REPO_OWNER}/${BASE_REF}"" -- | \\\n  grep -E -z -Z \'\\.py$\' | \\\n  xargs -0 flake8 --count --select=E9,F63,F7,F82 --show-source --statistics\n# Can optionally add `--exit-zero` to the flake8 arguments so that\n# this doesn\'t fail the build.\n# explicitly ignore docstring errors (start with D)\ngit diff --name-only --diff-filter=d -z ""refs/remotes/${BASE_REPO_OWNER}/${BASE_REF}"" -- | \\\n  grep -E -z -Z \'\\.py$\' | \\\n  xargs -0 flake8 --count --statistics --extend-ignore D\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install wheel flake8 pytest\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'DATA=$(jq --raw-output .before $GITHUB_EVENT_PATH)\n\necho ""DATA: ${DATA}""\n#######################################################################\n# stop the build if there are Python syntax errors or undefined names, ignore existing \n#######################################################################\n# We need to get just the *filenames* of only *python* files changed.\n# Using various -z/-Z/-0 to utilise NUL-terminated strings.\ngit diff --name-only --diff-filter=d -z ""$DATA"" | \\\n  grep -E -z -Z \'\\.py$\' | \\\n  xargs -0 flake8 --count --select=E9,F63,F7,F82 --show-source --statistics\n#######################################################################\n  \n#######################################################################\n# \'Full\' run, but ignoring docstring errors\n#######################################################################\n# explicitly ignore docstring errors (start with D)\n# Can optionally add `--exit-zero` to the flake8 arguments so that\ngit diff --name-only --diff-filter=d -z ""$DATA"" | \\\n  grep -E -z -Z \'\\.py$\' | \\\n  xargs -0 flake8 --count --statistics --extend-ignore D\n#######################################################################\n', './scripts/mypy-all.sh --platform win32\n', 'git config user.name github-actions\ngit config user.email github-actions@github.com\ngit submodule update --remote\n', 'changes=$(git status --porcelain)\ngit status --porcelain\nif [ -n ""${changes}"" ];\nthen\n  echo \'changes=true\' >> $GITHUB_OUTPUT\n  echo ""changes_text=${changes}"" >> $GITHUB_OUTPUT\nelse\n  echo \'changes=false\' >> $GITHUB_OUTPUT\nfi\nexit 0\n', 'git checkout -b ""submodule-change/$GITHUB_RUN_ID"" $CHECKOUT_BRANCH\ngit commit -am ""updating submodules""\ngit push --set-upstream origin ""submodule-change/$GITHUB_RUN_ID""\n', ""# For 'tar' we can only specify filename/glob exclusions, not any\n# directory location\ntar -c -v -z \\\n    -f ../EDMarketConnector-release-${{ needs.variables.outputs.sem_ver }}.tar.gz \\\n    -C .. \\\n    --exclude=EDMarketConnector-release-*.* \\\n    --exclude=.editorconfig \\\n    --exclude=.flake8 \\\n    --exclude=.git* \\\n    --exclude=.mypy.ini \\\n    --exclude=.pre-commit-config.yaml \\\n    --exclude=Build-exe-and-msi.py \\\n    --exclude=*.manifest \\\n    --exclude=coriolis-data \\\n    --exclude=img \\\n    --exclude=pyproject.toml \\\n    --exclude=scripts \\\n    --exclude=tests \\\n    --exclude=wix \\\n    EDMarketConnector\n  mv ../EDMarketConnector-release-${{ needs.variables.outputs.sem_ver }}.tar.gz .\n"", 'pip install wheel\npip install -r requirements-dev.txt\n', ""Invoke-Webrequest -UseBasicParsing https://github.com/vslavik/winsparkle/releases/download/v0.7.0/WinSparkle-0.7.0.zip -OutFile out.zip\nExpand-Archive out.zip\nMove-Item 'out\\WinSparkle-0.7.0\\Release\\*' '.\\'\n"", 'python Build-exe-and-msi.py\n', 'sha256sum EDMarketConnector_win_*.msi EDMarketConnector-release-*.{zip,tar.gz} > ./hashes.sum']"
"['python -m pip install --upgrade pip setuptools\npip install --upgrade tox tox-gh-actions\n', '.github/workflows/install_h2spec.sh\n', 'tox --parallel auto --notest\n', 'tox --parallel 0\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r test/requirements.txt\npython setup.py install\n', 'flake8 setup.py dropbox example test\n', 'pytest test/unit/test_dropbox_unit.py\n', 'python -m pip install --upgrade pip\npip install twine sphinx\npip install -r requirements.txt\npip install -r test/requirements.txt\npython setup.py install\n', 'sphinx-build -b html docs build/html\n', 'python setup.py sdist bdist_wheel\ntwine check dist/*\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r test/requirements.txt\npython setup.py install\n', 'pytest test/integration/test_dropbox.py', 'python -m pip install --upgrade pip\npip install coverage pytest\npip install -r requirements.txt\npip install -r test/requirements.txt\npython setup.py install\n', 'coverage run --rcfile=.coveragerc -m pytest test/unit/test_dropbox_unit.py\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install coverage pytest\npip install -r requirements.txt\npip install -r test/requirements.txt\npython setup.py install\n', 'coverage run --rcfile=.coveragerc -m pytest test/integration/test_dropbox.py\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py bdist_wheel\n', 'python setup.py sdist', 'twine check dist/*\ntwine upload dist/*\n', 'git submodule init\ngit submodule update --remote --recursive\n', 'echo ""::set-output name=branch::spec_update_${{ steps.current-time.outputs.formattedTime }}""\n', 'diffs=$(git diff --submodule spec | grep "">"" | wc -l)\necho ""Number of Spec diffs: $diffs""\necho ""::set-output name=num-diff::$diffs""\n', 'cd spec\ngitdiff=$(git log -n ${{ steps.git-diff-num.outputs.num-diff }} --pretty=""format:%n %H %n%n %b"")\ncommit=""Automated Spec Update $gitdiff""\ncommit=""${commit//\'%\'/\'%25\'}""\ncommit=""${commit//$\'\\n\'/\'%0A\'}""\ncommit=""${commit//$\'\\r\'/\'%0D\'}""\necho ""Commit Message: $commit""\necho ""::set-output name=commit::$commit""\ncd ..\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython generate_base_client.py\n']"
"['python -m pip install --upgrade pip wheel\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest --verbose --cov-config=.coveragerc --cov=pottery --cov=tests', 'mypy', 'flake8 *\\.py pottery/*\\.py tests/*\\.py --count --max-complexity=10 --statistics\nisort *\\.py pottery/*\\.py tests/*\\.py --check-only --diff\n', 'bandit --recursive pottery\nsafety check\n']"
"['sudo apt-get install --no-install-recommends pandoc\n', 'python --version\n', 'python -m pip install -r doc/requirements.txt\n', 'python -m sphinx -W --keep-going --color -d _build/doctrees/ doc/ _build/html/ -b html\n', 'python -m sphinx -W --keep-going --color -d _build/doctrees/ doc/ _build/linkcheck/ -b linkcheck\n', 'sudo apt-get install --no-install-recommends libportaudio2\n', 'python --version\n', 'python -m pip install .\n', 'python -m sounddevice\npython -c ""import sounddevice as sd; print(sd._libname)""\npython -c ""import sounddevice as sd; print(sd.get_portaudio_version())""\n', 'python --version\n', 'python -m pip install .\n', 'python -m sounddevice\npython -c ""import sounddevice as sd; print(sd._libname)""\npython -c ""import sounddevice as sd; print(sd.get_portaudio_version())""\n']"
"['gh pr merge --auto --squash ""$PR_URL""', 'make lint\n', 'echo ""::set-output name=dir::$(pip cache dir)""    # - name: Cache\n', 'pytest --cov=janus --cov=tests --cov-report=term --cov-report=xml:coverage.xml\n', 'python -m pip install twine wheel build\n', 'python -m build\n', 'twine upload dist/*\n']"
"['pip install -r requirements/maintainer.pip', 'pytest', 'python setup.py sdist bdist_wheel', 'cd test-infra\nterraform fmt -check -diff\n', 'cd test-infra\nterraform init\nterraform validate\n']"
"['python -m pip install --upgrade pip setuptools\npip install certifi urllib3 mock pytest\n', 'export PATH=${HOME}/.local/bin:${PATH}\nmake check\n', 'echo ""/Users/runner/Library/Python/3.11/bin"" >> $GITHUB_PATH\necho ""$HOME/.local/bin"" >> $GITHUB_PATH\necho ""/Users/runner/.local/bin"" >> $GITHUB_PATH\n', 'python setup.py install\npytest\n', 'wget --quiet -O /tmp/minio https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x /tmp/minio\nmkdir -p /tmp/minio-config/certs/\ncp tests/certs/* /tmp/minio-config/certs/\n/tmp/minio -C /tmp/minio-config server /tmp/fs{1...4} &\nSSL_CERT_FILE=/tmp/minio-config/certs/public.crt python tests/functional/tests.py\n', 'New-Item -ItemType Directory -Path ""$env:temp/minio-config/certs/""\nCopy-Item -Path tests\\certs\\* -Destination ""$env:temp/minio-config/certs/""\nInvoke-WebRequest -Uri https://dl.minio.io/server/minio/release/windows-amd64/minio.exe -OutFile $HOME/minio.exe\nStart-Process -NoNewWindow -FilePath ""$HOME/minio.exe"" -ArgumentList ""-C"", ""$env:temp/minio-config"", ""server"", ""$env:temp/fs{1...4}""\n$env:SSL_CERT_FILE = ""$env:temp/minio-config/certs/public.crt""\npython tests/functional/tests.py\n']"
"['python -m pip install --upgrade pip tox\n', 'tox\n']"
"['python -W ignore -m pip install --upgrade pip\npython -W ignore -m pip install -r requirements-all.txt\n', 'sphinx-build docs/source docs/build/html -W --keep-going -j auto -b linkcheck', 'python -W ignore -m pip install --upgrade pip\npython -W ignore -m pip install -r requirements-all.txt\n', 'pytest -v --tb=short tests/docs/admonition_inserter.py', 'sphinx-build docs/source docs/build/html -W --keep-going -j auto', 'python -W ignore -m pip install --upgrade pip\npython -W ignore -m pip install -U pytest-cov\npython -W ignore -m pip install -r requirements.txt\npython -W ignore -m pip install -r requirements-dev.txt\npython -W ignore -m pip install pytest-xdist[psutil]\n', '# We test without optional dependencies first. This includes:\n# - without pytz\n# - without jobqueue\n# - without ratelimiter\n# - without webhooks\n# - without arbitrary callback data\n# - without socks support\n# - without http2 support\nTO_TEST=""test_no_passport.py or test_datetime.py or test_defaults.py or test_jobqueue.py or test_applicationbuilder.py or test_ratelimiter.py or test_updater.py or test_callbackdatacache.py or test_request.py""\npytest -v --cov -k ""${TO_TEST}""\n# Rerun only failed tests (--lf), and don\'t run any tests if none failed (--lfnf=none)\npytest -v --cov --cov-append -k ""${TO_TEST}"" --lf --lfnf=none\n# No tests were selected, convert returned status code to 0\nopt_dep_status=$(( $? == 5 ? 0 : $? ))\n\n# Test the rest\nexport TEST_WITH_OPT_DEPS=\'true\'\npip install -r requirements-opts.txt\n# `-n auto --dist loadfile` uses pytest-xdist to run each test file on a different CPU\n# worker. Increasing number of workers has little effect on test duration, but it seems\n# to increase flakyness, specially on python 3.7 with --dist=loadgroup.\npytest -v --cov --cov-append -n auto --dist loadfile\npytest -v --cov --cov-append -n auto --dist loadfile --lf --lfnf=none\nmain_status=$(( $? == 5 ? 0 : $? ))\n# exit with non-zero status if any of the two pytest runs failed\nexit $(( ${opt_dep_status} || ${main_status} ))\n', 'python -W ignore -m pip install --upgrade pip\npython -W ignore -m pip install -r requirements.txt\npython -W ignore -m pip install -r requirements-opts.txt\npython -W ignore -m pip install -r requirements-dev.txt\n', 'pytest -v tests/test_official.py\nexit $?\n', 'git fetch --depth=1', 'python -W ignore -m pip install pyright~=1.1.291\n', 'pip install . -U\npyright --verifytypes telegram --ignoreexternal --outputjson > pr.json || true\npyright --verifytypes telegram --ignoreexternal > pr.readable || true\n', 'git checkout ${{ github.base_ref }}\npip install . -U\npyright --verifytypes telegram --ignoreexternal --outputjson > base.json || true\n']"
"['python -c ""import sys; print(sys.version)""\n', 'python -c ""import sys; print(sys.maxsize);""\npython -c ""import platform; print(platform.uname());""\npython -c ""import platform; print(platform.platform());""\npython -c ""import platform; print(platform.architecture());""\npython -c ""import platform; print(platform.processor());""\npython -c ""import platform; print(platform.python_compiler());""\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'echo ""::set-output name=date::$(/bin/date -u ""+%Y%m%d"")""\n', 'pip install -r requirements.txt\n', 'python setup.py sdist\npython setup.py bdist_wheel\n', 'python -c ""import sys; print(sys.version)""\n', 'python -c ""import sys; print(sys.maxsize);""\npython -c ""import platform; print(platform.uname());""\npython -c ""import platform; print(platform.platform());""\npython -c ""import platform; print(platform.architecture());""\npython -c ""import platform; print(platform.processor());""\npython -c ""import platform; print(platform.python_compiler());""\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'echo ""::set-output name=date::$(/bin/date -u ""+%Y%m%d"")""\n', 'pip install -r requirements.txt\n', 'pip install --upgrade -r test/requirements.txt\n', 'pip install coverage pytest-cov flake8\n', 'pip install .\n', 'flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics --exclude="".svn,CVS,.bzr,.hg,.git,__pycache__,poly_point_isect.py""\n', 'python -m pytest --verbose --xdoctest-modules -s --durations=50 -Walways\n', 'coverage run --source imgaug -m pytest --verbose -Walways\ncoverage xml\ncoverage report\n', 'python -c ""import sys; print(sys.version)""\n', 'python -c ""import sys; print(sys.maxsize);""\npython -c ""import platform; print(platform.uname());""\npython -c ""import platform; print(platform.platform());""\npython -c ""import platform; print(platform.architecture());""\npython -c ""import platform; print(platform.processor());""\npython -c ""import platform; print(platform.python_compiler());""\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'echo ""::set-output name=date::$(/bin/date -u ""+%Y%m%d"")""\n', 'pip install -r requirements.txt\n', 'pip install --upgrade -r test/requirements.txt\n', 'pip install coverage pytest-cov flake8\n', 'pip install .\n', 'flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics --exclude="".svn,CVS,.bzr,.hg,.git,__pycache__,poly_point_isect.py""\n', 'python -m pytest --verbose --xdoctest-modules -s --durations=50 -Walways\n', 'coverage run --source imgaug -m pytest --verbose -Walways\ncoverage xml\ncoverage report\n']"
""
""
"['python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python setup.py sdist bdist_wheel', 'python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python .github/workflows/run_tests.py combine', 'python -m pip install coveralls', 'python -m coveralls --service=github', 'sudo apt-get update -y\nsudo apt-get install -y wget ca-certificates gnupg debian-archive-keyring apt-transport-https\nsudo sh -c \'echo ""deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main"" > /etc/apt/sources.list.d/pgdg.list\'\nsudo sh -c \'wget -qO - https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor > /etc/apt/trusted.gpg.d/apt.postgresql.org.gpg\'\nsudo sh -c \'echo ""deb [signed-by=/etc/apt/trusted.gpg.d/citusdata_community.gpg] https://packagecloud.io/citusdata/community/ubuntu/ $(lsb_release -cs) main"" > /etc/apt/sources.list.d/citusdata_community.list\'\nsudo sh -c \'wget -qO - https://packagecloud.io/citusdata/community/gpgkey | gpg --dearmor > /etc/apt/trusted.gpg.d/citusdata_community.gpg\'\n', 'python .github/workflows/install_deps.py', 'python .github/workflows/run_tests.py', 'python -m coverage xml -o cobertura.xml', 'bash <(curl -Ls https://coverage.codacy.com/get.sh) report -r cobertura.xml -l Python --partial', 'python -m pip install coveralls', 'python -m coveralls --service=github --finish', 'bash <(curl -Ls https://coverage.codacy.com/get.sh) final', 'python -m pip install -r requirements.txt psycopg2-binary psycopg']"
"['docker create -t --name yadm-website --entrypoint test/validate yadm/jekyll:2019-10-17; docker cp ./ yadm-website:/srv/jekyll', 'docker start yadm-website -a', 'make test']"
"['python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox -eflake8', 'tox -edocs', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox']"
""
""
[]
"['sudo apt-get update', 'tests/ci-prepare-${{ matrix.deps }}.sh ${{ matrix.os }}', 'python3 setup.py build_cython', 'python3 setup.py build_ext --inplace', 'python3 -m pytest -r s tests/', './build_docs.sh', 'sudo python3 -m pip install ""black == 22.3.0""', 'black --line-length=100 --skip-string-normalization --check --diff --color --required-version 22.3.0 .']"
"['python -c ""import sys; print(sys.version)""', 'make init\n', 'make test\n', 'coveralls --service=github\n', 'pip3 install --upgrade coveralls\n', 'coveralls --finish\n', 'python -c ""import sys; print(sys.version)""', 'make init\nmake init_docs\n', 'make docs']"
"['$env:PATH = ""C:\\Program Files\\Git\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\ProgramData\\Chocolatey\\bin""\n[Environment]::SetEnvironmentVariable(""Path"", $env:PATH, ""Machine"")\nchoco install gnupg -y --no-progress\necho ""C:\\Program Files (x86)\\gnupg\\bin"" >> $env:GITHUB_PATH\ngit config --system gpg.program ""C:\\Program Files (x86)\\gnupg\\bin\\gpg.exe""\n', ""pip install -U 'setuptools>=61'"", 'pip install ""$(echo -n dist/*whl)[toml,test]""', '$(hg debuginstall --template ""{pythonexe}"") -m pip install hg-git --user\n', 'hg version', 'pytest']"
"['python -m pip install .[test]\npython -m pip install pytest-github-actions-annotate-failures\n', 'python -m pytest -v tests', 'python -m pip install .[mypy]', 'python -m mypy --ignore-missing-imports semantic_release', 'python -m pip install black', 'python -m black .', 'python -m pip install ""isort >=4,<5""', 'python -m isort -y -rc .', 'new_sha=$(git rev-parse HEAD)\necho ""SHA=$new_sha"" >> $GITHUB_OUTPUT\n', 'git fetch --prune origin +refs/heads/master:refs/remotes/origin/master', 'python -m pip install .[test]\npython -m pip install pytest-github-actions-annotate-failures\n', 'python -m pytest -v tests', 'python -m pip install .[mypy]', 'python -m mypy --ignore-missing-imports semantic_release']"
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox']"
"[""python -m pip install --upgrade pip\npip install poetry\ncd daemon\ncp core/constants.py.in core/constants.py\nsed -i 's/required=True/required=False/g' core/emulator/coreemu.py\npoetry install\n"", 'cd daemon\npoetry run isort -c -df\n', 'cd daemon\npoetry run black --check .\n', 'cd daemon\npoetry run flake8\n', 'cd daemon/proto\npoetry run python -m grpc_tools.protoc -I . --python_out=.. --grpc_python_out=.. core/api/grpc/*.proto\n', 'cd daemon\npoetry run pytest --mock tests\n']"
"['sudo apt-get update\nsudo apt-get install hunspell\n', 'cd scripts; make -k', 'cd ..']"
"['python -m pip install --upgrade pip setuptools wheel\npip install -U -r requirements.txt\n', 'python ./setup.py sdist bdist_wheel\n', 'pip install -e .[docs,speed,voice]\n', 'cd docs\nsphinx-build -b html -D language=${DOCS_LANGUAGE} -j auto -a -n -T -W --keep-going . _build/html\n', 'echo ""available=true"" >> $GITHUB_OUTPUT\n', 'wget -qO - https://artifacts.crowdin.com/repo/GPG-KEY-crowdin | sudo apt-key add -\necho ""deb https://artifacts.crowdin.com/repo/deb/ /"" | sudo tee -a /etc/apt/sources.list.d/crowdin.list\nsudo apt-get update -qq\nsudo apt-get install -y crowdin3\n', 'cd docs\ncrowdin download --all\n', 'wget -qO - https://artifacts.crowdin.com/repo/GPG-KEY-crowdin | sudo apt-key add -\necho ""deb https://artifacts.crowdin.com/repo/deb/ /"" | sudo tee -a /etc/apt/sources.list.d/crowdin.list\nsudo apt-get update -qq\nsudo apt-get install -y crowdin3\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -e .[docs,speed,voice]\n', 'cd docs\nmake gettext\n', 'cd docs\ncrowdin upload\n', 'python -m pip install --upgrade pip setuptools wheel black==22.6 requests\npip install -U -r requirements.txt\n', 'black --check discord examples\n', 'python -m pip install -e .[test]\n', 'PYTHONPATH=""$(pwd)"" pytest -vs --cov=discord --cov-report term-missing:skip-covered\n']"
"['pip install --user --upgrade pip wheel', 'pip install -e .[lint]', 'make lint', 'echo ""PIP_CACHE_DIR=$(pip cache dir)"" >> $GITHUB_ENV', 'python -m pip install --user --upgrade pip wheel', 'pip install -e .[test]', 'make test-unit-coverage', 'pip install -e .[test]', 'coverage run --source=lbry -m unittest tests/unit/test_conf.py', 'pip install coveralls\ncoveralls --service=github\n', 'sudo swapoff -a\nsudo sysctl -w vm.swappiness=1\nsudo sysctl -w fs.file-max=262144\nsudo sysctl -w vm.max_map_count=262144\n', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends ffmpeg\n', 'pip install tox coverage coveralls', 'rm -rf .tox', 'tox -e ${{ matrix.test }}', 'coverage combine tests\ncoveralls --service=github\n', 'pip install coveralls\ncoveralls --service=github --finish\n', 'echo ""PIP_CACHE_DIR=$(pip cache dir)"" >> $GITHUB_ENV', 'pip install pyinstaller==4.6', 'pip install -e .', 'python docker/set_build.py', 'pyinstaller --onefile --name lbrynet lbry/extras/cli.py\ndist/lbrynet --version\n', 'pip install pywin32==301\npyinstaller --additional-hooks-dir=scripts/. --icon=icons/lbry256.ico --onefile --name lbrynet lbry/extras/cli.py\ndist/lbrynet.exe --version\n', 'pip install githubrelease\nchmod +x lbrynet-macos/lbrynet\nchmod +x lbrynet-linux/lbrynet\nzip --junk-paths lbrynet-mac.zip lbrynet-macos/lbrynet\nzip --junk-paths lbrynet-linux.zip lbrynet-linux/lbrynet\nzip --junk-paths lbrynet-windows.zip lbrynet-windows/lbrynet.exe\nls -lh\ngithubrelease release lbryio/lbry-sdk info ${GITHUB_REF#refs/tags/}\ngithubrelease asset lbryio/lbry-sdk upload ${GITHUB_REF#refs/tags/} \\\n  lbrynet-mac.zip lbrynet-linux.zip lbrynet-windows.zip\ngithubrelease release lbryio/lbry-sdk publish ${GITHUB_REF#refs/tags/}\n']"
['pip install shiv\npython build_collector.py\n']
"['make docker-image', 'docker run --rm -v $(pwd):/mnt:rw dumb-init-build /mnt/ci/docker-python-test', 'docker run --init --rm -v $(pwd):/mnt:rw dumb-init-build make -C /mnt builddeb', 'docker run --rm -v $(pwd):/mnt:rw ${{ matrix.docker_image }} /mnt/ci/docker-deb-test', 'sudo make python-dists-${{ matrix.manylinux_arch }}']"
""
""
""
"['rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', ""git checkout -b pre-commit-autoupdate\npip install pre-commit\npre-commit --version\npre-commit autoupdate\ngit diff --exit-code && exit 0\ngit add -A\ngit config user.name 'github-actions[bot]'\ngit config user.email 'github-actions[bot]@users.noreply.github.com'\ngit commit -m '[CI/CD] pre-commit autoupdate'\ngit push -f origin pre-commit-autoupdate\ngh pr create -B dev -H pre-commit-autoupdate -f -l 'CI/CD'\n"", 'pip install pre-commit\npre-commit --version\npre-commit run --all-files --show-diff-on-failure\n', 'curl -sSfL ""$(curl -sSf \'https://api.github.com/repos/koalaman/shellcheck/releases/latest\' | mawk -F\\"" \'/""browser_download_url.*\\.linux\\.x86_64\\.tar\\.xz""/{print $4;exit}\')"" -o shellcheck.tar.xz\ntar --wildcards --strip-components=1 -xf shellcheck.tar.xz \'*/shellcheck\'\nrm shellcheck.tar.xz\n', 'mapfile -t FILES < <(find . -not \\( -path \'./.git\' -prune \\) -type f)  # read all files to array\nfor i in ""${!FILES[@]}""\ndo\n    [[ ${FILES[$i]##*/} =~ \'.\'[^.]*\'sh\'$ ]] && continue  # file has shell extension\n    [[ $(mawk \'NR==1 && $0 ~ /^#!.*sh([[:blank:]]|$)/{print;exit}\' ""${FILES[$i]}"") ]] && continue  # file has shell shebang\n    unset -v ""FILES[$i]""  # else remove from array\ndone\n./shellcheck -xC -o all -e SC2244,SC2250,SC2312 ""${FILES[@]}""\n', ""test '${{ matrix.dist }}' != 'ubuntu-22.04' || sudo apt-mark hold grub-efi-amd64-signed"", 'sudo apt-get -q update', 'sudo DEBIAN_FRONTEND=""noninteractive"" apt-get -qq --no-install-recommends dist-upgrade', 'sudo DEBIAN_FRONTEND=""noninteractive"" apt-get -qq --no-install-recommends install curl gcc ffmpeg libcurl4-openssl-dev libssl-dev motion v4l-utils', 'python3 -m pip install --upgrade pip setuptools wheel', 'python3 -m pip install --upgrade build mypy pytest safety', 'python3 -m build', 'python3 -m pip install .', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive --exclude build/ . || true', 'pytest --ignore=tests/test_utils/test_mjpeg.py --ignore=tests/test_utils/test_rtmp.py .', 'pytest --fixtures tests/test_utils/test_mjpeg.py || true', 'pytest --fixtures tests/test_utils/test_rtmp.py  || true', 'pytest . || pytest --doctest-modules . || true', 'safety check', 'sudo apt-get -q update', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -qq --no-install-recommends dist-upgrade', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -qq --no-install-recommends install ca-certificates curl gcc libcurl4-openssl-dev libssl-dev python3-dev', ""curl -sSfO 'https://bootstrap.pypa.io/get-pip.py'"", 'sudo python3 get-pip.py', 'sudo python3 -m pip install --upgrade pip setuptools wheel', 'REPO=$GITHUB_REPOSITORY BRANCH=$GITHUB_REF_NAME\n[ ${{ github.event_name }} = \'pull_request\' ] && REPO=${{ github.event.pull_request.head.repo.full_name }} BRANCH=${{ github.event.pull_request.head.ref }}\nsudo python3 -m pip install ""https://github.com/$REPO/archive/$BRANCH.tar.gz""\n', 'sudo motioneye_init --skip-apt-update', 'i=0; until ss -tln | grep 8765; do [ $i -le 10 ] || exit 0; sleep 1; i=$(expr $i + 1); done', 'sudo systemctl status motioneye', 'sudo systemctl is-active motioneye', 'sudo apt-mark hold grub-efi-amd64-signed', 'sudo apt-get -q update', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -qq --no-install-recommends dist-upgrade', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -qq --no-install-recommends install ca-certificates curl gcc libcurl4-openssl-dev libssl-dev python3-dev', ""curl -sSfO 'https://bootstrap.pypa.io/get-pip.py'"", 'sudo python3 get-pip.py', 'sudo python3 -m pip install --upgrade pip setuptools wheel', 'REPO=$GITHUB_REPOSITORY BRANCH=$GITHUB_REF_NAME\n[ ${{ github.event_name }} = \'pull_request\' ] && REPO=${{ github.event.pull_request.head.repo.full_name }} BRANCH=${{ github.event.pull_request.head.ref }}\nsudo python3 -m pip install ""https://github.com/$REPO/archive/$BRANCH.tar.gz""\n', 'sudo motioneye_init --skip-apt-update', 'i=0; until ss -tln | grep 8765; do [ $i -le 10 ] || exit 0; sleep 1; i=$(expr $i + 1); done', 'sudo systemctl status motioneye', 'sudo systemctl is-active motioneye', ""python3 -m pip install -U babel jinja2 &\nsudo apt-get -q update\nsudo DEBIAN_FRONTEND='noninteractive' apt-get -qq --no-install-recommends install gettext\n"", ""pybabel extract -F l10n/babel.cfg -o motioneye/locale/motioneye.pot motioneye/\n# Remove trailing empty line to satisfy pre-commit\nsed -i '${/^$/d}' motioneye/locale/motioneye.pot\n"", 'xgettext --no-wrap --from-code=UTF-8 -o motioneye/locale/motioneye.js.pot motioneye/static/js/*.js l10n/*.js', 'for i in motioneye/locale/*/LC_MESSAGES/motioneye.js.po\ndo\n    lang=${i#motioneye/locale/}\n    lang=${lang%/LC_MESSAGES/motioneye.js.po}\n    echo ""Generating motioneye.$lang.json""\n    l10n/po2json ""$i"" ""motioneye/static/js/motioneye.$lang.json""\ndone\n', 'git diff -I \'^""POT-Creation-Date: \' --exit-code && exit 0\ngit config user.name \'github-actions[bot]\'\ngit config user.email \'github-actions[bot]@users.noreply.github.com\'\ngit add -A\ngit commit -m \'Update translation files\'\ngit push\n']"
"['python -m pip install --upgrade pip\n', 'pip install .', './test/test.sh', 'python -m pip install --upgrade pip\npip install mypy\n', 'mypy --strict git_remote_dropbox']"
"['echo ""No build required""', 'pip install --user --upgrade pip', 'make clean install run-pre-commit', 'pip install --user --upgrade pip', 'python setup.py install\ndynaconf init -v FOO=running_on_ci -y\ndynaconf -i config.settings write toml -v OTHERVALUE=Hello_CI -y\ndynaconf -i config.settings list | grep -c running_on_ci\ndynaconf -i config.settings list | grep -c Hello_CI\ndynaconf --version\n', 'pip install --user --upgrade pip', 'make ciinstall', 'make citest', 'pip install --user --upgrade pip', 'make ciinstall', 'py.test -v --cov-config .coveragerc --cov=dynaconf -l tests/ --junitxml=junit/test-results.xml -m ""not integration""', 'pip install --user --upgrade pip', 'make ciinstall', 'make test_functional', 'pip install --user --upgrade pip', 'pip install --use-deprecated=legacy-resolver .[test]', 'py.test -v -l tests --junitxml=junit/test-results.xml -m ""not integration""', 'pip install --user --upgrade pip', 'pip install --use-deprecated=legacy-resolver .[test]', 'python tests_functional/runtests.py', 'pip install --user --upgrade pip', 'make ciinstall', 'make test_redis', 'pip install --user --upgrade pip', 'make ciinstall', 'make test_vault', 'echo ""All tests has passed""']"
""
"['pip3 install setuptools>=63.2.0', 'pip3 install wheel', 'python3 setup.py sdist', 'pip3 install dist/opencanary-*.tar.gz', 'pip3 install -r opencanary/test/requirements.txt', 'cp opencanary/test/opencanary.conf .', 'opencanaryd --start', 'pytest -s']"
"['if [[ -n $(git status --porcelain) ]]\nthen\n  echo ""::set-output name=status::true""\nfi\n', 'echo ""${{ github.workspace }}""\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -r requirements-tox.txt\n', 'python -m tox -e flake8', 'sudo apt update && sudo apt-get install libsnappy-dev\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -r requirements-tox.txt\npython -m pip install --upgrade tox-gh-actions\n', 'if [[ -z ""${{ matrix.tox-env }}"" ]]\nthen\n  python -m tox\nelse\n  python -m tox -e ${{ matrix.tox-env }}\nfi\n', 'echo Hello, world!']"
"['PGPASSWORD=postgres psql -c \'CREATE DATABASE eventsourcing;\' -U postgres -h localhost\nPGPASSWORD=postgres psql -c ""CREATE USER eventsourcing WITH PASSWORD \'eventsourcing\';"" -U postgres -h localhost\nPGPASSWORD=postgres psql eventsourcing -c ""CREATE SCHEMA myschema AUTHORIZATION eventsourcing"" -U postgres -h localhost\n', 'make install', 'make lint', 'make test', ""pip install -U pip\npip install wheel\npython setup.py sdist\npip install dist/*\nrm -r dist\nmv eventsourcing/tests eventsourcing_tests\nrm -r eventsourcing\nfind eventsourcing_tests -type f -exec sed -i -e 's/eventsourcing.tests/eventsourcing_tests/g' {} \\;\npython -m unittest -v eventsourcing_tests/domain_tests/test_aggregate.py\npython -m unittest -v eventsourcing_tests/application_tests/test_application_with_popo.py\npython -m unittest -v eventsourcing_tests/application_tests/test_processapplication.py\n""]"
"['python -m pip install --upgrade pip\npip install twine\n', 'twine upload --repository testpypi dist/*-manylinux*.whl']"
"['python -m pip install --upgrade pip setuptools wheel build\npip install .\n', 'python setup.py sdist bdist_wheel', 'curl -X POST -d {} ${{ secrets.NETLIFY_BUILD_HOOK }}', 'npm install', 'npm run build', 'pip install rich', 'python ${GITHUB_WORKSPACE}/.github/workflows/code_checks.py', 'python -m pip install --upgrade pip setuptools beautifulsoup4', 'pip install .', 'multiqc ${{ steps.single_module.outputs.value }} test_data/data --ignore test_data/data/modules/', 'multiqc ${{ steps.single_module.outputs.value }} --lint test_data/data/modules/ --filename full_report.html', 'multiqc ${{ steps.single_module.outputs.value }} --lint test_data/data/modules/ --fullnames --fn_as_s_name', ""multiqc ${{ steps.single_module.outputs.value }} test_data/data/modules/ --filename all_ignored.html --ignore-samples '*'\n[[ ! -f all_ignored.html ]]\n"", 'cd test_data\nmultiqc --file-list data/special_cases/dir_list.txt\n', 'multiqc --lint test_data/data/modules/ -m fastqc -f -d -dd 1 -i ""Forced Report"" -b ""This command has lots of options"" --filename custom_fn --no-data-dir', 'multiqc ${{ steps.single_module.outputs.value }} --lint test_data/data/modules/ -f --flat --tag methylation --exclude clusterflow --ignore-samples ngi --fullnames --zip-data-dir -c test/config_example.yaml', 'cd test_data\nmultiqc -m star -o tests/multiqc_report_dev -t default_dev -k json --file-list data/special_cases/file_list.txt\n', 'mkdir empty_dir\nmultiqc -n empty empty_dir ${{ steps.single_module.outputs.value }}\n[[ ! -f empty.html ]]\n', 'python test/print_missing_csp.py --report full_report.html --whitelist CSP.txt', 'cmd ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat"" ${{ matrix.platform-vcvars }}', 'python -m pip install --upgrade pip setuptools\n', 'pip install .', 'curl -fsSL https://github.com/ewels/MultiQC_TestData/archive/master.zip -o test_data.zip\n7z x test_data.zip -y -o""test_data""\ndir test_data\\MultiQC_TestData-master\n', 'multiqc ${{ steps.single_module.outputs.value }} test_data\\MultiQC_TestData-master\\data --ignore test_data\\MultiQC_TestData-master\\data\\modules\\', 'multiqc ${{ steps.single_module.outputs.value }} --lint test_data\\MultiQC_TestData-master\\data\\modules\\ --filename full_report.html', 'cd test_data\\MultiQC_TestData-master\nmultiqc ${{ steps.single_module.outputs.value }} --file-list data\\special_cases\\dir_list.txt\n', 'multiqc --lint test_data\\MultiQC_TestData-master\\data\\modules\\ -m fastqc -f -d -dd 1 -i ""Forced Report"" -b ""This command has lots of options"" --filename custom_fn --no-data-dir', 'multiqc ${{ steps.single_module.outputs.value }} --lint test_data\\MultiQC_TestData-master\\data\\modules\\ -f --flat --tag methylation --exclude clusterflow --ignore-samples ngi --fullnames --zip-data-dir -c test\\config_example.yaml', 'cd test_data\\MultiQC_TestData-master\nmultiqc -m star -o testsmultiqc_report_dev -t default_dev -k json --file-list data\\special_cases\\file_list.txt\n', 'pip install .', 'wget https://github.com/ewels/MultiQC_TestData/archive/refs/heads/master.zip\nunzip master.zip\nmv MultiQC_TestData-master/ test_data/\n']"
"['echo ""run: ${{ github.run_number }}""\necho ""job: ${{ github.job }}""\necho ""event_name: ${{ github.event_name }}""\necho ""event_path: ${{ github.event_path }}""\necho ""repository: ${{ github.repository }}""\necho ""ref: ${{ github.ref }}""\necho ""head_ref: ${{ github.head_ref }}""\necho ""base_ref: ${{ github.base_ref }}""\necho ""commit: ${{ github.sha }}""\n', 'pip install pipenv\npipenv install\n', 'pipenv run pyrevit check', 'pipenv run pyrevit set year\n', 'pipenv run pyrevit sign addcert\n', 'pipenv run pyrevit set build wip\n', 'pipenv run pyrevit set build release\n', 'pipenv run pyrevit set products\n', 'pipenv run pyrevit build products\n', 'pipenv run pyrevit sign products\n', 'pipenv run pyrevit build installers\n', 'pipenv run pyrevit sign installers\n', 'pipenv run pyrevit report releasenotes > release_notes.md\n', 'pipenv run pyrevit build commit\ngit push\ngit push --tags\n', 'choco apikey --key ${{ secrets.CHOCO_TOKEN}} --source https://push.chocolatey.org/\nchoco push dist/pyrevit-cli.${{ steps.installversion.outputs.content }}.nupkg -s https://push.chocolatey.org/\n', 'git checkout ${{ github.base_ref }}\ngit merge ${{ github.head_ref }}\ngit push\ngit checkout ${{ github.head_ref }}\n', 'pipenv run pyrevit notify wip https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n', 'pipenv run pyrevit notify release ${{ steps.publish_release.outputs.url }}\n', 'pipenv run pyrevit set next-version\ngit push\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'make unittests', 'pip install flake8 flake8-black flake8-isort\n', 'flake8 --count --statistics .']"
"['echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin\n', 'docker build . --tag $IMAGE_NAME', 'IMAGE_ID=""docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME""\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\nVERSION=$(cat VERSION)\n\ndocker tag $IMAGE_NAME $IMAGE_ID:latest\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\n\ndocker push $IMAGE_ID:latest\ndocker push $IMAGE_ID:$VERSION\n', 'mkdir -p artifacts\necho ""wfdt=$(date +\'%Y%m%d_%H%M%S\')"" >> $GITHUB_ENV\n', 'sudo apt update\nsudo apt install unzip zip\npython -m pip install --upgrade pip\npython -m pip install setuptools wheel\npython -m pip install flake8 pytest\ncd current\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f test-requirements.txt ]; then pip install -r test-requirements.txt; fi\n', 'cd current\nflake8 . --count --select=W291,W293,W391 --statistic\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=10 \\\n  --max-line-length=127 --statistics\n', 'cd current\npytest\n', 'cd current\ngit status\npython setup.py sdist bdist_wheel\npython3 -m pip -v install dist/capirca*py3*.whl\naclgen --output_directory ./output --logtostderr\ncd ./output/ && \\\n  zip -r ../../artifacts/capirca_output_${{ env.wfdt }}.zip .\ncd ..\npython3 -m pip -v uninstall -y capirca\n', 'cd master\ngit status\npython setup.py sdist bdist_wheel\npython3 -m pip -v install dist/capirca*py3*.whl\naclgen --output_directory ./output --logtostderr\npython3 -m pip -v uninstall -y capirca\n', 'mkdir -p artifacts-diff\nsudo npm install -g diff2html diff2html-cli\ndiff2html --version\ndiff -qr current/output master/output > \\\n  ./artifacts-diff/policy_output.diff | true\ncat ./artifacts-diff/policy_output.diff | grep Files | grep differ \\\n  | cut -d"" "" -f2 | cut -d ""/"" -f3 > ./artifacts-diff/files.list\nwhile read p; do diff -u master/output/$p current/output/$p | \\\n  diff2html -i stdin --file ./artifacts-diff/$p.html | \\\n  true; done < ./artifacts-diff/files.list\nsed -i \'/Diff to HTML by/d\' ./artifacts-diff/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install setuptools wheel\npython -m pip install flake8 pytest\npython -m pip install -r test-requirements.txt\npython -m pip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel\npython -m pip -v install --find-links=dist --no-index capirca\naclgen --output_directory .\\output --logtostderr\npowershell Compress-Archive -Force output\\* output.zip\n']"
"['make', 'JP_VERSION=100.100.100 ./scripts/build-all-platforms']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'echo ""RELEASE_VERSION=${GITHUB_REF/refs\\/tags\\//}"" >> $GITHUB_ENV', 'sed -i ""s/version = 0.1/version = $RELEASE_VERSION/g"" setup.cfg\npython setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest-cov\npip install -e "".[testing]""\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest --cov kmapper\n', 'bash <(curl -s https://codecov.io/bash)', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install -U setuptools\nsudo apt-get update -qq\nsudo apt-get install -qq swig python-dev libxml2-dev libxmlsec1-dev\nmake install-req\nmake install-test\n', 'make pytest', 'pip install -U setuptools\nsudo apt-get update -qq\nsudo apt-get install -qq swig python-dev libxml2-dev libxmlsec1-dev\nmake install-req\nmake install-test\n', 'make pycodestyle\nmake flake8\n', 'pip install coveralls\ncoverage run  setup.py test\ncoverage report -m\ncoveralls\n']"
"['pip3 install pipenv\npipenv install pyqt5 lxml\npipenv run pip install pyqt5==5.15.6 lxml\n', 'pipenv run make qt5py3\nrm -rf build dist\n', 'pipenv run python setup.py py2app\nopen dist/labelImg.app\n', 'cd dist/\ntar czf labelImg.tgz labelImg.app\n', 'pip3 install pyinstaller pyqt5==5.15.6 lxml\n', 'pyrcc5 -o libs/resources.py resources.qrc\n', 'pyinstaller --hidden-import=pyqt5 --hidden-import=lxml -F -n ""labelImg"" -c labelImg.py -p ./libs -p ./\n', 'pip3 install pyinstaller pyqt5==5.15.6 lxml\n', 'pyrcc5 -o libs/resources.py resources.qrc\n', 'pyinstaller --hidden-import=pyqt5 --hidden-import=lxml -F -n ""labelImg"" -c labelImg.py -p ./libs -p ./\n']"
"[""# The checkout actions doesn't clone to ~/zulip or allow\n# us to use the path option to clone outside the current\n# /__w/zulip/zulip directory. Since this directory is owned\n# by root we need to change it's ownership to allow the\n# github user to clone the code here.\n# Note: /__w/ is a docker volume mounted to $GITHUB_WORKSPACE\n# which is /home/runner/work/.\nsudo chown -R github .\n\n# This is the GitHub Actions specific cache directory the\n# the current github user must be able to access for the\n# cache action to work. It is owned by root currently.\nsudo chmod -R 0777 /__w/_temp/\n"", 'dirs=(/srv/zulip-{venv,emoji}-cache)\nsudo mkdir -p ""${dirs[@]}""\nsudo chown -R github ""${dirs[@]}""\n', './tools/ci/production-build', 'set -x\npath=""$(pnpm store path)""\n[[ ""$path"" == /__w/.pnpm-store/* ]]\n', 'tools/ci/generate-failure-message >> $GITHUB_OUTPUT', '# This is the GitHub Actions specific cache directory the\n# the current github user must be able to access for the\n# cache action to work. It is owned by root currently.\nsudo chmod -R 0777 /__w/_temp/\n\n# Since actions/download-artifact@v2 loses all the permissions\n# of the tarball uploaded by the upload artifact fix those.\nchmod +x /tmp/production-upgrade-pg\nchmod +x /tmp/production-pgroonga\nchmod +x /tmp/production-install\nchmod +x /tmp/production-verify\nchmod +x /tmp/generate-failure-message\n', 'dirs=(/srv/zulip-{venv,emoji}-cache)\nsudo mkdir -p ""${dirs[@]}""\nsudo chown -R github ""${dirs[@]}""\n', 'sudo /tmp/production-install ${{ matrix.extra-args }}', 'sudo /tmp/production-verify ${{ matrix.extra-args }}', 'sudo /tmp/production-pgroonga', 'sudo /tmp/production-verify ${{ matrix.extra-args }}', 'sudo /tmp/production-upgrade-pg', 'sudo /tmp/production-verify ${{ matrix.extra-args }}', '/tmp/generate-failure-message >> $GITHUB_OUTPUT', '# This is the GitHub Actions specific cache directory the\n# the current github user must be able to access for the\n# cache action to work. It is owned by root currently.\nsudo chmod -R 0777 /__w/_temp/\n\n# Since actions/download-artifact@v2 loses all the permissions\n# of the tarball uploaded by the upload artifact fix those.\nchmod +x /tmp/production-upgrade\nchmod +x /tmp/production-verify\nchmod +x /tmp/generate-failure-message\n', 'dirs=(/srv/zulip-{venv,emoji}-cache)\nsudo mkdir -p ""${dirs[@]}""\nsudo chown -R github ""${dirs[@]}""\n', 'sudo /tmp/production-upgrade', '/tmp/generate-failure-message >> $GITHUB_OUTPUT', 'export PATH=""$HOME/.local/bin:$PATH""\ngit clone https://github.com/zulip/marketplace-partners\npip3 install python-digitalocean zulip fab-classic PyNaCl\necho $PATH\npython3 tools/oneclickapps/prepare_digital_ocean_one_click_app_release.py\n', 'dirs=(/srv/zulip-{venv,emoji}-cache)\nsudo mkdir -p ""${dirs[@]}""\nsudo chown -R github ""${dirs[@]}""\n', ""# This is the main setup job for the test suite\n./tools/ci/setup-backend --skip-dev-db-build\n\n# Cleaning caches is mostly unnecessary in GitHub Actions, because\n# most builds don't get to write to the cache.\n# scripts/lib/clean_unused_caches.py --verbose --threshold 0\n"", 'source tools/ci/activate-venv\n./tools/test-tools\n', 'source tools/ci/activate-venv\n./tools/run-codespell\n', 'source tools/ci/activate-venv\necho ""Test suite is running under $(python --version).""\n./tools/lint --groups=backend --skip=gitlint,mypy # gitlint disabled because flaky\n', 'source tools/ci/activate-venv\n./tools/lint --groups=frontend --skip=gitlint # gitlint disabled because flaky\n', 'source tools/ci/activate-venv\n./tools/test-backend --coverage --xml-report --no-html-report --include-webhooks --no-cov-cleanup --ban-console-output\n', 'source tools/ci/activate-venv\n# We run mypy after the backend tests so we get output from the\n# backend tests, which tend to uncover more serious problems, first.\n./tools/run-mypy --version\n./tools/run-mypy\n', ""source tools/ci/activate-venv\n\n# Currently our compiled requirements files will differ for different\n# Python versions, so we will run test-locked-requirements only on the\n# platform with the oldest one.\n# ./tools/test-locked-requirements\n# ./tools/test-run-dev  # https://github.com/zulip/zulip/pull/14233\n#\n# This test has been persistently flaky at like 1% frequency, is slow,\n# and is for a very specific single feature, so we don't run it by default:\n# ./tools/test-queue-worker-reload\n\n./tools/test-migrations\n./tools/setup/optimize-svg --check\n./tools/setup/generate_integration_bots_avatars.py --check-missing\n./tools/ci/check-executables\n\n# Ban check-database-compatibility from transitively\n# relying on static/generated, because it might not be\n# up-to-date at that point in upgrade-zulip-stage-2.\nchmod 000 static/generated web/generated\n./scripts/lib/check-database-compatibility\nchmod 755 static/generated web/generated\n"", 'source tools/ci/activate-venv\n# In CI, we only test links we control in test-documentation to avoid flakes\n./tools/test-documentation --skip-external-links\n./tools/test-help-documentation --skip-external-links\n./tools/test-api\n', ""source tools/ci/activate-venv\n# Run the node tests first, since they're fast and deterministic\n./tools/test-js-with-node --coverage --parallel=1\n"", 'source tools/ci/activate-venv\n# Check that various schemas are consistent. (is fast)\n./tools/check-schemas\n', 'source tools/ci/activate-venv\n./manage.py makemessages --locale en\nPYTHONWARNINGS=ignore ./tools/check-capitalization --no-generate\nPYTHONWARNINGS=ignore ./tools/check-frontend-i18n --no-generate\n', 'source tools/ci/activate-venv\n./tools/test-js-with-puppeteer\n', 'pnpm dedupe --check', 'source tools/ci/activate-venv\n# This final check looks for untracked files that may have been\n# created by test-backend or provision.\nuntracked=""$(git ls-files --exclude-standard --others)""\nif [ -n ""$untracked"" ]; then\n    printf >&2 ""Error: untracked files:\\n%s\\n"" ""$untracked""\n    exit 1\nfi\n', '. /srv/zulip-py3-venv/bin/activate && \\\n./tools/test-locked-requirements\n', './tools/ci/setup-backend', 'set -x\npath=""$(pnpm store path)""\n[[ ""$path"" == /__w/.pnpm-store/* ]]\n', 'tools/ci/generate-failure-message >> $GITHUB_OUTPUT']"
"['python -m pip install --upgrade pip\npip install jsonschema==${{ matrix.jsonschema-version }}\npip install .[dev]\n# pip install ""selenium<4.3.0""\n# pip install altair_saver\n', 'python tools/generate_schema_wrapper.py\n# This gets the paths of all files which were either deleted, modified\n# or are not yet tracked by Git\nfiles=`git ls-files --deleted --modified --others --exclude-standard`\n# Depending on the shell it can happen that \'files\' contains empty\n# lines which are filtered out in the for loop below\nfiles_cleaned=()\nfor i in ""${files[@]}""; do\n  # Skip empty items\n  if [ -z ""$i"" ]; then\n    continue\n  fi\n  # Add the rest of the elements to a new array\n  files_cleaned+=(""${i}"")\ndone\nif [ ${#files_cleaned[@]} -gt 0 ]; then\n    echo ""The code generation modified the following files:""\n    echo $files\n    exit 1\nfi\n', 'pytest --doctest-modules tests\n', '# pip install vl-convert-python\npytest -m save_engine --doctest-modules tests\n', ""# We install all 'format' dependencies of jsonschema as check-jsonschema\n# only does the 'format' checks which are installed.\n# We can always use the latest jsonschema version here.\n# uri-reference check is disabled as the URIs in the Vega-Lite schema do\n# not conform RFC 3986.\npip install 'jsonschema[format]' check-jsonschema --upgrade\ncheck-jsonschema --check-metaschema altair/vegalite/v5/schema/vega-lite-schema.json --disable-formats uri-reference\n"", 'python -m pip install --upgrade pip\npip install hatch\n', 'hatch run doc:build-html\n', 'hatch run doc:doctest\n', 'python -m pip install --upgrade pip\npip install .[dev]\n', 'black --diff --color .\nblack --check .\n', 'ruff check .\n', 'mypy altair tests\n']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox', 'tox', 'python -m pip install --upgrade pip\npip install --upgrade setuptools wheel\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
""
"['source scripts/install_deepchem_conda.sh\n', 'source scripts/install_deepchem_conda.sh ${{ matrix.python-version }} cpu\n', 'conda activate deepchem\npython -V\npython -c \'import deepchem; print(""DeepChem version %s"" % deepchem.__version__)\'\npython -c \'import rdkit; print(""RDKIT version %s"" % rdkit.__version__)\'\npython -c \'import torch; print(""Torch version %s"" % torch.__version__)\'\npython -c \'import tensorflow; print(""Tensorflow version %s"" % tensorflow.__version__)\'\npython -c \'import jax; print (""Jax version %s"" % jax.__version__)\'\n', '. .\\scripts\\install_deepchem_conda.ps1\n', '. .\\scripts\\install_deepchem_conda.ps1 ${{ matrix.python-version }} cpu\n', 'conda activate deepchem\npython -V\npython -c ""import deepchem; print(\'DeepChem version %s\' % deepchem.__version__)""\npython -c ""import rdkit; print(\'RDKIT version %s\' % rdkit.__version__)""\npython -c ""import torch; print(\'Torch version %s\' % torch.__version__)""\npython -c ""import tensorflow; print(\'Tensorflow version %s\' % tensorflow.__version__)""\n', 'pip install -r requirements.txt', 'make clean html', 'make doctest_tutorials', 'make doctest_examples', 'python -m pip install --upgrade pip;\npip install conda-merge; \nconda-merge requirements/env_dqc.yml requirements/env_test.yml > env.yml\n', 'conda info', 'pip install -e .', 'pip freeze', 'python -c \'import numpy as np; print (""numpy version is %s"" % np.__version__)\'', 'python -c \'import deepchem; print(""DeepChem version %s"" % deepchem.__version__)\'\npython -c \'import torch; print(""Torch version %s"" % torch.__version__)\'\npython -c \'import dqc; print(""DQC version %s"" % dqc.__version__)\'\npython -c \'import xitorch; print(""Xitorch version %s"" % xitorch.__version__)\'\npython -c \'import pylibxc; print(""pylibxc version %s"" % pylibxc.version.__version__)\'\n', 'echo ""running dqc pytest""\npytest -v -m \'dqc\' deepchem\n', 'echo ""COMMIT_RANGE=${{ github.event.before }}.."" >> $GITHUB_ENV', 'git fetch origin master\necho ""COMMIT_RANGE=origin/master..."" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip;\npip install conda-merge;\ncd requirements\nconda-merge env_common.yml env_test.yml env_ubuntu.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml jax/env_jax.cpu.yml > env.yml\ncd ..\ncp requirements/env.yml env.yml\n', 'pip install -e .', 'CHANGED_FILES=`git diff --name-only $COMMIT_RANGE || true`\necho ""changed files are $CHANGED_FILES""\n', 'CHANGED_FILES=`git diff --name-only $COMMIT_RANGE | grep .py$ || true`\nif [ -n ""$CHANGED_FILES"" ]; then\n  yapf -d $CHANGED_FILES\nfi\n', 'source scripts/flake8_for_ci.sh', 'mypy -p deepchem\n', ""python -m pip install --upgrade pip\npip install -e '.[jax]'\n"", 'python -c ""import deepchem; import jax;""', 'echo ""COMMIT_RANGE=${{ github.event.before }}.."" >> $GITHUB_ENV', 'git fetch origin master\necho ""COMMIT_RANGE=origin/master..."" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip;\npip install conda-merge;\nconda-merge requirements/env_common.yml requirements/env_test.yml requirements/jax/env_jax.cpu.yml > env.yml\n', 'pip install -e .', ""pytest -v -m 'jax' deepchem"", 'python -m pip install --upgrade pip\npip install -e .\n', 'python -c ""import deepchem""', 'echo ""COMMIT_RANGE=${{ github.event.before }}.."" >> $GITHUB_ENV', 'git fetch origin master\necho ""COMMIT_RANGE=origin/master..."" >> $GITHUB_ENV\n', 'brew update && brew install libomp', 'python -m pip install --upgrade pip;\npip install conda-merge;\ncd requirements\nif [ ""$(uname)"" == \'Linux\' ]; then\n  conda-merge env_common.yml env_test.yml env_ubuntu.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml jax/env_jax.cpu.yml > env.yml\nelif [  ""$(uname)"" == \'Darwin\' ]; then\n  conda-merge env_common.yml env_test.yml env_mac.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.mac.cpu.yml jax/env_jax.cpu.yml > env.yml\nelif [[  ""$(uname)"" == ""MINGW64_NT""* ]]; then\n  conda-merge env_common.yml env_test.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml > env.yml\nfi;\ncd ..\ncp requirements/env.yml env.yml\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'python -m pip install --upgrade pip\npip install -e .\n', 'python -c ""import deepchem""', 'rm -rf ./build ./dist ./*egg-info\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel --release\ntwine upload dist/*\n', 'echo ::set-output name=VERSION::${GITHUB_REF#refs/tags/}', 'echo ${{ steps.docker_build.outputs.digest }}', ""python -m pip install --upgrade pip\npip install -e '.[tensorflow]'\n"", 'python -c ""import deepchem; import tensorflow;""', 'echo ""COMMIT_RANGE=${{ github.event.before }}.."" >> $GITHUB_ENV', 'git fetch origin master\necho ""COMMIT_RANGE=origin/master..."" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip;\npip install conda-merge;\nconda-merge requirements/env_common.yml requirements/env_test.yml requirements/tensorflow/env_tensorflow.cpu.yml > env.yml\n', 'pip install -e .', ""pytest -v -m 'tensorflow' deepchem"", 'python -m pip install --upgrade pip\npip install -e .\n', 'python -c ""import deepchem""', 'echo ""COMMIT_RANGE=${{ github.event.before }}.."" >> $GITHUB_ENV', 'git fetch origin master\necho ""COMMIT_RANGE=origin/master..."" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip;\npip install conda-merge;\ncd requirements\nif [ ""$(uname)"" == \'Linux\' ]; then\n  conda-merge env_common.yml env_test.yml env_ubuntu.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml jax/env_jax.cpu.yml env_py310_no_support.yml >  env.yml\nelif [  ""$(uname)"" == \'Darwin\' ]; then\n  conda-merge env_common.yml env_test.yml env_mac.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.mac.cpu.yml jax/env_jax.cpu.yml env_py310_no_support.yml > env.yml\nelif [[  ""$(uname)"" == ""MINGW64_NT""* ]]; then\n  conda-merge env_common.yml env_test.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml > env.yml\nfi;\ncd ..\ncp requirements/env.yml env.yml\n', 'python -m pip install --upgrade pip;\npip install conda-merge;\ncd requirements\nif [ ""$(uname)"" == \'Linux\' ]; then\n  conda-merge env_common.yml env_test.yml env_ubuntu.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml jax/env_jax.cpu.yml > env.yml\nelif [  ""$(uname)"" == \'Darwin\' ]; then\n  conda-merge env_common.yml env_test.yml env_mac.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.mac.cpu.yml jax/env_jax.cpu.yml > env.yml\nelif [[  ""$(uname)"" == ""MINGW64_NT""* ]]; then\n  conda-merge env_common.yml env_test.yml tensorflow/env_tensorflow.cpu.yml torch/env_torch.cpu.yml > env.yml\nfi;\ncd ..\ncp requirements/env.yml env.yml\n', 'pip install -e .', ""DGLBACKEND=pytorch pytest -v --ignore-glob='deepchem/**/test*.py' --doctest-modules deepchem --doctest-continue-on-failure"", ""DGLBACKEND=pytorch pytest -v --ignore-glob='deepchem/**/test*.py' --ignore-glob='deepchem/models/jax_models/*' --doctest-modules deepchem --doctest-continue-on-failure"", 'pytest -v -m ""not jax and not torch and not tensorflow and not dqc"" --ignore=\'deepchem/dock/tests/test_docking.py\'  --ignore=\'deepchem/dock/tests/test_pose_scoring.py\' --ignore=\'deepchem/dock/tests/test_pose_generation.py\' deepchem', 'pytest -v -m ""not jax and not torch and not tensorflow and not dqc"" deepchem', ""python -m pip install --upgrade pip\npip install -e '.[torch]'\n"", 'python -c ""import deepchem; import torch;""', 'echo ""COMMIT_RANGE=${{ github.event.before }}.."" >> $GITHUB_ENV', 'git fetch origin master\necho ""COMMIT_RANGE=origin/master..."" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip;\npip install conda-merge;\nconda-merge requirements/env_common.yml requirements/torch/env_torch.cpu.yml requirements/env_test.yml > env.yml\n', 'pip install -e .', ""pytest -v -m 'torch' deepchem""]"
"['pip install pre-commit', 'pre-commit run --all-files', 'sudo apt-get update && sudo apt-get install -y python3-dev openssl libssl-dev gcc pkg-config libffi-dev libxml2-dev libxmlsec1-dev', 'pip install -r piptools_requirements.txt && pip install -r requirements.txt', 'make test_unit', 'make actions_test_integration', 'npm install grunt-cli && npm install && grunt test', 'pip install virtualenv', 'sudo apt-get update -y && sudo apt-get install -y libxmlsec1-dev pkg-config', 'make docs', 'rm -rf docs', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
"['make develop\n', 'make test\n']"
"['sudo apt update\nsudo apt install libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-shape0 libxcb-cursor0 libxcb-xinerama0\nsudo apt install libegl1-mesa libxkbcommon-x11-0\n', 'python -m pip install ${{ matrix.pyqt }} ${{ matrix.pygments }}\npython -m pip install Markdown pymdown-extensions docutils chardet Markups\n', 'xvfb-run -a -s ""-screen 0 1024x768x24"" python -m unittest discover -s tests -v', 'python -m unittest discover -s tests -v', 'python -m pip install pyflakes', 'python -m pyflakes ReText/ tests/ setup.py']"
"['mamba install -c conda-forge boa conda-verify\n\nwhich python\npip list\nmamba list\n', '# suffix for pre-release package versions\nexport VERSION_SUFFIX=a`date +%y%m%d`\n\n# conda search for the latest dask-core pre-release\narr=($(conda search --override-channels -c dask/label/dev dask-core | tail -n 1))\n\n# extract dask-core pre-release version / build\nexport DASK_CORE_VERSION=${arr[1]}\n\n# distributed pre-release build\nconda mambabuild continuous_integration/recipes/distributed \\\n                 --channel dask/label/dev \\\n                 --no-anaconda-upload \\\n                 --output-folder .\n\n# dask pre-release build\nconda mambabuild continuous_integration/recipes/dask \\\n                 --channel dask/label/dev \\\n                 --no-anaconda-upload \\\n                 --output-folder .\n', '# install anaconda for upload\nmamba install -c conda-forge anaconda-client\n\nanaconda upload --label dev noarch/*.tar.bz2\n', 'mkdir artifacts && cd artifacts\n\nartifacts_url=${{ github.event.workflow_run.artifacts_url }}\n\ngh api --paginate ""$artifacts_url"" -q \'.artifacts[] | [.name, .archive_download_url] | @tsv\' | while read artifact\ndo\n  IFS=$\'\\t\' read name url <<< ""$artifact""\n  gh api $url > ""$name.zip""\n  unzip -d ""$name"" ""$name.zip""\ndone\n', 'bash continuous_integration/scripts/setup_ssh.sh', 'ls -ld ~ ~/.ssh ~/.ssh/*\nfor f in ~/.ssh/* /etc/ssh/sshd_config; do\n  echo ==================================\n  echo $f\n  echo ==================================\n  cat $f\ndone\n', ""ssh -vvv localhost 'echo hello world'"", ""ssh -vvv 127.0.0.1 'echo hello world'"", ""ssh -vvv $(hostname) 'echo hello world'"", 'conda config --show', 'mamba list', 'python continuous_integration/scripts/test_report.py --max-days 90 --max-runs 30 --nfails 1 -o test_report.html\npython continuous_integration/scripts/test_report.py --max-days 7 --max-runs 30 --nfails 2 -o test_short_report.html --title ""Test Short Report""\nmkdir deploy\nmv test_report.html test_short_report.html deploy/\n', 'export PARTITION_LABEL=$( echo ""${{ matrix.partition }}"" | sed ""s/ //"" )\nexport TEST_ID=""${{ matrix.os }}-${{ matrix.environment }}-${{ matrix.queuing || join(matrix.extra_packages, \'_\') }}-$PARTITION_LABEL""\n# Switch to this version for stress-test:\n# export TEST_ID=""$TEST_ID-${{ matrix.run }}""\necho ""TEST_ID: $TEST_ID""\necho ""TEST_ID=$TEST_ID"" >> $GITHUB_ENV\n', 'conda config --show', 'echo ""::set-output name=today::$(/bin/date -u \'+%Y%m%d\')""', 'mamba env update -n dask-distributed -f ${{ env.CONDA_FILE }}', 'python -m pip install --no-deps -e .\n', ""mamba install -y ${{ join(matrix.extra_packages, ' ') }}"", 'mamba list', 'echo -e ""--\\n--Conda Environment (re-create this with \\`mamba env create --name <name> -f <output_file>\\`)\\n--""\nmamba env export | grep -E -v \'^prefix:.*$\'\n', 'bash continuous_integration/scripts/setup_ssh.sh', 'sed -i.bak \'s/timeout_method = ""thread""/timeout_method = ""signal""/\' pyproject.toml', 'echo ""DISABLE_IPV6=1"" >> $GITHUB_ENV', 'echo ""DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=inf"" >> $GITHUB_ENV', 'python continuous_integration/scripts/host_info.py\n', 'source continuous_integration/scripts/set_ulimit.sh\nset -o pipefail\nmkdir reports\n\npytest distributed \\\n  -m ""not avoid_ci and ${{ matrix.partition }}"" --runslow \\\n  --leaks=fds,processes,threads \\\n  --junitxml reports/pytest.xml -o junit_suite_name=$TEST_ID \\\n  --cov=distributed --cov-report=xml \\\n| tee reports/stdout\n', 'if [ ! -e reports/pytest.xml ]\nthen\n  # This should only ever happen on Windows.\n  # On Linux and MacOS, pytest-timeout kills off the individual tests\n  # See (reconfigure pytest-timeout above)\n  python continuous_integration/scripts/parse_stdout.py < reports/stdout > reports/pytest.xml\nfi\n', 'sed -i\'\' -e \'s/filename=""/filename=""distributed\\//g\' coverage.xml', 'echo RAPIDS_VER=${{ steps.rapids_current.outputs.RAPIDS_VER_0 }} >> $GITHUB_ENV\necho UCX_PY_VER=$(curl -sL https://version.gpuci.io/rapids/${{ steps.rapids_current.outputs.RAPIDS_VER_0 }}) >> $GITHUB_ENV\necho NEW_RAPIDS_VER=${FULL_RAPIDS_VER::-4} >> $GITHUB_ENV\necho NEW_UCX_PY_VER=${FULL_UCX_PY_VER::-4} >> $GITHUB_ENV\n']"
"['pip3 install -r requirements.txt\npip3 install -r requirements-dev.txt\npip3 install --editable .\n', 'pip3 install -r requirements.txt --use-deprecated=legacy-resolver\npip3 install -r requirements-dev.txt --use-deprecated=legacy-resolver\npip3 install --editable . --use-deprecated=legacy-resolver\n', 'pip3 install -U python-dotenv\npy.test test/unit\n', 'pip3 install -U python-dotenv\npy.test test/unit --reruns 3\n', 'pip3 install -U python-dotenv\npy.test test/unit --reruns 3 --cov=ibm_watson\n', 'pip3 install -U python-dotenv\npy.test test/unit --reruns 3\n', 'pip3 install -U python-dotenv\npy.test test/unit --reruns 3\n', 'sudo apt-get install bumpversion\nnpm install -g semantic-release\nnpm install -g @semantic-release/changelog\nnpm install -g @semantic-release/exec\nnpm install -g @semantic-release/git\nnpm install -g @semantic-release/github\nnpm install -g @semantic-release/commit-analyzer\nnpm install -g @semantic-release/release-notes-generator\n', 'sudo apt-get install python3-sphinx\ndocs/publish_gha.sh\n', 'npx semantic-release', 'pip3 install setuptools wheel twine build\npython -m build --sdist --outdir dist/\n', 'pip3 install -r requirements.txt\npip3 install -r requirements-dev.txt\npip3 install --editable .\n', 'pip3 install -U python-dotenv\npytest test/integration/test_assistant_v1.py -rap\npytest test/integration/test_discovery_v1.py -rap\npytest test/integration/test_discovery_v2.py -rap\npytest test/integration/test_language_translator_v3.py -rap\npytest test/integration/test_natural_language_understanding_v1.py -rap\npytest test/integration/test_speech_to_text_v1.py -rap\npytest test/integration/test_text_to_speech_v1.py -rap\n']"
"['python -m venv venv-requirements-txt\nsource venv-requirements-txt/bin/activate\npip install -r requirements/py${{ matrix.python-version }}.txt\npython -m unittest discover pigar/tests/ -t . -v\n', 'python -m venv venv-pip-e\nsource venv-pip-e/bin/activate\npip install -e .\npython -m unittest discover pigar/tests/ -t . -v\n', 'pip install build', 'python -m build']"
"['docker compose build --build-arg dev_mode=""true""\n', 'docker compose build\n', 'cat > ~/.pypirc <<- EOF\n[distutils]\nindex-servers =\n    pypi\n    pypitest\n\n[autovizwidget]\nusername=__token__\npassword=$PYPI_TOKEN_AUTOVIZWIDGET\n\n[hdijupyterutils]\nusername=__token__\npassword=$PYPI_TOKEN_HDIJUPYTERUTILS\n\n[sparkmagic]\nusername=__token__\npassword=$PYPI_TOKEN_SPARKMAGIC\n\nEOF\n', 'cd hdijupyterutils\npython setup.py sdist\ncd ..\n', 'cd autovizwidget\npython setup.py sdist\ncd ..\n', 'cd sparkmagic\npython setup.py sdist\ncd ..\n', 'sudo apt-get install -y libkrb5-dev\n', 'python -m pip install --upgrade pip\npip install pytest mock\npip install -r hdijupyterutils/requirements.txt -e hdijupyterutils\npip install -r autovizwidget/requirements.txt -e autovizwidget\npip install -r sparkmagic/requirements.txt -e sparkmagic\n', 'pytest hdijupyterutils\n', 'pytest autovizwidget\n', 'mkdir ~/.sparkmagic\npytest sparkmagic\n']"
"['git config user.name ""pytest bot""\ngit config user.email ""pytestbot@gmail.com""\ngit tag --annotate --message=v${{ github.event.inputs.version }} v${{ github.event.inputs.version }} ${{ github.sha }}\ngit push origin v${{ github.event.inputs.version }}\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox run -e ${{ matrix.tox_env }} --installpkg `find dist/*.tar.gz`\n']"
"['pip install tox\ntox -e py\n', 'pip install coverage\ncoverage xml\n', 'pip install tox', 'tox -e lint', 'python -m pip install --upgrade build\npython -m build .\n', 'pip list | grep setuptools\npip list | grep build\n', 'python -m pip install --upgrade build\npython -m build .\n', 'pip list | grep setuptools\npip list | grep build\n', 'pip install tox\ntox\n']"
[]
"['python -m pip install wheel\npython -m pip install -r tests/requirements.txt\npython -m pip install .\n', 'git config --global init.defaultBranch main', 'pytest\n', 'pytest\n', 'git config --global core.autocrlf true\npytest\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['sudo apt-get update -q\nsudo apt-get install -qy jq\npip install -e .[babel]\n', 'python setup.py extract_messages', './scripts/poedit.sh upload', 'sudo apt-get update -q\nsudo apt-get install -qy jq gobject-introspection libgirepository-1.0-1 gir1.2-gtk-3.0 gir1.2-glib-2.0 gir1.2-gstreamer-1.0 gir1.2-poppler-0.18 python3-pip python3-setuptools python3-wheel libgirepository1.0-dev vlc\npip install pygobject pycairo .[build_sphinx] .[vlc_video]\n', './scripts/poedit.sh contributors\npython3 -m sphinx -bhtml docs/ build/sphinx/html\ntar czf pympress-docs.tar.gz -C build/sphinx/html/ .\n', 'tar xzf pympress-docs.tar.gz\nrm pympress-docs.tar.gz\ngit add .\ngit -c user.email=me@cimba.li -c user.name=""${GITHUB_ACTOR}"" commit -m ""Github Action-built docs update""\ngit push\n', 'ref=${{ github.ref }}\n[ ""${ref::10}"" = \'refs/tags/\' ] && tag=${ref:10} || tag=${{ github.event.inputs.tag }}\necho tag=${tag#v} | tee -a $GITHUB_OUTPUT\n', 'env PYTHONPATH=pympress python3 -c ""import importlib; assert importlib.import_module(\'pympress.__init__\').__version__ == \'${{ steps.name.outputs.tag }}\'""\n', 'sudo apt-get update -q\nsudo apt-get install -qy gettext\n', './pympress/scripts/poedit.sh download\ngit -C pympress status --porcelain $i18n_files | tee status\n\nif [ -s status ]; then\n  echo ""Unversioned translation updates:""\n  git diff -- $i18n_files\n  exit 1\nfi\n', 'tar czf $basename.tar.gz --exclude-vcs --exclude=.github --transform=""s/^pympress/$basename/"" pympress/', 'cd ""`brew --repo homebrew/core`""\n# Credentials and remotes\ngit remote add gh ""https://github.com/Cimbali/homebrew-core/""\ngit fetch gh\n# Attempt a rebase of changes in our repo copy\ngit checkout --detach\ngit rebase origin/master gh/master && git branch -f master HEAD || git rebase --abort\n# Now use master and update remote so we can use the bump-formula-pr\ngit checkout master\ngit log -1 --decorate\n', 'brew audit --strict pympress\n', 'brew install pympress --only-dependencies\nbrew install pympress --build-from-source --HEAD\n', 'pympress --help\n', 'pympress --quit\n', 'head ~/Library/Logs/pympress.log\n', 'brew test pympress\n', ""# NB. donâ€™t use --debug which is interactive\nbrew test --keep-tmp --verbose pympress | tee test.log\ntempdir=`sed -n '/Temporary files retained at/{n;p}' test.log`\ntree -a $tempdir\n"", 'sudo apt-get update -q\nsudo apt-get install -qy python3-rpm\npython3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel twine babel pysrpm rpmlint\n', 'python3 setup.py compile_catalog\n', ""mkdir srpm\npysrpm --dest-dir=srpm/ --source-only .\necho file=`find srpm/ -name '*.src.rpm' -printf '%P\\n' -quit` | tee -a $GITHUB_OUTPUT\n"", 'rpmlint srpm/${{ steps.name.outputs.file }}', 'dnf install -y rpm-build `rpm -qR ""${{ needs.srpm.outputs.file }}"" | awk \'$1 !~ /^rpmlib(.*)/ {print $1}\'`', 'rpm -q --qf ""dest=`rpm --eval %{_rpmfilename}`\\n"" ""$srpm"" | tee -a $GITHUB_OUTPUT\nrpmbuild -D ""_rpmdir ${PWD}"" -ra ""$srpm""\n', 'zypper install -y rpm-build rpmlint osc', 'trap \'rm -f ./osc-config\' EXIT && echo ""$OPENBUILDSERVICE_TOKEN_SECRET"" > ./osc-config\n\nosc --config ./osc-config co -o osc home:cimbali python-pympress\n\nsed -r \'\n  s/^(Version: *)[0-9.]+$/\\1${{ needs.checks.outputs.tag }}/\n  s/^(Source0: *pympress-)[0-9.]+(\\.tar\\.gz)/\\1${{ needs.checks.outputs.tag }}\\2/\n\' osc/pympress.spec > pympress.spec\n', 'rpmlint pympress.spec', 'filename=`rpm -q --qf ""%{name}-%{version}-%{release}.src.rpm"" --specfile pympress.spec`\necho ""filename=$filename"" | tee -a $GITHUB_OUTPUT\n\nrpmbuild -D ""_sourcedir $PWD"" -D ""_srcrpmdir $PWD"" -bs pympress.spec\n\n[ -f ""$filename"" ]  # Check itâ€™s the expected file name\n', 'rpmlint ${{ steps.srpm.outputs.filename }}', 'zypper install -y `rpm -qR ""${{ steps.srpm.outputs.filename }}""`', 'filename=`rpm -q --qf ""%{arch}/%{name}-%{version}-%{release}.%{arch}.rpm"" ""$srpm""`\necho ""filename=$filename"" | tee -a $GITHUB_OUTPUT\n\nrpmbuild -D ""_rpmdir ${PWD}"" -ra ""$srpm""\n', 'rpmlint ${{ steps.build.outputs.filename }}', 'zypper install -y --allow-unsigned-rpm ${{ steps.build.outputs.filename }}', 'env PYMPRESS_HEADLESS_TEST=1 pympress --quit', 'head ${XDG_CACHE_HOME:-$HOME/.cache}/pympress.log', 'trap \'rm -f ./osc-config\' EXIT && echo ""$OPENBUILDSERVICE_TOKEN_SECRET"" > ./osc-config\n\ncd osc/\nosc=""osc --config ../osc-config""\n\n$osc rm `sed -n \'s/^Source0: *//p\' pympress.spec`\ncp ../pympress-${{ needs.checks.outputs.tag }}.tar.gz ./\n$osc add pympress-${{ needs.checks.outputs.tag }}.tar.gz\n\ncp ../pympress.spec ./\n\n$osc ci -m ""Update build to v${{ needs.checks.outputs.tag }}""\n', 'python3 -m pip install --disable-pip-version-check --upgrade pip\npython3 -m pip install python-vlc\n', 'python3 setup.py compile_catalog', 'python3 setup.py --freeze build_exe', 'echo file=pympress-${{ needs.checks.outputs.tag }}-${{ matrix.arch }} | tee -a $GITHUB_OUTPUT\n', 'python3 setup.py --freeze bdist_msi --target-name ${{ steps.name.outputs.file }}.msi --skip-build', 'cd build\nmv exe.* pympress\ncp ../pympress/share/defaults.conf pympress/pympress.conf\nzip -r ../dist/${{ steps.name.outputs.file }}.zip pympress/\ncd -\n', '$installer = gci -path dist\\* -include *.msi -name\nStart-Process msiexec.exe -Wait -NoNewWindow -ArgumentList (\'/i ""dist\\{0}"" /qn /norestart /L* installer.log\' -f $installer)\n\necho ""::group::Installer log""\nget-content installer.log\necho ""::endgroup::""\n', '# Check pympress install dir is appended to one of the $PATH variables\n$dir = (\n  [System.Environment]::GetEnvironmentVariable(""Path"",""Machine"").split("";"") +\n  [System.Environment]::GetEnvironmentVariable(""Path"",""User"").split("";"")\n) | Select-String \'pympress\'\n\ngci -path $dir -filter *exe\n\nStart-Process ""$dir\\pympress.exe"" -Wait -NoNewWindow -ArgumentList ""--quit""\n\necho ""::group::Pympress log""\nget-content ""$env:LOCALAPPDATA\\pympress.log""\necho ""::endgroup::""\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 flake8-docstrings\n', 'git fetch origin ${{ github.base_ref }}', ""flake8 . --count --show-source --statistics | sed -r 'h;s/^(\\S+):([0-9]+):([0-9]+): /::error file=\\1,line=\\2,col=\\3::/p;g'\n"", 'git diff -z --name-only FETCH_HEAD -- \'**.py\' | xargs -r0 flake8 --exit-zero --ignore=D107,D200,D210,D413,E251,E302,E303,W504 > errors\ngit diff FETCH_HEAD -U0 -- \'**.py\' | sed -rn -e \'/^\\+\\+\\+ /{s,^\\+\\+\\+ ./,,;h}\' -e \'/^@@ /{G;s/^@@ -[0-9,]+ \\+([0-9,]+) @@.*\\n(.*)/\\2,\\1/p}\' | (\n   while IFS=, read file start lines; do for (( l = start ; l < $start + ${lines:-1}; ++l )); do echo ""^$file:$l:""; done; done\n) > changed_lines\n# Invert return value, i.e. error iff matches\n! grep -f changed_lines errors | sed -r \'h;s/^(\\S+):([0-9]+):([0-9]+): /::error file=\\1,line=\\2,col=\\3::/p;g\'\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 flake8-docstrings\n', ""flake8 . --count --show-source --statistics --select=E,F,W,C | sed -r 'h;s/^(\\S+):([0-9]+):([0-9]+): /::error file=\\1,line=\\2,col=\\3::/p;g'\n"", ""flake8 . --count --show-source --statistics --select=D | sed -r 'h;s/^(\\S+):([0-9]+):([0-9]+): /::warning file=\\1,line=\\2,col=\\3::/p;g'\n"", 'ref=${{ github.ref }}\n[ ""${ref::10}"" = \'refs/tags/\' ] && tag=${ref:10} || tag=${{ github.event.inputs.tag }}\nif echo ${tag#v} | grep -qxE \'[0-9]+(\\.[0-9]+)*\' ; then release=final; else release=prerelease; fi\necho tag=${tag#v} | tee -a $GITHUB_OUTPUT\necho release=$release | tee -a $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine babel\n', 'python setup.py compile_catalog\npython setup.py sdist bdist_wheel\n', 'twine upload dist/*\n', '# Get releases from pypi, exiting with non-zero if expected version not found\njq_script="".releases.\\""${tag}\\""[]? | select(.python_version == \\""source\\"")""\nwhile ! curl -s https://pypi.org/pypi/pympress/json | jq -r -e ""$jq_script"" > lastsource.json ; do\n  sleep 60  # be patient with pypi\ndone\necho url=`jq -r .url lastsource.json` | tee -a $GITHUB_OUTPUT\necho sha256=`jq -r .digests.sha256 lastsource.json` | tee -a $GITHUB_OUTPUT\n', 'if ! jq -r \'""\\(.digests.sha256) dist/\\(.filename)""\' lastsource.json | sha256sum -c ; then\n  echo \'::warning:: Generated sdist file did not match pypi sha256sum\'\nfi\n', 'git clone https://github.com/Cimbali/pympress-pkgbuild aur-repo', 'ref=${{ github.ref }}\n[ ""${ref::10}"" = \'refs/tags/\' ] && tag=${ref:10} || tag=${{ github.event.inputs.tag }}\ntag=${tag#v}\n\nprev_pkgver=`awk -F= \'$1 == ""pkgver"" {print $2}\' aur-repo/PKGBUILD | tr -d ""[()\\""\']""`\nif [[ ""$prev_pkgver"" = ""$tag"" ]]; then\n  prev_pkgrel=`awk -F= \'$1 == ""pkgrel"" {print $2}\' aur-repo/PKGBUILD | tr -d ""[()\\""\']""`\nelse\n  prev_pkrel=0\nfi\n\nurl=""https://github.com/Cimbali/pympress/releases/download/v${tag}/pympress-${tag}.tar.gz""\nsha256=`curl -sL ""$url"" | sha256sum | awk \'{ print $1 }\'`\n\nprintf \'%s\\n\' ""tag=$tag"" ""url=$url"" ""sha256=$sha256"" ""prev_pkgver=$prev_pkgver"" ""prev_pkgrel=$prev_pkgrel"" |\n  tee -a $GITHUB_OUTPUT\n', 'while read param value; do\n  sed -i -r ""s,^(\\\\s*$param ?=[(\'\\"" ]*)[A-Za-z0-9\\${\\}:/._-]+([ \'\\"")]*)$,\\1$value\\2,"" aur-repo/.SRCINFO aur-repo/PKGBUILD\ndone <<EOF\n  source ${{ steps.info.outputs.url }}\n  sha256sums ${{ steps.info.outputs.sha256 }}\n  pkgver ${{ steps.info.outputs.tag }}\n  pkgrel $(( ${{ steps.info.outputs.prev_pkgrel }} + 1 ))\nEOF\n', 'trap \'rm -f ./ssh-key\' EXIT && echo ""$AUR_PRIVATE_KEY"" > ./ssh-key && chmod 0600 ./ssh-key\nssh=\'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i ../ssh-key\'\n\ngit -C aur-repo -c user.name=Cimbali -c user.email=""me@cimba.li"" commit -am ""Update to v${{ steps.info.outputs.tag }}""\ngit -C aur-repo -c core.sshCommand=""$ssh"" push git@github.com:Cimbali/pympress-pkgbuild.git ""master:master""\n', 'sudo apt-get update -q\nsudo apt-get install -qy osc cpio rpm2cpio pandoc python3-m2crypto\npython3 -m pip install copr-cli\n', 'ref=${{ github.ref }}\n[ ""${ref::10}"" = \'refs/tags/\' ] && tag=${ref:10} || tag=${{ github.event.inputs.tag }}\ntag=${tag#v}\n\nurl=""https://github.com/Cimbali/pympress/releases/download/v${tag}/pympress-${tag}.tar.gz""\nsha256=`curl -sL ""$url"" | sha256sum | awk \'{ print $1 }\'`\n\nprintf \'%s\\n\' ""tag=$tag"" ""url=$url"" ""sha256=$sha256"" | tee -a $GITHUB_OUTPUT\n', 'curl -s -u ""Cimbali:$GITHUB_TOKEN"" -H ""X-GitHub-Api-Version: 2022-11-28"" -H ""Accept: application/vnd.github+json"" \\\n  ""https://api.github.com/repos/Cimbali/pympress/releases"" -o - |\n  jq "".[] | select(.tag_name == \\""v$tag\\"") | del(.author, .assets[].uploader)"" | tee release.json\n\njq -r .body release.json | sed \'/\\(New Contributors\\|Full Changelog\\)/,/^\\s*$/d;s/^- /  &/;1i- Update to v${{ steps.info.outputs.tag }}\' |\n  pandoc --from=markdown --to=markdown --columns=67 | sed -r \'s/^(\\s+)-/\\1*/;s/\\#/#/g\' | tee changes\n', 'trap \'rm -f ./osc-config\' EXIT && echo ""$OPENBUILDSERVICE_TOKEN_SECRET"" > ./osc-config\nosc=""osc --config $GITHUB_WORKSPACE/osc-config""\n\n$osc co -o osc home:cimbali python-pympress\ncd osc/\n\nif grep -qxFe \'- Update to v${{ steps.info.outputs.tag }}\' pympress.changes; then\n  echo ""Version already in changelog ; skipping request""\nelse\n  $osc vc -F ../changes\n  sed -i ""2s/Cimba Li <me@cimba.li>/me@cimba.li/"" pympress.changes\n  $osc ci -m ""Release ${{ steps.info.outputs.tag }}""\n\n  $osc sr --yes -m ""Version ${{ steps.info.outputs.tag }}"" \'X11:Utilities\' pympress\nfi\n', 'url=""https://github.com/Cimbali/pympress/releases/download/v${tag}/python3-pympress-${tag}-1.src.rpm""\ncurl -L ""$url"" -o ""python3-pympress-${tag}-1.src.rpm""\n', 'trap \'rm -f ./copr-config\' EXIT && echo ""$COPR_TOKEN_CONFIG"" > ./copr-config\ncopr-cli --config ./copr-config build --nowait cimbali/pympress ""python3-pympress-${{ steps.info.outputs.tag }}-1.src.rpm""\n', 'brew update\nbrew install pipgrip\n', 'cd ""`brew --repo homebrew/core`""\n# Credentials and remotes\ngit config user.name Cimbali\ngit config user.email me@cimba.li\ngit config credential.helper store\necho -e ""protocol=https\\nhost=github.com\\nusername=Cimbali\\npassword=$PASSWORD"" | git credential-store store\ngit remote add gh ""https://github.com/Cimbali/homebrew-core/""\ngit fetch gh\n# Attempt a rebase of changes in our repo copy\ngit checkout --detach\ngit rebase origin/master gh/master && git branch -f master HEAD || git rebase --abort\n# Now use master and update remote so we can use the bump-formula-pr\ngit checkout master\ngit push gh -f master:master\n', 'brew bump-formula-pr --strict --no-browse --url=""${{needs.pypi.outputs.url}}"" --sha256=""${{needs.pypi.outputs.sha256}}"" pympress\n']"
"['python -m pip install --upgrade pip\npython setup.py install\npython -m pip install -r test-requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\n# Code style checks disabled until the style of the project is set.\n./run_tests.sh lint\n', './run_tests.sh test\n', 'python -m pip install --upgrade pip\npython setup.py install\npython -m pip install -r test-requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\n# Code style checks disabled until the style of the project is set.\n./run_tests.sh lint\n', './run_tests.sh test\n', 'git remote add tag_target ""https://$GITHUB_TOKEN@github.com/MycroftAI/adapt.git""\nVERSION=$(python setup.py --version)\ngit tag -f release/v$VERSION || exit 0\nif git push tag_target --tags; then\n  echo ""New tag published on github, push to PyPI as well.""\n  pip install twine wheel\n  python setup.py sdist bdist_wheel\n  twine check dist/*\n  twine upload dist/*\n  echo ""Package pushed to PyPI. Prepare for mycroft-core PR.""\n  echo ""::set-output name=version::$VERSION""\nfi\n', 'VERSION=${{needs.tag-release-if-needed.outputs.version}}\nif [[ $VERSION != *"".""* ]]; then\n  echo ""Not a valid version number.""\n  exit 1\nelif [[ $VERSION == *""-""* ]]; then\n  echo ""Pre-release suffix detected. Not pushing to mycroft-core.""\nelse\n  sed -E ""s/adapt-parser==[0-9]+\\.[0-9]+\\.[0-9]+/adapt-parser==$VERSION/"" requirements/requirements.txt > tmp-requirements.txt\n  mv tmp-requirements.txt requirements/requirements.txt\n  echo ""ADAPT_VERSION=$VERSION"" >> $GITHUB_ENV\nfi\n']"
""
""
"['docker pull ${{ matrix.docker_base_image }}\ndocker pull us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker-dev:latest\n', 'docker build --build-arg PPA_TRACK=${{ matrix.gift_ppa_track }} --cache-from=${{ matrix.docker_base_image }},us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker-dev:latest -t turbinia-worker-dev -f docker/worker/Dockerfile .', 'docker run --name turbinia-worker --entrypoint ""/bin/bash"" -it -d -t turbinia-worker-dev:latest\n', 'docker exec -u root -t turbinia-worker bash -c ""update-alternatives --install /usr/bin/python python /usr/bin/python3 1""\ndocker exec -u root -t turbinia-worker bash -c ""/usr/bin/python -V""\n', 'docker exec -u root -t turbinia-worker bash -c ""/usr/bin/python -m pip install --quiet --upgrade pip""\ndocker exec -u root -t turbinia-worker bash -c ""/usr/bin/python -m pip install --quiet mock nose coverage yapf""\ndocker exec -u root -t turbinia-worker bash -c ""/usr/bin/python -m pip install --quiet tox""\n', 'docker exec -u root -t turbinia-worker bash -c ""cd /tmp && ./run_tests.py""\ndocker exec -u root -t turbinia-worker bash -c ""cd /tmp && tox --sitepackages ${TOXENV}""\n', 'docker pull ${{ matrix.docker_base_image }}\ndocker pull us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-api-server-dev:latest\ndocker pull us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-server-dev:latest\ndocker pull us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker-dev:latest\n', 'docker build --cache-from=${{ matrix.docker_base_image }},us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-api-server-dev:latest -t turbinia-api-server-dev -f docker/api_server/Dockerfile .', 'docker build --cache-from=${{ matrix.docker_base_image }},us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-server-dev:latest -t turbinia-server-dev -f docker/server/Dockerfile .', 'docker build --build-arg PPA_TRACK=${{ matrix.gift_ppa_track }} --cache-from=${{ matrix.docker_base_image }},us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker-dev:latest -t turbinia-worker-dev -f docker/worker/Dockerfile .', 'mkdir ./conf\nsed -f ./docker/local/local-config.sed ./turbinia/config/turbinia_config_tmpl.py > ./conf/turbinia.conf\n', 'cat ./conf/turbinia.conf\n', 'sed -i -e \'s/#image: ""t/image: ""t/g\' -e \'s/image: ""u/#image: ""u/g\' ./docker/local/docker-compose.yml\n', 'chmod +x ./turbinia/e2e/e2e-local.sh\n./turbinia/e2e/e2e-local.sh\n']"
"['echo ${{ secrets.DOCKER_HUB_PASSWORD }} | docker login -u ${{ secrets.DOCKER_HUB_USERNAME }} --password-stdin', 'docker build --target listenbrainz-prod --build-arg GIT_COMMIT_SHA=HEAD .', 'echo ::set-output name=TAG::${GITHUB_REF/refs\\/tags\\//}', 'echo ${{ secrets.DOCKER_HUB_PASSWORD }} | docker login -u ${{ secrets.DOCKER_HUB_USERNAME }} --password-stdin', './test.sh fe -b', './test.sh fe', './test.sh fe -f\n', './test.sh fe -t', 'echo ${{ secrets.DOCKER_HUB_PASSWORD }} | docker login -u ${{ secrets.DOCKER_HUB_USERNAME }} --password-stdin', 'docker build \\\n  --target listenbrainz-prod \\\n  --tag metabrainz/listenbrainz:""${{ github.event.inputs.tag }}"" \\\n  --build-arg GIT_COMMIT_SHA=""${{ github.sha }}"" .\n', 'docker push metabrainz/listenbrainz:""${{ github.event.inputs.tag }}""', 'echo ${{ secrets.DOCKER_HUB_PASSWORD }} | docker login -u ${{ secrets.DOCKER_HUB_USERNAME }} --password-stdin', 'docker pull metabrainz/listenbrainz-spark-base:latest', './test.sh spark -b', './test.sh spark', 'cp listenbrainz/config.py.sample listenbrainz/config.py', 'echo ${{ secrets.DOCKER_HUB_PASSWORD }} | docker login -u ${{ secrets.DOCKER_HUB_USERNAME }} --password-stdin', 'docker-compose -f docker/docker-compose.test.yml pull lb_db redis rabbitmq couchdb', './test.sh']"
"['pip install flake8\n', 'flake8 src\n', '# We need to install numpy by hand, to prevent manual compilation on py3.5 which fails...\npip install numpy\npip install .[plot] pytest\nsudo apt-get update --yes && sudo apt-get install graphviz\n', 'cd tests && MATPLOTLIBRC=""../ci"" pytest ./\n', 'pip install build && python -m build --sdist --wheel']"
"['pip install tox', 'tox -e flake8', 'tox -e spell', 'tox -e twine', 'tox -e gitarchive', 'pip install tox', 'tox -e docs', 'pip install tox', 'tox -e mypy', './.config/ci/install.sh ${{ matrix.installmode }}', 'UT_FLAGS=""${{ matrix.flags }}"" ./.config/ci/test.sh ${{ matrix.python }} ${{ matrix.mode }}', 'pip install tox', 'pip install cryptography', 'tox -e cryptography']"
""
"['pip install .\n', 'pip install flake8\nflake8 .\n', 'pip install isort\nisort --check .\n', 'pip install black\nblack --check .\n', 'pip install mypy\nmypy --install-types --non-interactive --ignore-missing-imports -p gdown\n', 'pip install pytest\npytest tests\n', ""output=/tmp/gdown_r\ngdown https://raw.githubusercontent.com/wkentaro/gdown/3.1.0/gdown/__init__.py -O $output --quiet\ntest $(md5sum $output | awk '{print $1}') = 2a51927dde6b146ce56b4d89ebbb5268\nrm -rf $output\n"", 'output=/tmp/spam.txt\nsuccess=0\nwhile read -r file_id\ndo\n  gdown $file_id -O $output --quiet || continue\n  test $(cat $output) = spam && success=1 && break\ndone < tests/data/file_ids.csv\ntest $success = 1\nrm -rf $output\n', ""output=/tmp/large_file\nsuccess=0\nwhile IFS=, read -r file_id md5\ndo\n    gdown $file_id -O $output --quiet || continue\n    test $(md5sum $output | awk '{print $1}') = $md5 && success=1 && break\ndone < tests/data/file_ids_large.csv\ntest $success = 1\nrm -rf $output\n"", 'gdown https://github.com/wkentaro/gdown/archive/refs/tags/v4.0.0.tar.gz -O - --quiet | tar zxf -\ntest -d gdown-4.0.0\n', 'output=/tmp/folder/\nsuccess=0\nwhile IFS=, read -r folder_id md5\ndo\n  gdown $folder_id -O $output --quiet --folder || continue\n  actual_hash=$(find $output -type f -exec md5sum {} \\; | awk \'{print $1}\' | sort | md5sum | awk \'{print $1}\')\n  test $actual_hash = $md5 || echo ""$folder_id, $actual_hash"" && success=1 && break\ndone < tests/data/folder_ids.csv\ntest $success = 1\nrm -rf $output\n', 'output=/tmp/folder-limit/\ngdown https://drive.google.com/drive/folders/1gd3xLkmjT8IckN6WtMbyFZvLR4exRIkn -O $output --quiet --folder && exit 1 || exit 0\n', ""output=/tmp/file.txt\nfile_id=1TFYNzuZJTgNGzGmjraZ58ZVOh9_YoKeBnU-opWgXQL4\nmd5=6c17d87d3d01405ac5c9bb65ee2d2fc2\ngdown $file_id -O $output --quiet --format txt\nactual_hash=$(md5sum $output | awk '{print $1}')\ntest $actual_hash = $md5\n"", ""output=/tmp/file.pdf\nfile_id=1h6wQX7ATSJDOSWFEjHPmv_nukJzZD_zZ30Jvy6XNiTE\nmd5=5be20dd8a23afa06365714edc24856f3\ngdown $file_id -O $output --quiet --format pdf\nactual_hash=$(md5sum $output | awk '{print $1}')\ntest $actual_hash = $md5\n"", ""output=/tmp/file.pdf\nfile_id=13AhW1Z1GYGaiTpJ0Pr2TTXoQivb6jx-a\nmd5=96704c6c40e308a68d3842e83a0136b9\ngdown $file_id -O $output --quiet --format pdf\nactual_hash=$(md5sum $output | awk '{print $1}')\ntest $actual_hash = $md5\n"", 'rm -f dist/*.tar.gz\npython setup.py sdist\npip install dist/*.tar.gz\n']"
"['git clone https://github.com/armory3d/armory_ci\ngit clone --recursive https://github.com/armory3d/Kha.git armory_ci/Kha\ngit clone https://github.com/armory3d/iron.git armory_ci/Libraries/iron\ngit clone https://github.com/armory3d/armory.git armory_ci/Libraries/armory\ngit clone https://github.com/armory3d/nodejs_bin.git armory_ci/nodejs_bin\n', 'cd armory_ci\nnodejs_bin/node-linux64 Kha/make.js krom --shaderversion 330\n']"
"['pip3 install .[image]', 'appstreamcli validate /usr/share/metainfo/com.github.jeromerobert.pdfarranger.metainfo.xml', 'desktop-file-validate /usr/share/applications/com.github.jeromerobert.pdfarranger.desktop', 'python3 -X tracemalloc -u -m unittest discover -v -f -s tests', 'python3 -m coverage combine', 'curl -s https://codecov.io/bash | bash', 'pip3 install .[image]', 'python3 -X tracemalloc -u -m unittest discover -v -f -s tests', 'pip3 install .[image]', 'python3 -X tracemalloc -u -m unittest discover -v -f -s tests', './setup.py build', 'HOME=/root wine python setup_win32.py bdist_msi', 'HOME=/root wine python setup_win32.py bdist_zip', '/pdfarranger/pdfarranger-build']"
"['python -m pip install --upgrade pip', 'pip install coveralls tox==3.2 tox-pip-extensions==1.3.0 ephemeral-port-reserve', 'tox -i https://pypi.python.org/simple -e ${{ matrix.toxenv }}', 'python -m pip install --upgrade pip virtualenv', 'curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/', 'python -m pip install --upgrade pip', 'pip install coveralls tox==3.2 tox-pip-extensions==1.3.0 ephemeral-port-reserve', 'make k8s_itests', 'sudo apt-get update', 'sudo apt-get install -yq devscripts', 'make itest_${{ matrix.dist }}', 'mkdir -p dist/', 'python -m pip install --upgrade pip', 'pip install coveralls tox==3.2 tox-pip-extensions==1.3.0 ephemeral-port-reserve', 'tox -i https://pypi.python.org/simple -e ${{ matrix.toxenv }}', 'python setup.py sdist']"
""
"['python -m pip install -U pip\n  pip install -e .\n  pip install -r requirements_dev.txt\n', 'python -V\npip -V\n', 'make lint', 'make test', 'pylint pdfx', 'mypy pdfx', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
"['python -m montreal_forced_aligner configure --disable_auto_server', 'python -m montreal_forced_aligner server init -p test', 'cat ~/Documents/MFA/pg_mfa_test/postgresql.conf', 'cat ~/Documents/MFA/pg_init_log_test.txt', 'cat ~/Documents/MFA/pg_log_test.txt', 'pytest -x ./tests', 'python -m montreal_forced_aligner server stop -p test', 'cat ~/Documents/MFA/pg_init_log_test.txt', 'cat ~/Documents/MFA/pg_log_test.txt', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
['sudo apt-get update && sudo apt-get install liblz4-dev\nmake -C hail shadowJar HAIL_COMPILE_NATIVES=1\n']
"['python -m pip install build', 'python -m build .', 'python -m pip install -e "".[dev,examples]""', 'cd k3d && python -m pytest']"
"['pip install --upgrade pip\npip install wheel\npython setup.py bdist_wheel sdist\n', 'pip install networkx>=2\npip install ./dist/recordlinkage-*.whl\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\n# remove recordlinkage to prevent relative imports (use installed package)\n# this is like wrapping stuff in a src folder\nrm -r recordlinkage/\npytest\n', 'echo ${GITHUB_REF/refs\\/tags\\/v/}\necho ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\/v/}\n', 'python -m pip install --upgrade pip\npip install setuptools wheel\n', 'python setup.py sdist bdist_wheel\n', 'brew install pandoc\n', 'python -m pip install .[all]\n', 'python -m pip install -r docs/requirements.txt\n', 'python -m sphinx -W --keep-going --color docs/ _build/html/\n']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\n', 'tox -e `python -c ""import sys; print(\'py\' + \'\'.join(sys.version.split(\'.\')[:2]))""`\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\n', 'tox -e ${{ matrix.tox-job }}\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['docker-compose run -u root web tests_with_coverage', 'docker-compose run -u root web coverage_lcov']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install pytest\n', 'pycodestyle -v awscurl\n', 'python --version\npytest -v --cov=awscurl --cov-fail-under=77 --cov-report html\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['pip install mkdocs-material', 'mkdocs gh-deploy --force', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'wget --no-check-certificate https://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat', 'python neural_style.py --content examples/1-content.jpg --styles examples/1-style.jpg --output test-$(date +%s).png --iterations 2']"
[]
"['export PATH=$PATH:$(go env GOPATH)/bin\ngo get -u github.com/letsencrypt/pebble/...\ncd $GOPATH/src/github.com/letsencrypt/pebble && go install ./...\npebble -h || true\n', 'python -m pip install --upgrade pip\npip install virtualenv\npip install -U -r tests/requirements.txt\n', 'export PEBBLE_BIN=$(go env GOPATH)/bin/pebble\ncoverage run --source . --omit ./setup.py -m unittest tests\n', 'coverage report -m', 'python -m pip install --upgrade coveralls\ncoveralls\n', 'python -m pip install --upgrade coveralls\ncoveralls --service=github --finish\n', 'export PATH=$PATH:$(go env GOPATH)/bin\ngo get -u github.com/letsencrypt/pebble/...\ncd $GOPATH/src/github.com/letsencrypt/pebble && go install ./...\npebble -h || true\n', 'python -m pip install --upgrade pip\npip install virtualenv\npip install -U -r tests/requirements.txt\n', 'export PEBBLE_BIN=$(go env GOPATH)/bin/pebble\ncoverage run --source . --omit ./setup.py -m unittest tests\n', 'coverage report -m', 'python -m pip install --upgrade pip\npip install virtualenv\npip install -U -r tests/requirements.txt\n', 'sudo apt install sshfs\nexport SSHFS_KEYFILE=$(pwd)/sshfs_key.pem\ntouch $SSHFS_KEYFILE\nchmod 600 $SSHFS_KEYFILE\necho ""$STAGING_SSH_KEY"" > $SSHFS_KEYFILE\nmkdir -p /tmp/challenge-files\nnohup sshfs -o StrictHostKeyChecking=no,debug,IdentityFile=$SSHFS_KEYFILE -p 2299 challengeuser@staging.gethttpsforfree.com:/acme-challenge /tmp/challenge-files &\nsleep 10\nls -lah /tmp/challenge-files\n', 'export ACME_TINY_USE_STAGING=""1""\nexport ACME_TINY_DOMAIN=""staging.gethttpsforfree.com""\nexport ACME_TINY_SSHFS_CHALLENGE_DIR=""/tmp/challenge-files""\ncoverage run --source . --omit ./setup.py -m unittest tests\n', 'coverage report -m']"
""
"['sudo apt-get install -y pandoc\npython -m pip install --upgrade pip\npython -m pip install .[docs]\npython -m pip install -r ./docs/doc-requirements.txt\n', 'cd docs\nmake html\n', 'python -m pip install --upgrade pip\npython -m pip install jupyterlab~=3.0\nnpm install -g codecov\n', 'echo ""::set-output name=dir::$(npm config get cache)""\n', 'python -m pip install --upgrade --upgrade-strategy=eager -e "".[test]""\nnpm test\ncodecov\n', 'python -m pip install --upgrade pip\npython -m pip install jupyterlab~=3.0\npython -m pip install --upgrade --upgrade-strategy=eager "".[test]""\npython -m pip install jupyter_server${{ matrix.jupyter_server-version }}\n', 'git config --global user.email CI@fake.com\ngit config --global user.name ""CI""\ntmpdir=$(mktemp -d)\necho ""TEST_TMPDIR=$tmpdir"" >> $GITHUB_ENV\npushd $tmpdir\npy.test -l --cov-report xml --cov=nbdime --pyargs nbdime\n', 'git config --global user.email CI@fake.com\ngit config --global user.name ""CI""\n$hgconfig = ""[ui]`r`nusername = CI <CI@fake.com>""\n$hgconfig | Set-Content ($HOME + ""\\mercurial.ini"")\necho ""TEST_TMPDIR=."" >> $Env:GITHUB_ENV\npy.test -l --cov-report xml --cov=nbdime --pyargs nbdime\n']"
"['echo \'experimantal = true\' > /tmp/buildkitd.toml\necho \'debug = true\' >> /tmp/buildkitd.toml\necho \'insecure-entitlements  = [ ""security.insecure"" ]\' >> /tmp/buildkitd.toml\n# echo \'[worker.oci]\' >> /tmp/buildkitd.toml\n# echo \'max-parallelism = 1\' >> /tmp/buildkitd.toml\ncat /tmp/buildkitd.toml\n', 'npm install -g yarn', 'poetry run pip install --upgrade setuptools wheel\npoetry run pip install --upgrade pytest-github-actions-annotate-failures\n', 'echo -e ""$(poetry --version --ansi) is at $(which poetry)\\n$(poetry env info --ansi)\\n\\v$(poetry --ansi run pip -V)""\n', 'poetry install --no-interaction --with dev --with speedups\n', 'poetry run poe yarn', 'poetry run poe lint', 'poetry run poe yarn test', 'poetry run poe pytest', 'poetry run poe codecov --env OS=${{ runner.os }} Interpreter=""$(poetry run python -V)"" -t ${{ secrets.CODECOV_TOKEN }}', 'docker run -d --rm -p 8081:8081 --name ${{ env.DOCKER_CONTAINER_NAME }} ${{ env.DOCKER_TEST_TAG }}\necho ""Waiting ${{ env.SLEEP }} seconds for the container to start..."" && sleep ${{ env.SLEEP }}\necho ""Checking if container is alive...""\n[ $(curl http://0.0.0.0:8081/ui/get_messages -s) == ""{}"" ] && echo ""Success!"" || (echo ""Faile"" && exit 1)\necho ""Checking if we have a working home page...""\n[ $(curl http://0.0.0.0:8081 -s) ~=\'site-notification-modal\' ] && echo ""Success!"" || (echo ""Faile"" && exit 1)\necho ""Stopping and removing container...""\ndocker stop ${{ env.DOCKER_CONTAINER_NAME }}\n', 'ls -R /tmp/sickchill-wheels\n', 'poetry install\n# -n to just print the version for now\npython scmaintools -n\n', 'git config --global user.name miigotu\ngit config --global user.email miigotu@gmail.com\npoetry build\npoetry publish\n', 'npm install -g yarn', 'pip install poethepoet\npoe depup\n']"
"['python -m pip install --upgrade pip wheel\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install wheel\npython setup.py bdist_wheel sdist\n', 'python -m pip install --upgrade pip wheel\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\n', 'pytest\n']"
"['python -m pip install --upgrade pip setuptools wheel\npip install pytest\npip install -r requirements.txt\n', 'pytest --junit-xml=tests.xml\n', 'python -m pip install pip setuptools wheel\npip install pytest\npip install -r requirements.txt\n', 'pytest --runslow  --disable-warnings\n']"
"['python -m pip install .[develop]', 'python -m pip install --upgrade nox', 'nox -s test-${{matrix.python-version}}', 'echo ""JAVA17_HOME=$JAVA_HOME_17_X64"" >> $GITHUB_ENV', 'echo ""JAVA11_HOME=$JAVA_HOME_11_X64"" >> $GITHUB_ENV', 'python -m pip install --upgrade nox', 'nox -s rally_tracks_compat', 'python3 --version | grep ""Python 3.10.9""', 'python3 -m pip install --user esrally', 'esrally --version']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip tox', 'tox --skip-missing-interpreters false', 'python -m pip install --upgrade pip tox', 'tox --skip-missing-interpreters false -e docs']"
['pip install tox\ntox -- --cov meshio --cov-report xml --cov-report term\n']
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -m spacy download en_core_web_sm\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['echo ""CFLAGS=-O3 -march=native -pipe"" >> $GITHUB_ENV\nncpu=""""\ncase ""${{ runner.os }}"" in\n""Linux"")\n  ncpu=""$(nproc)""\n  echo ""DEBIAN_FRONTEND=noninteractive"" >> $GITHUB_ENV\n  echo ""TZ=Etc/UTC"" >> $GITHUB_ENV\n  ;;\n""macOS"")\n  ncpu=""$(sysctl -n hw.ncpu)""\n  ;;\n""Windows"")\n  ncpu=""${NUMBER_OF_PROCESSORS}""\n  ;;\nesac\n[[ -z ""${ncpu}"" || ${ncpu} -le 0 ]] && ncpu=""1""\necho ""ncpu=${ncpu}"" >> $GITHUB_ENV\n', 'sudo -E dpkg --add-architecture i386\nsudo -E apt-get update -qq\nsudo -E apt-get install --no-install-recommends -yq gcc-multilib \\\n  libbz2-dev:i386 libffi-dev:i386 libgdbm-dev:i386 \\\n  liblzma-dev:i386 libncurses5-dev:i386 libreadline6-dev:i386 libsqlite3-dev:i386 \\\n  libssl-dev:i386 lzma:i386 uuid-dev:i386 zlib1g-dev:i386\nmkdir -p external/bin\ncat << EOF > external/bin/gcc\n#!/bin/bash\nexec $(which gcc) -m32 -mno-adx ""\\$@""\nEOF\ncat << EOF > external/bin/g++\n#!/bin/bash\nexec $(which g++) -m32 -mno-adx ""\\$@""\nEOF\nchmod 755 external/bin/gcc external/bin/g++\necho ""${{ github.workspace }}/external/bin"" >> $GITHUB_PATH\n', 'sudo -E apt-get update -qq\nsudo -E apt-get install --no-install-recommends -yq \\\n  build-essential gdb lcov libbz2-dev libffi-dev libgdbm-dev \\\n  liblzma-dev libncurses5-dev libreadline6-dev libsqlite3-dev \\\n  libssl-dev lzma lzma-dev tk-dev uuid-dev xvfb zlib1g-dev\n', '# https://bugs.python.org/msg376705\nOLD=""/etc/ssl/openssl.cnf""\nNEW=""openssl.cnf.new""\n\necho ""openssl_conf = default_conf"" > ""${NEW}""\ncat ""${OLD}"" >> ""${NEW}""\n\ncat << EOF >> ""${NEW}""\n[default_conf]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = ssl_default_sect\n\n[ssl_default_sect]\nMinProtocol = None\nCipherString = DEFAULT:@SECLEVEL=1\nEOF\n\nsudo mv ""${NEW}"" ""${OLD}""\n', './configure\nif [[ ""${{ matrix.target.cpu }}"" == ""amd64"" ]]; then\n  make -j${ncpu} regen-all\n  changes=""$(git status --porcelain)""\n  # Check for changes in regenerated files\n  if ! test -z ""${changes}""; then\n    echo ""Generated files not up to date""\n    echo ""${changes}""\n    exit 1\n  fi\nfi\nmake -j${ncpu}\nmake pythoninfo\nmake smelly\nxvfb-run make buildbottest TESTOPTS=""-j${ncpu} -uall,-cpu -x test_ttk_guionly""\n# doc checks:\nif [[ ""${{ matrix.target.cpu }}"" == ""amd64"" ]]; then\n  cd Doc\n  python3 -m pip install -r requirements.txt\n  make check suspicious html SPHINXOPTS=""-q -j auto"" PYTHON=python2\nfi\n', './configure\nmake -j${ncpu}\nmake pythoninfo\nmake smelly\nmake buildbottest TESTOPTS=""-j${ncpu} -uall,-cpu""\n', '.\\PCbuild\\build.bat -e -p x64\n.\\tauthon.bat -m test.pythoninfo\n.\\PCbuild\\rt.bat -p x64 -q -uall -u-cpu -u-largefile -rwW --slowest -j2 -x test_concurrent_futures\n']"
"['python -m pip install --upgrade pip\npip install nose\npip install scikit-learn==${{ matrix.sklearn }}\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'nosetests\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'make -B docs\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'make -B audit\n', 'make -B test\n']"
"['echo ""versions=[\'stable\', \'latest\']"" >> ""$GITHUB_OUTPUT""', ""if ${{ matrix.version == 'latest' }}; then\n  sha=$(git rev-parse --short ${{ github.sha }})\n  git checkout ${{ github.sha }}\nelif ${{ matrix.version == 'stable' }}; then\n  tag=$(git tag | sort -V | tail -1)\n  git checkout tags/$tag\nelse\n  git checkout ${{ matrix.version }}\nfi\n"", 'sudo apt-get update\nsudo apt-get install -y pandoc\npip install -e "".[all,ci,docs]""\n', 'sphinx-build -b doctest docs $(mktemp -d)', 'sphinx-build \\\n  -b html \\\n  -D doc_version=""${{ matrix.version }}"" \\\n  -D doc_versions=""${{ join(fromJSON(needs.build-matrix.outputs.versions)) }}"" \\\n  docs \\\n  website/docs/${{ matrix.version }}\n', 'pip install -e "".[ci]""', 'python -m build', 'pip install -e "".[dev,ci]""', 'isort --check .\nblack --check .\nflake8\n', 'pip install -e "".[all,ci]""', 'pytest -v --cov-report= --cov=src/pymanopt tests/\ncoverage lcov -o coverage/lcov.info\n']"
"['python -m pip install --upgrade build twine\npython -m build\ntwine check --strict dist/*\n', 'export PKG=$(ls dist/ | grep tar)\nset -- $PKG\necho ""name=$1"" >> $GITHUB_ENV\n']"
""
"['python -m pip install --upgrade pip\npip install twine wheel\n', 'python setup.py sdist bdist_wheel', 'twine upload dist/*', 'python -m pip install --upgrade pip\npip install flake8 flake8-comprehensions flake8-bugbear\nflake8 --version\n', 'flake8 .\ncd examples && flake8 .\n', 'python -m pip install --upgrade pip\npython -m pip install scikit-image opencv-python lmdb h5py pyarrow\npython -m pip install -e .\npython -c ""import tensorpack.dataflow""\n# check that dataflow can be imported alone without tensorflow\npython -c ""import cv2; print(\'OpenCV \'+ cv2.__version__)""\n', 'python -m pip install ${{ matrix.TF-version }}\npython -c ""import tensorflow as tf; print(tf.__version__)\n', './tests/run-tests.sh']"
""
"['make frontend/node_modules', 'python -m pip install --upgrade pip\npython -m pip install tox pre-commit\npython -m pip freeze --local\n', 'tox -e lint', 'pre-commit run --show-diff-on-failure --color=always --all-files', 'make', 'make test-js', 'sudo apt-get update\nsudo apt-get -y install ffmpeg\n', 'brew install ffmpeg', 'choco install --no-progress --timeout 600 ffmpeg', ""Out-File $env:GITHUB_ENV utf8 -Append -InputObject 'PYTHONIOENCODING=utf-8'"", 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions coverage[toml]\n', 'tox', 'python -m pip install build', 'python -m build']"
"['python -m pip install --upgrade pip\n# Until the next xdis release\npip install git+https://github.com/rocky/python-xdis#egg=xdis\npip install -e .\npip install -r requirements-dev.txt\n', 'make check\n', 'python -m pip install --upgrade pip\n# Until the next xdis release\npip install git+https://github.com/rocky/python-xdis#egg=xdis\npip install -e .\npip install -r requirements-dev.txt\n', 'make check\n', 'python -m pip install --upgrade pip\n# Until the next xdis release\npip install git+https://github.com/rocky/python-xdis#egg=xdis\npip install -e .\npip install -r requirements-dev.txt\n', 'make check\n']"
""
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade tox\n', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)']"
"['echo ""Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}""\necho ""Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}""\n', 'sudo chown -Rc $UID .git/']"
"['pip install bandit black codespell flake8 isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101,B105,B106,B110,B303,B404,B603 .', 'black --check . || true', 'codespell || true', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=29 --max-line-length=167 --show-source --statistics', 'isort --check-only --profile black .', 'pip install -e .', 'mypy --ignore-missing-imports . || true', 'mv setup.cfg setup.cfg.disabled', 'pytest .', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check', 'pip install tox', 'TOXENV=py36 tox', 'TOXENV=py37 tox', 'TOXENV=py38 tox', 'TOXENV=py39 tox']"
"['python -m pip install -c constraints.txt tox twine', 'cd frontend && npm ci', 'cd frontend && npm run build', 'tox -e build-dist', 'python -m twine upload dist/*', 'python -m pip install -c constraints.txt tox', 'tox -e ${{ matrix.toxenv }}', 'cd frontend && npm ci', 'cd frontend && npm run build', 'cd frontend && npm test', 'python -m pip install -c constraints.txt pre-commit', 'cd frontend && npm ci', 'pre-commit run -a eslint', 'cd frontend; npx tsc', 'cd frontend; npx svelte-check', 'python -m pip install -c constraints.txt tox', 'tox -e lint', 'python -m pip install -c constraints.txt tox', 'tox -e docs', 'python -m pip install -c constraints.txt tox', 'tox -e pyinstaller']"
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -e .\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'sudo apt update\nsudo apt install ffmpeg\nmake tests\n']"
"['python -c ""import sys; print(sys.version)""\npython -c ""import platform, struct; print(platform.machine(), struct.calcsize(\'P\') * 8)""\npython -c ""import sys; print(sys.executable)""\npython -m pip --version\npip --version\npip config list\npip freeze\n', 'pip install .[tests]\npip list\n', 'mkdir upload', 'python make.py win64\ndir dist/*.msi | rename-item -newname {  $_.name  -replace "".msi"","" (64 bit).msi""  }\nmv dist/*.msi upload/\n', 'make macos\nmv dist/*.dmg upload/\n', 'uname -a\ncat /etc/lsb-release\npython -c ""import sys; print(sys.version)""\npython -c ""import platform, struct; print(platform.machine(), struct.calcsize(\'P\') * 8)""\npython -c ""import sys; print(sys.executable)""\npython -m pip --version\npip --version\npip list --verbose\n', 'pip install .[tests]\npip list\n', 'mkdir upload', 'xvfb-run make linux', 'cd dist/\ntar -cvf Mu_Editor-AppImage-x86_64-${{ github.sha }}.tar *.AppImage\nls -la .\n', 'pip install .[dev]', 'python -c ""import sys; print(sys.version)""\npython -c ""import platform, struct; print(platform.machine(), struct.calcsize(\'P\') * 8)""\npython -c ""import sys; print(sys.executable)""\npython -m pip --version\npip --version\npip config list\npip freeze\n', 'sudo apt-get update\nsudo apt-get install -y libxkbcommon-x11-0 xvfb\n', 'pip install .[dev]\npip list\n', 'xvfb-run make check', 'python make.py check', 'sleep 150']"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade tox tox-py\n', 'tox --py current', 'python -m pip install --upgrade pip tox\n', 'tox -e qa']"
"['pip install build', 'python -m build', 'pip install -e .[test] coveralls coverage[toml]', 'coverage run -m pytest']"
"['conda activate minimal\npython -m pip install .\npytest -svx --timeout=300\n', 'conda activate minimal\nrm -rf fixture/\npytest -svx --timeout=300 zarr/tests/test_dim_separator.py zarr/tests/test_storage.py\n# This simulates fixture-less tests in conda and debian packaging\n', 'conda create -n zarr-env python==${{matrix.python-version}} bsddb3 numcodecs lmdb pip nodejs flake8 mypy\nconda activate zarr-env\nnpm install -g azurite\n', 'conda activate zarr-env\npython -m pip install --upgrade pip\npython -m pip install -U pip setuptools wheel line_profiler\npython -m pip install -rrequirements_dev_minimal.txt numpy${{matrix.numpy_version}} -rrequirements_dev_optional.txt pymongo redis\npython -m pip install .\npython -m pip freeze\n', 'conda activate zarr-env\nmkdir ~/blob_emulator\nazurite -l ~/blob_emulator --debug debug.log 2>&1 > stdouterr.log &\npytest --cov=zarr --cov-config=pyproject.toml --doctest-plus --cov-report xml --cov=./ --timeout=300\n', ""python -m pip install 'build!=0.1' setuptools-scm\n"", 'python -m build\ngit describe\npwd\nif [ -f dist/zarr-0.0.0.tar.gz ]; then\n  echo ""WRONG VERSION NUMBER""\n  exit 1\nelse\n  echo ""All seem good""\nfi\n', 'ls\nls dist\n', 'conda create -n zarr-env python==${{matrix.python-version}} numcodecs pip nodejs\n', 'conda activate zarr-env\npython -m pip install --upgrade pip\npython -m pip install -U pip setuptools wheel\npython -m pip install -r requirements_dev_numpy.txt -r requirements_dev_minimal.txt -r requirements_dev_optional.txt\npython -m pip install .\npython -m pip freeze\nnpm install -g azurite\n', 'conda activate zarr-env\nmkdir ~/blob_emulator\nazurite -l ~/blob_emulator --debug debug.log 2>&1 > stdouterr.log &\npytest -sv --timeout=300\n', 'conda info', 'conda list']"
"['pip install black flake8 isort', 'auth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', '# pip install flake8\n# stop the build if there are Python syntax errors or undefined names\n# flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n# flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install -e .\n', 'pip install -U pytest setuptools wheel twine\npytest\n', 'python setup.py sdist\ntwine check dist/*\n', 'python setup.py bdist_wheel\ntwine check dist/*\n', 'twine upload --skip-existing dist/*\n', 'twine upload --skip-existing dist/*\n', 'twine upload --skip-existing dist/*\n', 'twine upload --skip-existing dist/*\n']"
""
"['pip install tox', 'tox -e lint', 'pip install tox', 'tox -e py', 'tox -e coverage-report', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', '# GH checkout action doesn\'t preserve tag annotations, we must fetch them\n# https://github.com/actions/checkout/issues/290\ngit fetch --tags --force\n# strip leading \'refs/tags/\' to get the tag name\nTAG_NAME=""${GITHUB_REF##*/}""\n# Dump tag message to temporary .md file (excluding the PGP signature at the bottom)\nTAG_MESSAGE=$(git tag -l --format=\'%(contents)\' $TAG_NAME | sed -n \'/-----BEGIN PGP SIGNATURE-----/q;p\')\necho ""$TAG_MESSAGE"" > ""${{ runner.temp }}/release_notes.md""\n# if the tag has a pre-release suffix mark the Github Release accordingly\nif egrep -q ""$PRERELEASE_TAG_PATTERN"" <<< ""$TAG_NAME""; then\n  echo ""Tag contains a pre-release suffix""\n  echo ""IS_PRERELEASE=true"" >> ""$GITHUB_ENV""\nelse\n  echo ""Tag does not contain pre-release suffix""\n  echo ""IS_PRERELEASE=false"" >> ""$GITHUB_ENV""\nfi\n', 'if [ ""$IS_PRERELEASE"" == true ]; then\n  echo ""DEBUG: This is a pre-release""\nelse\n  echo ""DEBUG: This is a final release""\nfi\npython setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['git checkout HEAD^2', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends \\\n  build-essential \\\n  cmake \\\n  check \\\n  cython3 \\\n  libcurl4-openssl-dev \\\n  libemu-dev \\\n  libev-dev \\\n  libglib2.0-dev \\\n  libloudmouth1-dev \\\n  libnetfilter-queue-dev \\\n  libnl-3-dev \\\n  libpcap-dev \\\n  libssl-dev \\\n  libtool \\\n  libudns-dev \\\n  python3 \\\n  python3-dev \\\n  python3-bson \\\n  python3-yaml \\\n  python3-boto3 \\\n  fonts-liberation\n', 'DOCKER_IMAGE=dinotools/dionaea\nVERSION=noop\nif [ ""${{ github.event_name }}"" = ""schedule"" ]; then\n  VERSION=nightly\nelif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/}\nelif [[ $GITHUB_REF == refs/heads/* ]]; then\n  VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r \'s#/+#-#g\')\n  if [ ""${{ github.event.repository.default_branch }}"" = ""$VERSION"" ]; then\n    VERSION=edge\n  fi\nelif [[ $GITHUB_REF == refs/pull/* ]]; then\n  VERSION=pr-${{ github.event.number }}\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION}""\nif [[ $VERSION =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n  MINOR=${VERSION%.*}\n  MAJOR=${MINOR%.*}\n  TAGS=""$TAGS,${DOCKER_IMAGE}:${MINOR},${DOCKER_IMAGE}:${MAJOR},${DOCKER_IMAGE}:latest""\nelif [ ""${{ github.event_name }}"" = ""push"" ]; then\n  TAGS=""$TAGS,${DOCKER_IMAGE}:sha-${GITHUB_SHA::8}""\nfi\necho ::set-output name=version::${VERSION}\necho ::set-output name=tags::${TAGS}\necho ::set-output name=created::$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')\n']"
"['pip install --requirement requirements-dev.txt', 'pip install --editable .', 'pytest --ignore=tests/test_pattern_matching.py', 'pytest tests/test_pattern_matching.py', 'flake8', 'flake8 --extend-exclude tests/test_pattern_matching.py', 'mypy', 'pip install --upgrade build pip setuptools wheel\npython -m build\n']"
""
"[""echo ::set-output name=value::$(echo ${{ steps.latest_tag.outputs.tag }} | sed -r 's|([0-9]+.[0-9]+.[0-9]+).*|\\1|g')"", 'echo ${{ steps.semantic.outputs.value }}', 'echo ""::set-output name=value::$(($(git tag | grep -c ${{ steps.semantic.outputs.value }}.pre) + 1))""', 'echo ""::set-output name=value::${{ steps.semantic.outputs.value }}.pre${{ steps.build_number.outputs.value }}""', 'echo ""::set-output name=value::$(($(git tag | grep -c ${{ steps.semantic.outputs.new_release_version }}.rc) + 1))""', 'echo ""::set-output name=value::${{ steps.semantic.outputs.new_release_version }}.rc${{ steps.build_number.outputs.value }}""', 'echo ""value=$(echo ""${{ needs.prepare.outputs.release }}"" | sed ""s/\\./_/g"").md"" >> $GITHUB_OUTPUT', 'git config user.email ""azory@ydata.ai""\ngit config user.name ""Azory YData Bot""\ngit config core.autocrlf false\n', 'cd docsrc/source/pages/reference\nsed -i -e \'s/## \\[.*/### Changelog ${{ needs.prepare.outputs.release }}/g\' \\\n    -e \'s/### Bug Fixes/#### ðŸ› Bug fixes/g\' \\\n    -e \'s/### Features/#### ðŸŽ‰ Features/g\' \\\n    -e \'s/### BREAKING CHANGES/#### ðŸš¨ Breaking changes/g\' \\\n    -e \'s/### Documentation/#### ðŸ“– Documentation/g\' \\\n    changelog/${{ steps.path.outputs.value }}\n\ngrep -q "".. include:: changelog/${{ steps.path.outputs.value }}"" changelog.rst || sed -i ""4 a\\\\\n.. include:: changelog\\/${{ steps.path.outputs.value }}\\\\\n   :parser: myst_parser.sphinx_\\\\\n\n"" changelog.rst\n', 'if [[ `git status --porcelain` ]]; then\n  echo ""CHANGES=true"" >> $GITHUB_OUTPUT\n  git add docsrc/source/pages/reference/changelog/${{ steps.path.outputs.value  }}\n  git commit -m ""[skip ci] Update changelogs"" -a\nelse\n  echo ""CHANGES=false"" >> $GITHUB_OUTPUT\nfi\n', 'git config user.email ""azory@ydata.ai""\ngit config user.name ""Azory YData Bot""\ngit config core.autocrlf false\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip install -r requirements-test.txt\n', 'make install', 'make lint', 'if [[ `git status --porcelain --untracked-files=no` ]]; then\n  echo ""CHANGES=true"" >> $GITHUB_OUTPUT\n  git add -u\n  git commit -m ""fix(linting): code formatting"" -a\nelse\n  echo ""CHANGES=false"" >> $GITHUB_OUTPUT\nfi\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip install -r requirements-test.txt\n', 'make install', 'make lint', 'make package version=${{ github.event.inputs.version }}', 'echo ""value=${GITHUB_REF#refs/*/}"" >> $GITHUB_OUTPUT', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip install -r requirements-test.txt\n', 'make install', 'make lint', 'make package version=${{ steps.version.outputs.value }}', 'git config user.email ""azory@ydata.ai""\ngit config user.name ""Azory YData Bot""\ngit config core.autocrlf false\n', 'pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-test.txt\n', 'make install\n', 'make examples', 'make docs', 'mv docs master\nmkdir docs\nmv master docs/master\nmv examples/ master\nmkdir examples\nmv master examples/master\ngit add -f docs/master\ngit add -f examples/master\ngit stash push -- docs/master examples/master\ngit fetch origin gh-pages\ngit checkout -f gh-pages\ntouch .nojekyll\ngit add .nojekyll\nrm -rf docs/master\nrm -rf examples/master\ngit add docs/\ngit add examples/\ngit stash apply\ngit commit -m ""[skip ci] Updating documentation and examples"" -a || true\n', 'echo ""sonar.projectKey=${{ github.event.repository.name }}"" > sonar-project.properties\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements.txt ""${{ matrix.pandas }}"" ""${{ matrix.numpy }}""\npip install -r requirements-test.txt\n', 'make install', 'make test', 'pip install --upgrade pip setuptools wheel\npip install -r requirements.txt ""${{ matrix.pandas }}"" ""${{ matrix.numpy }}""\npip install -r requirements-test.txt\n', 'make install', 'make test_cov', 'pip install --upgrade pip setuptools wheel\npip install -r requirements.txt ""${{ matrix.pandas }}"" ""${{ matrix.numpy }}""\npip install -r requirements-test.txt\n', 'make install', 'make test_cov', 'codecov -F py${{ matrix.python-version }}-${{ matrix.os }}-${{ matrix.pandas }}-${{ matrix.numpy }}', 'pip install --upgrade pip setuptools wheel\npip install pytest-spark>=0.6.0 pyarrow==1.0.1 pyspark==""${{ matrix.spark }}""\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install ""${{ matrix.pandas }}"" ""${{ matrix.numpy }}""\n', 'echo ""ARROW_PRE_0_15_IPC_FORMAT=1"" >> $GITHUB_ENV', 'echo ""SPARK_LOCAL_IP=127.0.0.1"" >> $GITHUB_ENV', 'make install', 'make install-spark-ci', 'make test_spark']"
""
""
"['pip install flake8 flake8-import-order sphinx rstcheck[sphinx] doc8', 'pip install .', 'flake8 .', ""doc8 $(git ls-files '*.rst')"", ""rstcheck --ignore-directives automodule $(git ls-files '*.rst')"", ""yamllint --strict $(git ls-files '*.yaml' '*.yml')"", 'make -C docs html', 'make -C docs linkcheck', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'pip install coverage', 'pip install .', 'echo -e ""[run]\\nrelative_files = True"" > .coveragerc', 'coverage run -m unittest discover']"
[]
"['python -m pip install --upgrade pip\npip install --editable .\npip install flake8 pytest pytest-cov\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'make pytest\n', 'python -m pip install --upgrade pip\npip install --editable .\npip install flake8 pytest pytest-cov coveralls\n', 'make pytest\ncoveralls --service=github\n']"
"['npm install', 'npm test', 'npm install', 'npm run lint']"
""
"['python -m pip install -U build pip setuptools\npython -m pip install -U -r requirements.txt\npython -m build --sdist\n', 'python -m mypy $MODULE_NAME\n', 'rm -rf $MODULE_NAME\n', 'python -m pip freeze > installed.txt\npython -m pip uninstall -y -r installed.txt\n', 'SDIST=$(python -c ""import os;print(os.listdir(\'./dist\')[-1])"" 2>&1)\npip install dist/$SDIST\n', 'python -c ""import $MODULE_NAME"" -Werror\n', 'python -m pip install -U -r requirements.txt\n', 'python -m pytest --pyargs $MODULE_NAME -Werror\n']"
""
"['python -m pip install --upgrade pip\npip install build\npip install wheel\n', 'project_version=$(python3 setup.py --version)\necho ""project_version=$project_version"" >> $GITHUB_OUTPUT\n', 'rm -Rf build *.egg-info/ && python3 setup.py sdist bdist_wheel', 'git fetch origin\ngit checkout release\ngit pull origin release\ngit checkout master\ngit pull origin master\ngit merge release --ff-only\ngit push origin master\n']"
"['echo ""CI_REPOSITORY_NAME=$CI_REPOSITORY_NAME""\n', 'REPO=$CI_REPOSITORY_NAME\nOWNER=""$(echo ""${{ github.repository_owner }}"" | tr \'[:upper:]\' \'[:lower:]\')""\nDOCKER_IMAGE=${OWNER}/${REPO}\nif [ ""$CI_REF_NAME"" == ""master"" ];then VERSION=latest;fi\nif [ ""$CI_REF_NAME"" == ""dev"" ];then VERSION=mightly;fi\nif [ ""$CI_REF_NAME"" == ""dockserver"" ];then VERSION=dockserver;fi\nTAGS=""${DOCKER_IMAGE}:${VERSION}""\necho ::set-output name=tags::${TAGS}\necho ::set-output name=title::${GITHUB_REPOSITORY}\necho ::set-output name=version::${VERSION}\necho ::set-output name=created::$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')\n', 'echo ${{ steps.docker_build.outputs.digest }}']"
"['python -m pip install --upgrade pip\necho ""deb https://deb.torproject.org/torproject.org focal main"" | sudo tee -a /etc/apt/sources.list.d/torproject.list\nwget -O - -o /dev/null https://deb.torproject.org/torproject.org/A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89.asc | gpg --import\ngpg --export A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89 | sudo apt-key add -\nsudo apt update\nsudo apt install python3-setuptools python3-tk tor -y\npip install pytest pytest-cov pyyaml coveralls\npip install -r requirements-dev.txt\n', 'py.test --cov=src --cov-report=xml tests/\n']"
"['python -m pip install -U pip\npython -m pip install -U build twine wheel\n', 'python -m build\ntwine check --strict dist/*\n', 'python -m pip install -U pip\npython -m pip install -U tox\n', 'tox -e py\n']"
"['conda config --add pinned_packages python=${{ matrix.python-version }}\nmamba env update --file environment.yaml --name test\nmamba env update --file environment_dev.yaml --name test\npip install --no-cache-dir --no-deps -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'pytest --cov=./ --cov-report=xml\n', 'sudo apt-get install glpk-utils\n', 'brew install glpk hdf5\n', 'choco install glpk\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\npip install -e .[dev]\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'pytest\n', 'python setup.py sdist\n']"
"['conda info', 'conda install pip\npip install --upgrade setuptools\npip install -r dev_requirements.txt\npip install -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --show-source --statistics\n', 'py.test --cov=./\n', 'python -m pip install --upgrade pip\npip install --upgrade setuptools\npip install --upgrade wheel\npip install -r dev_requirements.txt\npip install -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --show-source --statistics\n', 'py.test --cov=./\n', 'python setup.py sdist bdist_wheel\n']"
"['echo ""dir=$(yarn cache dir)"" >> $GITHUB_OUTPUT', 'sudo apt-get update\npython -m pip install --upgrade pip\npython -m pip install wheel\n\nsudo apt-get install gettext\n\n# for wxPython\nsudo apt install libnotify4\nsudo apt install glib-networking libsdl2-dev libsdl2-2.0-0\n\n# for PyGObject\nsudo apt install libgirepository1.0-dev libcairo2-dev\n\n# for shapely\nsudo apt install build-essential libgtk-3-dev\n\nuname -a\npython --version\npython -m pip --version\npython -m pip debug\n\npython -m pip install pycairo\npython -m pip install PyGObject\n\npython -m pip install https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-20.04/wxPython-4.1.1-cp38-cp38-linux_x86_64.whl\n\npython -m pip install -r requirements.txt\npython -m pip install pyinstaller\n\n# scipy gives us a ELF error when stripped\nsudo apt-get install gcc g++ gfortran python3-dev libopenblas-dev liblapack-dev\npython -m pip uninstall --yes scipy\npython -m pip install scipy --no-binary scipy\n\necho ""${{ env.pythonLocation }}\\bin"" >> $GITHUB_PATH\n', 'make dist\n', 'git config --system core.longpaths true\npython -m pip install --upgrade pip\npython -m pip install wheel\n# scipy 1.9.1 is the last version to support 32-bit windows\npython -m pip install scipy==1.9.1\npython -m pip install wxpython==4.1.1\npython -m pip install -r requirements.txt\npython -m pip install pyinstaller\n\necho ""${{ env.pythonLocation }}\\bin"" >> $GITHUB_PATH\n', 'make dist\n', 'bash bin/build-windows-installer\n', 'brew update\n\nbrew install gtk+3 pkg-config gobject-introspection geos libffi gettext || true\n\nexport LDFLAGS=""-L/usr/local/opt/libffi/lib""\nexport PKG_CONFIG_PATH=""/usr/local/opt/libffi/lib/pkgconfig""\n\n# for msgfmt\necho ""/usr/local/opt/gettext/bin"" >> $GITHUB_PATH\n\necho ""GI_TYPELIB_PATH=/usr/local/lib/girepository-1.0/"" >> $GITHUB_ENV\n\npip install --upgrade pip\npip --version\npip install wheel\npip install PyGObject\npip install wxpython==4.1.1\npip install -r requirements.txt\n# with --no-binary argument may fix notary issues as well shapely speedups error issue\npip install -U lxml --no-binary lxml\npip uninstall --yes shapely\npip install -v -U Shapely==1.8.5 --no-binary Shapely\npip install pyinstaller\n\necho ""${{ env.pythonLocation }}/bin"" >> $GITHUB_PATH\n', 'make dist\n', 'if [[ ""${GITHUB_REF}"" =~ ^refs/tags/v[0-9.]+$ ]]; then\n  tag=""${GITHUB_REF#refs/tags/}""\n  echo ""release_tag=${tag}"" >> $GITHUB_ENV\n  echo ""prerelease=false"" >> $GITHUB_ENV\n  echo ""title=${tag}"" >> $GITHUB_ENV\nelse\n  branch=""${GITHUB_REF#refs/heads/}""\n  tag=""dev-build-$(echo $branch | tr / -)""\n  echo ""release_tag=${tag}"" >> $GITHUB_ENV\n  echo ""prerelease=true"" >> $GITHUB_ENV\n  echo ""title=development build of $branch"" >> $GITHUB_ENV\nfi\n', 'git submodule update --init --recursive\n', 'git config --global user.email ""inkstitch-crowdin@lex.gd""\ngit config --global user.name ""Ink/Stitch Crowdin integration""\n\nwget --quiet https://downloads.crowdin.com/cli/v2/crowdin-cli.zip\nunzip -j crowdin-cli.zip\n\nsudo apt-get update\nsudo apt install gettext\n# for wxPython\nsudo apt install glib-networking libsdl2-dev libsdl2-2.0-0\n# for PyGObject\nsudo apt install libgirepository1.0-dev libcairo2-dev\n# for shapely\nsudo apt install libgeos-dev build-essential libgtk-3-dev\n\npython -m pip install --upgrade pip\npython -m pip install wheel\npython -m pip install pycairo\npython -m pip install PyGObject\n\npython -m pip install https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-20.04/wxPython-4.1.1-cp38-cp38-linux_x86_64.whl\n\npython -m pip install -r requirements.txt\npython -m pip install Babel\n\nmake messages.po\necho ""uploading messages.po to crowdin""\njava -jar crowdin-cli.jar -v upload -b main\n\necho ""downloading new translations""\njava -jar crowdin-cli.jar -v pull -b main\n\n# Try to only commit if translations changed.  Crowdin will update all\n# files when a new translation string is added but we don\'t need to\n# commit those until folks actually translate the new strings.\nif git diff translations | grep -qE \'^[-+]msgstr "".+""$\'; then\n  make electron/src/renderer/assets/translations.json\n  git add translations electron/src/renderer/assets/translations.json\n  git commit -m ""new translations from Crowdin""\n  git push https://github.com/inkstitch/inkstitch main\nfi\n']"
"['sudo apt-get update -qq \nsudo apt-get install -qq -y --no-install-recommends fakeroot rename\n', './scripts/build_deb_package.sh', 'sudo dpkg -i ${{ env.DIST_DIR_NAME }}/${{ env.BIN_NAME }}_*.deb', 'echo ""TCCONFIG_VERSION=$(grep -Po ""(?<=__version__ = \\"")\\d+\\.\\d+\\.\\d+(?=\\"")"" tcconfig/__version__.py)"" >> $GITHUB_ENV', 'set -x\n\nWORK_DIR=work\nmkdir -p ""$WORK_DIR"" ""$SHA_DIR""\n\nfor asset in $(gh release view v${TCCONFIG_VERSION} --json assets | jq \'.assets[] | select(.contentType != ""text/plain"") | .url\'); do\n  wget -P ""$WORK_DIR"" ""$asset""\ndone\n\ncd ""$DIST_DIR_NAME""\nsha256sum ${BIN_NAME}_* > ""../${SHA_DIR}/${SHA_TEXT_FILE}""\n', 'python -m pip install --upgrade --disable-pip-version-check ""pip>=21.1""', 'echo ""::set-output name=dir::$(pip cache dir)""', 'python -m pip install --upgrade tox\n', 'tox -e py\n']"
"['python -m pip install --upgrade pip\npip install .\npip install mock\npython ./setup.py install\n', 'python -m unittest discover\n', 'example/import_from_python/import.py\n', 'shellpy example/allinone/test.spy\n', 'shellpy3 example/allinone/test3.spy\n']"
"['poetry install', 'poetry run mypy .', 'poetry run pytest']"
""
"['npm install -g yarn', 'yarn --cwd netbox/project-static', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pycodestyle coverage tblib\n', 'mkdocs build', 'python netbox/manage.py collectstatic --no-input', 'pycodestyle --ignore=W504,E501 --exclude=node_modules netbox/', 'yarn --cwd netbox/project-static validate', 'scripts/verify-bundles.sh', 'coverage run --source=""netbox/"" netbox/manage.py test netbox/ --parallel', 'coverage report --skip-covered --omit *migrations*']"
"['make deps', 'make clean && make test']"
[]
"['python -m pip install --upgrade pip\npython -m pip install -r .github/lint-requirements.txt\n', 'echo ""::add-matcher::.github/flake8_matcher.json""\nflake8 --count --statistics --show-source --append-config=tox.ini .\n', 'python -m pip install --upgrade pip\npython -m pip install black==22.3.0\n', 'black --diff --check .\n', 'python -m pip install --upgrade pip\npython -m pip install pytest pytest-randomly\n', 'pytest']"
"['curl -sSL https://install.python-poetry.org | python3 -\n', 'poetry build']"
"['message=$(echo ""$TITLE"" | grep -oP \'[{\\[][^}\\]]+[}\\]]\' | sed \'s/{\\|}\\|\\[\\|\\]//g\')\necho ""message=$message"" >> $GITHUB_ENV\nif [ -z $message ]; then\n    echo ""message=$(echo \'Thank you for your contribution! We will review the pull request and get back to you soon.\')"" >> $GITHUB_ENV\nfi\n']"
"['docker build -t vmaf .\n', 'python -m pip install --upgrade pip\npip install meson\n', 'sudo apt-get update\nsudo -E apt-get -yq install ninja-build gcc nasm\n', 'brew install -q ninja nasm\n', 'meson setup libvmaf libvmaf/build --buildtype release\nsudo ninja -vC libvmaf/build install\n', 'git clone -q --branch master --depth=1 ""https://github.com/FFmpeg/FFmpeg"" ffmpeg\ncd ffmpeg\n./configure --enable-version3 --enable-libvmaf --cc=""$CC"" --cxx=""$CXX"" || { less ffbuild/config.log; exit 1; }\n', 'sudo make -C ffmpeg --quiet -j $(getconf _NPROCESSORS_ONLN 2>/dev/null || sysctl -n hw.ncpu) install\n', 'curl ""https://gist.githubusercontent.com/1480c1/0c4575da638ef6e8203feffd0597de16/raw/akiyo_cif.tar.xz.base64"" | base64 -d | tar xJ\nvmaf_score=$(ffmpeg -hide_banner -nostats -i encoded.mkv -i orig.mkv -filter_complex libvmaf -f null - 2>&1 | grep \'VMAF score\' | tr \' \' \'\\n\' | tail -n1)\necho ""$vmaf_score""\nif [[ $vmaf_score != ""93.663925"" ]]; then\n  echo ""vmaf score doesn\'t match 93.663925""\n  exit 1\nelse\n  echo ""vmaf score matches""\n  exit 0\nfi\n', 'python -m pip install --upgrade pip\npip install meson\n', 'sudo apt-get update\nsudo -E apt-get -yq install ccache ninja-build\ncase ""$CC"" in\n*gcc) sudo -E apt-get -yq install gcc g++ nasm ;;\n*gcc-9)\n  sudo -E apt-add-repository -y ""ppa:ubuntu-toolchain-r/test""\n  sudo -E apt-get -yq install gcc-9 g++-9 nasm\n  ;;\n*clang) sudo -E apt-get -yq install clang nasm ;;\nesac\n$CC --version\nmeson --version\nccache --version\n', 'brew install -q ninja nasm ccache\n\n$CC --version\nmeson --version\nccache --version\n', 'meson setup libvmaf libvmaf/build --buildtype release --prefix $PWD/install -Denable_float=true\n', 'sudo ninja -vC libvmaf/build install\n', 'sudo ninja -vC libvmaf/build test\n', ""mkdir -p ~/.ccache && sudo chown -R $(whoami) ~/.ccache\npip install 'tox<4'\ntox -c python/ -e py -- -v -p no:warnings -m 'main or lib' --doctest-modules\n"", 'ldd ""./install/bin/vmaf"" || true\necho ""::set-output name=path::./install/bin/vmaf""\necho ""::set-output name=upload_url::$(curl -L https://api.github.com/repos/${{ github.repository }}/releases/tags/$(cut -d/ -f3 <<< ${{ github.ref }}) | jq -r .""upload_url"")""\n', 'echo ""name=CCACHE_DIR::$PWD/.ccache"" >> $GITHUB_ENV', 'meson setup libvmaf libvmaf/build --buildtype release --default-library static --prefix ""$MINGW_PREFIX""', 'meson install -C libvmaf/build', 'meson test -C libvmaf/build --num-processes $(nproc)', 'ldd ""$MINGW_PREFIX/bin/vmaf.exe"" || true\necho ""::set-output name=path::$(cygpath -m ""$(command -v vmaf)"")""\necho ""::set-output name=upload_url::$(curl -L https://api.github.com/repos/${{ github.repository }}/releases/tags/$(cut -d/ -f3 <<< ${{ github.ref }}) | jq -r .""upload_url"")""\n']"
[]
"['python3.7 -m pip install docker pytest', 'bash scripts/build-push.sh', 'python -m pip install --upgrade pip\npython -m pip install poetry\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'bash scripts/test.sh']"
"['python -m pip install --upgrade pip\nif [ -f test/requirements.txt ]; then pip install -r test/requirements.txt; fi\n', 'make --keep-going THEMIS_VIM=${{ steps.vim.outputs.executable }} test lint']"
"[""python -m pip install --upgrade pip wheel\npython -m pip install -e '.[check]'\n"", 'make download\n', 'python -m pytest tests --verbose --cov=textacy --cov-report=term-missing\n', ""python -m pip install --upgrade pip wheel\npython -m pip install -e '.[check]'\n"", 'python -m black --diff src\n', 'python -m isort --diff src\n', 'python -m ruff check --exit-zero src\n', ""python -m pip install --upgrade pip wheel\npython -m pip install -e '.[check]'\n"", 'python -m mypy --install-types --non-interactive src\n', ""python -m pip install --upgrade pip wheel\npython -m pip install -e '.[docs]'\n"", 'cd docs && make ${{ matrix.build-type }}\n', 'python -m pip install --upgrade pip\npip install build wheel\n', 'python -m build --sdist --wheel\n']"
"['python -m pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install tox', 'tox -e format', 'sudo apt-get update\nsudo apt-get install libssl-dev libxmlsec1 libxmlsec1-dev libxmlsec1-openssl libxslt1-dev pkg-config\n', 'brew install libxmlsec1 libxslt pkgconfig\n', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'mkdir .coverage-data && mv .coverage.* .coverage-data/', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e coverage-report']"
"['python -m pip install . -r requirements.txt\n', 'flake8 --config .flake8 httpobs', 'nosetests httpobs/tests -e insert_test_result -e scored_test -e select_test_results -e test_retrieve --with-coverage --cover-package=httpobs']"
"['pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.check }}\n', 'pip install --upgrade pip\npip install tox\n', 'tox -e py', 'sudo apt-get update\nsudo apt-get install libmemcached-dev\n', 'pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'python setup.py sdist', 'python setup.py bdist_wheel', 'ls -l dist/*\ntwine upload dist/*\n']"
"[""pip install dredd_hooks\npip install 'PyYAML>=5.1'\npip install 'six>=1.13.0'\n"", 'yarn install --ignore-scripts', 'yarn test-api', 'yarn install --ignore-scripts', 'mv ./.github/build-themes-check.sh ./themes-default/slim\ncd ./themes-default/slim\nbash build-themes-check.sh\n', 'yarn lint && yarn lint-css', 'yarn test', 'yarn coverage', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'python ./.github/check_version.py']"
[]
"['nox --non-interactive --error-on-missing-interpreter --session github_actions_default_tests', 'nox --non-interactive --error-on-missing-interpreter --session github_actions_all_tests', 'python -m pip install --disable-pip-version-check .\n', 'nox --non-interactive --error-on-missing-interpreter --session ""tests-${{ matrix.python-version }}"" -- --full-trace', 'python -m pip install --disable-pip-version-check .\n', 'nox --non-interactive --error-on-missing-interpreter --session ""lint""', 'python -m pip install --disable-pip-version-check .\n', 'nox --non-interactive --error-on-missing-interpreter --session ""docs""', 'pipx run build']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'make merge_unihan\n']"
"['python -m pip install --upgrade pip\npip install flake8 flake8-docstrings flake8-debugger flake8-bugbear pytest\n', 'pip install -e .\n', 'flake8 satpy/\n', 'echo ""DATE=$(date +\'%Y%m%d\')"" >> $GITHUB_ENV\nCONDA_PREFIX=$(python -c ""import sys; print(sys.prefix)"")\necho ""CONDA_PREFIX=$CONDA_PREFIX"" >> $GITHUB_ENV\n', 'mamba env update -n test-environment -f continuous_integration/environment.yaml', 'python -m pip install \\\n--index-url https://pypi.anaconda.org/scipy-wheels-nightly/simple/ \\\n--trusted-host pypi.anaconda.org \\\n--no-deps --pre --upgrade \\\nmatplotlib \\\nnumpy \\\npandas \\\nscipy; \\\npython -m pip install \\\n--no-deps --upgrade \\\ngit+https://github.com/dask/dask \\\ngit+https://github.com/dask/distributed \\\ngit+https://github.com/zarr-developers/zarr \\\ngit+https://github.com/Unidata/cftime \\\ngit+https://github.com/rasterio/rasterio \\\ngit+https://github.com/pydata/bottleneck \\\ngit+https://github.com/pydata/xarray \\\ngit+https://github.com/astropy/astropy;\nLD_PRELOAD=$(python -c ""import sys; print(sys.prefix)"")/lib/libstdc++.so\necho ""LD_PRELOAD=${LD_PRELOAD}"" >> $GITHUB_ENV\n', 'python -m pip install --no-deps -e .\n', 'export LD_PRELOAD=${{ env.LD_PRELOAD }};\npytest --cov=satpy satpy/tests --cov-report=xml --cov-report=\n', 'export LD_PRELOAD=${{ env.LD_PRELOAD }};\ncoverage run --source=satpy -m behave satpy/tests/features --tags=-download\ncoverage xml\n', 'python setup.py sdist']"
"['sudo apt-get --assume-yes update\nsudo apt-get --assume-yes install asciidoctor\n', 'python setup.py sdist', 'tools/win_provision.sh asciidoc', 'tools/win_build_librsync.sh ${{ matrix.arch }} ${WIN_LIBRSYNC_VERSION}', 'tools/win_build_rdiffbackup.sh ${{ matrix.arch }} ${WIN_PYTHON_VERSION}', 'tools/win_package_rdiffbackup.sh ${{ matrix.arch }}', 'ls -la\nif [[ ${{ matrix.arch }} == *64 ]]; then bits=64; else bits=32; fi \n7z x rdiff-backup-*.win${bits}exe.zip\ncd rdiff-backup-*-${bits}\npwd\nls -la\n./rdiff-backup.exe --help\n./rdiff-backup.exe info\n./rdiff-backup.exe -v5 backup . ../bak${bits}\n./rdiff-backup.exe -v5 verify ../bak${bits}\n./rdiff-backup.exe -v5 restore ../bak${bits} to\nls -la to/\n', 'echo artifact_files=${{ steps.download.outputs.download-path }}/*.* >> $GITHUB_OUTPUT', 'sudo python -m pip install --upgrade pip\nsudo pip install twine\n', ""twine upload --skip-existing --verbose dist/rdiff*.{whl,tar.gz}\n# old versions don't understand --non-interactive\n"", ""twine upload --skip-existing --verbose dist/rdiff*.{whl,tar.gz}\n# old versions don't understand --non-interactive\n"", 'sudo gem install bundler\ncd docs\nbundle install\nbundle exec jekyll build --baseurl ""${{ steps.pages.outputs.base_path }}"" .\n', 'sudo apt install librsync-dev libacl1-dev rdiff asciidoctor\nsudo pip3 install --upgrade -r requs/base.txt -r requs/optional.txt -r requs/test.txt\n', 'export RUN_COMMAND=\nexport SUDO=sudo\nmake test\n', 'tools/win_provision.sh asciidoc', 'tools/win_build_librsync.sh ${{ matrix.arch }} ${WIN_LIBRSYNC_VERSION}', 'tools/win_test_rdiffbackup.sh ${{ matrix.arch }} ${WIN_PYTHON_VERSION}']"
"['python -m pip install --upgrade pip\npip install flake8 pyinstaller\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=60 --max-line-length=127 --statistics\n', 'sh build-pkgs.sh\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 office365 --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 office365 --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'echo ""${{env.office365_python_sdk_securevars}}""\npytest\n']"
"['python -m pip install --upgrade pip\npip install .\npip install -r requirements-test.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 ecs_deploy tests --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 ecs_deploy tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['python3.7 -m pip install docker pytest', 'bash scripts/build-push.sh', 'python -m pip install --upgrade pip\npython -m pip install poetry\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry run bash scripts/test.sh']"
"['$env:PATH += "";C:\\msys64\\usr\\bin""\npacman -S --noconfirm --needed bison flex\n', 'git clone -b v1.2.11 --depth 1 https://github.com/madler/zlib.git\ncd zlib\ncmake -DCMAKE_INSTALL_PREFIX:PATH=C:\\dep\\zlib -G ""Visual Studio 16 2019"" .\ncmake --build . --config Release --target ALL_BUILD\ncmake --build . --config Release --target INSTALL\ncopy C:\\dep\\zlib\\lib\\zlibstatic.lib C:\\dep\\zlib\\lib\\zdll.lib\ncopy C:\\dep\\zlib\\bin\\zlib.dll C:\\dep\\zlib\\lib\n', 'git clone -b REL_14_STABLE https://github.com/postgres/postgres.git', 'git clone -b master --depth 1 https://github.com/postgrespro/ptrack.git\nCopy-Item -Path ptrack -Destination postgres\\contrib -Recurse\n(Get-Content ptrack\\patches\\REL_14_STABLE-ptrack-core.diff -Raw).Replace(""`r`n"",""`n"") | Set-Content ptrack\\patches\\REL_14_STABLE-ptrack-core.diff -Force -NoNewline\ncd postgres\ngit apply -3 ../ptrack/patches/REL_14_STABLE-ptrack-core.diff\n', '$env:PATH += "";C:\\msys64\\usr\\bin""\ncd postgres\\src\\tools\\msvc\n(Get-Content config_default.pl) -Replace ""zlib *=>(.*?)(?=,? *#)"", ""zlib => \'${{ env.zlib_dir }}\'"" | Set-Content config.pl\ncmd.exe /s /c ""`""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat`"" amd64 && .\\build.bat""\n', 'cmd.exe /s /c ""`""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat`"" amd64 && perl .\\gen_probackup_project.pl `""${{ github.workspace }}`""\\postgres""', 'cd postgres\nsrc\\tools\\msvc\\install.bat postgres_install\n', 'git clone -b no-port-for --single-branch --depth 1 https://github.com/postgrespro/testgres.git\ncd testgres\npython setup.py install\n', 'icacls.exe ""${{ github.workspace }}"" /grant ""${env:USERNAME}:(OI)(CI)F""\n$env:PATH += "";${{ github.workspace }}\\postgres\\postgres_install\\lib;${{ env.zlib_dir }}\\lib""\n$Env:LC_MESSAGES = ""English""\n$Env:PG_CONFIG = ""${{ github.workspace }}\\postgres\\postgres_install\\bin\\pg_config.exe""\n$Env:PGPROBACKUPBIN = ""${{ github.workspace }}\\postgres\\Release\\pg_probackup\\pg_probackup.exe""\n$Env:PG_PROBACKUP_PTRACK = ""ON""\nIf (!$Env:MODE -Or $Env:MODE -Eq ""basic"") {\n  $Env:PG_PROBACKUP_TEST_BASIC = ""ON""\n  python -m unittest -v tests\n  python -m unittest -v tests.init_test\n} else {\n  python -m unittest -v tests.$Env:MODE\n}\n']"
"['pip install -U --user pip setuptools setuptools-scm nox', 'python -m nox --non-interactive --session tests-${{ matrix.python-version }}', 'git checkout HEAD^2', 'pip install -U --user pip setuptools setuptools-scm nox', 'python -m nox --non-interactive --session docs', 'pip install -U --user pip setuptools setuptools-scm black isort', 'python -m black --check --diff .', 'python -m isort --check-only .', 'pip install -U --user pip setuptools setuptools-scm nox', 'python -m nox --non-interactive --session validate-${{ matrix.python-version }} -k flake8', 'pip install -U --user pip setuptools setuptools-scm nox', 'python -m nox --non-interactive --session validate-${{ matrix.python-version }} -k mypy']"
"['python -m pip install -r scripts/requirements.txt', 'python scripts/validate/format.py ${FILENAME}', 'scripts/github_pull_request.sh ${{ github.repository }} ${{ github.event.pull_request.number }} ${FILENAME}', 'python scripts/validate/links.py ${FILENAME} --only_duplicate_links_checker', 'python -m pip install -r scripts/requirements.txt', 'cd scripts\npython -m unittest discover tests/ --verbose\n', 'python -m pip install -r scripts/requirements.txt', 'python scripts/validate/links.py ${FILENAME}']"
"['git fetch --prune --unshallow', 'python --version\npython -m pip install --upgrade pip\npython -m pip install -r requirements/requirements.txt\n', 'bash runtests.sh', 'git fetch --prune --unshallow', 'brew install bash', 'python --version\npython -m pip install --upgrade pip\npip install -r requirements/requirements.txt\n', 'bash runtests.sh', 'git fetch --prune --unshallow', 'python --version\npython -m pip install --upgrade pip\npip install -r requirements/requirements.txt\n', 'bash runtests.sh', 'python --version\npython -m pip install --upgrade pip\npython -m pip install -r requirements/requirements.txt\n']"
"['cd docker/qa-community-rust/ && docker build . --file Dockerfile --tag $IMAGE_NAME', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'python -m pip install --upgrade pip flake8\npip install quantaxis -U\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install quantaxis\n', 'cd docker/qa-base/ && docker build . --file Dockerfile --tag $IMAGE_NAME', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'cd docker/qa-eventmq/ && docker build . --file Dockerfile --tag $IMAGE_NAME', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'cd docker/qa-jupyter/ && docker build . --file Dockerfile --tag $IMAGE_NAME', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
""
"['python -m pip install --upgrade pip\npip install Augmentor\n', 'python -c ""import Augmentor""\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\n# if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest -v\n']"
"['pip install build', 'python -m build -s -w .', 'docker compose up -d', 'pip install -e .[test]', 'pytest', 'pip install -e .[test]', 'pytest -m ""not external_service""']"
"['python -m pip install --upgrade pip\npip install ruff black\n', 'make lint\n', 'bash <(curl https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash)\n./actionlint -color\n', 'python -m pip install bandit\nmake analyzer-bandit\n', 'python -m pip install semgrep\nmake analyzer-semgrep\n', 'make sphinx\n', 'if [[ ""$COV_RUN"" == ""true"" ]]\nthen\n  make test-coverage COVERAGE_TYPE=term\n  poetry run coverage xml\nelse\n  make test\nfi\n', 'poetry run python tools/dev/license-check.py\n', 'mkdir -p .tfcache\npytest -m ""terraform and audited and not skiplive"" -v tests\n', 'poetry self add poetry-plugin-freeze\npip install twine\nmake pkg-build-wheel\npython tools/dev/poetrypkg --root . --output wheels-manifest.txt\n', 'python tools/dev/changelog.py --output release.md --root . --since last\ncat release.md\n', './tools/dev/staging-auth\nmake pkg-publish PKG_REPO=${{ env.PKG_REPO }}\n', './tools/dev/staging-auth.sh\npip install -r wheels-manifest.txt\n', 'set -euxo pipefail\ncustodian version --debug\ncustodian schema --json > schema.json\n', 'echo ""release=$(poetry version -s).0"" >> ""$GITHUB_ENV""\n', 'poetry config repositories.prodpypi https://upload.pypi.org/legacy\n', 'make pkg-publish PKG_REPO=prodpypi\n']"
"['diff -u <(cd data/devices; ls *.device) <(cd _instdir/share/libratbag; ls *.device)', '(test -d _instdir && tree _instdir) || exit 0']"
"['pip install -e .[build]', 'pyinstaller .github/pyinstaller/floss.spec', 'chmod +x ${{ matrix.artifact_name }}', './${{ matrix.artifact_name }} -h', './${{ matrix.artifact_name }} tests/data/test-decode-to-stack.exe', 'chmod +x ${{ matrix.artifact_name }}', 'echo ""zip_name=floss-${GITHUB_REF#refs/tags/}-${{ matrix.asset_name }}.zip"" >> $GITHUB_ENV', 'zip ${{ env.zip_name }} ${{ matrix.artifact_name }}', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload --skip-existing dist/*\n', 'pip install -e .[dev]', 'isort --profile black --length-sort --line-width 120 -c .', 'black -l 120 --check .', 'mypy --config-file .github/mypy/mypy.ini --check-untyped-defs floss/ scripts/ tests/', 'sudo apt-get install -y libyaml-dev', 'pip install -e .[dev]', 'pytest tests/']"
"['python -m pip install --upgrade pip\npip install .\npip install -r doc/requirements.txt\n', 'make --directory=doc html\ntouch ./doc/build/html/.nojekyll\n', 'wget https://raw.githubusercontent.com/peaceiris/actions-gh-pages/v2/entrypoint.sh\nbash ./entrypoint.sh\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo apt-get install libsndfile1\n', 'pip install -e .[dev,testing]\n', 'export PYANNOTE_DATABASE_CONFIG=$GITHUB_WORKSPACE/tests/data/database.yml\npytest --cov-report=xml\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python piku.py\n', 'python piku.py setup\n', ""pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# Notes: \n# --exit-zero treats all errors as warnings, but we don't use it anymore\n# Allow longer lines for inlining SSH entries and set a complexity threshold that will pass for now\n# Ignore W605 (https://lintlyci.github.io/Flake8Rules/rules/W605.html) because \n# flake8 does not understand escaping dots inside templates for nginx and SSH\n# TODO: pare down complexity and line length as we shrink piku core\nflake8 . --ignore=W605 --count --max-complexity=78 --max-line-length=255 --statistics\n"", 'docker build .github/workflows/${{ matrix.distro }} -t local/${{ matrix.distro }}\ndocker run -v ""$PWD"":/run local/${{ matrix.distro }}\n']"
""
[]
"[""set -x\npython --version\npip install 'tox<4'\ntox --version\n"", 'tox -e ${{ matrix.config.tox }}\n', 'python -m pip install --upgrade build hatch twine\n', 'VERSION=""$(python -m hatch version)""\nTAG=""v$VERSION""\nGIT_TAG=""$(git tag -l --points-at ""$GITHUB_REF"" \'v*\')""\nif [ ""x$GIT_TAG"" != ""x$TAG"" ]; then\n  echo ""::error ::package version \'$TAG\' does not match git tag \'$GIT_TAG\'""\n  exit 1\nfi\necho ""version=$VERSION"" >> $GITHUB_OUTPUT\necho ""tag=$TAG"" >> $GITHUB_OUTPUT\necho ""anchor=${TAG//[^[:alnum:]]/-}"" >> $GITHUB_OUTPUT\n', 'rm -rf build dist django_anymail.egg-info\npython -m build\npython -m twine check dist/*\n', 'python -m twine upload --disable-progress-bar --non-interactive dist/*\n', '# gh release create-or-edit ""$TAG"" --target ""$GITHUB_REF"" --title ""$TITLE"" --notes ""$NOTES"" ./dist/*\n# (gh release doesn\'t support edit - 6/2021)\n# (hub requires separate --attach=FILE arg for each file)\nFILES=(./dist/*)\nif ! hub release edit --message ""$TITLE"" --message ""$NOTES"" ""${FILES[@]/#/--attach=}"" ""$TAG""; then\n  hub release create -t ""$GITHUB_SHA"" --message ""$TITLE"" --message ""$NOTES"" ""${FILES[@]/#/--attach=}"" ""$TAG""\nfi\n', ""python -m pip install 'tox<4' 'tox-gh-matrix<0.3'\npython -m tox --version\n"", 'python -m tox --gh-matrix\npython -m tox --gh-matrix-dump  # for debugging\n', ""set -x\npython -VV\npython -m pip install 'tox<4'\npython -m tox --version\n"", 'python -m tox -e ${{ matrix.tox.name }}\n']"
"['openssl aes-256-cbc -d -md sha256 -in settings.yml.ghenc -out settings.yml -pass env:AES_256_CBC_PASS\n', 'python -m pip install --upgrade pip wheel\n', 'sudo apt-get install libxml2-dev libxslt1-dev\npython -m pip install hg+https://foss.heptapod.net/pypy/cffi\npython -m pip install git+https://github.com/cython/cython.git\npython -m pip install git+https://github.com/lxml/lxml.git\npython -m pip install git+https://github.com/yaml/pyyaml.git\n', 'python -m pip install .\npython -m pip install -r test-requirements.txt\n', 'black --check --diff exchangelib tests scripts setup.py\nisort --check --diff exchangelib tests scripts setup.py\nflake8 exchangelib tests scripts setup.py\nunittest-parallel -j 4 --class-fixtures --coverage --coverage-source exchangelib\ncoveralls --service=github\n', 'openssl aes-256-cbc -d -md sha256 -in settings.yml.ghenc -out settings.yml -pass env:AES_256_CBC_PASS\n', 'python -m pip install --upgrade pip wheel\n', 'python -m pip install .\npython -m pip install -r test-requirements.txt\n', 'PYTHONPATH=./ python scripts/wipe_test_account.py\n']"
"['python -m pip install --upgrade pip setuptools\n', 'python -m pip install tox tox-gh-actions\n', 'tox\n', 'python -m pip install --upgrade pip setuptools twine wheel\n', 'python setup.py bdist_wheel\npython setup.py sdist\n', ""twine upload --verbose -u '__token__' dist/*\n""]"
"['wget https://www.antlr.org/download/antlr-4.11.1-complete.jar\n', 'java -jar /tmp/antlr-4.11.1-complete.jar PBXProj.g4 -Dlanguage=Python3\n', 'git add kin/grammar\ngit status\nif [ ""$(git status --porcelain)"" ]; then\n  git config --global user.email ""actions@users.noreply.github.com""\n  git config --global user.name ""Kin grammar bot""\n  git commit -m ""Update grammar""\n  git push\nfi\n', 'wget https://www.antlr.org/download/antlr-4.11.1-complete.jar\n', 'java -jar /tmp/antlr-4.11.1-complete.jar PBXProj.g4 -Dlanguage=Python3\n', 'python -m pip install --upgrade pip\npython -m pip install pytest\npip install .\n', './tests/tester.py\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install git+https://github.com/fsspec/filesystem_spec\npip install . --no-deps\n', 'pytest -vv s3fs']"
"['sudo apt-get update\nsudo apt-get install -y libusb-1.0-0-dev libudev-dev\n', 'pip3 install setuptools\npip3 install nox\n', 'nox --session gendoc\n', 'sudo apt-get update\nsudo apt-get install -y libusb-1.0-0-dev libudev-dev\n', 'pip3 install setuptools\npip3 install nox\n', 'python3 -m nox --session lint\n', 'python3 -m nox --session test-${{ matrix.python-version }}\n', 'pip install wheel nox\n', 'python setup.py sdist\n', 'python setup.py bdist_wheel\n', 'mkdir dist/\nmv rivalcfg-dist/* dist/\n']"
"['python -m pip install --upgrade pip pycodestyle pep257 mkdocs mkdocs-material\ncurl -L -o bazelisk ""https://github.com/bazelbuild/bazelisk/releases/download/v1.3.0/bazelisk-linux-amd64""\nchmod +x bazelisk && sudo mv bazelisk /usr/local/bin/bazel\n', 'mkdocs build --verbose --clean --strict;']"
"['imgtags=$(echo ""${{ github.ref }}"" | sed \'s/refs\\/tags\\//latest,/; s/refs\\/heads\\///\')\necho ""::set-output name=imgtags::$imgtags""\n', 'git checkout HEAD^2', 'pip install -r test-requirements.txt', 'pip install .', 'python -m pip install --user --upgrade setuptools wheel', 'python setup.py sdist bdist_wheel', 'npm install standard\nnode_modules/.bin/standard\n', 'pip install --user pycodestyle\npython -m pycodestyle --max-line-length=88\n', 'pip install -r requirements.txt\npip install -r test-requirements.txt\n', 'py.test -s --cov=./']"
"['docker build -f py.Dockerfile \\\n  --build-arg PYTHON_VERSION=${{ matrix.python-version }} \\\n  --tag gym-docker .\n', 'docker run gym-docker pytest', 'pip install pre-commit', 'pre-commit --version', 'pre-commit install', 'pre-commit run --all-files']"
"['choco install wget --no-progress\nwget -nv -O windrivers.zip https://github.com/jopohl/sdrbuild/releases/download/v1.1/win-$ARCH.zip\n7z x windrivers.zip -osrc/urh/dev/native/lib/shared\nls src/urh/dev/native/lib/shared\n', 'pip install wheel\npip install -r data/requirements.txt\nif [[ $OS == ubuntu* ]]\nthen\n  sudo apt-get update\n  sudo apt-get install libhackrf-dev librtlsdr-dev xvfb libxkbcommon-x11-0 x11-utils libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0\n  pip install PyVirtualDisplay==0.2.5\nelif [[ $OS == windows* ]]\nthen\n  pip install pywin32 pyaudio\nelif [[ $OS == mac* ]] \nthen\n  brew tap pothosware/homebrew-pothos\n  brew install airspy hackrf librtlsdr libbladerf limesuite portaudio uhd\n  \n  wget -nv https://github.com/analogdevicesinc/libiio/releases/download/v0.23/macOS-10.15.pkg\n  sudo installer -pkg macOS-10.15.pkg -target /\n  sudo cp /Library/Frameworks/iio.framework/iio /usr/local/lib/libiio.dylib\n  sudo install_name_tool -id ""/usr/local/lib/libiio.dylib"" /usr/local/lib/libiio.dylib\n  file /usr/local/lib/libiio.dylib\n  otool -L /usr/local/lib/libiio.dylib\n  sudo cp /Library/Frameworks/iio.framework/Versions/0.23/Headers/iio.h /usr/local/include\n  \n  wget -nv https://www.sdrplay.com/software/SDRplay_RSP_API-MacOSX-2.13.2.pkg\n  sudo installer -pkg SDRplay_RSP_API-MacOSX-2.13.2.pkg -target /  \n\n  pip install pyaudio\n  pip install pillow\nfi\npip install twine setuptools pytest pytest-xvfb pytest-cov pytest-faulthandler six appdirs packaging pyinstaller\npython -c ""import tempfile, os; open(os.path.join(tempfile.gettempdir(), \'urh_releasing\'), \'w\').close()""\n', 'python src/urh/cythonext/build.py', 'python setup.py sdist', 'python setup.py bdist_wheel', 'docker run --rm \\\n-e PYVER=$(python -c ""import sys; print(\'%s%s\' % (sys.version_info.major, sys.version_info.minor))"") \\\n-v `pwd`:/io jopohl/urh_manylinux2014 /io/data/make_manylinux2014_wheels.sh\n', 'pip install dist/*.whl\nurh --version\nxvfb-run urh autoclose\npython data/check_native_backends.py\n', ""python data/pyinstaller_helper.py\nfor /f %%i in ('python src/urh/version.py') do set URHVERSION=%%i\niscc /dMyAppVersion=%URHVERSION% /dArch=%PYARCH% data/inno.iss\n"", 'cd pyinstaller\\urh\nurh_debug.exe autoclose\n', 'cp data/pyinstaller_macos.spec urh.spec\npyinstaller --clean --distpath ./pyinstaller --workpath ./urh_build urh.spec\nmkdir -p dist\ncat pyinstaller/main.app/Contents/Info.plist\nhdiutil create -volname Universal.Radio.Hacker \\\n               -srcfolder pyinstaller/main.app \\\n               -ov -format UDZO \\\n               dist/Universal.Radio.Hacker-""$(python src/urh/version.py)"".dmg\n', 'touch tests/show_gui\ncp tests/.coveragerc .\npytest -s -v --junitxml=junit/test-results.xml --cov=src --cov-config=.coveragerc tests\ncoverage xml\ncoverage html\n', 'pytest -s -v --junitxml=junit/test-results.xml tests', 'if [[ $OS == ubuntu* ]]\nthen\n  twine upload --skip-existing dist/*\nelse\n  twine upload --skip-existing dist/*.whl\nfi\n']"
"['python -m pip install -U pip setuptools wheel', 'python -m pip install -r requirements-test.txt', 'python -m pip install -e .', 'python -m pytest']"
"['echo ""RELEASE=${{ startsWith(github.ref_name, \'v\') || github.ref_name == \'master\' }}"" >> $GITHUB_ENV\n', 'curl https://www.python.org/ftp/python/${PYTHON_VERSION}/python-${PYTHON_VERSION}-macosx10.9.pkg -o ""python.pkg""\nsudo installer -pkg python.pkg -target /\necho ""/Library/Frameworks/Python.framework/Versions/${{ matrix.python_version }}/bin"" >> $GITHUB_PATH\n""/Applications/Python ${{ matrix.python_version }}/Install Certificates.command""\n', 'echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT\n', 'sudo apt-get update\n# Unsure which of these are actually necessary...\nsudo apt-get install -y \\\n  appstream \\\n  qt5-default \\\n  qtwayland5 \\\n  libqt5x11extras5 \\\n  libfontconfig1 \\\n  libxcb1 \\\n  libfontconfig1-dev \\\n  libfreetype6-dev \\\n  libx11-dev \\\n  libxcursor-dev \\\n  libxext-dev \\\n  libxfixes-dev \\\n  libxft-dev \\\n  libxi-dev \\\n  libxrandr-dev \\\n  libxrender-dev\n', 'if [ ""$RUNNER_OS"" == ""Windows"" ]; then\n  choco install innosetup\nfi\npip3 install poetry==1.3.2\n', 'python3 -m venv venv\nsource venv/bin/activate || source venv/Scripts/activate\npoetry install\nmake build SKIP_WEBUI=${{ matrix.skip_webui }} SKIP_SERVER_RUST=${{ matrix.skip_rust }}\n', 'source venv/bin/activate || source venv/Scripts/activate\nmake test SKIP_SERVER_RUST=${{ matrix.skip_rust }}\n', 'source venv/bin/activate || source venv/Scripts/activate\nmake test-integration\n', 'source venv/bin/activate || source venv/Scripts/activate\npoetry install  # run again to ensure we have the correct version of PyInstaller\nmake package SKIP_SERVER_RUST=${{ matrix.skip_rust }}\n', '# Load certificates\n# Only load key & sign if env vars for signing exists\nif [ -n ""$APPLE_EMAIL"" ]; then\n  ./scripts/ci/import-macos-p12.sh\nfi\n\n# Build .app and .dmg\nsource venv/bin/activate\nmake dist/ActivityWatch.dmg\n\n# codesign and notarize\nif [ -n ""$APPLE_EMAIL"" ]; then\n  codesign --verbose -s ${APPLE_PERSONALID} dist/ActivityWatch.dmg\n\n  # Run prechecks\n  brew install akeru-inc/tap/xcnotary\n  xcnotary precheck dist/ActivityWatch.app\n  xcnotary precheck dist/ActivityWatch.dmg\n\n  # Notarize\n  make dist/notarize\nfi\nmv dist/ActivityWatch.dmg dist/activitywatch-$(scripts/package/getversion.sh)-macos-x86_64.dmg\n', './scripts/package/package-appimage.sh\n', '# The entire process is deferred to a shell file for consistency.\n./scripts/package/package-deb.sh\n', 'echo ""${{ steps.version.outputs.full }} (stable: ${{ steps.version.outputs.is_stable }})""\n', 'pip install requests\n', 'LAST_RELEASE=`STABLE_ONLY=${{ steps.version.output.is_stable }} ./scripts/get_latest_release.sh`\n./scripts/build_changelog.py --range ""$LAST_RELEASE...${{ steps.version.outputs.full }}""\n', 'mv changelog.md release_notes.md\n', 'ls -R', 'git clone https://github.com/ActivityWatch/docs\ngit clone https://github.com/ActivityWatch/activitywatch.github.io\n', 'mkdir dist\npushd dist\nwget -q https://github.com/ActivityWatch/activitywatch/releases/download/${{ env.new_version }}/activitywatch-${{ env.new_version }}-linux-x86_64.zip\n', 'pushd dist\n\n# New version\nunzip activitywatch-*-linux-x86_64.zip\nmv activitywatch/ aw-new\n\n# Old version\nwget -q -O aw-old.zip https://github.com/ActivityWatch/activitywatch/releases/download/${{ env.old_version }}/activitywatch-${{ env.old_version }}-linux-x86_64.zip\nunzip aw-old.zip\nmv activitywatch/ aw-old\n', 'ls -R', 'bin=dist/aw-old/${{ matrix.aw_server_old }}/${{ matrix.aw_server_old }}\nurl=""http://localhost:5600""\n\n# Check version\n$bin --version || true  # due to bug in old aw-server\n\n# Run server and log output\n$bin ${{ matrix.aw_server_old_args }} >> log-old.txt 2>&1 &\nsleep 5  # wait for startup\n\n# Set server URL\n\n# Get server info\ncurl ""$url/api/0/info"" --fail-with-body\n\n# Create bucket\ncurl -X \'POST\' --fail-with-body \\\n  ""$url/api/0/buckets/aw-test"" \\\n  -H \'accept: application/json\' \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n  ""client"": ""test"",\n  ""type"": ""test"",\n  ""hostname"": ""test""\n}\'\n\n# Get buckets\ncurl ""$url/api/0/buckets/"" -H \'accept: application/json\'\n\n# Send a heartbeat\ntimestamp=$(date -u +""%Y-%m-%dT%H:%M:%SZ"")\ncurl -X \'POST\' \\\n  ""$url/api/0/buckets/aw-test/heartbeat?pulsetime=0"" \\\n  -H \'accept: application/json\' \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n  ""timestamp"": ""\'$timestamp\'"",\n  ""duration"": 0,\n  ""data"": {""key"": ""test value""}\n}\'\n\n# Give a sec, then kill server process\nsleep 1\nkill $!\n', 'bin=dist/aw-new/${{ matrix.aw_server_new }}/${{ matrix.aw_server_new }}\nurl=""http://localhost:5600""\n\n# Check version\n$bin --version\n\n# Run server and log output\n$bin ${{ matrix.aw_server_new_args }} >> log-new.txt 2>&1 &\nsleep 5  # wait for startup\n\n# Get server info\ncurl ""$url/api/0/info""\n\n# Get buckets\ncurl ""$url/api/0/buckets/"" -H \'accept: application/json\'\n\n# Send a heartbeat\ntimestamp=$(date -u +""%Y-%m-%dT%H:%M:%SZ"")\ncurl -X \'POST\' --fail-with-body \\\n  ""$url/api/0/buckets/aw-test/heartbeat?pulsetime=60"" \\\n  -H \'accept: application/json\' \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n  ""timestamp"": ""\'$timestamp\'"",\n  ""duration"": 0,\n  ""data"": {""key"": ""test value""}\n}\'\n\n# Give a sec, then kill server process\nsleep 1\nkill $!\n', 'cat log-old.txt || true\necho ""\\n---\\n""\ncat log-new.txt || true\n']"
""
"['python -m pip install -U pip\n', 'python --version\n', 'pip install -e "".[test]""\nif [ -n ""${{ matrix.pynvml-version }}"" ]; then\n  pip install nvidia-ml-py==${{ matrix.pynvml-version }}\nfi\npython -m gpustat --version\n', 'pytest --color=yes -v -s\n']"
"['python -c ""import sys; print(sys.version)""\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\n', 'pip install --upgrade numpy\n', 'make build-dependencies\nmake install-executable\n', 'make benchmark\n', 'python -c ""import sys; print(sys.version)""\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\n', 'sudo apt-get install -y shellcheck\npip install --upgrade flake8\n', 'flake8 --ignore=E501,W503 src/crunch.py\nshellcheck --exclude=2046 src/*.sh\n', 'python -c ""import sys; print(sys.version)""\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\n', 'pip install --upgrade pytest\nsudo apt-get install -y pngcheck\n', 'make build-dependencies\nmake install-executable\n', 'pytest src\nmake test-valid-png-output\n', 'python -c ""import sys; print(sys.version)""\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\n', 'pip install --upgrade pytest\nbrew install pngcheck\n', 'make build-dependencies\nmake install-executable\n', 'pytest src\nmake test-valid-png-output\n']"
[]
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', ""# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=11 --max-line-length=127 --statistics\nflake8 --filename='*.pyx,*.px*' --ignore E901,E225,E226,E227,E402,E999\n"", 'black --check .\n', ""sudo apt install clang-format\nfind implicit | grep -E '\\.(cu|cuh|h|cpp)$' | xargs clang-format --dry-run --Werror\n"", 'isort -c .\n', 'codespell\n', 'pylint implicit tests benchmarks examples\n', 'pip install build \npython -m build . --sdist -o wheelhouse\n', 'python ci/rename_wheels.py\n', 'python -m pip install --upgrade pip\npip install pytest\npip install -r implicit_source/requirements.txt\n', 'pip install h5py', 'pip install annoy nmslib', 'pip install --force-reinstall --no-deps --no-index --find-links . implicit\n', 'python -m pytest implicit_source/tests\n', 'pip install --upgrade wheel pip setuptools twine\ntwine upload *\nrm *\n']"
"[""from os import environ\nfrom pathlib import Path\n\nFILE_APPEND_MODE = 'a'\n\nwith Path(environ['GITHUB_OUTPUT']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print('is-untagged-devel=true', file=outputs_file)\n"", ""from os import environ\nfrom pathlib import Path\n\nFILE_APPEND_MODE = 'a'\n\nwith Path(environ['GITHUB_OUTPUT']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print('release-requested=true', file=outputs_file)\n"", ""from hashlib import sha512\nfrom os import environ\nfrom pathlib import Path\nfrom sys import version\n\nFILE_APPEND_MODE = 'a'\n\nhash = sha512(version.encode()).hexdigest()\n\nwith Path(environ['GITHUB_OUTPUT']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(f'py-hash-key={hash}', file=outputs_file)\n"", 'from os import environ\nfrom pathlib import Path\n\nFILE_APPEND_MODE = \'a\'\n\nwith Path(environ[\'GITHUB_OUTPUT\']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(\n        ""files-hash-key=${{\n            hashFiles(\n              \'setup.cfg\', \'setup.py\', \'tox.ini\', \'pyproject.toml\',\n              \'.pre-commit-config.yaml\', \'pytest.ini\'\n            )\n        }}"",\n        file=outputs_file,\n    )\n', 'echo ""dir=$(python -m pip cache dir)"" >> ""${GITHUB_OUTPUT}""', 'git tag --points-at HEAD | xargs git tag --delete', 'python -m pip install --user setuptools-scm', 'from os import environ\nfrom pathlib import Path\n\nimport setuptools_scm\n\nFILE_APPEND_MODE = \'a\'\n\nver = setuptools_scm.get_version(\n  ${{\n      steps.untagged-check.outputs.is-untagged-devel == \'true\'\n      && \'local_scheme=""no-local-version""\' || \'\'\n  }}\n)\nwith Path(environ[\'GITHUB_OUTPUT\']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(f\'dist-version={ver}\', file=outputs_file)\n', 'from os import environ\nfrom pathlib import Path\n\nFILE_APPEND_MODE = \'a\'\n\nwith Path(environ[\'GITHUB_OUTPUT\']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(\n        ""tag=v${{\n            steps.request-check.outputs.release-requested == \'true\'\n            && github.event.inputs.release-version\n            || steps.scm-version.outputs.dist-version\n        }}"",\n        file=outputs_file,\n    )\n', 'from os import environ\nfrom pathlib import Path\n\nFILE_APPEND_MODE = \'a\'\n\nwith Path(environ[\'GITHUB_OUTPUT\']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(""name=${{ env.dists-name }}"", file=outputs_file)\n', 'from os import environ\nfrom pathlib import Path\n\nFILE_APPEND_MODE = \'a\'\n\nwith Path(environ[\'GITHUB_OUTPUT\']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(\n        ""sdist=${{ steps.dist.outputs.name }}-${{\n            steps.request-check.outputs.release-requested == \'true\'\n            && github.event.inputs.release-version\n            || steps.scm-version.outputs.dist-version\n        }}.tar.gz"",\n        file=outputs_file,\n    )\n    print(\n        ""wheel=${{ steps.dist.outputs.name }}-${{\n            steps.request-check.outputs.release-requested == \'true\'\n            && github.event.inputs.release-version\n            || steps.scm-version.outputs.dist-version\n        }}-py3-none-any.whl"",\n        file=outputs_file,\n    )\n', ""from hashlib import sha512\nfrom os import environ\nfrom pathlib import Path\nfrom sys import version\n\nFILE_APPEND_MODE = 'a'\n\nhash = sha512(version.encode()).hexdigest()\n\nwith Path(environ['GITHUB_OUTPUT']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(f'py-hash-key={hash}', file=outputs_file)\n"", 'echo ""dir=$(python -m pip cache dir)"" >> ""${GITHUB_OUTPUT}""', ""python -m pip install --user '${{ env.TOX_VERSION }}'"", 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --notest', 'git tag --points-at HEAD | xargs git tag --delete', ""git tag -m '${{ needs.pre-setup.outputs.git-tag }}' '${{ needs.pre-setup.outputs.git-tag }}' -- ${{\n  fromJSON(needs.pre-setup.outputs.release-requested)\n  && github.event.inputs.release-committish || ''\n}}"", 'python -m pip install --user tomlkit', ""from pathlib import Path\n\nimport tomlkit\n\npyproject_toml_path = Path.cwd() / 'pyproject.toml'\npyproject_toml_txt = pyproject_toml_path.read_text()\npyproject_toml = tomlkit.loads(pyproject_toml_txt)\nsetuptools_scm_section = pyproject_toml['tool']['setuptools_scm']\nsetuptools_scm_section['local_scheme'] = 'no-local-version'\npatched_pyproject_toml_txt = tomlkit.dumps(pyproject_toml)\npyproject_toml_path.write_text(patched_pyproject_toml_txt)\n"", 'git diff --color=always\ngit update-index --assume-unchanged pyproject.toml\n', 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --skip-pkg-install', ""ls -1 'dist/${{ needs.pre-setup.outputs.sdist-artifact-name }}' 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}'"", ""from hashlib import sha512\nfrom os import environ\nfrom pathlib import Path\nfrom sys import version\n\nFILE_APPEND_MODE = 'a'\n\nhash = sha512(version.encode()).hexdigest()\n\nwith Path(environ['GITHUB_OUTPUT']).open(\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(f'py-hash-key={hash}', file=outputs_file)\n"", 'echo ""dir=$(pip cache dir)"" >> ""${GITHUB_OUTPUT}""', ""python -m pip install --user '${{ env.TOX_VERSION }}'"", 'shopt -s extglob\nrm -rf !tox.ini\n', 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --notest', 'test -d .tox/pre-commit && .tox/pre-commit/bin/python -m pre_commit install-hooks || :', 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --skip-pkg-install', ""from os import environ\nfrom sys import version_info\n\nFILE_APPEND_MODE = 'a'\n\nis_stable_abi = version_info.releaselevel == 'final'\n\nwith open(\n        environ['GITHUB_OUTPUT'],\n        mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(\n        'is-stable-abi={is_stable_abi}'.\n        format(is_stable_abi=str(is_stable_abi).lower()),\n        file=outputs_file,\n    )\n"", ""from hashlib import sha512\nfrom os import environ\nfrom sys import version\n\nFILE_APPEND_MODE = 'a'\n\nhash = sha512(version.encode()).hexdigest()\n\nwith open(\n        environ['GITHUB_OUTPUT'], mode=FILE_APPEND_MODE,\n) as outputs_file:\n    print(f'py-hash-key={hash}', file=outputs_file)\n"", 'echo ""dir=$(python -m pip cache dir)"" >> ""${GITHUB_OUTPUT}""', 'python -m pip install --user --upgrade --force-reinstall pip-with-requires-python', ""python -m pip install --user '${{ env.TOX_VERSION }}'"", ""sed -i 's/^package_env\\(\\s\\)\\?=.*/package_env = py36-win-dummy/g' tox.ini"", ""python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --installpkg 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}' --notest"", 'systeminfo', 'python -m platform', 'python -c ""import platform; print(platform.version())""', 'python -c ""import platform; print(platform.uname())""', 'python -c ""import platform; print(platform.release())""', 'python -c ""import ssl; print(\'\\nOPENSSL_VERSION: \' + ssl.OPENSSL_VERSION + \'\\nOPENSSL_VERSION_INFO: \' + repr(ssl.OPENSSL_VERSION_INFO) + \'\\nOPENSSL_VERSION_NUMBER: \' + repr(ssl.OPENSSL_VERSION_NUMBER))""', ""python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --installpkg 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}'"", ""compgen -G '.test-results/pytest/cov.xml' && ( echo 'present=true' >> ${GITHUB_OUTPUT} ) ; exit 0"", 'cat code-coverage-results.md >> ""${GITHUB_STEP_SUMMARY}""', ""python -m tox --parallel auto --parallel-live --skip-missing-interpreters false -vvvvv --installpkg 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}' -- --no-cov -vvvvv --lf && exit 1"", 'REMOTE_TAGGED_COMMIT_SHA=""$(\n  git ls-remote --tags --refs $(git remote get-url origin) \'${{\n    needs.pre-setup.outputs.git-tag\n  }}\' | awk \'{print $1}\'\n)""\n\nif [[ ""${REMOTE_TAGGED_COMMIT_SHA}"" == \'${{\n  github.event.inputs.release-committish\n}}\' ]]\nthen\n  echo ""already-exists=true"" >> ""${GITHUB_OUTPUT}""\nfi\n', ""git tag -m '${{ needs.pre-setup.outputs.git-tag }}' -m 'Published at https://pypi.org/project/${{\n  needs.pre-setup.outputs.dist-name\n}}/${{\n  needs.pre-setup.outputs.dist-version\n}}' -m 'This release has been produced by the following workflow run: ${{\n  github.server_url\n}}/${{\n  github.repository\n}}/actions/runs/${{\n  github.run_id\n}}' '${{ needs.pre-setup.outputs.git-tag }}' -- ${{ github.event.inputs.release-committish }}"", ""git push --atomic origin '${{ needs.pre-setup.outputs.git-tag }}'""]"
['python -m pip install --requirement requirements.txt\npython -m pip install --requirement requirements-dev.txt\npython -m pip install --editable .\npython -m flake8\npython -m pytest --cov\n']
"['pip3 install -e .\npip3 install pytest-faulthandler\napt-get -y install xvfb\n', 'pip3 install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F72,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip3 install -e .\n', 'python3 dragonfire/odqa.py\n', 'pip3 install -e .\n', 'dpkg-buildpackage -us -uc\nRELEASE_VERSION=$(echo ${{ github.ref }} | cut -c12-)\nmv ../dragonfire_${RELEASE_VERSION}_amd64.deb .\nls -al\necho ::set-output name=filename::dragonfire_${RELEASE_VERSION}_amd64.deb\n', 'pip3 install -e .\npip3 install pytest-faulthandler\napt-get -y install xvfb\n', 'pip3 install pytest pytest-cov codecov\nxvfb-run --auto-servernum python3 -m pytest --cov=dragonfire/ --capture=sys --disable-pytest-warnings -vv\n# send the code coverage report to Codeconv\ncodecov --token=${{ secrets.CODECOV_TOKEN }}\n']"
"['python -m pip install --upgrade poetry\npoetry install\n', 'poetry run coverage run --branch setup.py test\npoetry run coverage xml -i\n', 'poetry run mypy .\n', 'poetry run flake8\n', 'poetry run sphinx-build -W -a -E -b html -n docs docs/_build']"
"['echo ::error::PR is not approved yet.\nexit 1\n', 'mkdir -p dist/\necho ""${VERSION}"" > dist/VERSION\n', 'python -m pip install -U setuptools wheel pip\npython setup.py sdist\n', 'set -e\necho ::set-output name=version::$(cat dist/VERSION)\nrm dist/VERSION\n', 'ls -al dist/\n', 'python -m pip install -U pip setuptools wheel\npython -m pip install -e .[test]\npython -m unittest -v tests.suite\n']"
""
['git checkout HEAD^2']
"['poetry run invoke black', 'poetry run invoke bandit', 'poetry run invoke pydocstyle', 'poetry run invoke flake8', 'poetry run invoke yamllint', 'echo IMAGE_VER=`poetry version -s`-py${{ matrix.python-version }} >> $GITHUB_ENV', 'echo IMAGE_VER=`poetry version -s`-py${{ matrix.python-version }} >> $GITHUB_ENV', 'docker image ls', 'poetry run invoke pylint', 'echo IMAGE_VER=`poetry version -s`-py${{ matrix.python-version }} >> $GITHUB_ENV', 'docker image ls', 'poetry run invoke pytest', 'pip install poetry', 'echo RELEASE_VERSION=${GITHUB_REF:10} >> $GITHUB_ENV', 'poetry version $RELEASE_VERSION', 'poetry build', 'pip install poetry', 'echo RELEASE_VERSION=${GITHUB_REF:10} >> $GITHUB_ENV', 'poetry version $RELEASE_VERSION', 'poetry build']"
"['python -m pip install --upgrade pip\npip install .[doc]\n', 'make --directory=docs html\ntouch ./docs/build/html/.nojekyll\n', 'sudo apt-get update\nsudo apt-get install espeak-ng festival mbrola mbrola-fr1\n', 'pip install --upgrade pip pytest pytest-cov\npython setup.py install\n', 'phonemize --version', 'pytest -v --cov=phonemizer --cov-report=xml test/', 'sudo apt-get update\nsudo apt-get install festival mbrola mbrola-fr1\n', 'sudo apt install espeak', 'sudo apt-get install make autoconf automake libtool pkg-config gcc libsonic-dev libpcaudio-dev git\ngit clone --depth 1 --branch $ESPEAK_VERSION https://github.com/espeak-ng/espeak-ng.git\ncd espeak-ng\n./autogen.sh\n./configure\nmake\nsudo make install\nsudo ldconfig\nespeak --version\n', 'pip install --upgrade pip pytest\npython setup.py install\n', 'phonemize --version', 'pytest -v', 'brew update\nbrew install espeak\n', 'cd festival\nbash setup_festival_mbrola.sh . festival\n', 'pip install --upgrade pip\npython setup.py install\npip install --upgrade pytest\n', 'phonemize --version', 'pytest -v', 'sudo apt-get update\nsudo apt-get install espeak-ng festival mbrola mbrola-fr1\n', 'pip install --upgrade pip pytest wheel\npython setup.py install\npytest\npython setup.py sdist bdist_wheel\n', ""$source = 'https://github.com/espeak-ng/espeak-ng/releases/download/1.50/espeak-ng-20191129-b702b03-x64.msi'\nInvoke-WebRequest -Uri $source -OutFile espeak.msi\nStart-Process msiexec.exe -Wait -ArgumentList '/I espeak.msi /qn'\n"", '$uri = ""https://sourceforge.net/projects/e-guidedog/files/related-third-party-software/0.3""\n\n$webclient = New-Object System.Net.WebClient\n$webclient.DownloadFile(""$uri"" + ""/festival-2.5-win.7z"", ""festival-2.5.7z"")\n$webclient.DownloadFile(""$uri"" + ""/speech_tools-2.5-win.7z"", ""speech_tools-2.5.7z"")\n\nset-alias sz ""$env:ProgramFiles\\7-Zip\\7z.exe""\nsz x -oC:\\ festival-2.5.7z\nsz x -oC:\\ speech_tools-2.5.7z\n', 'pip install pytest\npython setup.py install\n', 'phonemize --version\n', 'pytest -v']"
"['docker --version\ndocker-compose --version\n', 'pip install pytest requests', 'cat ci/config_token.py >> ${{ matrix.example }}/jupyterhub_config.py', 'docker-compose up --detach', 'pytest --verbose --capture=no', 'docker-compose logs']"
"['sudo apt update\nsudo apt-get -y install ffmpeg build-essential libasound2-dev libjack-dev sox\npip install -e .[test]\n', 'pytest --ignore=magenta/models/score2perf', 'pylint magenta', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox -e coverage']"
"['conda install -q -y python=${{ matrix.python-version }}\nwhich python\npython --version\npip --version\n', 'if [ ""${{ matrix.PYTEST_QT_API }}"" = ""pyside2"" ]; then\n  conda install -q -y pyside2 -c conda-forge\n# should be installed via pip\n# else\n#   conda install -q -y pyqt=5\nfi\npip install -r requirements-dev.txt\n', 'pip install .\n', 'flake8 labelme/\n', 'black --line-length 79 --check --diff labelme/\n', '# # open virtual display\n# if [ ""${{ matrix.os }}"" = ""ubuntu-latest"" ]; then\n#   export DISPLAY=:99.0\n#   /sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1920x1200x24 -ac +extension GLX +render -noreset\n#   (herbstluftwm )&\n# else\n#   (sudo Xvfb :99 -ac -screen 0 1024x768x8 )&\n# fi\npytest -vs -m \'not gui\' tests/\n', 'labelme --help\nlabelme --version\n(cd examples/primitives && labelme_json_to_dataset primitives.json && rm -rf primitives_json)\n(cd examples/tutorial && rm -rf apc2016_obj3_json && labelme_json_to_dataset apc2016_obj3.json && python load_label_png.py && git checkout -- .)\n(cd examples/semantic_segmentation && rm -rf data_dataset_voc && ./labelme2voc.py data_annotated data_dataset_voc --labels labels.txt && git checkout -- .)\n(cd examples/instance_segmentation && rm -rf data_dataset_voc && ./labelme2voc.py data_annotated data_dataset_voc --labels labels.txt && git checkout -- .)\n(cd examples/video_annotation && rm -rf data_dataset_voc && ./labelme2voc.py data_annotated data_dataset_voc --labels labels.txt && git checkout -- .)\n\npip install lxml  # for bbox_detection/labelme2voc.py\n(cd examples/bbox_detection && rm -rf data_dataset_voc && ./labelme2voc.py data_annotated data_dataset_voc --labels labels.txt && git checkout -- .)\n\npip install cython && pip install pycocotools  # for instance_segmentation/labelme2coco.py\n(cd examples/instance_segmentation && rm -rf data_dataset_coco && ./labelme2coco.py data_annotated data_dataset_coco --labels labels.txt && git checkout -- .)\n', '# Build the standalone executable\npip install pyinstaller\npyinstaller labelme.spec\ndist/labelme --version\n', 'echo ""${{ steps.create_release.outputs.upload_url }}"" > release_url.txt', 'pip install .\n', 'pip install pyinstaller\npyinstaller labelme.spec\n', 'echo ""::set-output name=upload_url::$(cat release_url/release_url.txt)""\n', 'npm install -g create-dmg\ncd dist\ncreate-dmg Labelme.app || test -f Labelme\\ 0.0.0.dmg\nmv Labelme\\ 0.0.0.dmg Labelme.dmg\n']"
"['make install-dev-deps\n', 'make prcheck', 'pip install -r requirements-test.txt --upgrade --upgrade-strategy eager -e .\n', 'make coverage', 'npm install -g aws-cdk', 'pip install -r requirements-dev.txt\npip install -e .[${{ matrix.cdk-version }}]\n', 'python -m pytest tests/functional/cdk']"
"['sudo apt-get update -qq\nsudo apt-get install -y gcc g++\n', 'pip install -r requirements/tests.txt', 'if [[ ${{ matrix.python-version }} == 3.9 ]]; then ./dev_setup.sh; fi\nif [[ ${{ matrix.python-version }} != 3.9 ]]; then ./dev_setup.sh -sm; fi\n', 'pycodestyle mycroft test\nflake8 mycroft test --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'if [[ ${{ matrix.python-version }} == 3.9 ]]; then ./start-mycroft.sh unittest --cov-report html; fi\nif [[ ${{ matrix.python-version }} != 3.9 ]]; then ./start-mycroft.sh unittest; fi\n', 'if [[ ${{ matrix.python-version }} == 3.9 ]]; then bash <(curl -s https://codecov.io/bash); fi']"
"['echo icloudpd_version=$(cat setup.py | grep version= | cut -d\'""\' -f 2) >> $GITHUB_OUTPUT\necho \'icloudpd_changelog<<EOF\' >> $GITHUB_OUTPUT\nscripts/extract_releasenotes CHANGELOG.md >> $GITHUB_OUTPUT\necho \'EOF\' >> $GITHUB_OUTPUT\n', 'echo ""icloudpd_version=${{steps.get_version.outputs.icloudpd_version}}""\necho ""icloudpd_changelog=${{steps.get_version.outputs.icloudpd_changelog}}""\n', 'python.exe -m pip install -r requirements-pip.txt', 'scripts/install_deps\n', 'scripts/build\n', 'pyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloudpd.py icloud.py --name icloudpd-${{needs.get_version.outputs.icloudpd_version}}-windows-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloud.py --name icloud-${{needs.get_version.outputs.icloudpd_version}}-windows-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile exec.py --name icloudpd-ex-${{needs.get_version.outputs.icloudpd_version}}-windows-amd64\n', 'pyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloudpd.py icloud.py --name icloudpd-${{needs.get_version.outputs.icloudpd_version}}-linux-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloud.py --name icloud-${{needs.get_version.outputs.icloudpd_version}}-linux-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile exec.py --name icloudpd-ex-${{needs.get_version.outputs.icloudpd_version}}-linux-amd64\n', 'pyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile exec.py --name icloudpd-ex-${{needs.get_version.outputs.icloudpd_version}}-macos-amd64\n', 'echo ${{ steps.buildx.outputs.name }}', 'echo ${{ steps.buildx.outputs.platforms }}', 'scripts/install_deps\n', 'python3 -m twine upload --non-interactive --disable-progress-bar dist/*.whl\n', 'echo icloudpd_version=$(cat setup.py | grep version= | cut -d\'""\' -f 2) >> $GITHUB_OUTPUT\necho \'icloudpd_changelog<<EOF\' >> $GITHUB_OUTPUT\nscripts/extract_releasenotes CHANGELOG.md >> $GITHUB_OUTPUT\necho \'EOF\' >> $GITHUB_OUTPUT\n', 'echo ""icloudpd_version=${{steps.get_version.outputs.icloudpd_version}}""\necho ""icloudpd_changelog=${{steps.get_version.outputs.icloudpd_changelog}}""\n', 'echo ${{ steps.buildx.outputs.name }}', 'echo ${{ steps.buildx.outputs.platforms }}', 'scripts/install_deps\n', 'scripts/lint\n', 'scripts/test\n', 'echo icloudpd_version=$(cat setup.py | grep version= | cut -d\'""\' -f 2) >> $GITHUB_OUTPUT\necho \'icloudpd_changelog<<EOF\' >> $GITHUB_OUTPUT\nscripts/extract_releasenotes CHANGELOG.md >> $GITHUB_OUTPUT\necho \'EOF\' >> $GITHUB_OUTPUT\n', 'echo ""icloudpd_version=${{steps.get_version.outputs.icloudpd_version}}""\necho ""icloudpd_changelog=${{steps.get_version.outputs.icloudpd_changelog}}""\n', 'scripts/install_deps\n', 'scripts/build\n', 'pyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloudpd.py icloud.py --name icloudpd-${{needs.get_version.outputs.icloudpd_version}}-windows-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloud.py --name icloud-${{needs.get_version.outputs.icloudpd_version}}-windows-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile exec.py --name icloudpd-ex-${{needs.get_version.outputs.icloudpd_version}}-windows-amd64\n', 'pyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloudpd.py icloud.py --name icloudpd-${{needs.get_version.outputs.icloudpd_version}}-linux-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile icloud.py --name icloud-${{needs.get_version.outputs.icloudpd_version}}-linux-amd64\npyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile exec.py --name icloudpd-ex-${{needs.get_version.outputs.icloudpd_version}}-linux-amd64\n', 'pyinstaller --collect-all keyrings.alt --hidden-import pkgutil --collect-all tzdata --onefile exec.py --name icloudpd-ex-${{needs.get_version.outputs.icloudpd_version}}-macos-amd64\n', 'echo ${{ steps.buildx.outputs.name }}', 'echo ${{ steps.buildx.outputs.platforms }}']"
"['python -m pip install --upgrade pip\npip install -r tests/requirements.txt\npython -m nltk.downloader popular\n', 'make test\n', 'conda info', '# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', 'conda config --set always_yes yes --set changeps1 no\nconda update -n base conda --yes\nconda env create -f tests/requirements.txt -n yellowbrick python=${{ matrix.python-version }}\nconda activate yellowbrick\npython -m nltk.downloader popular\n', 'conda activate yellowbrick\nmake test\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r docs/requirements.txt\n', 'python -m pip install --upgrade pip\npip install pre-commit\npre-commit install\n', 'pre-commit run --from-ref origin/${{ github.base_ref }} --to-ref HEAD --show-diff-on-failure\n']"
"['git checkout HEAD^2', ""python -m pip install --upgrade pip\npip install '.[test]'\npip install doit\npip install --requirement ./docs/requirements.txt\n"", 'wget https://artifacts.crowdin.com/repo/deb/crowdin.deb -O crowdin.deb\nsudo dpkg -i crowdin.deb\nsudo apt-get update\nsudo apt-get install -y gettext\n', ""mkdir .cache\nmkdir -p parent_dirextory\nfind . -mindepth 1 -maxdepth 1 '!' -name parent_dirextory -exec mv '{}' parent_dirextory/ ';'\nmv parent_dirextory ehForwarderBot\n"", 'config=$HOME/.crowdin.yaml\nhere=$(pwd)\necho project_identifier: ehforwarderbot > $config\necho api_key: $CROWDIN_TOKEN >> $config\necho base_path: $here >> $config\n', 'cd ehForwarderBot\ndoit run --verbosity=2 crowdin\n', ""python -m pip install --upgrade pip\npip install '.[tests]'\n"", 'mypy -p ehforwarderbot\n', 'pytest -vv -r a -l --color=yes\n']"
""
""
"['sudo apt-get update\nsudo apt-get install -y pyqt5-dev-tools xvfb jq\n', ""pip install 'tox==3.28.0'"", 'tox -vvve ${{ matrix.environment }} -- --forked --verbose']"
"['python -m pip install --upgrade pip\npip install twine wheel setuptools pybind11\n', 'sudo test -d dist || mkdir -v dist\nsudo find . -name \\*.whl | grep -v /dist/ | xargs -n1 -i mv -v ""{}"" dist/\n', 'sudo rm -rfv dist/*-linux_x86_64.whl\n', 'python -m pip install --upgrade pip\npip install twine wheel setuptools pybind11\n', 'python setup.py --without-cython sdist --format=gztar\n', 'python -m pip install --upgrade pip\npip install twine wheel setuptools cython pybind11\n', 'python setup.py  --with-cython --with-distributable-extensions sdist --format=gztar bdist_wheel\n', '$env:PYTHONWARNINGS=""ignore::UserWarning""\nInvoke-Expression ""python -m pip install --upgrade pip""\nInvoke-Expression ""pip install setuptools twine wheel cython pybind11""\n', '$env:PYTHONWARNINGS=""ignore::UserWarning""\nInvoke-Expression ""python setup.py  --with-cython --with-distributable-extensions sdist --format=gztar bdist_wheel""\n', 'pip install black\nblack . -S -C --check --diff --exclude examples/pyomobook/python-ch/BadIndent.py\n', 'JOB=""${{matrix.TARGET}}/${{matrix.python}}${{matrix.other}}""\necho ""GHA_JOBNAME=$JOB"" | sed \'s|/|_|g\' >> $GITHUB_ENV\nif test -z ""${{matrix.other}}""; then\n    echo ""GHA_JOBGROUP=${{matrix.TARGET}}"" >> $GITHUB_ENV\nelse\n    echo ""GHA_JOBGROUP=other"" >> $GITHUB_ENV\nfi\n# Note: pandas 1.0.3 causes gams 29.1.0 import to fail in python 3.8\nEXTRAS=tests\nif test -z ""${{matrix.slim}}""; then\n    EXTRAS=""$EXTRAS,docs,optional""\nfi\necho ""EXTRAS=$EXTRAS"" >> $GITHUB_ENV\nPYTHON_PACKAGES=""${{matrix.PACKAGES}}""\necho ""PYTHON_PACKAGES=$PYTHON_PACKAGES"" \\\n    | tr \'\\n\' \' \' | sed \'s/ \\+/ /g\' >> $GITHUB_ENV\n', 'CURLRC=""$(cat <<EOF\n   retry = 0\n   max-time = 30\nEOF\n)""\necho ""$CURLRC"" > ${GITHUB_WORKSPACE}/.curlrc\necho ""$CURLRC"" > ${GITHUB_WORKSPACE}/_curlrc\necho ""CURL_HOME=$GITHUB_WORKSPACE"" >> $GITHUB_ENV\n', 'mkdir -p ${GITHUB_WORKSPACE}/cache/os\nexport HOMEBREW_CACHE=${GITHUB_WORKSPACE}/cache/os\n# Be cautious running brew update: it can break\n#    setup-python on OSX\n# brew update\n#\n# Notes:\n#  - install glpk\n#  - pyodbc needs: gcc pkg-config unixodbc freetds\nfor pkg in bash pkg-config unixodbc freetds glpk; do\n    brew list $pkg || brew install $pkg\ndone\n', 'mkdir -p ${GITHUB_WORKSPACE}/cache/os\n# Notes:\n#  - install glpk\n#  - ipopt needs: libopenblas-dev gfortran liblapack-dev\nsudo apt-get -o Dir::Cache=${GITHUB_WORKSPACE}/cache/os \\\n    install libopenblas-dev gfortran liblapack-dev glpk-utils\nsudo chmod -R 777 ${GITHUB_WORKSPACE}/cache/os\n', 'echo ""SETUPTOOLS_USE_DISTUTILS=local"" >> $GITHUB_ENV\n', 'python -c \'import sys;print(sys.executable)\'\npython -m pip install --cache-dir cache/pip --upgrade pip\nPYOMO_DEPENDENCIES=`python setup.py dependencies \\\n    --extras ""$EXTRAS"" | tail -1`\nPACKAGES=""${PYTHON_CORE_PKGS} ${PYTHON_PACKAGES} ${PYOMO_DEPENDENCIES} ""\nif [[ ${{matrix.python}} == pypy* ]]; then\n    EXCLUDE=""$PYPY_EXCLUDE $EXCLUDE""\nfi\nEXCLUDE=`echo ""$EXCLUDE"" | xargs`\nif test -n ""$EXCLUDE""; then\n    for WORD in $EXCLUDE; do\n        PACKAGES=${PACKAGES//$WORD / }\n    done\nfi\npython -m pip install --cache-dir cache/pip ${PACKAGES}\npython -m pip install --cache-dir cache/pip pymysql || \\\n    python -m pip install --cache-dir cache/pip pymysql\nif test -z ""${{matrix.slim}}""; then\n    python -m pip install --cache-dir cache/pip cplex docplex \\\n        || echo ""WARNING: CPLEX Community Edition is not available""\n    python -m pip install --cache-dir cache/pip \\\n        -i https://pypi.gurobi.com gurobipy \\\n        || echo ""WARNING: Gurobi is not available""\n    python -m pip install --cache-dir cache/pip xpress \\\n        || echo ""WARNING: Xpress Community Edition is not available""\nfi\npython -c \'import sys; print(""PYTHON_EXE=%s"" \\\n    % (sys.executable,))\' >> $GITHUB_ENV\necho """"\necho ""Final pip environment:""\npython -m pip list | sed \'s/^/    /\'\n', '# Set up environment\nmkdir -p $GITHUB_WORKSPACE/cache/conda\nconda config --set always_yes yes\nconda config --set auto_update_conda false\nconda config --prepend pkgs_dirs $GITHUB_WORKSPACE/cache/conda\n# Try to install mamba\nconda install -q -y -n base conda-libmamba-solver || MAMBA_FAILED=1\nif test -z ""$MAMBA_FAILED""; then\n    echo ""*** Activating the mamba environment solver ***""\n    conda config --set solver libmamba\nfi\n# Print environment info\necho ""*** CONDA environment: ***""\nconda info\nconda config --show-sources\nconda config --show channels\nconda list --show-channel-urls\nwhich python\npython --version\n# Note: some pypi packages are not available through conda\nPYOMO_DEPENDENCIES=`python setup.py dependencies \\\n    --extras ""$EXTRAS"" | tail -1`\nPACKAGES=""${PYTHON_CORE_PKGS} ${PYTHON_PACKAGES} ${PYOMO_DEPENDENCIES} ""\nif [[ ${{matrix.python}} == pypy* ]]; then\n    EXCLUDE=""$PYPY_EXCLUDE $EXCLUDE""\nfi\n# HACK: Remove problem packages on conda+Linux\nif test ""${{matrix.TARGET}}"" == linux; then\n    EXCLUDE=""casadi numdifftools pint $EXCLUDE""\nfi\nEXCLUDE=`echo ""$EXCLUDE"" | xargs`\nif test -n ""$EXCLUDE""; then\n    for WORD in $EXCLUDE; do\n        PACKAGES=${PACKAGES//$WORD / }\n    done\nfi\nfor PKG in $PACKAGES; do\n    if [[ "" $PYPI_ONLY "" == *"" $PKG ""* ]]; then\n        PYPI_DEPENDENCIES=""$PYPI_DEPENDENCIES $PKG""\n    else\n        CONDA_DEPENDENCIES=""$CONDA_DEPENDENCIES $PKG""\n    fi\ndone\necho ""*** Install Pyomo dependencies ***""\nconda install -q -y $CONDA_DEPENDENCIES\nif test -z ""${{matrix.slim}}""; then\n    echo ""*** Install CPLEX ***""\n    conda install -q -y \'cplex>=12.10\' docplex \\\n        || echo ""WARNING: CPLEX Community Edition is not available""\n    echo ""*** Install Gurobi ***""\n    conda install -q -y gurobi \\\n        || echo ""WARNING: Gurobi is not available""\n    echo ""*** Install Xpress ***""\n    conda install -q -y xpress \\\n        || echo ""WARNING: Xpress Community Edition is not available""\n    for PKG in cyipopt pymumps scip; do\n        echo ""*** Install $PKG ***""\n        conda install -q -y $PKG \\\n            || echo ""WARNING: $PKG is not available""\n    done\n    # TODO: This is a hack to stop test_qt.py from running until we\n    # can better troubleshoot why it fails on GHA\n    for QTPACKAGE in qt pyqt; do\n        # Because conda is insane, removing packages can cause\n        # unrelated packages to be updated (breaking version\n        # specifications specified previously, e.g., in\n        # setup.py).  There doesn\'t appear to be a good\n        # workaround, so we will just force-remove (recognizing\n        # that it may break other conda cruft).\n        conda remove --force-remove $QTPACKAGE \\\n            || echo ""$QTPACKAGE not in this environment""\n    done\nfi\n# Re-try Pyomo (optional) dependencies with pip\nif test -n ""$PYPI_DEPENDENCIES""; then\n    python -m pip install --cache-dir cache/pip $PYPI_DEPENDENCIES\nfi\n# remember this python interpreter\npython -c \'import sys; print(""PYTHON_EXE=%s"" \\\n    % (sys.executable,))\' >> $GITHUB_ENV\n#\n# conda activate puts itself first in the PATH, which overrides\n# any paths we add through GITHUB_PATH.  We will update .profile\n# to move the local runner paths back to the front (before conda).\nfor profile in $HOME/.profile $HOME/.bash_profile; do\n    if test ! -e $profile; then\n        continue\n    fi\n    echo \'\' >> $profile\n    echo \'export PATH=`echo ""$PATH"" \\\n        | tr "":"" ""\\\\n"" | grep runner | tr ""\\n"" "":""`:`echo ""$PATH"" \\\n        | tr "":"" ""\\\\n"" | grep -v runner | tr ""\\n"" "":""`\' >> $profile\ndone\necho """"\necho ""Final conda environment:""\nconda list | sed \'s/^/    /\'\n', 'TPL_DIR=""${GITHUB_WORKSPACE}/cache/tpl""\nmkdir -p ""$TPL_DIR""\nDOWNLOAD_DIR=""${GITHUB_WORKSPACE}/cache/download""\nmkdir -p ""$DOWNLOAD_DIR""\necho ""TPL_DIR=$TPL_DIR"" >> $GITHUB_ENV\necho ""DOWNLOAD_DIR=$DOWNLOAD_DIR"" >> $GITHUB_ENV\n', 'IPOPT_DIR=$TPL_DIR/ipopt\necho ""$IPOPT_DIR"" >> $GITHUB_PATH\necho ""LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$IPOPT_DIR"" >> $GITHUB_ENV\nmkdir -p $IPOPT_DIR\nIPOPT_TAR=${DOWNLOAD_DIR}/ipopt.tar.gz\nif test ! -e $IPOPT_TAR; then\n    echo ""...downloading Ipopt""\n    if test ""${{matrix.TARGET}}"" == osx; then\n        echo ""IDAES Ipopt not available on OSX""\n        exit 0\n    fi\n    URL=https://github.com/IDAES/idaes-ext\n    RELEASE=$(curl --max-time 150 --retry 8 \\\n        -L -s -H \'Accept: application/json\' ${URL}/releases/latest)\n    VER=$(echo $RELEASE | sed -e \'s/.*""tag_name"":""\\([^""]*\\)"".*/\\1/\')\n    URL=${URL}/releases/download/$VER\n    if test ""${{matrix.TARGET}}"" == linux; then\n        curl --max-time 150 --retry 8 \\\n            -L $URL/idaes-solvers-ubuntu2004-x86_64.tar.gz \\\n            > $IPOPT_TAR\n    else\n        curl --max-time 150 --retry 8 \\\n            -L $URL/idaes-solvers-windows-x86_64.tar.gz \\\n            $URL/idaes-lib-windows-x86_64.tar.gz > $IPOPT_TAR\n    fi\nfi\ncd $IPOPT_DIR\ntar -xzi < $IPOPT_TAR\necho """"\necho ""$IPOPT_DIR""\nls -l $IPOPT_DIR\n', '$GAMS_DIR = ""${env:TPL_DIR}/gams""\necho ""$GAMS_DIR"" | `\n    Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\necho ""LD_LIBRARY_PATH=${env:LD_LIBRARY_PATH}:$GAMS_DIR"" `\n    Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\necho ""DYLD_LIBRARY_PATH=${env:DYLD_LIBRARY_PATH}:$GAMS_DIR"" `\n    Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n$INSTALLER = ""${env:DOWNLOAD_DIR}/gams_install.exe""\n$URL = ""https://d37drm4t2jghv5.cloudfront.net/distributions/29.1.0""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    $URL = ""$URL/windows/windows_x64_64.exe""\n} elseif ( ""${{matrix.TARGET}}"" -eq ""osx"" ) {\n    $URL = ""$URL/macosx/osx_x64_64_sfx.exe""\n} else {\n    $URL = ""$URL/linux/linux_x64_64_sfx.exe""\n}\nif (-not (Test-Path ""$INSTALLER"" -PathType Leaf)) {\n    echo ""...downloading GAMS""\n    Invoke-WebRequest -Uri ""$URL"" -OutFile ""$INSTALLER"" `\n        -RetryIntervalSec 30 -MaximumRetryCount 8 -TimeoutSec 150\n}\necho ""...installing GAMS""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    Start-Process -FilePath ""$INSTALLER"" -ArgumentList `\n        ""/SP- /NORESTART /VERYSILENT /DIR=$GAMS_DIR /NOICONS"" `\n        -Wait\n} else {\n    chmod 777 $INSTALLER\n    Start-Process -FilePath ""$INSTALLER"" -ArgumentList `\n        ""-q -d $GAMS_DIR"" -Wait\n    mv $GAMS_DIR/*/* $GAMS_DIR/.\n}\necho """"\necho ""$GAMS_DIR""\nls -l $GAMS_DIR\n', 'GAMS_DIR=""${env:TPL_DIR}/gams""\npy_ver=$($PYTHON_EXE -c \'import sys;v=""_%s%s"" % sys.version_info[:2] \\\n    ;print(v if v != ""_27"" else """")\')\nif test -e $GAMS_DIR/apifiles/Python/api$py_ver; then\n    echo ""Installing GAMS Python bindings""\n    pushd $GAMS_DIR/apifiles/Python/api$py_ver\n    $PYTHON_EXE setup.py install\n    popd\nfi\n', '$BARON_DIR = ""${env:TPL_DIR}/baron""\necho ""$BARON_DIR"" | `\n    Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\n$URL = ""https://www.minlp.com/downloads/xecs/baron/current/""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    $INSTALLER = ""${env:DOWNLOAD_DIR}/baron_install.exe""\n    $URL += ""baron-win64.exe""\n} elseif ( ""${{matrix.TARGET}}"" -eq ""osx"" ) {\n    $INSTALLER = ""${env:DOWNLOAD_DIR}/baron_install.zip""\n    $URL += ""baron-osx64.zip""\n} else {\n    $INSTALLER = ""${env:DOWNLOAD_DIR}/baron_install.zip""\n    $URL += ""baron-lin64.zip""\n}\nif (-not (Test-Path ""$INSTALLER"" -PathType Leaf)) {\n    echo ""...downloading BARON ($URL)""\n    Invoke-WebRequest -Uri ""$URL"" -OutFile ""$INSTALLER"" `\n        -RetryIntervalSec 30 -MaximumRetryCount 8 -TimeoutSec 150\n}\necho ""...installing BARON""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    Start-Process -FilePath ""$INSTALLER"" -ArgumentList `\n        ""/SP- /NORESTART /VERYSILENT /DIR=$BARON_DIR /NOICONS"" `\n        -Wait\n} else {\n    unzip -q $INSTALLER\n    mv baron-* $BARON_DIR\n}\necho """"\necho ""$BARON_DIR""\nls -l $BARON_DIR\n', 'GJH_DIR=""$TPL_DIR/gjh""\necho ""${GJH_DIR}"" >> $GITHUB_PATH\nINSTALL_DIR=""${DOWNLOAD_DIR}/gjh""\nif test ! -e ""$INSTALL_DIR/bin""; then\n    mkdir -p ""$INSTALL_DIR""\n    INSTALLER=""$INSTALL_DIR/gjh_asl_json.zip""\n    URL=""https://codeload.github.com/ghackebeil/gjh_asl_json/zip/master""\n    curl --max-time 150 --retry 8 -L $URL > $INSTALLER\n    cd $INSTALL_DIR\n    unzip -q $INSTALLER\n    cd gjh_asl_json-master/Thirdparty\n    ./get.ASL\n    cd ..\n    make\n    mv bin ""$INSTALL_DIR/bin""\nfi\ncp -rp ""$INSTALL_DIR/bin"" ""$GJH_DIR""\necho """"\necho ""$GJH_DIR""\nls -l $GJH_DIR\n', 'echo """"\necho ""Clone Pyomo-model-libraries...""\nURL=https://github.com/Pyomo/pyomo-model-libraries.git\ngit clone -b ${SRC_REF##*/} $URL || git clone -b main $URL\necho """"\necho ""Install Pyomo...""\necho """"\n$PYTHON_EXE setup.py develop ${{matrix.setup_options}}\necho """"\necho ""Set custom PYOMO_CONFIG_DIR""\necho """"\necho ""PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config"" >> $GITHUB_ENV\n', '$PYTHON_EXE -m pip install --cache-dir cache/pip highspy \\\n    || echo ""WARNING: highspy is not available""\n', 'if test ""${{matrix.TARGET}}"" == win; then\n    COVERAGE_BASE=${GITHUB_WORKSPACE}\\\\.cover\nelse\n    COVERAGE_BASE=${GITHUB_WORKSPACE}/.cover\nfi\nCOVERAGE_RC=${COVERAGE_BASE}_rc\necho ""COVERAGE_RCFILE=$COVERAGE_RC"" >> $GITHUB_ENV\necho ""COVERAGE_PROCESS_START=$COVERAGE_RC"" >> $GITHUB_ENV\ncp ${GITHUB_WORKSPACE}/.coveragerc ${COVERAGE_RC}\necho ""data_file=${COVERAGE_BASE}age"" >> ${COVERAGE_RC}\nSITE_PACKAGES=$($PYTHON_EXE -c ""from distutils.sysconfig import \\\n    get_python_lib; print(get_python_lib())"")\necho ""Python site-packages: $SITE_PACKAGES""\necho \'import coverage; coverage.process_startup()\' \\\n    > ${SITE_PACKAGES}/run_coverage_at_startup.pth\n', 'echo """"\necho ""Pyomo download-extensions""\necho """"\npyomo download-extensions || exit 1\necho """"\necho ""Pyomo build-extensions""\necho """"\npyomo build-extensions --parallel 2\n', 'echo ""$PATH""\npyomo help --solvers || exit 1\npyomo help --transformations || exit 1\npyomo help --writers || exit 1\n', 'CATEGORY=\nfor cat in ${{matrix.category}}; do\n    CATEGORY+="" -m $cat""\ndone\n$PYTHON_EXE -m pytest -v \\\n    -W ignore::Warning $CATEGORY \\\n    pyomo `pwd`/pyomo-model-libraries \\\n    `pwd`/examples/pyomobook --junitxml=""TEST-pyomo.xml""\n', '# Manually invoke the DAT parser so that parse_table_datacmds.py\n# is fully generated by a single process before invoking MPI\n$PYTHON_EXE -c ""from pyomo.dataportal.parse_datacmds import \\\n    parse_data_commands; parse_data_commands(data=\'\')""\nmpirun -np ${{matrix.mpi}} --oversubscribe pytest -v \\\n    --junit-xml=TEST-pyomo-mpi.xml \\\n    -m ""mpi"" -W ignore::Warning \\\n    pyomo `pwd`/pyomo-model-libraries\n', 'make -C doc/OnlineDocs doctest -d\n', 'coverage combine\ncoverage report -i\ncoverage xml -i\n', 'echo """"\necho ""Install Pyomo...""\necho """"\npython setup.py develop\necho """"\necho ""Set custom PYOMO_CONFIG_DIR""\necho """"\necho ""PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config"" >> $GITHUB_ENV\n', 'echo ""$PATH""\npyomo help --solvers || exit 1\npyomo help --transformations || exit 1\npyomo help --writers || exit 1\n', 'echo """"\necho ""Running standalone Pyomo test""\necho """"\npython `pwd`/pyomo/environ/tests/standalone_minimal_pyomo_driver.py \\\n    || exit 1\n', 'python -c \'import sys;print(sys.executable)\'\npython -m pip install --cache-dir cache/pip --upgrade pip\nPYOMO_DEPENDENCIES=`python setup.py dependencies \\\n    --extras ""tests"" | tail -1`\npython -m pip install --cache-dir cache/pip \\\n    ${PYTHON_CORE_PKGS} ${PYOMO_DEPENDENCIES}\npython -c \'import sys; print(""PYTHON_EXE=%s"" \\\n    % (sys.executable,))\' >> $GITHUB_ENV\n', 'echo """"\necho ""Clone Pyomo-model-libraries...""\ngit clone https://github.com/Pyomo/pyomo-model-libraries.git\necho """"\necho ""Install Pyomo...""\necho """"\n$PYTHON_EXE setup.py develop ${{matrix.setup_options}}\necho """"\necho ""Set custom PYOMO_CONFIG_DIR""\necho """"\necho ""PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config"" >> $GITHUB_ENV\n', '# Manually invoke the DAT parser so that parse_table_datacmds.py\n# is generated before running ""coverage xml""\n$PYTHON_EXE -c ""from pyomo.dataportal.parse_datacmds import \\\n    parse_data_commands; parse_data_commands(data=\'\')""\n', 'set +e\nCODECOV=""${GITHUB_WORKSPACE}/codecov.sh""\necho ""CODECOV=$CODECOV"" >> $GITHUB_ENV\nfor i in `seq 3`; do\n    echo ""Downloading current codecov script (attempt ${i})""\n    curl -L https://codecov.io/bash -o $CODECOV\n    if test $? == 0; then\n        break\n    fi\n    DELAY=$(( RANDOM % 30 + 30))\n    echo ""Pausing $DELAY seconds before re-attempting download""\n    sleep $DELAY\ndone\nif test ! -e $CODECOV; then\n    echo ""Failed to download codecov.sh""\n    exit 1\nfi\n', 'set +e\nif [ ""$GITHUB_EVENT_NAME"" == ""pull_request"" ]; then\n    SHA=$(jq --raw-output .pull_request.head.sha ""$GITHUB_EVENT_PATH"")\nelse\n    SHA=$GITHUB_SHA\nfi\nfor ARTIFACT in artifacts/*_*${{matrix.TARGET}}_*; do\n    NAME=`echo $ARTIFACT | cut -d/ -f2`\n    cp -v $ARTIFACT/.coverage .coverage-$NAME\ndone\nrm -vf .coverage coverage.xml\necho ""Build: ${{ matrix.TARGET }}/other""\necho """"\nFILES=.coverage-*_other-*\ncoverage combine --debug=dataio $FILES\nif test ! -e .coverage; then\n    echo ""No coverage to upload.""\nelse\n    coverage xml || coverage xml -i\n    mv -v coverage.xml coverage-other.xml\nfi\necho """"\necho ""Build: ${{ matrix.TARGET }}""\necho """"\nFILES=.coverage-*_${{matrix.TARGET}}-*\ncoverage combine --debug=dataio $FILES\nrm -vf artifacts/*/*.xml\nif test ! -e .coverage; then\n    echo ""No coverage to upload.""\nelse\n    coverage xml || coverage xml -i\nfi\n', 'pip install black\nblack . -S -C --check --diff --exclude examples/pyomobook/python-ch/BadIndent.py\n', 'JOB=""${{matrix.TARGET}}/${{matrix.python}}${{matrix.other}}""\necho ""GHA_JOBNAME=$JOB"" | sed \'s|/|_|g\' >> $GITHUB_ENV\nif test -z ""${{matrix.other}}""; then\n    echo ""GHA_JOBGROUP=${{matrix.TARGET}}"" >> $GITHUB_ENV\nelse\n    echo ""GHA_JOBGROUP=other"" >> $GITHUB_ENV\nfi\n# Note: pandas 1.0.3 causes gams 29.1.0 import to fail in python 3.8\nEXTRAS=tests\nif test -z ""${{matrix.slim}}""; then\n    EXTRAS=""$EXTRAS,docs,optional""\nfi\necho ""EXTRAS=$EXTRAS"" >> $GITHUB_ENV\nPYTHON_PACKAGES=""${{matrix.PACKAGES}}""\necho ""PYTHON_PACKAGES=$PYTHON_PACKAGES"" \\\n    | tr \'\\n\' \' \' | sed \'s/ \\+/ /g\' >> $GITHUB_ENV\n', 'CURLRC=""$(cat <<EOF\n   retry = 0\n   max-time = 30\nEOF\n)""\necho ""$CURLRC"" > ${GITHUB_WORKSPACE}/.curlrc\necho ""$CURLRC"" > ${GITHUB_WORKSPACE}/_curlrc\necho ""CURL_HOME=$GITHUB_WORKSPACE"" >> $GITHUB_ENV\n', 'mkdir -p ${GITHUB_WORKSPACE}/cache/os\nexport HOMEBREW_CACHE=${GITHUB_WORKSPACE}/cache/os\n# Be cautious running brew update: it can break\n#    setup-python on OSX\n# brew update\n#\n# Notes:\n#  - install glpk\n#  - pyodbc needs: gcc pkg-config unixodbc freetds\nfor pkg in bash pkg-config unixodbc freetds glpk; do\n    brew list $pkg || brew install $pkg\ndone\n', 'mkdir -p ${GITHUB_WORKSPACE}/cache/os\n# Notes:\n#  - install glpk\n#  - ipopt needs: libopenblas-dev gfortran liblapack-dev\nsudo apt-get -o Dir::Cache=${GITHUB_WORKSPACE}/cache/os \\\n    install libopenblas-dev gfortran liblapack-dev glpk-utils\nsudo chmod -R 777 ${GITHUB_WORKSPACE}/cache/os\n', 'echo ""SETUPTOOLS_USE_DISTUTILS=local"" >> $GITHUB_ENV\n', 'python -c \'import sys;print(sys.executable)\'\npython -m pip install --cache-dir cache/pip --upgrade pip\nPYOMO_DEPENDENCIES=`python setup.py dependencies \\\n    --extras ""$EXTRAS"" | tail -1`\nPACKAGES=""${PYTHON_CORE_PKGS} ${PYTHON_PACKAGES} ${PYOMO_DEPENDENCIES} ""\nif [[ ${{matrix.python}} == pypy* ]]; then\n    EXCLUDE=""$PYPY_EXCLUDE $EXCLUDE""\nfi\nEXCLUDE=`echo ""$EXCLUDE"" | xargs`\nif test -n ""$EXCLUDE""; then\n    for WORD in $EXCLUDE; do\n        PACKAGES=${PACKAGES//$WORD / }\n    done\nfi\npython -m pip install --cache-dir cache/pip ${PACKAGES}\npython -m pip install --cache-dir cache/pip pymysql || \\\n    python -m pip install --cache-dir cache/pip pymysql\nif test -z ""${{matrix.slim}}""; then\n    python -m pip install --cache-dir cache/pip cplex docplex \\\n        || echo ""WARNING: CPLEX Community Edition is not available""\n    python -m pip install --cache-dir cache/pip \\\n        -i https://pypi.gurobi.com gurobipy \\\n        || echo ""WARNING: Gurobi is not available""\n    python -m pip install --cache-dir cache/pip xpress \\\n        || echo ""WARNING: Xpress Community Edition is not available""\nfi\npython -c \'import sys; print(""PYTHON_EXE=%s"" \\\n    % (sys.executable,))\' >> $GITHUB_ENV\necho """"\necho ""Final pip environment:""\npython -m pip list | sed \'s/^/    /\'\n', '# Set up environment\nmkdir -p $GITHUB_WORKSPACE/cache/conda\nconda config --set always_yes yes\nconda config --set auto_update_conda false\nconda config --prepend pkgs_dirs $GITHUB_WORKSPACE/cache/conda\n# Try to install mamba\nconda install -q -y -n base conda-libmamba-solver || MAMBA_FAILED=1\nif test -z ""$MAMBA_FAILED""; then\n    echo ""*** Activating the mamba environment solver ***""\n    conda config --set solver libmamba\nfi\n# Print environment info\necho ""*** CONDA environment: ***""\nconda info\nconda config --show-sources\nconda config --show channels\nconda list --show-channel-urls\nwhich python\npython --version\n# Note: some pypi packages are not available through conda\nPYOMO_DEPENDENCIES=`python setup.py dependencies \\\n    --extras ""$EXTRAS"" | tail -1`\nPACKAGES=""${PYTHON_CORE_PKGS} ${PYTHON_PACKAGES} ${PYOMO_DEPENDENCIES} ""\nif [[ ${{matrix.python}} == pypy* ]]; then\n    EXCLUDE=""$PYPY_EXCLUDE $EXCLUDE""\nfi\n# HACK: Remove problem packages on conda+Linux\nif test ""${{matrix.TARGET}}"" == linux; then\n    EXCLUDE=""casadi numdifftools pint $EXCLUDE""\nfi\nEXCLUDE=`echo ""$EXCLUDE"" | xargs`\nif test -n ""$EXCLUDE""; then\n    for WORD in $EXCLUDE; do\n        PACKAGES=${PACKAGES//$WORD / }\n    done\nfi\nfor PKG in $PACKAGES; do\n    if [[ "" $PYPI_ONLY "" == *"" $PKG ""* ]]; then\n        PYPI_DEPENDENCIES=""$PYPI_DEPENDENCIES $PKG""\n    else\n        CONDA_DEPENDENCIES=""$CONDA_DEPENDENCIES $PKG""\n    fi\ndone\necho ""*** Install Pyomo dependencies ***""\nconda install -q -y $CONDA_DEPENDENCIES\nif test -z ""${{matrix.slim}}""; then\n    echo ""*** Install CPLEX ***""\n    conda install -q -y \'cplex>=12.10\' docplex \\\n        || echo ""WARNING: CPLEX Community Edition is not available""\n    echo ""*** Install Gurobi ***""\n    conda install -q -y gurobi \\\n        || echo ""WARNING: Gurobi is not available""\n    echo ""*** Install Xpress ***""\n    conda install -q -y xpress \\\n        || echo ""WARNING: Xpress Community Edition is not available""\n    for PKG in cyipopt pymumps scip; do\n        echo ""*** Install $PKG ***""\n        conda install -q -y $PKG \\\n            || echo ""WARNING: $PKG is not available""\n    done\n    # TODO: This is a hack to stop test_qt.py from running until we\n    # can better troubleshoot why it fails on GHA\n    for QTPACKAGE in qt pyqt; do\n        # Because conda is insane, removing packages can cause\n        # unrelated packages to be updated (breaking version\n        # specifications specified previously, e.g., in\n        # setup.py).  There doesn\'t appear to be a good\n        # workaround, so we will just force-remove (recognizing\n        # that it may break other conda cruft).\n        conda remove --force-remove $QTPACKAGE \\\n            || echo ""$QTPACKAGE not in this environment""\n    done\nfi\n# Re-try Pyomo (optional) dependencies with pip\nif test -n ""$PYPI_DEPENDENCIES""; then\n    python -m pip install --cache-dir cache/pip $PYPI_DEPENDENCIES\nfi\n# remember this python interpreter\npython -c \'import sys; print(""PYTHON_EXE=%s"" \\\n    % (sys.executable,))\' >> $GITHUB_ENV\n#\n# conda activate puts itself first in the PATH, which overrides\n# any paths we add through GITHUB_PATH.  We will update .profile\n# to move the local runner paths back to the front (before conda).\nfor profile in $HOME/.profile $HOME/.bash_profile; do\n    if test ! -e $profile; then\n        continue\n    fi\n    echo \'\' >> $profile\n    echo \'export PATH=`echo ""$PATH"" \\\n        | tr "":"" ""\\\\n"" | grep runner | tr ""\\n"" "":""`:`echo ""$PATH"" \\\n        | tr "":"" ""\\\\n"" | grep -v runner | tr ""\\n"" "":""`\' >> $profile\ndone\necho """"\necho ""Final conda environment:""\nconda list | sed \'s/^/    /\'\n', 'TPL_DIR=""${GITHUB_WORKSPACE}/cache/tpl""\nmkdir -p ""$TPL_DIR""\nDOWNLOAD_DIR=""${GITHUB_WORKSPACE}/cache/download""\nmkdir -p ""$DOWNLOAD_DIR""\necho ""TPL_DIR=$TPL_DIR"" >> $GITHUB_ENV\necho ""DOWNLOAD_DIR=$DOWNLOAD_DIR"" >> $GITHUB_ENV\n', 'IPOPT_DIR=$TPL_DIR/ipopt\necho ""$IPOPT_DIR"" >> $GITHUB_PATH\necho ""LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$IPOPT_DIR"" >> $GITHUB_ENV\nmkdir -p $IPOPT_DIR\nIPOPT_TAR=${DOWNLOAD_DIR}/ipopt.tar.gz\nif test ! -e $IPOPT_TAR; then\n    echo ""...downloading Ipopt""\n    if test ""${{matrix.TARGET}}"" == osx; then\n        echo ""IDAES Ipopt not available on OSX""\n        exit 0\n    fi\n    URL=https://github.com/IDAES/idaes-ext\n    RELEASE=$(curl --max-time 150 --retry 8 \\\n        -L -s -H \'Accept: application/json\' ${URL}/releases/latest)\n    VER=$(echo $RELEASE | sed -e \'s/.*""tag_name"":""\\([^""]*\\)"".*/\\1/\')\n    URL=${URL}/releases/download/$VER\n    if test ""${{matrix.TARGET}}"" == linux; then\n        curl --max-time 150 --retry 8 \\\n            -L $URL/idaes-solvers-ubuntu2004-x86_64.tar.gz \\\n            > $IPOPT_TAR\n    else\n        curl --max-time 150 --retry 8 \\\n            -L $URL/idaes-solvers-windows-x86_64.tar.gz \\\n            $URL/idaes-lib-windows-x86_64.tar.gz > $IPOPT_TAR\n    fi\nfi\ncd $IPOPT_DIR\ntar -xzi < $IPOPT_TAR\necho """"\necho ""$IPOPT_DIR""\nls -l $IPOPT_DIR\n', '$GAMS_DIR = ""${env:TPL_DIR}/gams""\necho ""$GAMS_DIR"" | `\n    Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\necho ""LD_LIBRARY_PATH=${env:LD_LIBRARY_PATH}:$GAMS_DIR"" `\n    Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\necho ""DYLD_LIBRARY_PATH=${env:DYLD_LIBRARY_PATH}:$GAMS_DIR"" `\n    Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n$INSTALLER = ""${env:DOWNLOAD_DIR}/gams_install.exe""\n$URL = ""https://d37drm4t2jghv5.cloudfront.net/distributions/29.1.0""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    $URL = ""$URL/windows/windows_x64_64.exe""\n} elseif ( ""${{matrix.TARGET}}"" -eq ""osx"" ) {\n    $URL = ""$URL/macosx/osx_x64_64_sfx.exe""\n} else {\n    $URL = ""$URL/linux/linux_x64_64_sfx.exe""\n}\nif (-not (Test-Path ""$INSTALLER"" -PathType Leaf)) {\n    echo ""...downloading GAMS""\n    Invoke-WebRequest -Uri ""$URL"" -OutFile ""$INSTALLER"" `\n        -RetryIntervalSec 30 -MaximumRetryCount 8 -TimeoutSec 150\n}\necho ""...installing GAMS""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    Start-Process -FilePath ""$INSTALLER"" -ArgumentList `\n        ""/SP- /NORESTART /VERYSILENT /DIR=$GAMS_DIR /NOICONS"" `\n        -Wait\n} else {\n    chmod 777 $INSTALLER\n    Start-Process -FilePath ""$INSTALLER"" -ArgumentList `\n        ""-q -d $GAMS_DIR"" -Wait\n    mv $GAMS_DIR/*/* $GAMS_DIR/.\n}\necho """"\necho ""$GAMS_DIR""\nls -l $GAMS_DIR\n', 'GAMS_DIR=""${env:TPL_DIR}/gams""\npy_ver=$($PYTHON_EXE -c \'import sys;v=""_%s%s"" % sys.version_info[:2] \\\n    ;print(v if v != ""_27"" else """")\')\nif test -e $GAMS_DIR/apifiles/Python/api$py_ver; then\n    echo ""Installing GAMS Python bindings""\n    pushd $GAMS_DIR/apifiles/Python/api$py_ver\n    $PYTHON_EXE setup.py install\n    popd\nfi\n', '$BARON_DIR = ""${env:TPL_DIR}/baron""\necho ""$BARON_DIR"" | `\n    Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\n$URL = ""https://www.minlp.com/downloads/xecs/baron/current/""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    $INSTALLER = ""${env:DOWNLOAD_DIR}/baron_install.exe""\n    $URL += ""baron-win64.exe""\n} elseif ( ""${{matrix.TARGET}}"" -eq ""osx"" ) {\n    $INSTALLER = ""${env:DOWNLOAD_DIR}/baron_install.zip""\n    $URL += ""baron-osx64.zip""\n} else {\n    $INSTALLER = ""${env:DOWNLOAD_DIR}/baron_install.zip""\n    $URL += ""baron-lin64.zip""\n}\nif (-not (Test-Path ""$INSTALLER"" -PathType Leaf)) {\n    echo ""...downloading BARON ($URL)""\n    Invoke-WebRequest -Uri ""$URL"" -OutFile ""$INSTALLER"" `\n        -RetryIntervalSec 30 -MaximumRetryCount 8 -TimeoutSec 150\n}\necho ""...installing BARON""\nif ( ""${{matrix.TARGET}}"" -eq ""win"" ) {\n    Start-Process -FilePath ""$INSTALLER"" -ArgumentList `\n        ""/SP- /NORESTART /VERYSILENT /DIR=$BARON_DIR /NOICONS"" `\n        -Wait\n} else {\n    unzip -q $INSTALLER\n    mv baron-* $BARON_DIR\n}\necho """"\necho ""$BARON_DIR""\nls -l $BARON_DIR\n', 'GJH_DIR=""$TPL_DIR/gjh""\necho ""${GJH_DIR}"" >> $GITHUB_PATH\nINSTALL_DIR=""${DOWNLOAD_DIR}/gjh""\nif test ! -e ""$INSTALL_DIR/bin""; then\n    mkdir -p ""$INSTALL_DIR""\n    INSTALLER=""$INSTALL_DIR/gjh_asl_json.zip""\n    URL=""https://codeload.github.com/ghackebeil/gjh_asl_json/zip/master""\n    curl --max-time 150 --retry 8 -L $URL > $INSTALLER\n    cd $INSTALL_DIR\n    unzip -q $INSTALLER\n    cd gjh_asl_json-master/Thirdparty\n    ./get.ASL\n    cd ..\n    make\n    mv bin ""$INSTALL_DIR/bin""\nfi\ncp -rp ""$INSTALL_DIR/bin"" ""$GJH_DIR""\necho """"\necho ""$GJH_DIR""\nls -l $GJH_DIR\n', 'echo """"\necho ""Clone Pyomo-model-libraries...""\nURL=https://github.com/Pyomo/pyomo-model-libraries.git\ngit clone -b ${SRC_REF##*/} $URL || git clone -b main $URL\necho """"\necho ""Install Pyomo...""\necho """"\n$PYTHON_EXE setup.py develop ${{matrix.setup_options}}\necho """"\necho ""Set custom PYOMO_CONFIG_DIR""\necho """"\necho ""PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config"" >> $GITHUB_ENV\n', '$PYTHON_EXE -m pip install --cache-dir cache/pip highspy \\\n    || echo ""WARNING: highspy is not available""\n', 'if test ""${{matrix.TARGET}}"" == win; then\n    COVERAGE_BASE=${GITHUB_WORKSPACE}\\\\.cover\nelse\n    COVERAGE_BASE=${GITHUB_WORKSPACE}/.cover\nfi\nCOVERAGE_RC=${COVERAGE_BASE}_rc\necho ""COVERAGE_RCFILE=$COVERAGE_RC"" >> $GITHUB_ENV\necho ""COVERAGE_PROCESS_START=$COVERAGE_RC"" >> $GITHUB_ENV\ncp ${GITHUB_WORKSPACE}/.coveragerc ${COVERAGE_RC}\necho ""data_file=${COVERAGE_BASE}age"" >> ${COVERAGE_RC}\nSITE_PACKAGES=$($PYTHON_EXE -c ""from distutils.sysconfig import \\\n    get_python_lib; print(get_python_lib())"")\necho ""Python site-packages: $SITE_PACKAGES""\necho \'import coverage; coverage.process_startup()\' \\\n    > ${SITE_PACKAGES}/run_coverage_at_startup.pth\n', 'echo """"\necho ""Pyomo download-extensions""\necho """"\npyomo download-extensions || exit 1\necho """"\necho ""Pyomo build-extensions""\necho """"\npyomo build-extensions --parallel 2\n', 'echo ""$PATH""\npyomo help --solvers || exit 1\npyomo help --transformations || exit 1\npyomo help --writers || exit 1\n', 'CATEGORY=\nfor cat in ${{matrix.category}}; do\n    CATEGORY+="" -m $cat""\ndone\n$PYTHON_EXE -m pytest -v \\\n    -W ignore::Warning $CATEGORY \\\n    pyomo `pwd`/pyomo-model-libraries \\\n    `pwd`/examples/pyomobook --junitxml=""TEST-pyomo.xml""\n', '# Manually invoke the DAT parser so that parse_table_datacmds.py\n# is fully generated by a single process before invoking MPI\n$PYTHON_EXE -c ""from pyomo.dataportal.parse_datacmds import \\\n    parse_data_commands; parse_data_commands(data=\'\')""\nmpirun -np ${{matrix.mpi}} --oversubscribe pytest -v \\\n    --junit-xml=TEST-pyomo-mpi.xml \\\n    -m ""mpi"" -W ignore::Warning \\\n    pyomo `pwd`/pyomo-model-libraries\n', 'make -C doc/OnlineDocs doctest -d\n', 'coverage combine\ncoverage report -i\ncoverage xml -i\n', 'echo """"\necho ""Install Pyomo...""\necho """"\npython setup.py develop\necho """"\necho ""Set custom PYOMO_CONFIG_DIR""\necho """"\necho ""PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config"" >> $GITHUB_ENV\n', 'echo ""$PATH""\npyomo help --solvers || exit 1\npyomo help --transformations || exit 1\npyomo help --writers || exit 1\n', 'echo """"\necho ""Running standalone Pyomo test""\necho """"\npython `pwd`/pyomo/environ/tests/standalone_minimal_pyomo_driver.py \\\n    || exit 1\n', 'python -c \'import sys;print(sys.executable)\'\npython -m pip install --cache-dir cache/pip --upgrade pip\nPYOMO_DEPENDENCIES=`python setup.py dependencies \\\n    --extras ""tests"" | tail -1`\npython -m pip install --cache-dir cache/pip \\\n    ${PYTHON_CORE_PKGS} ${PYOMO_DEPENDENCIES}\npython -c \'import sys; print(""PYTHON_EXE=%s"" \\\n    % (sys.executable,))\' >> $GITHUB_ENV\n', 'echo """"\necho ""Clone Pyomo-model-libraries...""\ngit clone https://github.com/Pyomo/pyomo-model-libraries.git\necho """"\necho ""Install Pyomo...""\necho """"\n$PYTHON_EXE setup.py develop ${{matrix.setup_options}}\necho """"\necho ""Set custom PYOMO_CONFIG_DIR""\necho """"\necho ""PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config"" >> $GITHUB_ENV\n', '# Manually invoke the DAT parser so that parse_table_datacmds.py\n# is generated before running ""coverage xml""\n$PYTHON_EXE -c ""from pyomo.dataportal.parse_datacmds import \\\n    parse_data_commands; parse_data_commands(data=\'\')""\n', 'set +e\nCODECOV=""${GITHUB_WORKSPACE}/codecov.sh""\necho ""CODECOV=$CODECOV"" >> $GITHUB_ENV\nfor i in `seq 3`; do\n    echo ""Downloading current codecov script (attempt ${i})""\n    curl -L https://codecov.io/bash -o $CODECOV\n    if test $? == 0; then\n        break\n    fi\n    DELAY=$(( RANDOM % 30 + 30))\n    echo ""Pausing $DELAY seconds before re-attempting download""\n    sleep $DELAY\ndone\nif test ! -e $CODECOV; then\n    echo ""Failed to download codecov.sh""\n    exit 1\nfi\n', 'set +e\nif [ ""$GITHUB_EVENT_NAME"" == ""pull_request"" ]; then\n    SHA=$(jq --raw-output .pull_request.head.sha ""$GITHUB_EVENT_PATH"")\nelse\n    SHA=$GITHUB_SHA\nfi\nfor ARTIFACT in artifacts/*_*${{matrix.TARGET}}_*; do\n    NAME=`echo $ARTIFACT | cut -d/ -f2`\n    cp -v $ARTIFACT/.coverage .coverage-$NAME\ndone\nrm -vf .coverage coverage.xml\necho ""Build: ${{ matrix.TARGET }}/other""\necho """"\nFILES=.coverage-*_other-*\ncoverage combine --debug=dataio $FILES\nif test ! -e .coverage; then\n    echo ""No coverage to upload.""\nelse\n    coverage xml || coverage xml -i\n    mv -v coverage.xml coverage-other.xml\nfi\necho """"\necho ""Build: ${{ matrix.TARGET }}""\necho """"\nFILES=.coverage-*_${{matrix.TARGET}}-*\ncoverage combine --debug=dataio $FILES\nrm -vf artifacts/*/*.xml\nif test ! -e .coverage; then\n    echo ""No coverage to upload.""\nelse\n    coverage xml || coverage xml -i\nfi\n']"
"['pip install --no-deps -e .\n', 'flake8 . \n', 'black --check . \n', 'isort --check . \n', ""CFLAGS='-stdlib=libc++' pytest --cov=./ --cov-report=xml\n"", 'pytest --cov=./ --cov-report=xml\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'python setup.py sdist']"
"['nix-build', 'nix-shell --run ""make linkcheck""', 'nix-shell --run ""./run_code_block_tests.sh""']"
"['sudo apt-get update\n\n# SCSS\nsudo apt-get install sassc\n\n# Python Lint\nsudo apt-get install pylint\n\n# Man\nsudo apt-get install scdoc\n\n# Build System\nsudo apt-get install meson\n\n# Locales\nsudo apt-get install gettext intltool\n\n# Install OpenRazer Python library for validation\nsudo add-apt-repository ppa:openrazer/daily\nsudo apt-get install python3-openrazer\n\n# Application Dependencies\nsudo apt-get install python3-colorama python3-colour python3-requests python3-setproctitle imagemagick\nsudo apt-get install gir1.2-gtk-3.0 gir1.2-appindicator3-0.1\nsudo apt-get install python3-pyqt5 python3-pyqt5.qtsvg python3-pyqt5.qtwebengine\n', './scripts/validate-json.py\n', './scripts/validate-scss.sh\n', './scripts/validate-py.sh\n', './scripts/validate-scdoc.sh\n', 'meson build --prefix=/tmp/pkg/\nninja -C build install\n', './scripts/create-locales.sh\n', './scripts/build-locales.sh\n', './scripts/create-files-for-opt.sh /tmp/dist\n', 'sudo apt-get update\n\n# Build Dependencies\nsudo apt-get install gettext sassc\n\n# Application Dependencies\nsudo apt-get install python3-colorama python3-colour python3-requests python3-setproctitle imagemagick\nsudo apt-get install gir1.2-gtk-3.0 gir1.2-appindicator3-0.1\nsudo apt-get install python3-pyqt5 python3-pyqt5.qtsvg python3-pyqt5.qtwebengine\n', 'git clone https://github.com/openrazer/openrazer --depth=1 ""$GITHUB_WORKSPACE/openrazer""\nsudo add-apt-repository ppa:openrazer/daily\nsudo apt-get install openrazer-daemon python3-openrazer dbus-x11\n', './scripts/build-styles.sh\n./scripts/build-locales.sh\n', './tests/run.sh --verbose\n', 'eval $(sudo dbus-launch --sh-syntax)\nsudo -E ./tests/openrazer/run_daemon.sh ""$GITHUB_WORKSPACE/openrazer""\n', 'sudo apt-get update\n\n# Build Dependencies\nsudo apt-get install sassc meson gettext intltool\n\n# Debian Packaging\nsudo apt-get install debhelper devscripts\n', 'git clean -df\ngit checkout .\nsed -i ""s/focal/UNRELEASED/g"" debian/changelog\ndpkg-buildpackage -S -us -uc -d\nlintian --no-tag-display-limit\ndebuild -b\n', 'mkdir -p ~/.gnupg/\nchmod 700 ~/.gnupg\nprintf ""$BOT_GPG_KEY_BASE64"" | base64 --decode > ~/.gnupg/private.key\ngpg --import ~/.gnupg/private.key\ngit fetch --unshallow --tags\n./scripts/packaging/prepare-preview-ppa.sh ./\n']"
"['conda install -c conda-forge python-graphviz\nconda install pip\npip install .\npip install .[dev]\npip install .[jax]\n', 'rm -rd docs/_build/html\nsphinx-build docs docs/_build/html\ntouch docs/_build/html/.nojekyll\n', 'conda install -c conda-forge python-graphviz\nconda install pip\npip install .\npip install .[dev]\npip install .[jax]\n', 'BUILDDIR=_build/html/main make -C docs/ local', 'sphinx-build docs docs/_build/html\ntouch docs/_build/html/.nojekyll\n', 'python3 -m pip install --upgrade build', 'python3 -m build', 'python3 -m pip install --upgrade twine', 'python3 -m twine upload --skip-existing --repository testpypi dist/*', 'python -m venv venv-test-pypi\nvenv-test-pypi/bin/python -m pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple bambi\nvenv-test-pypi/bin/python -c ""import bambi; assert bambi.__version__ == \'${{  github.ref_name }}\'""\n', 'python3 -m pip install --upgrade twine', 'python3 -m twine upload dist/*', 'conda info\nconda list\n', 'conda install -c conda-forge python-graphviz\nconda install pip\npip install .\npip install .[dev]\npip install .[jax]\n\npython --version\nconda list\npip freeze\n', 'echo ""Running black...""\nblack bambi --check\n\necho ""Checking code style with pylint...""\npylint bambi\n', 'python -m pytest -vv --cov=bambi --cov-report=term --cov-report=xml tests']"
"['echo ""PUBLISH=$(echo true)"" >> $GITHUB_ENV', 'pipx install ""poetry==1.5.0""', 'poetry install --extras ""docs lint""', 'python -V\npoetry run python -V\n', ""pushd docs; make SPHINXBUILD='poetry run sphinx-build' html; popd\n"", 'pipx install ""poetry==1.5.0""', 'sudo apt install libevent-dev libncurses5-dev libtinfo-dev libutempter-dev bison\nmkdir ~/tmux-builds\nmkdir ~/tmux-src\ngit clone https://github.com/tmux/tmux.git ~/tmux-src/tmux-${{ matrix.tmux-version }}\ncd ~/tmux-src/tmux-${{ matrix.tmux-version }}\ngit checkout ${{ matrix.tmux-version }}\nsh autogen.sh\n./configure --prefix=$HOME/tmux-builds/tmux-${{ matrix.tmux-version }} && make && make install\nexport PATH=$HOME/tmux-builds/tmux-${{ matrix.tmux-version }}/bin:$PATH\ncd ~\ntmux -V\n', 'poetry install -E ""test coverage lint""\n', 'poetry run ruff .\n', 'poetry run mypy .', 'python -V\npoetry run python -V\n', 'sudo apt install libevent-2.1-7\nexport PATH=$HOME/tmux-builds/tmux-${{ matrix.tmux-version }}/bin:$PATH\nls $HOME/tmux-builds/tmux-${{ matrix.tmux-version }}/bin\ntmux -V\npoetry run py.test --cov=./ --cov-append --cov-report=xml\n', 'pipx install ""poetry==1.5.0""', 'poetry build']"
"['sudo apt update\nsudo apt upgrade -y\nsudo apt install -y build-essential python3-dev python3-pip python3-wheel python3-setuptools\n', 'echo todo\n', 'mkdir build\nmkdir build/bin\npython --version\npython -m pip --version\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --user --upgrade -r requirements.txt\npython -m pip install --user --upgrade .\n', 'Copy-Item %APPDATA%\\Python\\Python*\\Scripts\\cemu.exe build\\bin\\\n', 'cp -v ~/.local/bin/cemu build/bin/\n']"
"['python -m pip install --upgrade --disable-pip-version-check ""pip>=21.1""', 'python -m pip install --upgrade tox', 'make build-deb\n\n# installation test\nDEB_PACKAGE=$(find dist -type f -name \\*.deb)\nsudo dpkg -i $DEB_PACKAGE\n', './scripts/build_macos_binary.sh\n', 'BIN_PATH=${DIST_DIR_NAME}/${BIN_NAME}.exe\nSYSTEM=$(python3 -c ""import platform; print(platform.system().casefold())"")\nMACHINE=$(python3 -c ""import platform; machine=platform.machine().casefold(); print(\'amd64\' if machine == \'x86_64\' else machine)"")\nARCHIVE_PATH=${DIST_DIR_NAME}/${BIN_NAME}_${SYSTEM}_${MACHINE}.zip\n\npython -m pip install -q --upgrade .[all,buildexe]\n\npyinstaller cli.py --onefile --name ""$BIN_NAME"" --clean --noconfirm --specpath build\n${BIN_PATH} version\n\npowershell compress-archive -Force ""$BIN_PATH"" ""$ARCHIVE_PATH""\n', 'set -x\n\nmkdir -p ""$SHA_DIR""\ncd ""$DIST_DIR_NAME""\nls -l --time-style=long-iso --file-type --human-readable --group-directories-first\nsha256sum ${BIN_NAME}_* > ""../${SHA_DIR}/${SHA_TEXT_FILE}""\n', 'echo ""version=$(./docker/extract_version.sh)"" >> $GITHUB_OUTPUT', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ""$REGISTRY"" -u ${{ github.actor }} --password-stdin', 'MAX_ATTEMPT=30\nBASE_SLEEP=1\nCMD=""python3 -m pip install --retries 30 --disable-pip-version-check sqlitebiter==${PKG_VERSION}""\n\n$CMD && exit 0\nfor attempt_num in $(seq $MAX_ATTEMPT); do\n    SLEEP=$(echo ""$BASE_SLEEP * $attempt_num"" | bc)\n    echo ""\'$CMD\' failed. retrying in $SLEEP seconds..."" 1>&2\n    sleep ""$SLEEP""\n\n    $CMD && exit 0\ndone\n\nexit 1\n', 'IMAGE_TAG=${REGISTRY}/${OWNER}/${REPO}:${PKG_VERSION}\n\ndocker buildx build \\\n  -t ""$IMAGE_TAG"" \\\n  --platform=$PLATFORMS \\\n  --build-arg version=$PKG_VERSION \\\n  --push .\n', 'python -m pip install --upgrade --disable-pip-version-check ""pip>=21.1""', 'python -m pip install --upgrade tox', 'tox -e py', 'python -m pip install --upgrade .[test] -c constraints.txt\npytest test\n', 'curl -sSL https://raw.githubusercontent.com/thombashi/sqlitebiter/master/scripts/installer.sh | sudo bash -x']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -e .[testing]\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=20 --statistics\n', 'make\nmake coverage\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install flake8\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --builtins unicode --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --builtins unicode --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine build\n', 'python -m build --sdist --wheel --outdir dist/ .        ']"
"['pip install -r dev-requirements.txt', 'pytest --cov=./pur --cov-report=xml']"
"['sudo apt install -y libgirepository1.0-dev\npython -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python setup.py bdist\npython setup.py build\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['make check\n', 'python -m pip install --upgrade --disable-pip-version-check ""pip>=21.1""', 'make setup-ci', 'tox --verbose -e py', 'make setup-ci', 'tox -e cov', 'python -m pip install --upgrade --disable-pip-version-check coveralls tomli\ncoveralls --service=github\n']"
"[""python -m pip install pipenv==2023.2.4\npipenv --python `python --version | grep -Eo '3\\.[0-9]+'` sync --dev\n"", 'pipenv --python `python --version | grep -Eo \'3\\.[0-9]+\'` run make -C docs html SPHINXOPTS=""-W -n""', ""python -m pip install pipenv==2023.2.4\npipenv --python `python --version | grep -Eo '3\\.[0-9]+'` sync --dev\n"", ""pipenv --python `python --version | grep -Eo '3\\.[0-9]+'` run pylint instaloader"", ""pipenv --python `python --version | grep -Eo '3\\.[0-9]+'` run mypy -m instaloader"", 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\/v/}', 'python setup.py sdist', 'deploy/arch/deploy.sh $VERSION_TAG', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\/v/}', 'python deploy/windows/create_exe.py']"
"['echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")\n', 'curl -fsS https://install.python-poetry.org | python - --preview -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'echo ""$APPDATA\\Python\\Scripts"" >> $GITHUB_PATH', 'poetry config virtualenvs.create false', 'poetry install --only main --only test --only benchmark -vvv', 'curl -fsS https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry build --format sdist', 'ls -la dist\n', '[[ ""${GITHUB_REF#refs/tags/}"" =~ ^[0-9]+\\.[0-9]+\\.[0-9]+$ ]] \\\n  || echo ::set-output name=prerelease::true\n', 'poetry publish\n', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")\n', 'curl -fsS https://install.python-poetry.org | python - --preview -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'echo ""$APPDATA\\Python\\Scripts"" >> $GITHUB_PATH', 'poetry config virtualenvs.in-project true', 'timeout 10s poetry run pip --version || rm -rf .venv', 'poetry install --only main --only test -vvv', 'PENDULUM_EXTENSIONS=0 poetry run pytest -q tests\n', 'poetry run pytest -q tests\n']"
"['pip install flake8', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --max-complexity=12 --max-line-length=127 --statistics', 'python -m pip install --upgrade pip\npip install build\n', 'sed -i -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py', 'python -m build --sdist --wheel --outdir dist/', 'pip install pyinstaller', 'sed -i.tmp -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py && rm run.py.tmp', 'cp /etc/ssl/certs/ca-certificates.crt cert.pem && export SSL_CERT_FILE=${PWD}/cert.pem', 'python ./run.py -h', 'python -O -m PyInstaller --noconfirm --clean .build/ddns.spec', './dist/ddns || test -e config.json', './dist/ddns -h', 'sed -i -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py', 'docker run --rm ddns:test -h', 'docker run --rm -v ""$(pwd)"":/config/ ddns:test -c /config/config.json || test -e config.json', 'sed -i -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py', 'sed -i -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py', 'sed -i -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build --sdist --wheel --outdir dist/', 'pip install pyinstaller', 'sed -i.tmp -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" -e ""s/\\${BUILD_DATE}/$(date --iso-8601=seconds)/"" run.py && rm run.py.tmp', 'cp /etc/ssl/certs/ca-certificates.crt cert.pem && export SSL_CERT_FILE=${PWD}/cert.pem', 'python -O -m PyInstaller --noconfirm --clean .build/ddns.spec', './dist/ddns || test -e config.json', './dist/ddns -h', 'mv ./dist/ddns ./dist/ddns-osx', 'sed -i -e ""s#\\${BUILD_VERSION}#${{ github.ref_name }}#"" .release/README.md']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox -e lint', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox -e docs', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox -e playback']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist\npython setup.py bdist_wheel --universal\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest pytest-cov\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'make test\n']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip wheel\npip install tox\n', 'tox', 'python -m pip install --upgrade pip wheel\npip install tox tox-gh-actions\n', 'tox', 'python -m pip install --upgrade pip wheel\npip install .[${{ matrix.dependency }},test_no_transport]\n', 'pytest tests --${{ matrix.dependency }}-only', 'python -m pip install --upgrade pip wheel\npip install -e.[test]\n', 'pytest --cov=gql --cov-report=xml --cov-report=term-missing tests']"
"['python -m pip install --upgrade pip\npip install -U pycodestyle poetry\npoetry install\n', 'pycodestyle --max-line-length=200 starred', 'poetry build', 'poetry run starred --username maguowei --token ${GITHUB_TOKEN} --sort\n', 'python -m pip install --upgrade pip\npip install poetry\n', 'poetry publish --build\n']"
"['mkdir -p ~/.ssh/\ntouch ~/.ssh/id_rsa\ntouch ~/.ssh/id_rsa.pub\necho test | podman secret create pypitoken -\n./key.sh make\n', './key.sh envirotest full\n', 'mkdir -p ~/.ssh/\ntouch ~/.ssh/id_rsa\ntouch ~/.ssh/id_rsa.pub\necho test | podman secret create pypitoken -\n./key.sh make\n', './key.sh regression\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install -r test-requirements.txt\npip install codecov\n', 'flake8 stockstats.py test.py', 'py.test --cov=stockstats test.py\n', 'codecov', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['mkdir -p ros_ws/src', 'export SETUPTOOLS_USE_DISTUTILS=stdlib']"
"['python -m pip install --upgrade pip\npython -m pip install pytest pytest-runner\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py test\n', 'python setup.py install', 'pip install setuptools wheel twine\npython setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'curl --silent \\\n  -X POST \\\n  -H ""Authorization: Bearer ${{ secrets.BREW_GH_TOKEN }}"" \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  https://api.github.com/repos/gautamkrishnar/homebrew-socli/actions/workflows/generate.yml/dispatches \\\n  -d \'{""ref"":""master""}\'']"
""
"['echo ""$GITHUB_CONTEXT""\n', 'sleep 300s', 'echo ""Approving the pull request.""\ngh pr review --approve ""$PR_URL""\necho ""Waiting tii all the checks are done""\ngh pr checks --required --interval 60 --watch ""$PR_URL""\necho ""Merging the pull request""\ngh pr merge --squash ""$PR_URL""\n', 'echo ""$GITHUB_CONTEXT""\n', 'poetry install --with ci\n', 'echo ""PYTHONPATH=$GITHUB_WORKSPACE"" >> $GITHUB_ENV\n', 'git config --global user.email ""bot@demisto.com""\ngit config --global user.name ""Content Bot""\necho ""Someone merged to master. Starting to check if conflicts in release notes created.""\npoetry run python Utils/github_workflow_scripts/autobump_release_notes/autobump_rn.py -g $CONTENTBOT_GH_ADMIN_TOKEN -r $RUN_ID\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\ncd Utils/github_workflow_scripts\npipenv sync\n', 'echo ""Checking if contribution form needs to be filled for PR: $PR_NUMBER""\ncd Utils/github_workflow_scripts\npipenv run ./check_if_needs_to_fill_contribution_form.py --pr_number $PR_NUMBER --github_token $GITHUB_TOKEN\n', 'poetry install --with ci\n', 'echo ""PR number is: $PR_NUMBER""\necho ""Target branch name is: $BRANCH_NAME""\necho ""Starting check of contributor packs""\npoetry run python ./Utils/request_contributor_review.py --pr_number $PR_NUMBER --github_token $GITHUB_TOKEN --email_api_token $SENDGRID_EMAIL_API_KEY\necho ""Finished check of contributor packs""\n', 'poetry install --with ci\n', 'echo ""Checking for related Jira issues to PR: $PR_NUMBER""\ncd Utils/github_workflow_scripts/jira_integration_scripts\npoetry run python ./link_pr_to_jira_issue.py --url ""$INSTANCE_URL"" --pr_num ""$PR_NUMBER"" --pr_link ""$PR_LINK"" --pr_title ""$PR_TITLE"" --pr_body ""$PR_BODY"" --is_merged --username $USERNAME --password $PASSWORD\n', 'echo ""$GITHUB_CONTEXT""\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\n', 'echo ""Creating an internal PR from original merged external PR ${{ github.event.pull_request.html_url }}""\ncd Utils/github_workflow_scripts\npipenv sync\npipenv run ./create_internal_pr.py\necho ""Finished Creating Internal PR""\n', 'echo ""$GITHUB_CONTEXT""\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\n', 'echo ""Updating External PR ${{ github.event.pull_request.html_url }}""\ncd Utils/github_workflow_scripts\npipenv sync\n\npipenv run ./handle_external_pr.py\necho ""Finished Handling External PR""\n', 'echo ""Sending notification about External PR ${{ github.event.pull_request.html_url }}""\ncd Utils/github_workflow_scripts\npipenv run ./send_slack_message.py\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\n', 'echo ""Deleting stale contribution base branches (contrib/*)""\ncd Utils/github_workflow_scripts\npipenv sync\npipenv run ./delete_stale_contrib_branches.py\necho ""Finished deleting stale branches""\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\n', 'echo ""Deleting stale non contribution branches""\ncd Utils/github_workflow_scripts\npipenv sync\npipenv run ./delete_stale_non_contrib_branches.py\necho ""Finished deleting stale branches""\n', 'poetry install --with ci\n', 'echo ""Checking for related Jira issues to PR: $PR_NUMBER""\ncd Utils/github_workflow_scripts/jira_integration_scripts\necho --pr_num $PR_NUMBER --pr_link $PR_LINK --pr_title $PR_TITLE --pr_body $PR_BODY --no-is_merged\npoetry run python ./link_pr_to_jira_issue.py --url ""$INSTANCE_URL"" --pr_num ""$PR_NUMBER"" --pr_link ""$PR_LINK"" --pr_title ""$PR_TITLE"" --pr_body ""$PR_BODY"" --no-is_merged --username $USERNAME --password $PASSWORD\n', 'pipx install poetry', 'poetry install', 'npm install\npoetry run demisto-sdk pre-commit -g --unit-test --validate --no-secrets --show-diff-on-failure --verbose\n', 'cat code-coverage-results.md >> $GITHUB_STEP_SUMMARY || echo ""Missing coverage report""\n', 'pip install --upgrade pip\npip install github-automation\n', 'github-automation manage -c .github/project_conf/contributions.ini\n', 'pip install --upgrade pip\npip install github-automation\n', 'github-automation manage -c .github/project_conf/contributions.ini\n', 'python Utils/check_protected_directories.py ${{ steps.changed-files.outputs.all_changed_files }}\n', 'python3 -m pip install demisto-sdk\n', 'echo ""PYTHONPATH=$GITHUB_WORKSPACE"" >> $GITHUB_ENV\n', 'release_notes=""${{ steps.changed-files.outputs.all_changed_files }}""\necho ""Release notes found in PR: ${release_notes}. Starting docs review...""\n\nUtils/github_workflow_scripts/run_docs_review.py --changed_files ""$release_notes"" --delimiter ""${{ env.CHANGED_FILES_DELIMITER }}""\n', 'poetry install --with ci\n', 'echo ""Run secrets detection for PR: $PR_NUMBER on branch: $BRANCH_NAME""\ninvestigation_id=$(poetry run Utils/github_workflow_scripts/run_secrets_detection.py --pr_number $PR_NUMBER --branch_name $BRANCH_NAME --username $USERNAME --password $PASSWORD --gold_server_url $GOLD_SERVER_URL)\necho ""INVESTIGATION_ID=$investigation_id"" >> $GITHUB_ENV\n', 'echo ""Invastigation id is: $INVESTIGATION_ID ""\npoetry run python ./Utils/github_workflow_scripts/run_secrets_detection_get_playbook_status.py -i $INVESTIGATION_ID -k $GOLD_API_KEY --gold_server_url $GOLD_SERVER_URL\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\n', 'echo ""Updating contribution base branch (contrib/*)""\ncd Utils/github_workflow_scripts\npipenv sync\npipenv run ./sync_contrib_base.py --branch_name ${{ github.event.pull_request.base.ref }}\necho ""Finished updating base branch""\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\n', 'echo ""Updating contribution base branches (contrib/*)""\ncd Utils/github_workflow_scripts\npipenv sync\npipenv run ./sync_contrib_base.py\necho ""Finished updating base branches""\n', 'poetry install --with ci\n', 'echo ""Trigger contribution build for PR: $PR_NUMBER with base branch: $BASE_BRANCH contrib branch: $CONTRIB_BRANCH""\npoetry run python ./Utils/github_workflow_scripts/trigger_contribution_build.py --pr_number $PR_NUMBER --base_branch $BASE_BRANCH --contrib_branch $CONTRIB_BRANCH --username $USERNAME --password $PASSWORD --gold_server_url $GOLD_SERVER_URL\n']"
"['python -m pip install --upgrade pip\npip install pytest coverage coveralls\nif [ -f docker_requirements.txt ]; then pip install -r docker_requirements.txt; fi\npip install deepcut\npip install .[full]\npip install boto smart_open sphinx sphinx-rtd-theme\npython -m nltk.downloader omw-1.4\n', 'cd docs && make html\ncd ..\n', 'python -m pip install --upgrade pip \nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install ""h5py>=2.10.0,<3"" ""tensorflow>=2.3.1,<3""\npip install torch==1.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\npip install deepcut\npip install .[full]\npip install flake8 flake8-commas flake8-comprehensions flake8-tidy-imports\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'source ~/miniconda3/etc/profile.d/conda.sh\nconda activate pythainlp38\nconda info\nconda list\npython -m pip install --upgrade pip\npip uninstall --y pythainlp\npip install --no-deps fastai==1.0.61\npip install PyYAML attacut emoji epitran gensim nltk numpy pandas sacremoses sentencepiece ssg bpemb transformers sefr_cut phunspell spylls symspellpy tltk oskut nlpo3 onnxruntime thai_nner wunsen spacy_thai ufal.chu-liu-edmonds\npip install -e .\npython -m nltk.downloader omw-1.4\npython -m unittest discover\n', 'conda info\nconda list\n', 'pip install torch==1.10.0\n', 'python -m pip install --upgrade pip\npip install pytest coverage coveralls\nconda install -c conda-forge icu\nconda install -c conda-forge pyicu\nif [ -f docker_requirements.txt ]; then SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True pip install -r docker_requirements.txt; fi\npip install deepcut tltk\npip install .[full]\npython -m nltk.downloader omw-1.4\n', 'coverage run -m unittest discover\ncoveralls\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\npython setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install deepcut tltk\nSKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True pip install -r https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/docker_requirements.txt\npip install pythainlp[full]\npython -m nltk.downloader omw-1.4\n', 'mkdir pythainlp_test\ncd pythainlp_test\npip download --no-binary=:all: --no-dependencies pythainlp\nfile=""find . -name *.tar.gz""\nfile=$(eval ""$file"")\ntar -xvzf $file --one-top-level\nsecond=""/""\npath=${file//.tar.gz/$second}\ncd $path\nls\ncd tests\nmkdir tests\nmv data tests/\npython -m unittest discover\n', 'python -m pip install --upgrade pip\npip install pytest coverage coveralls\nif [ -f docker_requirements.txt ]; then SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True pip install -r docker_requirements.txt; fi\npip install deepcut tltk\npip install .[full]\npython -m nltk.downloader omw-1.4\n', 'coverage run -m unittest discover\ncoveralls\n', 'conda info\nconda list\n', 'pip install torch==1.8.1\n', 'python -m pip install --disable-pip-version-check --user --upgrade pip setuptools\npython -m pip install backports.zoneinfo[tzdata]\npython -m pip --version\npython -m pip install pytest coverage coveralls\nconda install -y -c conda-forge fairseq\npython -m pip install https://www.dropbox.com/s/o6p2sj5z50iim1e/PyICU-2.3.1-cp38-cp38-win_amd64.whl?dl=1\npython -m pip install -r docker_requirements.txt\npython -m pip install .[full]\npython -m nltk.downloader omw-1.4\npython -m pip install spacy deepcut tltk\n', 'coverage run -m unittest discover\ncoveralls\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python3 setup.py install\npip install pytest\npip install mock\npython3 -m pytest test\n', 'pip install pytest\npip install mock\npip install boto3\n./test-integration/run/run.sh ${{ matrix.test-type }} 1000 100 7\n']"
"['git config --global core.symlinks true', 'python3 -m pip install setuptools wheel\nmkdir wheelhouse\npython3 helpers/build_package.py wheel wheelhouse\n', 'python3 -m pip install auditwheel twine readme_renderer[md]\nauditwheel show wheelhouse/*.whl\ntwine check wheelhouse/*.whl\n', 'python3 -m pip install pytest wheelhouse/*.whl\npytest tests\n', 'sudo apk -U add samurai llvm lld linux-headers binutils-gold\ncp -f /usr/local/bin/gn py_mini_racer/extension/v8/buildtools/linux64/gn\nrm -f py_mini_racer/extension/depot_tools/ninja\n', 'python helpers/v8_build.py --no-update --no-sysroot --target py_mini_racer_shared_lib\ncp py_mini_racer/extension/out/libmini_racer.so py_mini_racer/libmini_racer.muslc.so\n', 'sudo apk add py3-pip py3-wheel\nmkdir wheelhouse\npython3 setup.py sdist --dist-dir wheelhouse\n', 'python3 -m pip install pytest wheelhouse/*.tar.gz\npytest tests\n', ""mkdir wheelhouse\nfind tmp -name '*.whl' -exec mv {} wheelhouse \\;\nfind tmp -name '*.tar.gz' -exec mv {} wheelhouse \\;\n""]"
"['pip install Django==${{ matrix.django-version }}\npip install coverage\npip install pytz\n', 'make test\n', 'make coverage\n', 'pip install --upgrade setuptools wheel', 'pip install -r requirements-dev.txt', 'make lint', 'pip install --upgrade setuptools wheel', 'python setup.py sdist bdist_wheel --universal']"
"['pip install tox', 'tox -e py', 'python -m pip install --user --upgrade build\n', 'python -m build\n']"
"['python -m pip install --upgrade pip\npip install -e .[test]\n', 'pytest --junitxml=pytest.xml --cov=tableauserverclient tests/ | tee pytest-coverage.txt', 'python -m pip install --upgrade pip\npip install -e .[test]\n', 'black --check --line-length 120 tableauserverclient samples test\n', 'mypy --show-error-codes --disable-error-code misc --disable-error-code import --implicit-optional tableauserverclient test\n', 'python -m pip install --upgrade pip\npip install -e .[test] build\npython -m build\ngit describe --tag --dirty --always\n', 'pip uninstall tableauserverclient\npip install tableauserverclient\n', 'python -c ""import tableauserverclient as TSC\nserver = TSC.Server(\'http://example.com\', use_server_version=False)""\n', 'python -m pip install --upgrade pip\npip install -e .[test] build\n', 'pytest test\n', 'python -m build', 'echo ""The result was ${{ steps.notify.outputs.slack-result }}""']"
"['python -m pip install --upgrade pip setuptools six wheel\npython -m pip install pytest-cov -r requirements.txt\n', 'pytest --ignore=computer_vision/cnn_classification.py --ignore=machine_learning/lstm/lstm_prediction.py --ignore=quantum/ --ignore=project_euler/ --ignore=scripts/validate_solutions.py --cov-report=term-missing:skip-covered --cov=. .', 'scripts/build_directory_md.py 2>&1 | tee DIRECTORY.md', ""scripts/build_directory_md.py 2>&1 | tee DIRECTORY.md\ngit config --global user.name github-actions\ngit config --global user.email '${GITHUB_ACTOR}@users.noreply.github.com'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\n"", 'git add DIRECTORY.md\ngit commit -am ""updating DIRECTORY.md"" ||  true\ngit push --force origin HEAD:$GITHUB_REF || true\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade pytest pytest-cov\n', 'pytest --doctest-modules --cov-report=term-missing:skip-covered --cov=project_euler/ project_euler/', 'python -m pip install --upgrade pip\npython -m pip install --upgrade pytest requests\n', 'pytest scripts/validate_solutions.py', 'pip install --user ruff', 'ruff --format=github .']"
"['pipx install poetry', 'poetry env use ""3.10""\npoetry install\n', 'poetry run mkdocs gh-deploy --force\n', 'pipx install poetry', 'poetry env use ""3.10""\npoetry install\n', 'pip install twine', 'poetry build\n', 'twine upload dist/*', 'pipx install poetry', 'poetry env use ""3.10""\npoetry install\n', 'poetry run python ./scripts/build.py', 'chmod u+x ./scripts/make_binzip.sh\n./scripts/make_binzip.sh\n', 'pipx install poetry', 'poetry env use ""3.10""\npoetry install\n', 'poetry run pylint --limit-inference-results 0 --disable all --enable missing-function-docstring missing-module-docstring missing-class-docstring empty-docstring ./spotdl\n', 'poetry run pylint --fail-under 10 --limit-inference-results 0 ./spotdl\n', 'poetry run mypy --ignore-missing-imports --follow-imports silent --install-types --non-interactive ./spotdl\n', 'poetry run black --check ./spotdl\n', 'poetry run isort --check --diff ./spotdl\n', 'pipx install poetry', 'poetry env use ${{ matrix.python-version }}\npoetry install\n', 'poetry run pytest -vvv --ignore tests/test_matching.py --ignore tests/utils/test_ffmpeg.py --ignore tests/utils/test_metadata.py\n', 'pipx install poetry', 'poetry env use ""3.10""\npoetry install\n', 'poetry run pytest -vvv --disable-vcr --vcr-record=none --ignore tests/providers/lyrics --ignore tests/utils/test_github.py --ignore tests/utils/test_ffmpeg.py --ignore tests/utils/test_metadata.py\n']"
"['echo ::error::PR is not approved yet.\nexit 1\n', 'mkdir -p dist/\necho ""${VERSION}"" > dist/VERSION\n', 'pip install -U setuptools wheel pip\npython setup.py sdist\n', 'pip install cibuildwheel==2.10.2', 'MATRIX_INCLUDE=$(\n  {\n    cibuildwheel --print-build-identifiers --platform linux --arch x86_64,aarch64 | grep cp |  jq -nRc \'{""only"": inputs, ""os"": ""ubuntu-latest""}\' \\\n    && cibuildwheel --print-build-identifiers --platform macos --arch x86_64,arm64 | grep cp |  jq -nRc \'{""only"": inputs, ""os"": ""macos-latest""}\' \\\n    && cibuildwheel --print-build-identifiers --platform windows --arch x86,AMD64 | grep cp |  jq -nRc \'{""only"": inputs, ""os"": ""windows-latest""}\'\n  } | jq -sc\n)\necho ""include=$MATRIX_INCLUDE"" >> $GITHUB_OUTPUT\n', 'pip install -e .[dev]\nmake htmldocs\n', 'rsync -a docs/_build/html/ docs/gh-pages/current/\n', 'set -e\necho ""version=$(cat dist/VERSION)"" >> $GITHUB_OUTPUT\nrm dist/VERSION\n', 'ls -al dist/\n', 'python -m pip install -U pip setuptools wheel\npython -m pip install -e .[test]\n', 'if [ ""${LOOP_IMPL}"" = ""uvloop"" ]; then\n  env USE_UVLOOP=1 python -m unittest -v tests.suite\nelse\n  python -m unittest -v tests.suite\nfi\n', 'sudo env DISTRO_NAME=""${DISTRO_NAME}"" PGVERSION=""${PGVERSION}"" \\\n  .github/workflows/install-postgres.sh\necho PGINSTALLATION=""/usr/lib/postgresql/${PGVERSION}/bin"" \\\n  >> ""${GITHUB_ENV}""\n', 'python -m pip install -U pip setuptools wheel\npython -m pip install -e .[test]\n', 'python -m unittest -v tests.suite\n', 'echo OK']"
"['python -m pip install build --user', 'python -m build --sdist --outdir dist/', '# Install TA-Lib\n  wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n  tar -xzvf ta-lib-0.4.0-src.tar.gz\n  cd ta-lib\n  ./configure --prefix=/usr\n  make\n  sudo make install\n  cd ..\n# Install Test Deps\n  pip install -U pip\n  pip install ""numpy>=1.15.4""\n  pip install TA-Lib\n  pip install coveralls\n  pip install -U setuptools\n  pip install ipython==5.3.0\n  pip install --prefer-binary .\n\n# download-bundle\n  rqalpha download-bundle\n', 'coverage run --source=rqalpha test.py\nls -al $HOME/.rqalpha/\nls -al $HOME/.rqalpha/bundle/\n', 'coveralls --service=github\n', 'pip3 install --upgrade coveralls\ncoveralls --finish\n']"
"['python setup.py install\nconda list\nconda info --all\n', 'isort . --check-only\nblack . --line-length 100 --check --diff\nflake8 .\npydocstyle .\n', 'make -C ./docs html\npython -m sphinx -b linkcheck -D linkcheck_ignore=""osmnx.html"" docs/ docs/_build/linkcheck\n', 'coverage run --source ./osmnx --module pytest --verbose\ncoverage xml -i\ncoverage report -m\n']"
""
[]
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\npip install ""coveralls<3.0.0""\n', 'tox\ncoveralls\n']"
""
"['sudo apt-get update && sudo apt-get install libgnutls28-dev libcurl4-openssl-dev libssl-dev libdiscid-dev\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'flake8 . --max-complexity=15 --max-line-length=120 --show-source --statistics\n# stop the build if there are Python syntax errors or undefined names\n#flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n#flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'cp setup/arm.yaml arm.yaml\nsudo mkdir -p /etc/arm/config/ /home/arm/db /home/arm /home/arm/config /home/arm/media /home/arm/music\nsudo cp setup/arm.yaml /etc/arm/config/arm.yaml\nsudo cp setup/apprise.yaml /etc/arm/config/apprise.yaml\nsudo cp setup/.abcde.conf /etc/arm/config/abcde.conf\nsudo mkdir -p /opt/arm/arm /opt/arm/setup\nsudo cp -r arm/migrations /opt/arm/arm\ncp setup/arm.yaml /opt/arm/setup/arm.yaml\n', 'timeout 1 python ./arm/runui.py || code=$?; if [[ $code -ne 124 && $code -ne 0 ]]; then exit $code; fi', 'echo ""Branch name is ${{ steps.branch-name.outputs.ref_branch }}""\necho ""Main name is ${{ steps.branch-name.outputs.default_branch }}""\necho ""TAG=${{ steps.branch-name.outputs.ref_branch }}"" >> $GITHUB_ENV\n', 'if [ ${{ steps.branch-name.outputs.is_default }} = true ]; then\n  VER=$(cat VERSION)\n  echo ""VERSION=$VER"" >> $GITHUB_ENV\nelse\n  echo ""VERSION=${{ env.TAG }}"" >> $GITHUB_ENV\n  echo ""${{ env.TAG }}"" > ./VERSION\nfi\n', 'TIMESTAMP=$(date -u +\'%Y-%m-%d T%H:%M:%SZ\')\necho ""BUILD_DATE=$TIMESTAMP"" >> $GITHUB_ENV\n', 'echo ""Branch name is ${{ steps.branch-name.outputs.ref_branch }}""\necho ""Main name is ${{ steps.branch-name.outputs.default_branch }}""\necho ""TAG=${{ steps.branch-name.outputs.current_branch }}"" >> $GITHUB_ENV\n', 'echo ""github ref = ${{ github.ref }} \nand ${{ steps.branch-name.outputs.current_branch }} \nand ${{ github.GITHUB_HEAD_REF }}""\n', 'git switch ${{ steps.branch-name.outputs.current_branch }}', 'SEM_VER=$(cat VERSION | awk -F. -v OFS=. \'NF==1{print ++$NF}; NF>1{if(length($NF+1)>length($NF))$(NF-1)++; $NF=sprintf(""%0*d"", length($NF), ($NF+1)%(10^length($NF))); print}\')\nCOMMIT_MSG=$(git log --format=%B -n 1 ${{ github.event.after }})\n# Write files\necho $SEM_VER > VERSION\n# Add new git commit\ngit add VERSION\ngit config --local user.email ""github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\ngit commit -m ""[Automated] Increment Version""      \n']"
""
"['make html', 'make check', 'make linkcheck']"
"['pip install -r requirements/linting.txt -r requirements/pyproject.txt pre-commit', 'pre-commit run -a', 'pip install -r requirements/docs.txt -r requirements/pyproject.txt', 'pip install .', 'make docs', 'pip install -r requirements/testing.txt -r requirements/pyproject.txt', 'make test', 'coverage xml', 'pip install -U twine build packaging', 'python <(curl -Ls https://gist.githubusercontent.com/samuelcolvin/4e1ad439c5489e8d6478cdee3eb952ef/raw/check_version.py)', 'python -m build', 'twine check dist/*', 'twine upload dist/*', 'make publish-docs']"
"['GO_VERSION=$(awk \'/^go/ {print $2};\' go.mod)\necho ""::set-output name=version::${GO_VERSION}""\n', 'mkdir -p test-results/tests\nmake JUNIT_XML=test-results/tests/junit.xml all\n', 'make\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'kubectl apply -f tests/e2e/success\n', './kubediff tests/e2e/success\n', 'kubectl scale deployment kubediff-app --replicas=3\n', './kubediff -e tests/e2e/success | diff tests/e2e/results/nojson.txt -\n', './kubediff -e -j tests/e2e/success | diff tests/e2e/results/json.txt -\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', './kubediff -e tests/e2e/error/apimissing.yaml | diff tests/e2e/results/apimissing.txt -\n./kubediff -e tests/e2e/error/kindmissing.yaml | diff tests/e2e/results/kindmissing.txt -\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', './kubediff -e tests/e2e/error/apimissing.yaml | diff tests/e2e/results/apimissing27.txt -\n./kubediff -e tests/e2e/error/kindmissing.yaml | diff tests/e2e/results/kindmissing27.txt -\n', 'make test\n']"
"['echo ""deb-src http://archive.ubuntu.com/ubuntu/ $(lsb_release -cs) main restricted universe multiverse"" | sudo tee -a /etc/apt/sources.list\nsudo apt-get update\nsudo apt-get build-dep systemd\nsudo apt-get install pandoc\n\ngit clone https://github.com/systemd/systemd --depth=1\nmeson setup systemd/build systemd\n\nninja -C systemd/build systemd-analyze\n\nsudo ln -svf $PWD/systemd/build/systemd-analyze /usr/bin/systemd-analyze\nsystemd-analyze --version\n', 'python3 -m pip install pytest mypy types-cryptography isort pyflakes\nnpm install -g pyright\n', 'python3 -m isort --verbose --check-only mkosi/', 'python3 -m pyflakes mkosi/ tests/', 'sh -c \'! git grep -P ""\\\\t"" ""*.py""\'', 'python3 -m mypy mkosi/ tests/', 'pyright mkosi/ tests/', 'python3 -m pytest -sv tests', 'python3 -m mkosi -h', 'sudo python3 -m mkosi -h', 'python3 -m pip install --user .\n$HOME/.local/bin/mkosi -h\npython3 -m pip uninstall --yes mkosi\n', 'python3 -m pip install --user --no-use-pep517 --editable .\n$HOME/.local/bin/mkosi -h\npython3 -m pip uninstall --yes mkosi\n', 'python3 -m venv testvenv\ntestvenv/bin/python3 -m pip install .\ntestvenv/bin/mkosi -h\n', 'sudo testvenv/bin/mkosi -h\n', 'sudo python3 -m pip install .\nsudo mkosi -h\nsudo python3 -m pip uninstall --yes mkosi\n', './tools/generate-zipapp.sh\n./builddir/mkosi -h\n', ""sudo apt-get update && sudo apt-get install --no-install-recommends shellcheck\nbash -c 'shopt -s globstar; shellcheck bin/mkosi tools/*.sh'\n"", 'sudo apt-get update && sudo apt-get install --no-install-recommends python3-pexpect python3-pytest', 'sudo tee /usr/lib/sysusers.d/acct-user-portage.conf > /dev/null <<- EOF\n# /usr/lib/sysusers.d/portage.conf\nu portage - ""Portage system user"" /var/lib/portage/home -\nEOF\nsudo systemd-sysusers --no-pager\n\nsudo install --owner=portage --group=portage --mode=0755 --directory /var/db/repos\nsudo install --owner=portage --group=portage --mode=0755 --directory /etc/portage/repos.conf\nsudo install --owner=portage --group=portage --mode=0755 --directory /var/cache/binpkgs\nsudo tee /etc/portage/repos.conf/eselect-repo.conf > /dev/null <<- EOF\n[gentoo]\nlocation = /var/db/repos/gentoo\nsync-type = git\nsync-uri = https://anongit.gentoo.org/git/repo/gentoo.git\nEOF\n\ngit clone https://anongit.gentoo.org/git/proj/portage.git --depth=1\ncd portage\ntee setup.cfg > /dev/null <<- EOF\n[build_ext]\nportage-ext-modules=true\nEOF\n\nsudo python setup.py install\n\nsudo ln -s --relative /var/db/repos/gentoo/profiles/default/linux/amd64/17.1/no-multilib/systemd/merged-usr /etc/portage/make.profile\n', 'sudo python3 -m pip install ..', 'tee mkosi.conf.d/00-ci.conf <<- EOF\n[Distribution]\nDistribution=${{ matrix.distro }}\n\n[Output]\nFormat=${{ matrix.format }}\nEOF\n', 'curl -I geo.mirror.pkgbuild.com', 'python3 -m mkosi --debug build', 'sudo systemctl --root image mask systemd-resolved', 'sudo python3 -m mkosi --debug boot', 'timeout -k 30 10m python3 -m mkosi --debug qemu']"
"['sudo apt-get update\nsudo apt-get install libmemcached-dev memcached redis\nsudo systemctl stop memcached\nsudo systemctl stop redis-server\n', 'pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
"['if [ $RUNNER_OS = Linux ]; then\n    sudo apt install libolm3 libolm-dev -y\nelse\n    brew install libolm\nfi\n', 'if [ $RUNNER_OS = Linux ]; then\n    sudo apt update || true\n    sudo apt install ffmpeg -y\nfi\n', 'choco feature enable -n=allowGlobalConfirmation\nchoco install ffmpeg\n', 'python -m pip install --upgrade pip\npython -m pip install -U tox tox-gh-actions\n', 'tox -vv', 'python -m pip install --upgrade pip\npython -m pip install -U tox\n', 'tox -vv -e ${{ matrix.component }}', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install -r doc-requirements.txt\n', 'python -m sphinx -T -E -b html -d _build/doctrees -D language=en -W --keep-going . _build/html', 'python -m pip install --upgrade pip\npip install -r dev-requirements.txt\n', 'python -m build', 'python -m pip install --upgrade pip\npip install -r dev-requirements.txt\n', 'python -m pytest\n']"
""
"['git checkout HEAD^2', 'pipx install poetry', 'poetry install', '# Export exact versions\npoetry export --without-hashes > requirements.txt\ngrep cryptography requirements.txt > cryptography.txt\ngrep cffi requirements.txt > source-reqs.txt\ngrep pyscard requirements.txt >> source-reqs.txt\n# Remove non-universal packages\npoetry run pip uninstall -y cryptography cffi pyscard\n# Build cffi from source to get universal build\npoetry run pip install --upgrade -r source-reqs.txt --no-binary :all:\n# Explicitly install pre-build universal build of cryptography\npoetry run pip download -r cryptography.txt --platform macosx_10_12_universal2 --only-binary :all: --no-deps --dest .\npoetry run pip install -r cryptography.txt --no-cache-dir --no-index --find-links .\n', 'poetry run pyinstaller ykman.spec\ndist/ykman/ykman --version\n[[ -z ""$(dist/ykman/ykman --version | grep -E ""not found|missing"")"" ]]\n', 'cp -r resources/macos dist/scripts', './scripts/make_pkg.sh', 'sudo apt-get update\nsudo apt-get install -qq swig libpcsclite-dev\npython -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'poetry run pytest', ""poetry build\nmkdir artifacts\nexport REF=$(echo ${GITHUB_REF} | cut -d '/' -f 3)\nmv dist/yubikey_manager-*.tar.gz artifacts/yubikey_manager-$REF.tar.gz\nmv dist/yubikey_manager-*.whl artifacts/yubikey_manager-$REF.whl\n"", 'pipx install poetry', 'sudo apt-get install -qq swig libpcsclite-dev\npoetry install\n', 'python -m pip install --upgrade pip\npip install pre-commit\npre-commit install\npre-commit run --all-files --verbose\n', 'poetry run pytest', 'pip install .\nykman --version\n[[ -z ""$(ykman --version | grep -E ""not found|missing"")"" ]]\n', 'poetry run pyinstaller ykman.spec\ndist/ykman/ykman --version\n[[ -z ""$(dist/ykman/ykman --version | grep -E ""not found|missing"")"" ]]\nexport REF=$(echo ${GITHUB_REF} | cut -d \'/\' -f 3)\nmv dist/ykman dist/ykman-$REF\n', 'pipx install poetry', 'poetry install', 'poetry run pytest -v', 'poetry build', 'python -m pip install --upgrade pip\npip install dist/*.tar.gz\nykman --version\n[[ -z ""$(ykman --version | grep -E ""not found|missing"")"" ]]\npip uninstall -y yubikey-manager\n', 'pip install dist/*.whl\nykman --version\n[[ -z ""$(ykman --version | grep -E ""not found|missing"")"" ]]\npip uninstall -y yubikey-manager\n', 'poetry run pyinstaller ykman.spec\ndist/ykman/ykman.exe --version\n[[ -z ""$(dist/ykman/ykman.exe --version | grep -E ""not found|missing"")"" ]]\n', 'cp -r resources/win dist/scripts', '.\\scripts\\make_msi.ps1']"
"['python -m pip install cibuildwheel==2.12.3', 'python -m cibuildwheel --output-dir wheelhouse', 'python setup.py sdist', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions wheel\n', 'tox']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 black\n', 'black . --safe --quiet', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade django~=${{ matrix.django-version }}\nif [ -f requirements/local.txt ]; then pip install -r requirements/local.txt; fi\n', 'coverage run -m pytest\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get install graphviz\npip install --upgrade pip\npip install tox\n', 'tox\n']"
"['wget https://raw.githubusercontent.com/pingcap/docs/master/scripts/pr_reminder.py;\npip3 install lxml;\npython3 pr_reminder.py ""$WEBHOOK""\n', 'wget https://raw.githubusercontent.com/pingcap/docs/master/scripts/pr_reminder_basedon_assignee.py;\npip3 install lxml;\npython3 pr_reminder_basedon_assignee.py ""$WEBHOOK""\n', './scripts/verify-duplicated-file-name.sh', 'npm i\nnode ./scripts/filterNonCloudDoc.js\ncp -r ./scripts ./tmp\ncp -r ./media ./tmp\ncp .gitignore ./tmp/\ncd ./tmp\n./scripts/verify-links.sh\n./scripts/verify-link-anchors.sh\n', 'npm i\nnode ./scripts/filterCloudDoc.js\ncp -r ./scripts ./tmp\ncp -r ./media ./tmp\ncp .gitignore ./tmp/\ncd ./tmp\n./scripts/verify-links.sh\n./scripts/verify-link-anchors.sh\n', 'sudo apt install tree -y\n', 'export GH_TOKEN=${{github.token}}\ncd docs\nnpm i\nnode scripts/filterUpdateFiles.js\ntree tmp\ncd ..\n', 'cp -r docs/tmp markdown-translator/markdowns\n', 'cd markdown-translator\necho ${{secrets.GCP_KEY}} | base64 --decode >> key.json\nexport GOOGLE_APPLICATION_CREDENTIALS=key.json\nexport PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}\nexport GLOSSARY_ID=${{ secrets.GCP_GLOSSARY_ID }}\nyarn\nnode src/index.js\ncd ..\n', 'cp -r markdown-translator/output/markdowns/* docs/\n', 'cd docs\ngit status\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""update translated files""\ngit push\n', 'sudo apt install tree -y\n', 'export GH_TOKEN=${{github.token}}\ncd docs\nnpm i\nnode scripts/filterUpdateFiles.js\ntree tmp\ncd ..\n', 'cp -r docs/tmp markdown-translator/markdowns\n', 'cd markdown-translator\necho ${{secrets.GCP_KEY}} | base64 --decode >> key.json\nexport GOOGLE_APPLICATION_CREDENTIALS=key.json\nexport PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}\nexport GLOSSARY_ID=${{ secrets.GCP_GLOSSARY_ID }}\nyarn\nnode src/index.js\ncd ..\n', 'cp -r markdown-translator/output/markdowns/* docs/\n', 'cd docs\ngit status\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""update translated files""\ngit push\n', 'echo ""sha=$(sha=${{ github.sha }}; echo ${sha:0:6})"" >> $GITHUB_OUTPUT\n', 'curl \\\n-X POST \\\n-H ""Accept: application/vnd.github+json"" \\\n-H ""Authorization: token ${{ secrets.DOCS_STAGING }}"" \\\nhttps://api.github.com/repos/pingcap/docs-staging/actions/workflows/update.yml/dispatches \\\n-d \'{""ref"":""main"",""inputs"":{""full"": ""false"", ""repo"":""${{ github.repository }}"",""branch"":""${{ github.ref_name }}""}}\'\n', 'CHANGED_FILES=$(git diff-tree --name-only --diff-filter \'AM\' -r HEAD^1 HEAD -- ""*.md"" | sed -z ""s/\\n$//;s/\\n/\' \'/g"")\necho ""all_changed_files=${CHANGED_FILES}"" >> $GITHUB_OUTPUT\n', 'curl https://raw.githubusercontent.com/pingcap/docs/master/.lycheeignore -O\n', 'curl https://raw.githubusercontent.com/pingcap/docs/master/.lycheeignore --output .lycheeignore\n', 'curl -L https://github.com/pingcap/cloud-assets-utils/releases/download/v0.2.0/cloud_assets_utils-ubuntu-latest -o cloud_assets_utils\nchmod +x cloud_assets_utils\nsudo mv cloud_assets_utils /usr/local/bin/cloud-assets-utils\n', 'curl http://devtools.qiniu.com/qshell-linux-x64-v2.4.1.zip -o qshell.zip\nunzip qshell.zip\nsudo mv qshell-linux-x64-v2.4.1 /usr/local/bin/qshell\nqshell account ${{ secrets.QINIU_ACCESS_KEY }} ${{ secrets.QINIU_SECRET_KEY }} test\n', 'cloud-assets-utils verify-and-sync -qiniu true -qiniu-bucket ${{ secrets.QINIU_BUCKET_NAME }} media -replace-first-path-to images/docs -cdn-refresh https://download.pingcap.com/', 'git remote add head ${{ github.event.pull_request.head.repo.clone_url }}\ngit fetch --depth=1 head ${{ github.event.pull_request.head.ref }}\n', 'git rev-parse \'${{ github.event.pull_request.head.sha }}\'\nif git diff --name-only --diff-filter \'D\' HEAD \'${{ github.event.pull_request.head.sha }}\' | grep -E \'^media/.*\\.(jpg|png|jpeg|gif)$\' >/tmp/changed_files; then\n  cat /tmp/changed_files\n  echo \'{""name"":""Image Deletion Check"",""head_sha"":""${{ github.event.pull_request.head.sha }}"",""status"":""completed"",""conclusion"":""failure""}\' > /tmp/body.json\n  jq \\\n    --arg count ""$(wc -l /tmp/changed_files | awk \'{print $1}\')"" \\\n    --arg summary ""$(cat /tmp/changed_files | sed \'s/^/- /\')"" \\\n    \'.output.title = ""Found "" + $count + "" deleted images"" | .output.summary = $summary\' \\\n  /tmp/body.json > /tmp/body2.json\nelse\n  echo \'{""name"":""Image Deletion Check"",""head_sha"":""${{ github.event.pull_request.head.sha }}"",""status"":""completed"",""conclusion"":""success"",""output"":{""title"":""OK"",""summary"":""No deleted images""}}\' > /tmp/body2.json\nfi\n', 'cat /tmp/body2.json\ncurl \\\n  -sSL \\\n  -X POST \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: token ${{ github.token }}"" \\\n  -T \'/tmp/body2.json\' \\\n\'https://api.github.com/repos/${{ github.repository }}/check-runs\'\n']"
"['echo `python3 --version`\nsudo apt-get install -y python-setuptools\nsudo apt-get install -y python3-sphinx\npython3 -m pip install --upgrade pip\npython3 -m pip install setuptools\n', 'cd docs\npip install -r requirements.txt\nmake html \n', 'echo ""The time was ${{ steps.build.outputs.time }}""', 'python -m pip install --upgrade pip\n# Install CPU-based pytorch\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n# Maybe use the CUDA 10.2 version instead?\n# pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n', './run_python_examples.sh ""install_deps,run_all,clean""\n']"
[]
"['pipx install poetry\npipx inject poetry poetry-bumpversion\n', 'poetry version ${{ github.event.release.tag_name }}\n', 'LATEST_COMMIT_HASH=$(echo ${{ github.event.after }} | cut -b -7)\necho ""LATEST_COMMIT_HASH=${LATEST_COMMIT_HASH}"" >> $GITHUB_ENV\n', 'curl https://api.github.com/repos/${{ secrets.DISPATCH_OWNER }}/${{ secrets.DISPATCH_REPO }}/dispatches -H ""Accept: application/vnd.github+json"" -H ""Authorization: Bearer ${{ secrets.ACCESS_TOKEN }}"" -H ""X-GitHub-Api-Version: 2022-11-28"" --data \'{""event_type"":""dispatch"",""client_payload"":{""version"":""latest"", ""tag"": ""${{ env.LATEST_COMMIT_HASH }}""}}\'\n', 'curl https://api.github.com/repos/${{ secrets.DISPATCH_OWNER }}/${{ secrets.DISPATCH_REPO }}/dispatches -H ""Accept: application/vnd.github+json"" -H ""Authorization: Bearer ${{ secrets.ACCESS_TOKEN }}"" -H ""X-GitHub-Api-Version: 2022-11-28"" --data \'{""event_type"":""dispatch"",""client_payload"":{""version"":""release"", ""tag"":""${{ github.event.release.tag_name }}""}}\'\n', 'python -m pip install --upgrade pip\npipx install poetry\n', 'poetry install\npoetry run pip list\nVERSION=$(curl --silent ""https://api.github.com/repos/hadolint/hadolint/releases/latest"" | \\\n  grep \'""tag_name"":\' | \\\n  sed -E \'s/.*""v([^""]+)"".*/\\1/\' \\\n  ) && curl -L -o /tmp/hadolint ""https://github.com/hadolint/hadolint/releases/download/v${VERSION}/hadolint-Linux-x86_64"" \\\n  && chmod +x /tmp/hadolint\n', 'poetry lock --check\n', 'poetry run flake8 . --ignore=E266,W503,E203,E501,W605,E128 --exclude contrib\n', 'poetry run black --check .\n', 'poetry run pylint --disable=W,C,R,E -j 0 -rn -sn prowler/\n', ""poetry run bandit -q -lll -x '*_test.py,./contrib/' -r .\n"", 'poetry run safety check\n', 'poetry run vulture --exclude ""contrib"" --min-confidence 100 .\n', '/tmp/hadolint Dockerfile --ignore=DL3013\n', 'poetry run pytest tests -n auto\n', 'pipx install poetry\npipx inject poetry poetry-bumpversion\n', 'poetry version ${{ env.RELEASE_TAG }}\ngit config user.name ""github-actions""\ngit config user.email ""<noreply@github.com>""\ngit add prowler/config/config.py pyproject.toml\ngit commit -m ""chore(release): ${{ env.RELEASE_TAG }}"" --no-verify\ngit tag -fa ${{ env.RELEASE_TAG }} -m ""chore(release): ${{ env.RELEASE_TAG }}""\ngit push -f origin ${{ env.RELEASE_TAG }}\ngit checkout -B release-${{ env.RELEASE_TAG }}\ngit push origin release-${{ env.RELEASE_TAG }}\npoetry build\n', 'poetry config pypi-token.pypi ${{ secrets.PYPI_API_TOKEN }}\npoetry publish\n', 'rm -rf ./dist && rm -rf ./build && rm -rf prowler.egg-info\npip install toml\npython util/replicate_pypi_package.py\npoetry build\n', 'poetry config pypi-token.pypi ${{ secrets.PYPI_API_TOKEN }}\npoetry publish\n', 'python -m pip install --upgrade pip\npip install boto3\n', 'python3 util/update_aws_services_regions.py']"
""
"['conda update --all --yes\nconda config --set always_yes yes --set changeps1 no\nconda info -a\nconda install -c conda-forge r-xfun=0.27 r-rlang=1.1.0 r-extrafont\nconda install -c conda-forge r-base=4.1.1 r-rmarkdown=2.13 r-ggplot2=3.3.5 r-plotly=4.10.0 pandoc=2.17.1.1\nconda install -c conda-forge regex\npip install pycodestyle\npip install .[all]\nbash run_tests.sh\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['pip install tox\ntox -e ${{ matrix.tox }}\n', 'pip install tox twine wheel\n\necho -e ""[pypi]"" >> ~/.pypirc\necho -e ""username = __token__"" >> ~/.pypirc\necho -e ""password = $PYPI_TOKEN"" >> ~/.pypirc\n\npython setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
""
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox -vv']"
""
"['python3 -m venv .venv', 'source .venv/bin/activate\npython -m pip install .\npython -m pip install -r docs/requirements.txt\n', 'source ../.venv/bin/activate\n\nset +e\nmake linkcheck\nexit_code=$?\n\nset -e\n\nif [ $exit_code -eq 0 ]; then\n    echo -e ""\\n\\n=================\\nAll links are valid!""\n\n    echo ""# :heavy_check_mark: Sphinx links"" >> $GITHUB_STEP_SUMMARY\n    echo ""All links are valid!"" >> $GITHUB_STEP_SUMMARY\nelse\n    echo -e ""\\n\\n=================\\nFound broken links. Look at the build logs.\\n""\n\n    echo ""# :x: Sphinx links"" >> $GITHUB_STEP_SUMMARY\n    echo ""Found broken links. Look at the build logs for additional information."" >> $GITHUB_STEP_SUMMARY\n    echo \'```\' >> $GITHUB_STEP_SUMMARY\n    cat _build/linkcheck/output.txt >> $GITHUB_STEP_SUMMARY\n    echo \'```\' >> $GITHUB_STEP_SUMMARY\nfi\n\nexit $exit_code\n', 'python3 -m venv .venv', 'source .venv/bin/activate\npython -m pip install .\npython -m pip install -r docs/requirements.txt\n', 'source ../.venv/bin/activate\n\nset +e\nmake htmlstrict\n\nexit_code=$?\n\nset -e\n\nif [ $exit_code -eq 0 ]; then\n    echo -e ""\\n\\n=================\\nNo warnings or errors detected!""\n    echo ""# :heavy_check_mark: Sphinx warnings/errors"" >> $GITHUB_STEP_SUMMARY\n    echo ""No errors or warnings detected!"" >> $GITHUB_STEP_SUMMARY\nelse\n    echo -e ""\\n\\n=================\\nWarnings and or errors detected; See the summary bellow:\\n""\n    cat _build/htmlstrict/output.txt\n\n    echo ""# :x: Sphinx warnings/errors"" >> $GITHUB_STEP_SUMMARY\n    echo ""Found some warnings or errors:"" >> $GITHUB_STEP_SUMMARY\n    echo \'```\' >> $GITHUB_STEP_SUMMARY\n    cat _build/htmlstrict/output.txt >> $GITHUB_STEP_SUMMARY\n    echo \'```\' >> $GITHUB_STEP_SUMMARY\nfi\n\nexit $exit_code\n', ""echo 'sha: ${{ github.sha }}'\necho 'ref: ${{ github.ref }}'\n"", 'sudo apt-get install lcov\n', 'cmake -E make_directory ${{ env.OTIO_BUILD_DIR }}\ncd ${{ env.OTIO_BUILD_DIR }}\ncmake ${{ github.workspace }} -DCMAKE_INSTALL_PREFIX=${{ env.OTIO_INSTALL_DIR }} -DOTIO_SHARED_LIBS=OFF -DOTIO_CXX_COVERAGE=ON\ncmake --build . --config ${{ env.OTIO_BUILD_CONFIG }}\n', 'cd ${{ env.OTIO_BUILD_DIR }}\ncmake --build . --target ${{ matrix.OTIO_TEST_TARGET }} --config ${{ env.OTIO_BUILD_CONFIG }}\n', 'cd ${{ env.OTIO_BUILD_DIR }}\nlcov --capture -b . --directory . --output-file=coverage.info -q\ncat coverage.info | sed ""s/SF:.*src/SF:src/g"" > coverage.filtered.info\nlcov --remove coverage.filtered.info \'*/usr/*\' --output-file=coverage.filtered.info -q\nlcov --remove coverage.filtered.info \'*/deps/*\' --output-file=coverage.filtered.info -q\nlcov --remove coverage.filtered.info \'*/tests/*\' --output-file=coverage.filtered.info -q\nlcov --list coverage.filtered.info\n', 'cd ${{ env.OTIO_BUILD_DIR }}\ncmake --build . --target install --config ${{ env.OTIO_BUILD_CONFIG }}\n', ""echo 'OTIO_CXX_DEBUG_BUILD=1' >> $GITHUB_ENV\nsudo apt-get install lcov\n"", 'python -m pip install --upgrade pip setuptools wheel ""flake8>=3.5"" check-manifest\n', 'make ci-prebuild', '# compile and install into virtualenv/virtual machine (verbosely)\npip install .[dev] -v\n', 'make ci-postbuild', 'make lcov', 'python -m pip install build --user', 'python -m build -s .']"
"['pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install -r requirements/ci.txt\npip install -e .\n', 'make test']"
"['sudo mkdir -p /usr/local/bin\ncurl -L -s https://github.com/prometheus/prometheus/releases/download/v2.8.1/prometheus-2.8.1.linux-amd64.tar.gz |\\\nsudo tar -xz -C /usr/local/bin --strip-components=1 prometheus-2.8.1.linux-amd64/promtool\nsudo chmod +x /usr/local/bin/promtool\n', 'make test', 'make docs']"
[]
"['python -m pip install --upgrade pip\npython -m pip install ""Django==${{ matrix.django-version }}""\npython -m pip install ""elasticsearch-dsl==${{ matrix.es-dsl-version }}""\npython -m pip install -r requirements_test.txt\n', 'TOX_ENV=$(echo ""py${{ matrix.python-version }}-django-${{ matrix.django-version }}-es${{ matrix.es-dsl-version }}"" | tr -d .)\npython -m tox -e $TOX_ENV -- --elasticsearch\n']"
"['python setup.py install\n', 'make lint\n', 'sudo apt install libenchant-2-dev\npip install -r requirements/doc-spelling.txt\n', 'make doc-spelling\n', 'pip install -U twine wheel\npython setup.py sdist bdist_wheel\n', 'twine check dist/*\n', 'make cythonize\n', 'python -m pytest tests -vv\npython -m coverage xml\n', 'echo ""Predeploy step""\n', 'make cythonize\n', 'python setup.py sdist', 'if [[ -n ""${{ matrix.qemu }}"" ]]; then\n  # Build emulated architectures only if QEMU is set,\n  # use default ""auto"" otherwise\n  echo ""CIBW_ARCHS_LINUX=${{ matrix.qemu }}"" >> $GITHUB_ENV\nfi\n', 'make cythonize\n', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | gh auth login --with-token\n']"
"['python setup.py sdist\n', 'pip install twine\nlast_dist=$(ls -t dist/smac-*.tar.gz | head -n 1)\ntwine_output=`twine check ""$last_dist""`\nif [[ ""$twine_output"" != ""Checking $last_dist: PASSED"" ]]\nthen\n  echo $twine_output\nelse\n  pip install $last_dist\nfi\n', 'pip install "".[gpytorch,dev]""\n\n# Getting the version\nSMAC_VERSION=$(python -c ""import smac; print(\'v\' + str(smac.version));"")\n\n# Make it a global variable\necho ""SMAC_VERSION=$SMAC_VERSION"" >> $GITHUB_ENV\n', 'make clean\nmake docs\n', 'cd ..\ngit clone https://github.com/${{ github.repository }}.git --branch gh-pages --single-branch gh-pages\n', 'branch_name=${GITHUB_REF##*/}\ncd ../gh-pages\nrm -rf $branch_name\ncp -r ../${{ env.name }}/docs/build/html $branch_name\n\n# we also copy the current SMAC_VERSION\nrm -rf $SMAC_VERSION\ncp -r ../${{ env.name }}/docs/build/html $SMAC_VERSION\n', 'last_commit=$(git log --pretty=format:""%an: %s"")\ncd ../gh-pages\nbranch_name=${GITHUB_REF##*/}\ngit add $branch_name/\ngit add $SMAC_VERSION/\ngit config --global user.name \'Github Actions\'\ngit config --global user.email \'not@mail.com\'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}\ngit commit -am ""$last_commit""\ngit push\n', 'pre-commit run --all-files\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files\n', 'python -m pip install --upgrade pip\npython -m pip install wheel\npython -m pip install -e "".${{ env.extra-requires }}""\n', 'echo ""::set-output name=BEFORE::$(git status --porcelain -b)""\n', 'pytest ${{ env.pytest-args }} ${{ env.test-dir }}\n\n# Post-hoc clean-up\nmake clean\n', 'before=""${{ steps.status-before.outputs.BEFORE }}""\nafter=""$(git status --porcelain -b)""\nif [[ ""$before"" != ""$after"" ]]; then\n    echo ""git status from before: $before""\n    echo ""git status from after: $after""\n    echo ""Not all generated files have been deleted!""\n    exit 1\nfi\n', 'python -V\npython -m pip install --upgrade pip\npython -m pip install wheel\npython -m pip install -e "".${{ env.extra-requires }}""\n', 'pytest ${{ env.pytest-args }} ${{ env.test-dir }}\n', 'python -m pip install --upgrade pip\npython setup.py sdist\necho ""${{env.package-name}}""\necho ""sdist_name=$(ls -t dist/${{ env.package-name }}-*.tar.gz | head -n 1)"" >> $GITHUB_ENV\n', 'python -m pip install ${{ env.sdist_name }}${{ env.extra-requires }}\n', 'pytest ${{ env.pytest-args }} ${{ env.test-dir }}\n', 'echo ""CUTOFF_DATE=$(date -d \'-46 days\' \'+%Y-%m-%d\')"" >> $GITHUB_ENV\necho ""RECENT_ISSUE_CUTOFF_DATE=$(date -d \'-7 days\' \'+%Y-%m-%d\')"" >> $GITHUB_ENV\n', 'echo ""## Issues that have had interaction in the last 7 days <br />"" >> mail.html\necho ""$(<recent_issues.md) <br />"" >> mail.html\necho ""## Issues that may become stale in <= 14 days <br />"" >> mail.html\necho ""$(<potentially_stale_issues.md) <br />"" >> mail.html\necho ""## Issues that have not had interaction in the last 46 days but will not go stale due to their labels<br />"" >> mail.html\necho ""$(<old_issues.md) <br />"" >> mail.html\n']"
"['python -m pip install --upgrade pip\nsudo apt-get install -y graphviz\npip install tox poetry\n', 'tox']"
"['python -m pip install --upgrade pip wheel\npip install -r requirements-dev.txt\n', 'echo ""::add-matcher::.github/matchers/flake8.json""\npre-commit run flake8 --all-files\necho ""::remove-matcher owner=flake8::""\n', 'python -m pip install --upgrade pip\npip install wheel twine\n', 'CURRENT_TAG=${GITHUB_REF#refs/tags/}\nCURRENT_VERSION=$(head -n1 social_core/__init__.py | awk \'{print $3}\' | sed \'s/[^0-9\\.]//g\')\nif [ ""${CURRENT_VERSION}"" != ""${CURRENT_TAG}"" ]; then\n    echo ""========================================================================""\n    echo ""Error: tag \'${CURRENT_TAG}\' and version \'${CURRENT_VERSION}\' don\'t match""\n    echo ""========================================================================""\n    exit 1;\nfi\n', 'python setup.py sdist bdist_wheel --python-tag py3', 'twine check dist/*', 'twine upload --non-interactive -u __token__ -p ""${PYPI_API_TOKEN}"" dist/*\n', 'sudo apt-get update\nsudo apt-get install -qq -y --no-install-recommends libxmlsec1-dev swig\n', 'python -m pip install --upgrade pip\npip install coverage tox\n', 'tox -e ""py${PYTHON_VERSION/\\./}""']"
"['python -m pip install --upgrade pip setuptools\npython -m pip install -e .\n', 'python -c ""import sys; print(sys.version)""', 'python -u -m pyperformance.tests', 'pip install -e .[dev]', 'pip freeze --all', 'mypy', 'python -m pip install --upgrade build\n', 'python -m build\n']"
"['pip install --upgrade pip wheel setuptools\npip install --upgrade firebase-admin-sdk\n', 'output=$(python bin/new_max.py)\necho ""::set-output name=new-number::$output""\n', 'python -m pip install tox tox-gh-actions', 'tox']"
"['pip install --upgrade pip\npip install --only-binary=numpy,scipy numpy scipy\npip install -e .[dev]\n', 'python -m isort --check PyEMD', 'python -m pycodestyle PyEMD', 'python -m black --check PyEMD', 'pip install --only-binary=numpy,scipy numpy scipy\npip install -e .[test]\n', 'python -m PyEMD.tests.test_all\n', 'pip install --only-binary=numpy,scipy numpy scipy\npip install -e .[test]\n', 'python -m PyEMD.tests.test_all\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip wheel pdm\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox', 'python -m pip install pdm twine check-wheel-contents', 'pdm build', 'ls -l dist', 'check-wheel-contents --toplevel cattr,cattrs dist/*.whl', 'python -m twine check dist/*']"
""
""
[]
"['pip install tox', 'tox -v -- -v', 'pip install tox', 'tox -v -- -v']"
"['echo ::set-output name=TAG::${GITHUB_REF/refs\\/tags\\//}', 'echo ::set-output name=TAG::${GITHUB_REF/refs\\/tags\\//}', 'pip3 install -e . -r requirements/dev.txt', 'flake8 kibitzr tests', 'pytest']"
"['cd docusaurus\nyarn install\nyarn build\n', 'pip install --upgrade pip sdist wheel\npip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade pip pytest\npip install -r requirements.txt\n', 'python -m pytest', 'python -m pip install --upgrade pip black flake8', 'flake8 .', 'black -t py34 --check .']"
"['pip install -r tools/scripts/requirements.txt', './tools/scripts/structure_mastg.sh', 'cp -r tests docs/MASTG/', 'python3 tools/scripts/transform_files.py', 'mv Document/Images/ docs/assets/Images/', 'find docs/MASTG/tests -name ""*.md"" -exec sed -i ""s#<img src=\\""Images/#<img src=\\""../../../../../../assets/Images/#g"" {} \\;', 'find docs/MASTG -name ""*.md"" -exec sed -i ""s#<img src=\\""Images/#<img src=\\""../../../assets/Images/#g"" {} \\;', 'echo ""MASVS_VERSION=$(curl -s https://api.github.com/repos/OWASP/owasp-masvs/releases/latest | jq \'.tag_name\' | sed \'s/\\""//g\')"" >> $GITHUB_ENV', 'python3 ./owasp-masvs/tools/generate_masvs_yaml.py -v ${{env.MASVS_VERSION}} -i ./owasp-masvs/Document -c ./owasp-masvs/controls', './tools/scripts/structure_masvs.sh', 'mkdir docs/assets/Images/MASVS', 'mv owasp-masvs/Document/images/* docs/assets/Images/MASVS', 'sed -i ""s#images/#../../../assets/Images/MASVS/#g"" docs/MASVS/**/*.md', 'sed -i ""s#images/#../../assets/Images/MASVS/#g"" docs/MASVS/*.md', 'python3 tools/scripts/write_masvs_control_md_files.py', 'python3 tools/scripts/populate_dynamic_pages.py', 'mkdocs gh-deploy --force --clean --verbose', 'pip install -r tools/scripts/requirements.txt', 'echo ""MASTG_VERSION=$(curl  ""https://api.github.com/repos/OWASP/owasp-mastg/tags"" | jq -r \'.[0].name\')"" >> $GITHUB_ENV', 'echo ""MASTG_VERSION=${{env.MASTG_VERSION}}-$(git rev-parse --short HEAD)"" >> $GITHUB_ENV', 'echo ""MASVS_VERSION=$(curl -s https://api.github.com/repos/OWASP/owasp-masvs/releases/latest | jq \'.tag_name\' | sed \'s/\\""//g\')"" >> $GITHUB_ENV', 'python3 tools/scripts/assemble_test_chapters.py', 'python3 tools/scripts/transform_files.py', './tools/docker/pandoc_makedocs.sh Document ${{env.MASTG_VERSION}} ${{env.MASVS_VERSION}}', 'pip install -r tools/scripts/requirements.txt', 'echo ""MASTG_VERSION=$(curl -s https://api.github.com/repos/OWASP/owasp-mastg/tags | jq \'.[0].name\' | sed \'s/\\""//g\')"" >> $GITHUB_ENV', 'echo ""MASTG_VERSION=${{env.MASTG_VERSION}}-$(git rev-parse --short HEAD)"" >> $GITHUB_ENV', 'echo ${{env.MASTG_VERSION}}', 'echo ""MASTG_COMMIT=$(git rev-parse --short HEAD)"" >> $GITHUB_ENV', 'echo ${{env.MASTG_COMMIT}}', 'echo ""MASVS_VERSION=$(curl -s https://api.github.com/repos/OWASP/owasp-masvs/releases/latest | jq \'.tag_name\' | sed \'s/\\""//g\')"" >> $GITHUB_ENV', 'echo ${{env.MASVS_VERSION}}', 'cd tools/scripts/owasp-masvs && echo ""MASVS_COMMIT=$(git rev-parse --short HEAD)"" >> $GITHUB_ENV', 'echo ${{env.MASVS_COMMIT}}', 'python3 tools/scripts/yaml_to_excel.py --mastgversion ${{env.MASTG_VERSION}} --mastgcommit ${{env.MASTG_COMMIT}} --masvsversion ${{env.MASVS_VERSION}} --masvscommit ${{env.MASVS_COMMIT}}', 'ls -l OWASP_MASTG*', 'pwd', 'ls -l']"
"[""! grep -Ev '^644' <(git ls-files src/ tests/ | xargs stat '--format=%a %n')"", ""! grep -E -e 'UTF-[^8]' -e 'UTF-[^ ]+ \\(with BOM\\)' <(git ls-files src/ tests/ | xargs stat '--format=%a %n')"", ""! grep 'with CRLF line terminators' <(git ls-files | file -nNf-)"", ""! git grep -EIn $'[\\u2066\\u2067\\u2068\\u2069\\u202A\\u202B\\u202C\\u202D\\u202E]'"", './script/install-dependencies.sh', 'python -m ruff .\n', './script/install-dependencies.sh\npython -m pip install -r docs-requirements.txt\n', 'python -m mypy --no-incremental\npython -m mypy --no-incremental --python-version 3.8 docs\n', 'bash ./script/install-dependencies.sh', 'pytest -r a --cov --cov-branch --cov-report=xml --durations 10', 'bash ./script/build-shell-completions.sh', 'git fetch --depth=300 origin +refs/tags/*:refs/tags/*', './script/install-dependencies.sh\npython -m pip install -r docs-requirements.txt\n', 'make --directory=docs html man', 'git fetch --depth=300 origin +refs/tags/*:refs/tags/*', './script/install-dependencies.sh\npython -m pip install -r docs-requirements.txt\n', 'make --directory=docs html', './script/deploy-docs.sh', 'git fetch --depth=300 origin +refs/tags/*:refs/tags/*', './script/install-dependencies.sh\npython -m pip install -r docs-requirements.txt\npython -m pip install --upgrade Jinja2 build wheel twine\n', './script/build-shell-completions.sh', 'make --directory=docs man', './script/build-and-sign.sh', './script/github-release.py ""${STREAMLINK_DIST_DIR}""/*.tar.gz{,.asc}', './script/deploy-pypi.sh', 'python -m pip install requests', 'python ./script/update-user-agents.py']"
"['set -ex\n\n# for some reason, pip installs it in a different place than what is looked at in the py file\npip3 install requests==2.26\npyTorchDockerImageTag=$(python3 .jenkins/get_docker_tag.py)\n\necho ""docker-image=${DOCKER_IMAGE}:${pyTorchDockerImageTag}"" >> ""${GITHUB_OUTPUT}""\n', 'set -ex\n\nchmod +x "".jenkins/build.sh""\n\ncontainer_name=$(docker run \\\n  ${GPU_FLAG:-} \\\n  -e WORKER_ID \\\n  -e NUM_WORKERS \\\n  -e COMMIT_ID \\\n  -e JOB_TYPE \\\n  -e COMMIT_SOURCE \\\n  --env-file=""/tmp/github_env_${GITHUB_RUN_ID}"" \\\n  --tty \\\n  --detach \\\n  --user jenkins \\\n  --name=""${container_name}"" \\\n  -v ""${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace"" \\\n  -w /var/lib/jenkins/workspace \\\n  ""${DOCKER_IMAGE}""\n)\n\necho ""rm /opt/cache/bin/*"" | docker exec -u root -i ""${container_name}"" bash\n\ndocker exec -t ""${container_name}"" sh -c "".jenkins/build.sh""\n', 'set -ex\n\n# for some reason, pip installs it in a different place than what is looked at in the py file\npip3 install requests==2.26\npyTorchDockerImageTag=$(python3 .jenkins/get_docker_tag.py)\n\necho ""docker-image=${DOCKER_IMAGE}:${pyTorchDockerImageTag}"" >> ""${GITHUB_OUTPUT}""\n', 'set -ex\n\nchmod +x "".jenkins/build.sh""\n\ncontainer_name=$(docker run \\\n  ${GPU_FLAG:-} \\\n  -e WORKER_ID \\\n  -e NUM_WORKERS \\\n  -e COMMIT_ID \\\n  -e JOB_TYPE \\\n  -e COMMIT_SOURCE \\\n  -e GITHUB_PYTORCHBOT_TOKEN \\\n  --env-file=""/tmp/github_env_${GITHUB_RUN_ID}"" \\\n  --tty \\\n  --detach \\\n  --user jenkins \\\n  --name=""${container_name}"" \\\n  -v ""${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace"" \\\n  -w /var/lib/jenkins/workspace \\\n  ""${DOCKER_IMAGE}""\n)\n\necho ""rm /opt/cache/bin/*"" | docker exec -u root -i ""${container_name}"" bash\n\ndocker exec -t ""${container_name}"" sh -c "".jenkins/build.sh""\n', 'npm i @octokit/core @octokit/rest\n', 'pip install requests\npip install PyGithub\n', 'python ./.github/scripts/docathon-label-sync.py ${{ github.event.pull_request.number }}', 'pip install pyspelling', 'sudo apt-get install aspell aspell-en', 'pyspelling']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['pip install --upgrade --user pip', 'pip install poetry', 'poetry install --no-interaction', 'poetry run task minify\npoetry build\npoetry run pip install --quiet --disable-pip-version-check ""twine>=1.12""\npoetry run twine check --strict dist/*\n', 'poetry publish\n', 'pip install --upgrade --user pip', 'pip install poetry', 'poetry install --no-interaction', 'make test', 'make lint', 'pip install --upgrade --user pip', 'pip install poetry', 'poetry install --no-interaction', 'poetry run pytest']"
"['python -m pip install .', 'tox r -e py${{ matrix.py }} -vv --notest', 'tox r -e py${{ matrix.py }} --skip-pkg-install', 'python -m pip install .', 'tox r -e ${{ matrix.tox_env }}', 'python -m pip install build', 'pyproject-build -s -w . -o dist']"
"['pip install .\npip install scipy\n', 'git clone https://github.com/erikbern/git-of-theseus\ngit-of-theseus-analyze git-of-theseus --outdir got\ngit-of-theseus-stack-plot got/cohorts.json\ngit-of-theseus-stack-plot got/cohorts.json --normalize\ngit-of-theseus-stack-plot got/exts.json\ngit-of-theseus-stack-plot got/authors.json\ngit-of-theseus-line-plot got/authors.json\ngit-of-theseus-line-plot got/dirs.json\ngit-of-theseus-survival-plot got/survival.json --exp-fit\ngit-of-theseus-analyze --help\ngit-of-theseus-stack-plot --help\ngit-of-theseus-survival-plot --help\n']"
"['python -m pip install --upgrade pip\npip install build twine\n', 'python -m build\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install nox\npip install .\nnox\n']"
"['git fetch --prune --unshallow', 'python -m pip install --upgrade pip setuptools wheel', 'pip install .[dev]', 'black --version', 'black -v --check .', 'flake8 .', 'isort --check --profile=black .', 'mypy binarytree', 'py.test --cov=binarytree --cov-report=xml', 'python -m sphinx -b doctest docs docs/_build', 'python -m sphinx -b html -W docs docs/_build', 'git fetch --prune --unshallow', 'python -m pip install --upgrade pip\npip install setuptools wheel twine setuptools_scm[toml]\n', 'python setup.py sdist bdist_wheel', 'twine upload --repository testpypi dist/*', 'twine upload --repository pypi dist/*']"
"['python -m pip install --upgrade pip\npip install wheel twine\n', 'CURRENT_TAG=${GITHUB_REF#refs/tags/}\nCURRENT_VERSION=$(head -n1 social_django/__init__.py | awk \'{print $3}\' | sed \'s/[^0-9\\.]//g\')\nif [ ""${CURRENT_VERSION}"" != ""${CURRENT_TAG}"" ]; then\n    echo ""========================================================================""\n    echo ""Error: tag \'${CURRENT_TAG}\' and version \'${CURRENT_VERSION}\' don\'t match""\n    echo ""========================================================================""\n    exit 1;\nfi\n', 'python setup.py sdist bdist_wheel --python-tag py3', 'twine check dist/*', 'twine upload --non-interactive -u __token__ -p ""${PYPI_API_TOKEN}"" dist/*\n', 'python -m pip install --upgrade pip wheel\npip install -r requirements-dev.txt\n', 'pre-commit run ruff --all-files', 'sudo apt update -qq -y\nsudo apt install -qq -y --no-install-recommends libxmlsec1-dev swig\n', 'python -m pip install --upgrade pip\npip install tox coverage\n', 'tox -e ""py${PYTHON_VERSION/\\./}-django32""\n', 'tox -e ""py${PYTHON_VERSION/\\./}-django40""\n', 'tox -e ""py${PYTHON_VERSION/\\./}-django41""\n', 'tox -e ""py${PYTHON_VERSION/\\./}-django42""\n', 'tox -e ""py${PYTHON_VERSION/\\./}-djangomain""\n', 'tox -e ""py${PYTHON_VERSION/\\./}-socialmaster""\n', 'coverage combine\ncoverage xml\n']"
"['set -eux\npython --version\npip install pre-commit\n', 'pre-commit run --all-files --show-diff-on-failure\n', 'set -eux\n${{ env.CC }} --version\npython --version\npip install -U pip\npip install -r requirements.txt\n# Configuration for scons\necho \'godot_binary = ""${{ env.GODOT_BINARY_VERSION }}""\' >> custom.py\necho \'platform = ""${{ env.PLATFORM }}""\' >> custom.py\necho \'CC = ""${{ env.CC }}""\' >> custom.py\n', 'set -eux\nscons build -j2\n', '/usr/bin/Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &\necho "">>> Started xvfb""\n', 'set -eux\nscons tests headless=true\n', 'set -eux\nscons release\n', 'set -eux\npython --version\npython -m pip install --user -U pip\npython -m pip install --user -r requirements.txt\n# Configuration for scons\necho \'godot_binary = ""${{ env.GODOT_BINARY_VERSION }}""\' >> custom.py\necho \'platform = ""${{ matrix.PLATFORM }}""\' >> custom.py\necho \'MSVC_USE_SCRIPT = True\' >> custom.py\necho \'TARGET_ARCH = ""${{ matrix.VS_ARCH }}""\' >> custom.py\necho \'CC = ""cl.exe""\' >> custom.py\n', 'set -eux\nscons build -j2\n', 'set -eux\n# Azure pipelines doesn\'t provide a GPU with an OpenGL driver,\n# hence we use Mesa3D as software OpenGL driver\npushd build/${{ matrix.PLATFORM }}/platforms/\nif [ ""${{ matrix.PLATFORM }}"" = ""windows-64"" ]\nthen\n  curl https://downloads.fdossena.com/Projects/Mesa3D/Builds/MesaForWindows-x64-20.0.7.7z -o mesa.7z\nelse\n  curl https://downloads.fdossena.com/Projects/Mesa3D/Builds/MesaForWindows-20.0.7.7z -o mesa.7z\nfi\n# opengl32.dll must be extracted in the same directory than Godot binary\n7z.exe x mesa.7z\nls -lh opengl32.dll  # Sanity check\npopd\n', 'set -eux\nscons tests\n', 'scons release\n', 'set -eux\n${{ env.CC }} --version\npython --version\nbrew update\nbrew install zlib openssl\nbrew install --cask xquartz\npip install -U pip\npip install -r requirements.txt\n# Configuration for scons\necho \'godot_binary = ""${{ env.GODOT_BINARY_VERSION }}""\' >> custom.py\necho \'platform = ""${{ env.PLATFORM }}""\' >> custom.py\necho \'CC = ""${{ env.CC }}""\' >> custom.py\n', 'set -eux\nscons build -j2\n', 'set -eux\nscons tests\n', 'set -eux\nscons release\n']"
"['pip install tox', 'tox -e coverage', 'pip install tox', 'tox -e docs', 'pip install tox', 'tox -e style', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install tox', 'tox -e mypy', 'pip install tox', 'tox -e py', 'tox -e flask21']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements_dev.txt\n', 'pytest tests --cov=tabpy --cov-config=setup.cfg', 'coveralls --service=github', 'docker build . --file Dockerfile --tag $IMAGE_NAME', 'echo ""${{ secrets.CR_PAT }}"" | docker login https://ghcr.io -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=ghcr.io/${{ github.repository_owner }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements_dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements_dev.txt\n', 'pytest tests\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements_dev.txt\n', 'pytest tests\n']"
"['python -m pip install --upgrade pip\npip install wheel\npip install flake8 flake8-quotes\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. ignore magic numbers and use double quotes and ignore numbers with zeroes before them.\n# and ignore lowercase hex numbers and ignore isort incorrect imports\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=90 --ignore=WPS432,WPS339,WPS341,I --inline-quotes double --statistics\n']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox -e gh\n', 'python -m pip install --upgrade pip\npip install -e .\npip install -r requirements-lint.txt\n', 'mypy --python-version 3.7 .\n', 'mypy --python-version 3.8 .\n', 'mypy --python-version 3.9 .\n', 'mypy --python-version 3.10 .\n', 'pylint --rcfile=.pylintrc \\\ncan/**.py \\\nsetup.py \\\ndoc.conf \\\nscripts/**.py \\\nexamples/**.py\n', 'python -m pip install --upgrade pip\npip install -r requirements-lint.txt\n', 'black --check --verbose .\n', 'python -m pip install --upgrade pip\npip install -r requirements-lint.txt\n', 'black --verbose .\n']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['gh pr merge --auto --squash ""$PR_URL""', 'pip install .\n', 'make lint\n', 'pip install -U build twine wheel\npython -m build\n', 'twine check dist/*\n', 'pip install ujson', 'pytest tests --cov-report xml --cov-report html\npython -m coverage xml\n', 'bash examples/run_all.sh', 'python -m pip install -U pip wheel setuptools build twine\n', 'python -m build\n']"
"['sudo apt-get install -y libimage-exiftool-perl\npython -m pip install --upgrade pip\npip install pytest\npip install -r requirements-dev.txt\n', 'pytest', 'LATEST_VERSION=`git tag | sort -t. -k 1.2,1n -k 2,2n -k 3,3n -k 4,4n | tail -1`\n\nCURRENT_VERSION=`echo -e ""import urllib.request, json\\n\\nrequest = urllib.request.Request(\'http://api.snapcraft.io/v2/snaps/info/phockup\')\\nrequest.add_header(\'Snap-Device-Series\', \'16\')\\narchitectures = {\'amd64\': \'linux/amd64\', \'i386\': \'linux/386\', \'arm64\': \'linux/arm64\', \'armhf\': \'linux/arm/v7\', \'ppc64el\': \'linux/ppc64le\', \'s390x\': \'linux/s390x\'}\\nwith urllib.request.urlopen(request) as url:\\n    data = json.loads(url.read().decode())\\n    for c in data[\'channel-map\']:\\n        channel = c[\'channel\']\\n        if architectures.get(channel[\'architecture\']) == \'${{ matrix.architecture }}\' and channel[\'name\'] == \'stable\':\\n            print(c[\'version\'])"" | python3`\n\nif [ ""$LATEST_VERSION"" = ""$CURRENT_VERSION"" ]; then\n  echo ::set-output name=deploy::0\n  exit\nfi\n\necho \'{""experimental"": true}\' | sudo tee /etc/docker/daemon.json > /dev/null\nsudo systemctl restart docker\n\ndocker run --rm --tty \\\n  --security-opt apparmor:unconfined \\\n  --cap-add SYS_ADMIN \\\n  multiarch/qemu-user-static --reset -p yes\n\ndocker run --rm --tty \\\n  --security-opt apparmor:unconfined \\\n  --cap-add SYS_ADMIN \\\n  --device /dev/fuse \\\n  --volume /sys \\\n  --volume /sys/fs/cgroup:/sys/fs/cgroup:ro \\\n  --volume $GITHUB_WORKSPACE:$GITHUB_WORKSPACE \\\n  --workdir $GITHUB_WORKSPACE \\\n  --platform ""${{ matrix.architecture }}"" \\\n  --env PLAYTEST=""${{ matrix.playtest }}"" \\\n  diddledan/snapcraft:core18\n\nSNAP=`find $GITHUB_WORKSPACE -maxdepth 1 -type f -name \'*.snap\' | head -n1`\necho ::set-output name=snap::""$SNAP""\necho ::set-output name=deploy::1\n', 'cd phockup\n\nLATEST_VERSION=`git tag | sort -t. -k 1.2,1n -k 2,2n -k 3,3n -k 4,4n | tail -1`\n\nCURRENT_VERSION=`cat ../homebrew-contrib/Formula/phockup.rb | tr \'\\n\' \'\\r\' | sed \'s/.*archive\\/\\([0-9.]*\\)\\.tar.*/\\1/g\'`\n\nif [ ""$LATEST_VERSION"" = ""$CURRENT_VERSION"" ]; then\n  exit\nfi\n\ncurl -sLo $LATEST_VERSION.tar.gz https://github.com/ivandokov/phockup/archive/$LATEST_VERSION.tar.gz\nSHASUM=`shasum -a 256 $LATEST_VERSION.tar.gz | awk \'{print $1}\'`\nrm $LATEST_VERSION.tar.gz\n\ncd ../homebrew-contrib\n\nsed -i ""s/archive\\/[0-9.]*\\.tar/archive\\/$LATEST_VERSION\\.tar/"" Formula/phockup.rb\nsed -i ""s/sha256 .*/sha256 \\""$SHASUM\\""/"" Formula/phockup.rb\n\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m $LATEST_VERSION\ngit push\n', 'pip install -r requirements-dev.txt', 'pre-commit run -a', 'sudo apt-get install -y libimage-exiftool-perl\npython -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'pytest']"
"['git branch docs -t origin/docs', 'pnpm i\npnpm lint\n', 'python -m pip install -U pip setuptools', 'git config user.name ""github-actions[bot]""\ngit config user.email ""41898282+github-actions[bot]@users.noreply.github.com""\n', 'sudo apt-get install libcairo2-dev libfreetype6-dev libffi-dev libjpeg-dev libpng-dev libz-dev', 'pip install wheel', 'pip install git+https://${GH_TOKEN}@github.com/squidfunk/mkdocs-material-insiders.git', 'pip install -r docs/requirements.txt', 'git fetch --tags -f', 'mike deploy --alias-type copy -b docs -up $(git describe --tags --abbrev=0) latest page', 'mike deploy --alias-type copy -b docs -p dev', 'mike deploy --config-file mkdocs.base.yml -u $(git describe --tags) latest', 'sudo dpkg --add-architecture i386\nsudo apt-get update\necho steam steam/question select ""I AGREE"" | sudo debconf-set-selections\necho steam steam/license note \'\' | sudo debconf-set-selections\nsudo DEBIAN_FRONTEND=noninteractive apt-get -yq --no-install-recommends install steamcmd\n/usr/games/steamcmd +login anonymous +download_depot 232250 232256 +quit\nsudo bash -c \'cat <<\\EOF > /usr/local/bin/vpk\n#!/bin/bash\nVPK_LINUX=/home/runner/.steam/steamcmd/linux32/steamapps/content/app_232250/depot_232256/bin/vpk_linux32\nVALVE_LIB_DIR=/home/runner/.steam/steamcmd/linux32/steamapps/content/app_232250/depot_232256/bin\nLD_LIBRARY_PATH=""${VALVE_LIB_DIR}:${LD_LIBRARY_PATH}"" ""${VPK_LINUX}"" ""${@}""\nEOF\'\nsudo chmod +x /usr/local/bin/vpk\n', 'sudo bash -c \'cat <<\\EOF > /usr/local/bin/vpk\n#!/bin/bash\nVPK_LINUX=/home/runner/.steam/steamcmd/linux32/steamapps/content/app_232250/depot_232256/bin/vpk_linux32\nVALVE_LIB_DIR=/home/runner/.steam/steamcmd/linux32/steamapps/content/app_232250/depot_232256/bin\nLD_LIBRARY_PATH=""${VALVE_LIB_DIR}:${LD_LIBRARY_PATH}"" ""${VPK_LINUX}"" ""${@}""\nEOF\'\nsudo chmod +x /usr/local/bin/vpk\n', 'if [[ ${{ github.event.ref }} == refs/tags/* ]]; then\n  echo ""release=true"" >> $GITHUB_OUTPUT\nfi\n', 'dev/release.sh', 'dev/release.sh', 'dev/package.sh', '{\n  echo ""${{ github.event.inputs.version }}""\n  echo ""${{ github.event.inputs.highlights }}""\n  echo ""${{ github.event.inputs.hours }}""\n  echo """"\n} > dev/info.txt\n', 'git config user.email ""support@mastercomfig.com""\ngit config user.name ""mastercoms""\ngit commit -am ""release ${{ github.event.inputs.version }}"" || true\n', 'git checkout release', 'git merge ${GITHUB_REF##*/}', 'git tag ${{ github.event.inputs.version }}', 'git push --atomic origin release ${{ github.event.inputs.version }}']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\n#flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n#flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'python -m pip install build --user\npython -m build --sdist --wheel --outdir dist/ .\n']"
"['python -m pip install --upgrade pip wheel', 'pip install tox', 'tox -elint', 'python -m pip install --upgrade pip wheel\npip install tox\ntox -e${{ matrix.tox }}\n', 'python -m pip install --upgrade pip wheel', 'pip install twine', 'python setup.py sdist bdist_wheel', 'twine upload -u __token__ -p ${{ secrets.PYPI_API_TOKEN }} dist/*']"
"['sudo apt-get update\nsudo apt-get install libsasl2-dev jq\n', 'make install-dev', 'source .venv/bin/activate\npython3 -m pip install --upgrade botocore\n', 'source .venv/bin/activate\npython3 -m localstack.aws.scaffold upgrade\n', '# Check if there are changed files and store the result in target/diff-check.log\n# Check against the PR branch if it exists, otherwise against the master\n# Store the result in target/diff-check.log and store the diff count in the GitHub Action output ""diff-count""\nmkdir -p target\n(git diff --name-only origin/asf-auto-updates localstack/aws/api/ 2>/dev/null || git diff --name-only origin/master localstack/aws/api/ 2>/dev/null) | tee target/diff-check.log\necho ""diff-count=$(cat target/diff-check.log | wc -l)"" >> $GITHUB_OUTPUT\n\n# Store a (multiline-sanitized) list of changed services (compared to the master) in the GitHub Action output ""changed-services""\necho ""changed-services<<EOF"" >> $GITHUB_OUTPUT\necho ""$(git diff --name-only origin/master localstack/aws/api/ | sed \'s#localstack/aws/api/#- #g\' | sed \'s#/__init__.py##g\' | sed \'s/_/-/g\')"" >> $GITHUB_OUTPUT\necho ""EOF"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip wheel setuptools\npip install -e .\npip install pytest\n', 'python -m pytest tests/bootstrap/\n', 'make install-test &\n\n# install podman\nsudo apt update\nsudo apt install -y podman\npodman ps\npodman system info\n\n# disable Docker, to ensure the tests are running against Podman only\ndocker ps\nsudo mv /var/run/docker.sock /var/run/docker.sock.bk\ndocker ps && exit 1\ndockerCmd=$(which docker)\nsudo mv $dockerCmd $dockerCmd"".bk""\n\n# wait for async installation process to finish\nwait\n', '# determine path of podman socket\npodmanSocket=$(podman system info --format json | jq -r \'.host.remoteSocket.path\')\necho ""Running tests against local podman socket $podmanSocket""\n\nDOCKER_CMD=podman DOCKER_HOST=$podmanSocket TEST_PATH=tests/integration/docker_utils DEBUG=1 make test\n', 'sudo apt-get update\nsudo apt-get install -y --allow-downgrades libsasl2-dev jq postgresql-14=14.8-0ubuntu0* postgresql-client postgresql-plpython3\n', './scripts/build_common_test_functions.sh `pwd`/tests/integration/awslambda/functions/common', 'make install', 'source .venv/bin/activate\npip install -e ../localstack[runtime,test]\n', 'VENV_DIR=""../localstack-ext/.venv"" make entrypoints\ncat localstack_core.egg-info/entry_points.txt\n', 'make entrypoints\ncat localstack_ext.egg-info/entry_points.txt\n', 'source .venv/bin/activate\nbin/test_localstack_pro.sh\n', '# Remove the host tmp folder (might contain remnant files with different permissions)\nsudo rm -rf ../localstack/.filesystem/var/lib/localstack/tmp\nmake test\n']"
"['git config --global user.email ""no-reply@github.com""\ngit config --global user.name ""Swk""\ngit config --global pull.rebase false\ngit submodule add https://github.com/swisskyrepo/PayloadsAllTheThings/ docs\n', 'pip install mkdocs-material', 'pip install mkdocs-git-revision-date-localized-plugin', 'pip install mkdocs-git-committers-plugin', 'mkdocs gh-deploy --force']"
"['set -xe\ngit clone https://github.com/ray-project/buildkite-ci-pipelines.git ./pipelines \n\n# Find changed files\nGIT_DIFF=$(git diff --name-only ${{ github.event.pull_request.base.sha }}..${{ github.event.pull_request.head.sha }})\n\necho ""All changed files:""\necho ""$GIT_DIFF""\n\nGIT_DIFF_SERIALIZED=$(echo ""$GIT_DIFF"" | tr \'\\n\' \'|\')\necho ""GIT_DIFF_SERIALIZED=$GIT_DIFF_SERIALIZED"" >> $GITHUB_ENV\n']"
"['python .github/workflows/ci.py build', 'python .github/workflows/ci.py test', ""if grep -Inr '\\s$' kitty kitty_tests kittens docs *.py *.asciidoc *.rst *.go .gitattributes .gitignore; then echo Trailing whitespace found, aborting.; exit 1; fi"", 'python -m pip install -r docs/requirements.txt ruff mypy types-requests types-docutils', 'ruff .', 'go version && python .github/workflows/ci.py gofmt', 'python .github/workflows/ci.py package', 'python setup.py build --debug', 'python setup.py build-static-binaries', 'which python && python -m mypy --version && ./test.py mypy', 'go version && go vet ./...', 'make FAIL_WARN=1 man', 'make FAIL_WARN=1 html', 'which python3 && python3 .github/workflows/ci.py build', 'python3 .github/workflows/ci.py test', 'python3 .github/workflows/ci.py build', 'python3 .github/workflows/ci.py test', 'python3 -m pip install -r docs/requirements.txt', 'make FAIL_WARN=1 docs', 'python3 .github/workflows/ci.py package', 'python3 .github/workflows/ci.py build']"
"['git fetch --prune --unshallow --tags', 'GITHUB_BRANCH=${GITHUB_REF/refs\\/heads\\//}\necho ""release_branch=${GITHUB_BRANCH}"" >> $GITHUB_OUTPUT\necho ""new_branch=merge-${GITHUB_BRANCH}-main-${GITHUB_SHA:0:7}"" >> $GITHUB_OUTPUT\n', 'LATEST_RASA_MINOR=$(git tag --list | grep -P \'^\\d+\\.\\d+\\.\\d+$\' | tail -n1 | sed -e \'s/.\\([0-9]\\)*$/.0/g\')\necho ""Latest minor: ${LATEST_RASA_MINOR}""\n# bash doesn\'t support nested variable access\nCURRENT_RASA_MINOR=${GITHUB_REF/refs\\/heads\\//}\nCURRENT_RASA_MINOR=${CURRENT_RASA_MINOR/\\.x/\\.0}\n\nif [[ ${LATEST_RASA_MINOR} == ${CURRENT_RASA_MINOR} ]]\nthen\n  echo ""labels=${LABEL_TYPE},${LABEL_STATUS}"" >> $GITHUB_OUTPUT\nelse\n  echo ""labels=${LABEL_TYPE}"" >> $GITHUB_OUTPUT\nfi\n', '# fetch all open merge-PRs that have been opened from the current release branch\ngh pr list -S ""is:open label:${LABEL_TYPE} head:merge-${{ steps.get-branch-name.outputs.release_branch }}-main"" > prs.txt\nless prs.txt\n\n# delete newly opened PR from the list\nawk \'!/${{ steps.get-branch-name.outputs.new_branch }}/\' prs.txt > temp && mv temp prs.txt\n\n# extract the PR ids\nawk \'{print $1}\' prs.txt > pr_ids.txt\n\n# close all outdated PRs\nwhile read id; do\n  gh pr close $id -d\ndone <pr_ids.txt\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'sudo apt-get -y install libpq-dev\nmake install-full install-docs\n', 'npm install -g swagger-cli\nswagger-cli validate docs/static/spec/action-server.yml\nswagger-cli validate docs/static/spec/rasa.yml\n', 'make test-docs', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'sudo apt-get -y install libpq-dev\nmake install-full install-docs\n', 'make lint-docs', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'make install-full\n', 'poetry run ruff check .github --extend-ignore D\npoetry run black --check .github\n', 'make test-gh-actions\n', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', 'test -f .github/configs/mr-test-schedule.json', 'matrix=$(gomplate -d mapping=./dataset/dataset_config_mapping.json -d github=.github/configs/mr-test-schedule.json -f .github/templates/model_regression_test_config_to_json.tmpl)\nmatrix_length=$(echo $matrix | jq \'.[] | length\')\necho ""matrix_length=$matrix_length"" >> $GITHUB_OUTPUT\necho ""matrix=$matrix"" >> $GITHUB_OUTPUT', 'echo ""CLOUDSDK_PYTHON=${{ steps.python.outputs.python-path }}"" >> $GITHUB_OUTPUT\nexport CLOUDSDK_PYTHON=${{ steps.python.outputs.python-path }}\n', 'curl -o gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nchmod 755 gomplate', '# Read TF version from poetry.lock file\npip install toml\nTF_VERSION=$(scripts/read_tensorflow_version.sh)\necho ""TensorFlow version: $TF_VERSION""\necho TF_VERSION=$TF_VERSION >> $GITHUB_ENV', 'GH_RUNNER_IMAGE_TAG=$(jq -r \'if (.config | any(.TF == ""${{ env.TF_VERSION }}"" )) then (.config[] | select(.TF == ""${{ env.TF_VERSION }}"") | .IMAGE_TAG) else .default_image_tag end\' .github/configs/tf-cuda.json)\necho ""GitHub runner image tag for TensorFlow ${{ env.TF_VERSION }} is ${GH_RUNNER_IMAGE_TAG}""\necho GH_RUNNER_IMAGE_TAG=$GH_RUNNER_IMAGE_TAG >> $GITHUB_ENV\n\nnum_max_replicas=3\nmatrix_length=${{ needs.read_test_configuration.outputs.matrix_length }}\nif [[ $matrix_length -gt $num_max_replicas ]]; then\n  NUM_REPLICAS=$num_max_replicas\nelse\n  NUM_REPLICAS=$matrix_length\nfi\necho NUM_REPLICAS=$NUM_REPLICAS >> $GITHUB_ENV', 'echo ""::warning file=${TF_CUDA_FILE},line=3,col=1,endColumn=3::Missing cuda config for tf ${{ env.TF_VERSION }}. If you are not sure how to config CUDA, please reach out to infrastructure.""', 'export GH_RUNNER_IMAGE_TAG=${{ env.GH_RUNNER_IMAGE_TAG }}\nexport GH_RUNNER_IMAGE=${{ secrets.GH_RUNNER_IMAGE }}\n./gomplate -f .github/runner/github-runner-deployment.yaml.tmpl -o runner_deployment.yaml', 'kubectl apply -f runner_deployment.yaml\nkubectl -n github-runner rollout status --timeout=15m deployment/github-runner-$GITHUB_RUN_ID', '# determine extra environment variables\n# - CONFIG\n# - DATASET\n# - IS_EXTERNAL\n# - EXTERNAL_DATASET_REPOSITORY_BRANCH\n# - TRAIN_DIR\n# - TEST_DIR\n# - DOMAIN_FILE\nsource <(gomplate -d mapping=./dataset/dataset_config_mapping.json -f .github/templates/configuration_variables.tmpl)\n\n# Not all configurations are available for all datasets.\n# The job will fail and the workflow continues, if the configuration file doesn\'t exist\n# for a given dataset\n\necho ""is_dataset_exists=true"" >> $GITHUB_OUTPUT\necho ""is_config_exists=true"" >> $GITHUB_OUTPUT\necho ""is_external=${IS_EXTERNAL}"" >> $GITHUB_OUTPUT\n\nif [[ ""${IS_EXTERNAL}"" == ""true"" ]]; then\n  echo ""DATASET_DIR=dataset_external"" >> $GITHUB_ENV\nelse\n  echo ""DATASET_DIR=dataset"" >> $GITHUB_ENV\n  test -d dataset/$DATASET || (echo ""::warning::The ${{ matrix.dataset }} dataset doesn\'t exist. Skipping the job."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0)\nfi\n\n# Skip job if dataset is Hermit and config is BERT + DIET(seq) + ResponseSelector(t2t) or Sparse + BERT + DIET(seq) + ResponseSelector(t2t)\nif [[ ""${{ matrix.dataset }}"" == ""Hermit"" && ""${{ matrix.config }}"" =~ ""BERT + DIET(seq) + ResponseSelector(t2t)"" ]]; then\n  echo ""::warning::This ${{ matrix.dataset }} dataset / ${{ matrix.config }} config is currently being skipped due to OOM associated with the upgrade to TF 2.6."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0\nfi\n\n# Skip job if a given type is not available for a given dataset\nif [[ -z ""${DOMAIN_FILE}"" && ""${{ matrix.type }}"" == ""core"" ]]; then\n  echo ""::warning::The ${{ matrix.dataset }} dataset doesn\'t include core type. Skipping the job."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0\nfi\n\ntest -f dataset/configs/$CONFIG || (echo ""::warning::The ${{ matrix.config }} configuration file doesn\'t exist. Skipping the job."" \\\n    && echo ""is_dataset_exists=false"" >> $GITHUB_OUTPUT && exit 0)\n\necho ""DATASET=${DATASET}"" >> $GITHUB_ENV\necho ""CONFIG=${CONFIG}"" >> $GITHUB_ENV\necho ""DOMAIN_FILE=${DOMAIN_FILE}"" >> $GITHUB_ENV\necho ""EXTERNAL_DATASET_REPOSITORY_BRANCH=${EXTERNAL_DATASET_REPOSITORY_BRANCH}"" >> $GITHUB_ENV\necho ""IS_EXTERNAL=${IS_EXTERNAL}"" >> $GITHUB_ENV\n\nif [[ -z ""${TRAIN_DIR}"" ]]; then\n  echo ""TRAIN_DIR=train"" >> $GITHUB_ENV\nelse\n  echo ""TRAIN_DIR=${TRAIN_DIR}"" >> $GITHUB_ENV\nfi\n\nif [[ -z ""${TEST_DIR}"" ]]; then\n  echo ""TEST_DIR=test"" >> $GITHUB_ENV\nelse\n  echo ""TEST_DIR=${TEST_DIR}"" >> $GITHUB_ENV\nfi\n\nHOST_NAME=`hostname`\necho ""HOST_NAME=${HOST_NAME}"" >> $GITHUB_ENV', 'DATASET_COMMIT=$(git rev-parse HEAD)\necho $DATASET_COMMIT\necho ""dataset_commit=$DATASET_COMMIT"" >> $GITHUB_OUTPUT\n', '.github/scripts/start_dd_agent.sh ""${{ secrets.DD_API_KEY }}"" ""${{ env.ACCELERATOR_TYPE }}"" ${{ env.NVML_INTERVAL_IN_SEC }}\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'make install-full\npoetry run python -m spacy download de_core_news_md\n', 'poetry run pip install -U datadog-api-client', 'poetry run python .github/scripts/validate_gpus.py', 'poetry run python .github/scripts/download_pretrained.py --config dataset/configs/${CONFIG}', 'poetry run rasa --version\n\nexport NOW_TRAIN=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\ncd ${{ github.workspace }}\n\nif [[ ""${{ steps.set_dataset_config_vars.outputs.is_external }}"" == ""true"" ]]; then\n  export DATASET=.\nfi\n\nif [[ ""${{ matrix.type }}"" == ""nlu"" ]]; then\n  poetry run rasa train nlu --quiet -u ""${DATASET_DIR}/${DATASET}/${TRAIN_DIR}"" -c ""dataset/configs/${CONFIG}"" --out ""${DATASET_DIR}/models/${DATASET}/${CONFIG}""\n  echo ""train_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\n  export NOW_TEST=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\n  poetry run rasa test nlu --quiet -u ""${DATASET_DIR}/$DATASET/${TEST_DIR}"" -m ""${DATASET_DIR}/models/$DATASET/$CONFIG"" --out ""${{ github.workspace }}/results/$DATASET/$CONFIG""\n\n  echo ""test_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TEST"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n  echo ""total_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\nelif [[ ""${{ matrix.type }}"" == ""core"" ]]; then\n  poetry run rasa train core --quiet -s ${DATASET_DIR}/$DATASET/$TRAIN_DIR -c dataset/configs/$CONFIG -d ${DATASET_DIR}/${DATASET}/${DOMAIN_FILE}\n  echo ""train_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\n  export NOW_TEST=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\n  poetry run rasa test core -s ""${DATASET_DIR}/${DATASET}/${TEST_DIR}"" --out ""${{ github.workspace }}/results/${{ matrix.dataset }}/${{ matrix.config }}""\n\n  echo ""test_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TEST"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n  echo ""total_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\nfi', 'poetry run pip install analytics-python\npoetry run python .github/scripts/mr_publish_results.py\ncat $SUMMARY_FILE', '# Create dummy files to preserve directory structure when uploading artifacts\n# See: https://github.com/actions/upload-artifact/issues/174\ntouch ""results/${{ matrix.dataset }}/.keep""\ntouch ""results/${{ matrix.dataset }}/${{ matrix.config }}/.keep""\n', 'sudo service datadog-agent stop\n', 'ls -R', 'python .github/scripts/mr_generate_summary.py\ncat $SUMMARY_FILE\n', '# Get ID of last on-schedule workflow\nSCHEDULE_ID=$(curl -X GET -s -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -H ""Accept: application/vnd.github.v3+json"" \\\n  ""https://api.github.com/repos/${{ github.repository }}/actions/workflows"" \\\n  | jq -r \'.workflows[] | select(.name == ""${{ github.workflow }}"") | select(.path | test(""schedule"")) | .id\')\n\nARTIFACT_URL=$(curl -s -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -H ""Accept: application/vnd.github.v3+json"" \\\n  ""https://api.github.com/repos/${{ github.repository }}/actions/workflows/${SCHEDULE_ID}/runs?event=schedule&status=completed&branch=main&per_page=1"" | jq -r .workflow_runs[0].artifacts_url)\n\nDOWNLOAD_URL=$(curl -s -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -H ""Accept: application/vnd.github.v3+json"" ""${ARTIFACT_URL}"" \\\n  | jq -r \'.artifacts[] | select(.name == ""report.json"") | .archive_download_url\')\n\n# Download the artifact\ncurl -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -LJO -H ""Accept: application/vnd.github.v3+json"" $DOWNLOAD_URL\n\n# Unzip and change name\nunzip report.json.zip && mv report.json report_main.json\n', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', 'OUTPUT=""$(gomplate -d data=report.json -d results_main=report_main.json -f .github/templates/model_regression_test_results.tmpl)""\nOUTPUT=""${OUTPUT//$\'\\n\'/\'%0A\'}""\nOUTPUT=""${OUTPUT//$\'\\r\'/\'%0D\'}""\nOUTPUT=""$(echo $OUTPUT | sed \'s|`|\\\\`|g\')""\necho ""report_description=${OUTPUT}"" >> $GITHUB_OUTPUT\n\nIS_DROPPED=false\n\n# Loop through all negative values within parentheses\n# Set IS_DROPPED to true if there is any value lower\n# than the threshold\nfor x in $(grep -o \'\\(-[0-9.]\\+\\)\' <<< $OUTPUT); do\n  if (( $(bc -l <<< ""${{ env.PERFORMANCE_DROP_THRESHOLD }} > $x"") )); then\n    IS_DROPPED=true\n    echo ""The decrease of some test performance is > ${{ env.PERFORMANCE_DROP_THRESHOLD }}. Executing the following steps...""\n    break\n  fi\ndone\n\necho ""is_dropped=$IS_DROPPED"" >> $GITHUB_OUTPUT\n', 'echo ""CLOUDSDK_PYTHON=${{ steps.python.outputs.python-path }}"" >> $GITHUB_OUTPUT\nexport CLOUDSDK_PYTHON=${{ steps.python.outputs.python-path }}\n', 'kubectl -n github-runner delete deployments github-runner-${GITHUB_RUN_ID} --grace-period=30', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', 'echo ${{ steps.fc_config.outputs.comment-id }}', 'source <(gomplate -d github=https://api.github.com/repos/${{ github.repository }}/issues/comments/${{ steps.fc_config.outputs.comment-id }} -H \'github=Authorization:token ${{ secrets.GITHUB_TOKEN }}\' -f .github/templates/model_regression_test_read_dataset_branch.tmpl)\necho ""dataset_branch=${DATASET_BRANCH}"" >> $GITHUB_OUTPUT', 'OUTPUT=$(gomplate -d mapping=./dataset/dataset_config_mapping.json -f .github/templates/model_regression_test_config_comment.tmpl)\nOUTPUT=""${OUTPUT//$\'\\n\'/\'%0A\'}""\nOUTPUT=""${OUTPUT//$\'\\r\'/\'%0D\'}""\necho ""help_description=$OUTPUT"" >> $GITHUB_OUTPUT\n', 'echo ""::error::Cannot find a comment with the configuration""', 'matrix=$(gomplate -d mapping=./dataset/dataset_config_mapping.json -d github=https://api.github.com/repos/${{ github.repository }}/issues/comments/${{ steps.fc_config.outputs.comment-id }} -H \'github=Authorization:token ${{ secrets.GITHUB_TOKEN }}\' -f .github/templates/model_regression_test_config_to_json.tmpl)\n\nif [ $? -ne 0 ]; then\n  echo ""::error::Cannot read config from PR. Please double check your config.""\n  exit 1\nfi\n\nmatrix_length=$(echo $matrix | jq \'.[] | length\')\necho ""matrix_length=$matrix_length"" >> $GITHUB_OUTPUT\necho ""matrix=$matrix"" >> $GITHUB_OUTPUT', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', '# Read TF version from poetry.lock file\npip install toml\nTF_VERSION=$(scripts/read_tensorflow_version.sh)\n# Keep the first 3 characters, e.g. we keep 2.3 if TF_VERSION is 2.3.4\nTF_VERSION=${TF_VERSION::3}\necho ""TensorFlow version: $TF_VERSION""\necho TF_VERSION=$TF_VERSION >> $GITHUB_ENV', 'GH_RUNNER_IMAGE_TAG=$(jq -r \'if (.config | any(.TF == ""${{ env.TF_VERSION }}"" )) then (.config[] | select(.TF == ""${{ env.TF_VERSION }}"") | .IMAGE_TAG) else .default_image_tag end\' .github/configs/tf-cuda.json)\necho ""GitHub runner image tag for TensorFlow ${{ env.TF_VERSION }} is ${GH_RUNNER_IMAGE_TAG}""\necho GH_RUNNER_IMAGE_TAG=$GH_RUNNER_IMAGE_TAG >> $GITHUB_ENV\n\nnum_max_replicas=3\nmatrix_length=${{ needs.read_test_configuration.outputs.matrix_length }}\nif [[ $matrix_length -gt $num_max_replicas ]]; then\n  NUM_REPLICAS=$num_max_replicas\nelse\n  NUM_REPLICAS=$matrix_length\nfi\necho NUM_REPLICAS=$NUM_REPLICAS >> $GITHUB_ENV', 'echo ""::warning file=${TF_CUDA_FILE},line=3,col=1,endColumn=3::Missing cuda config for tf ${{ env.TF_VERSION }}. If you are not sure how to config CUDA, please reach out to infrastructure.""', 'export GH_RUNNER_IMAGE_TAG=${{ env.GH_RUNNER_IMAGE_TAG }}\nexport GH_RUNNER_IMAGE=${{ secrets.GH_RUNNER_IMAGE }}\ngomplate -f .github/runner/github-runner-deployment.yaml.tmpl -o runner_deployment.yaml', 'kubectl apply -f runner_deployment.yaml\nkubectl -n github-runner rollout status --timeout=15m deployment/github-runner-$GITHUB_RUN_ID', '# determine extra environment variables\n# - CONFIG\n# - DATASET\n# - IS_EXTERNAL\n# - EXTERNAL_DATASET_REPOSITORY_BRANCH\n# - TRAIN_DIR\n# - TEST_DIR\n# - DOMAIN_FILE\nsource <(gomplate -d mapping=./dataset/dataset_config_mapping.json -f .github/templates/configuration_variables.tmpl)\n\n# Not all configurations are available for all datasets.\n# The job will fail and the workflow continues, if the configuration file doesn\'t exist\n# for a given dataset\n\necho ""is_dataset_exists=true"" >> $GITHUB_OUTPUT\necho ""is_config_exists=true"" >> $GITHUB_OUTPUT\necho ""is_external=${IS_EXTERNAL}"" >> $GITHUB_OUTPUT\n\n# Warn about job if dataset is Hermit and config is BERT + DIET(seq) + ResponseSelector(t2t) or Sparse + BERT + DIET(seq) + ResponseSelector(t2t)\nif [[ ""${{ matrix.dataset }}"" == ""Hermit"" && ""${{ matrix.config }}"" =~ ""BERT + DIET(seq) + ResponseSelector(t2t)"" ]]; then\n  echo ""::warning::This ${{ matrix.dataset }} dataset / ${{ matrix.config }} config is currently being skipped on scheduled tests due to OOM associated with the upgrade to TF 2.6. You may see OOM here.""\nfi\n\nif [[ ""${IS_EXTERNAL}"" == ""true"" ]]; then\n  echo ""DATASET_DIR=dataset_external"" >> $GITHUB_ENV\nelse\n  echo ""DATASET_DIR=dataset"" >> $GITHUB_ENV\n  test -d dataset/$DATASET || (echo ""::warning::The ${{ matrix.dataset }} dataset doesn\'t exist. Skipping the job."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0)\nfi\n\n# Skip job if a given type is not available for a given dataset\nif [[ -z ""${DOMAIN_FILE}"" && ""${{ matrix.type }}"" == ""core"" ]]; then\n  echo ""::warning::The ${{ matrix.dataset }} dataset doesn\'t include core type. Skipping the job."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0\nfi\n\ntest -f dataset/configs/$CONFIG || (echo ""::warning::The ${{ matrix.config }} configuration file doesn\'t exist. Skipping the job."" \\\n    && echo ""is_dataset_exists=false"" >> $GITHUB_OUTPUT && exit 0)\n\necho ""DATASET=${DATASET}"" >> $GITHUB_ENV\necho ""CONFIG=${CONFIG}"" >> $GITHUB_ENV\necho ""DOMAIN_FILE=${DOMAIN_FILE}"" >> $GITHUB_ENV\necho ""EXTERNAL_DATASET_REPOSITORY_BRANCH=${EXTERNAL_DATASET_REPOSITORY_BRANCH}"" >> $GITHUB_ENV\necho ""IS_EXTERNAL=${IS_EXTERNAL}"" >> $GITHUB_ENV\n\nif [[ -z ""${TRAIN_DIR}"" ]]; then\n  echo ""TRAIN_DIR=train"" >> $GITHUB_ENV\nelse\n  echo ""TRAIN_DIR=${TRAIN_DIR}"" >> $GITHUB_ENV\nfi\n\nif [[ -z ""${TEST_DIR}"" ]]; then\n  echo ""TEST_DIR=test"" >> $GITHUB_ENV\nelse\n  echo ""TEST_DIR=${TEST_DIR}"" >> $GITHUB_ENV\nfi\n\nHOST_NAME=`hostname`\necho ""HOST_NAME=${HOST_NAME}"" >> $GITHUB_ENV', 'DATASET_COMMIT=$(git rev-parse HEAD)\necho $DATASET_COMMIT\necho ""dataset_commit=$DATASET_COMMIT"" >> $GITHUB_OUTPUT\n', 'export PR_URL=""https://github.com/${GITHUB_REPOSITORY}/pull/${{ github.event.number }}""\n.github/scripts/start_dd_agent.sh ""${{ secrets.DD_API_KEY }}"" ""${{ env.ACCELERATOR_TYPE }}"" ${{ env.NVML_INTERVAL_IN_SEC }}\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'make install-full\npoetry run python -m spacy download de_core_news_md\n', 'poetry run pip install -U datadog-api-client ddtrace', 'poetry run python .github/scripts/validate_gpus.py', 'poetry run python .github/scripts/download_pretrained.py --config dataset/configs/${CONFIG}', 'poetry run rasa --version\n\nexport NOW_TRAIN=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\ncd ${{ github.workspace }}\n\nif [[ ""${{ steps.set_dataset_config_vars.outputs.is_external }}"" == ""true"" ]]; then\n  export DATASET=.\nfi\n\nif [[ ""${{ matrix.type }}"" == ""nlu"" ]]; then\n  poetry run ddtrace-run rasa train nlu --quiet -u ${DATASET_DIR}/${DATASET}/${TRAIN_DIR} -c dataset/configs/${CONFIG} --out ${DATASET_DIR}/models/${DATASET}/${CONFIG}\n  echo ""train_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\n  export NOW_TEST=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\n  poetry run ddtrace-run rasa test nlu --quiet -u ${DATASET_DIR}/$DATASET/${TEST_DIR} -m ${DATASET_DIR}/models/$DATASET/$CONFIG --out ${{ github.workspace }}/results/$DATASET/$CONFIG\n\n  echo ""test_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TEST"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n  echo ""total_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\nelif [[ ""${{ matrix.type }}"" == ""core"" ]]; then\n  poetry run ddtrace-run rasa train core --quiet -s ${DATASET_DIR}/$DATASET/$TRAIN_DIR -c dataset/configs/$CONFIG -d ${DATASET_DIR}/${DATASET}/${DOMAIN_FILE}\n  echo ""train_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\n  export NOW_TEST=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\n  poetry run ddtrace-run rasa test core -s ${DATASET_DIR}/${DATASET}/${TEST_DIR} --out ${{ github.workspace }}/results/${{ matrix.dataset }}/${CONFIG}\n\n  echo ""test_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TEST"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n  echo ""total_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\nfi', 'export PR_URL=""https://github.com/${GITHUB_REPOSITORY}/pull/${{ github.event.number }}""\npoetry run pip install analytics-python\npoetry run python .github/scripts/mr_publish_results.py\ncat $SUMMARY_FILE', 'sudo service datadog-agent stop\n', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', '# determine extra environment variables\n# - CONFIG\n# - DATASET\n# - IS_EXTERNAL\n# - EXTERNAL_DATASET_REPOSITORY_BRANCH\n# - TRAIN_DIR\n# - TEST_DIR\n# - DOMAIN_FILE\nsource <(gomplate -d mapping=./dataset/dataset_config_mapping.json -f .github/templates/configuration_variables.tmpl)\n\n# Not all configurations are available for all datasets.\n# The job will fail and the workflow continues, if the configuration file doesn\'t exist\n# for a given dataset\n\necho ""is_dataset_exists=true"" >> $GITHUB_OUTPUT\necho ""is_config_exists=true"" >> $GITHUB_OUTPUT\necho ""is_external=${IS_EXTERNAL}"" >> $GITHUB_OUTPUT\n\nif [[ ""${IS_EXTERNAL}"" == ""true"" ]]; then\n  echo ""DATASET_DIR=dataset_external"" >> $GITHUB_ENV\nelse\n  echo ""DATASET_DIR=dataset"" >> $GITHUB_ENV\n  test -d dataset/$DATASET || (echo ""::warning::The ${{ matrix.dataset }} dataset doesn\'t exist. Skipping the job."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0)\nfi\n\n# Skip job if a given type is not available for a given dataset\nif [[ -z ""${DOMAIN_FILE}"" && ""${{ matrix.type }}"" == ""core"" ]]; then\n  echo ""::warning::The ${{ matrix.dataset }} dataset doesn\'t include core type. Skipping the job."" \\\n    && echo ""is_config_exists=false"" >> $GITHUB_OUTPUT && exit 0\nfi\n\ntest -f dataset/configs/$CONFIG || (echo ""::warning::The ${{ matrix.config }} configuration file doesn\'t exist. Skipping the job."" \\\n    && echo ""is_dataset_exists=false"" >> $GITHUB_OUTPUT && exit 0)\n\necho ""DATASET=${DATASET}"" >> $GITHUB_ENV\necho ""CONFIG=${CONFIG}"" >> $GITHUB_ENV\necho ""DOMAIN_FILE=${DOMAIN_FILE}"" >> $GITHUB_ENV\necho ""EXTERNAL_DATASET_REPOSITORY_BRANCH=${EXTERNAL_DATASET_REPOSITORY_BRANCH}"" >> $GITHUB_ENV\necho ""IS_EXTERNAL=${IS_EXTERNAL}"" >> $GITHUB_ENV\n\nif [[ -z ""${TRAIN_DIR}"" ]]; then\n  echo ""TRAIN_DIR=train"" >> $GITHUB_ENV\nelse\n  echo ""TRAIN_DIR=${TRAIN_DIR}"" >> $GITHUB_ENV\nfi\n\nif [[ -z ""${TEST_DIR}"" ]]; then\n  echo ""TEST_DIR=test"" >> $GITHUB_ENV\nelse\n  echo ""TEST_DIR=${TEST_DIR}"" >> $GITHUB_ENV\nfi\n\nHOST_NAME=`hostname`\necho ""HOST_NAME=${HOST_NAME}"" >> $GITHUB_ENV', 'DATASET_COMMIT=$(git rev-parse HEAD)\necho $DATASET_COMMIT\necho ""dataset_commit=$DATASET_COMMIT"" >> $GITHUB_OUTPUT\n', 'export PR_URL=""https://github.com/${GITHUB_REPOSITORY}/pull/${{ github.event.number }}""\n.github/scripts/start_dd_agent.sh ""${{ secrets.DD_API_KEY }}"" ""${{ env.ACCELERATOR_TYPE }}"" ${{ env.NVML_INTERVAL_IN_SEC }}\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'make install-full\npoetry run python -m spacy download de_core_news_md\n', 'poetry run pip install -U datadog-api-client ddtrace', 'poetry run python .github/scripts/validate_cpu.py', 'poetry run python .github/scripts/download_pretrained.py --config dataset/configs/${CONFIG}', 'poetry run rasa --version\n\nexport NOW_TRAIN=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\ncd ${{ github.workspace }}\n\nif [[ ""${{ steps.set_dataset_config_vars.outputs.is_external }}"" == ""true"" ]]; then\n  export DATASET=.\nfi\n\nif [[ ""${{ matrix.type }}"" == ""nlu"" ]]; then\n  poetry run ddtrace-run rasa train nlu --quiet -u ${DATASET_DIR}/${DATASET}/${TRAIN_DIR} -c dataset/configs/${CONFIG} --out ${DATASET_DIR}/models/${DATASET}/${CONFIG}\n  echo ""train_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\n  export NOW_TEST=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\n  poetry run ddtrace-run rasa test nlu --quiet -u ${DATASET_DIR}/$DATASET/${TEST_DIR} -m ${DATASET_DIR}/models/$DATASET/$CONFIG --out ${{ github.workspace }}/results/$DATASET/$CONFIG\n\n  echo ""test_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TEST"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n  echo ""total_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\nelif [[ ""${{ matrix.type }}"" == ""core"" ]]; then\n  poetry run ddtrace-run rasa train core --quiet -s ${DATASET_DIR}/$DATASET/$TRAIN_DIR -c dataset/configs/$CONFIG -d ${DATASET_DIR}/${DATASET}/${DOMAIN_FILE}\n  echo ""train_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n\n  export NOW_TEST=$(gomplate -i \'{{ (time.Now).Format time.RFC3339}}\');\n  poetry run ddtrace-run rasa test core -s ${DATASET_DIR}/${DATASET}/${TEST_DIR} --out ${{ github.workspace }}/results/${{ matrix.dataset }}/${CONFIG}\n\n  echo ""test_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TEST"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\n  echo ""total_run_time=$(gomplate -i \'{{ $t := time.Parse time.RFC3339 (getenv ""NOW_TRAIN"") }}{{ (time.Since $t).Round (time.Second 1) }}\')"" >> $GITHUB_OUTPUT\nfi', 'export PR_URL=""https://github.com/${GITHUB_REPOSITORY}/pull/${{ github.event.number }}""\npoetry run pip install analytics-python\npoetry run python .github/scripts/mr_publish_results.py\ncat $SUMMARY_FILE', 'sudo service datadog-agent stop\n', 'succeeded=${{ needs.model_regression_test_cpu.result == \'success\' || needs.model_regression_test_gpu.result == \'success\' }}\nif [[ $succeeded == ""false"" ]]; then\n  success_status=""Failed""\nelif [[ $succeeded == ""true"" ]]; then\n  success_status=""Succeeded""\nelse\n  success_status=""Unknown""\nfi\necho $success_status\necho ""success_status=$success_status"" >> $GITHUB_OUTPUT', 'ls -R', 'python .github/scripts/mr_generate_summary.py\ncat $SUMMARY_FILE\n', 'success_status=${{ needs.combine_reports.outputs.success_status }}\necho ""Status: $success_status""\nif [[ $success_status == ""Succeeded"" ]]; then\n  exit 0\nelse\n  exit 1\nfi\n', '# Get ID of last on-schedule workflow\nSCHEDULE_ID=$(curl -X GET -s -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -H ""Accept: application/vnd.github.v3+json"" \\\n  ""https://api.github.com/repos/${{ github.repository }}/actions/workflows"" \\\n  | jq -r \'.workflows[] | select(.name == ""CI - Model Regression on schedule"") | select(.path | test(""schedule"")) | .id\')\n\nARTIFACT_URL=$(curl -s -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -H ""Accept: application/vnd.github.v3+json"" \\\n  ""https://api.github.com/repos/${{ github.repository }}/actions/workflows/${SCHEDULE_ID}/runs?event=schedule&status=completed&branch=main&per_page=1"" | jq -r .workflow_runs[0].artifacts_url)\n\nDOWNLOAD_URL=$(curl -s -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -H ""Accept: application/vnd.github.v3+json"" ""${ARTIFACT_URL}"" \\\n  | jq -r \'.artifacts[] | select(.name == ""report.json"") | .archive_download_url\')\n\n# Download the artifact\ncurl -H \'Authorization: token ${{ secrets.GITHUB_TOKEN }}\' -LJO -H ""Accept: application/vnd.github.v3+json"" $DOWNLOAD_URL\n\n# Unzip and change name\nunzip report.json.zip && mv report.json report_main.json\n', 'sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.9.0/gomplate_linux-amd64\nsudo chmod +x /usr/local/bin/gomplate', 'OUTPUT=""$(gomplate -d data=report.json -d results_main=report_main.json -f .github/templates/model_regression_test_results.tmpl)""\nOUTPUT=""${OUTPUT//$\'\\n\'/\'%0A\'}""\nOUTPUT=""${OUTPUT//$\'\\r\'/\'%0D\'}""\necho ""result=$OUTPUT"" >> $GITHUB_OUTPUT\n\n# Get time of current commit as start time\nTIME_ISO_COMMIT=$(gomplate -d github=https://api.github.com/repos/rasaHQ/rasa/commits/${{ github.sha }} -H \'github=Authorization:token ${{ secrets.GITHUB_TOKEN }}\' -i \'{{ (ds ""github"").commit.author.date }}\')  # Example ""2022-02-17T14:06:38Z""\nTIME_UNIX_COMMIT=$(date -d ""${TIME_ISO_COMMIT}"" +%s%3N)  # Example: ""1645106798""\n\n# Get current time\nTIME_ISO_NOW=$(gomplate -i \'{{ (time.Now).UTC.Format time.RFC3339}}\') # Example: ""2022-02-17T14:50:54Z%""\nTIME_UNIX_NOW=$(date -d ""${TIME_ISO_NOW}"" +%s%3N)  # Example: ""1645118083""\n\necho ""from_ts=$TIME_UNIX_COMMIT"" >> $GITHUB_OUTPUT\necho ""to_ts=$TIME_UNIX_NOW"" >> $GITHUB_OUTPUT\n', 'kubectl -n github-runner delete deployments github-runner-${GITHUB_RUN_ID} --grace-period=30', 'echo ""backend=true"" >> $GITHUB_OUTPUT\necho ""docker=true"" >> $GITHUB_OUTPUT\necho ""docs=true"" >> $GITHUB_OUTPUT\n', '# Get current tagged Rasa version\nCURRENT_TAG=${GITHUB_REF#refs/tags/}\nif [[ ""$CURRENT_TAG"" =~ ^[0-9.]+$ ]]; then\n  echo ""is_pre_release_version=false"" >> $GITHUB_OUTPUT\nelse\n  echo ""is_pre_release_version=true"" >> $GITHUB_OUTPUT\nfi\n', 'echo ""Could not find the doc tests run.""\nexit 1\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'sudo apt-get -y install libpq-dev\nmake install-full || make install-full || make install-full\n', 'git fetch origin ${{ github.base_ref }}\necho ""DOCSTRING_DIFF_BRANCH=origin/${{ github.base_ref }}"" >> $GITHUB_ENV\n\n# Fetch entire history for current branch so that `make lint-docstrings`\n# can calculate the proper diff between the branches\ngit fetch --unshallow origin ""${{ github.ref }}""\n', 'echo ""::add-matcher::.github/matchers/flake8-error-matcher.json""\n', ""# If it's not a pull request, $DOCSTRING_DIFF_BRANCH is unset.\n# This will result in an empty diff, which effictively means that\n# make lint-docstrings will be skipped for other events than `pull_request`\nmake lint BRANCH=$DOCSTRING_DIFF_BRANCH\n"", 'make types', 'make lint-changelog', 'poetry run rasa --help', '# List all unexpected files in changelog/\nUNEXPECTED_FILES=$(ls -A --ignore={""README.md"","".gitignore"",""_template.md.jinja2""})\n\n# Exit with error if found any unexpected files\n[[ ""$UNEXPECTED_FILES"" ]] && \\\necho ""Found the following unexpected files in changelogs/"" && \\\necho ""$UNEXPECTED_FILES"" && \\\nexit 1 || \\\necho ""Release includes all changelog entries.""\n', 'docker run --name dd_agent -p 8126:8126 -d -e ""DD_API_KEY=${{ secrets.DD_API_KEY }}"" -e ""DD_INSIDE_CI=true"" -e ""DD_HOSTNAME=none"" -e ""DD_SITE=datadoghq.eu"" -e GITHUB_ACTIONS=true -e CI=true datadog/agent:latest\ndocker ps --all --filter name=dd_agent --filter status=running --no-trunc --format ""{{.ID}} {{.Status}}""\ndocker port dd_agent\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', ""sudo apt-get -y install libpq-dev\nmake install-full | tee .output  || make install-full | tee .output || make install-full | tee .output\nif grep 'The lock file is not up to date' .output; then exit 1; fi\nmake prepare-tests-ubuntu\n"", '$spacy_data_dir = "".venv\\lib\\site-packages\\spacy\\data""\nif (Test-Path $spacy_data_dir) {\n  Get-ChildItem -Force -ErrorAction Stop $spacy_data_dir | Where-Object { if($_.Attributes -match ""ReparsePoint""){$_.Delete()} }\n  Remove-Item -Force -Recurse $spacy_data_dir\n  New-Item -Path $spacy_data_dir -Type Directory\n}\nmake install-full  || make install-full || make install-full\nmake prepare-tests-windows-gha\n', 'pip install pytest-github-actions-annotate-failures', '(Get-ItemProperty ""HKLM:System\\CurrentControlSet\\Control\\FileSystem"").LongPathsEnabled\nSet-ItemProperty \'HKLM:\\System\\CurrentControlSet\\Control\\FileSystem\' -Name \'LongPathsEnabled\' -value 0\n', 'poetry run pip install -U ddtrace', 'make ${{ matrix.test }}\nif [[ ""${{ matrix.os }}"" != ""windows-2019"" ]]; then\n  mv .coverage ${{ github.workspace }}/${{ matrix.test }}-coverage\nfi\n', 'docker run --name dd_agent -p 8126:8126 -d -e ""DD_API_KEY=${{ secrets.DD_API_KEY }}"" -e ""DD_INSIDE_CI=true"" -e ""DD_HOSTNAME=none"" -e ""DD_SITE=datadoghq.eu"" -e GITHUB_ACTIONS=true -e CI=true datadog/agent:latest\ndocker ps --all --filter name=dd_agent --filter status=running --no-trunc --format ""{{.ID}} {{.Status}}""\ndocker port dd_agent\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', ""sudo apt-get -y install libpq-dev\nmake install-full | tee .output\nif grep 'The lock file is not up to date' .output; then exit 1; fi\nmake prepare-tests-ubuntu\n"", '$spacy_data_dir = "".venv\\lib\\site-packages\\spacy\\data""\nif (Test-Path $spacy_data_dir) {\n  Get-ChildItem -Force -ErrorAction Stop $spacy_data_dir | Where-Object { if($_.Attributes -match ""ReparsePoint""){$_.Delete()} }\n  Remove-Item -Force -Recurse $spacy_data_dir\n  New-Item -Path $spacy_data_dir -Type Directory\n}\nmake install-full\nmake prepare-tests-windows-gha\n', 'pip install pytest-github-actions-annotate-failures', '(Get-ItemProperty ""HKLM:System\\CurrentControlSet\\Control\\FileSystem"").LongPathsEnabled\nSet-ItemProperty \'HKLM:\\System\\CurrentControlSet\\Control\\FileSystem\' -Name \'LongPathsEnabled\' -value 0\n', 'poetry run pip install -U ddtrace', 'make test-flaky\nif [[ ""${{ matrix.os }}"" != ""windows-2019"" ]]; then\n  mv .coverage ${{ github.workspace }}/test-flaky-coverage\nfi\n', 'subs=`ls ${{ github.workspace }}/tests_coverage`\ndownload_dir=""${{ github.workspace }}/tests_coverage""\nfinal_dir=""${{ github.workspace }}/tests_coverage/final""\n\n# Downloaded artifacts go into folders, gotta extract them all into one folder for upload\nmkdir ""${final_dir}/""\nfor i in $subs; do\n  mv ""${download_dir}/$i""/* ""${final_dir}/""\ndone\n\npip install coverage\ncoverage combine ""${final_dir}/""*\ncoverage xml\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', ""sudo apt-get -y install libpq-dev\nmake install-full | tee .output\nif grep 'The lock file is not up to date' .output; then exit 1; fi\nmake prepare-tests-ubuntu\n"", 'docker-compose -f tests_deployment/docker-compose.kafka.yml up -d\n', 'make test-integration\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', ""sudo apt-get -y install libpq-dev\nmake install-full | tee .output\nif grep 'The lock file is not up to date' .output; then exit 1; fi\nmake prepare-tests-ubuntu\n"", 'make test-integration\n', 'docker-compose -f tests_deployment/docker-compose.kafka.yml down\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'echo ${{ secrets.DOCKERHUB_PASSWORD }} | docker login -u ${{ env.DOCKERHUB_USERNAME }} --password-stdin || true', '# Get latest tagged Rasa version\ngit fetch --depth=1 origin ""+refs/tags/*:refs/tags/*""\n# Fetch branch history\ngit fetch --prune --unshallow\nLATEST_TAGGED_NON_ALPHA_RASA_VERSION=$(git tag | sort -r -V | grep -E ""^[0-9.]+$"" | head -n1)\nCURRENT_TAG=${GITHUB_REF#refs/tags/}\n# Return \'true\' if tag version is equal or higher than the latest tagged Rasa version\nIS_NEWEST_VERSION=$((printf \'%s\\n%s\\n\' ""${LATEST_TAGGED_NON_ALPHA_RASA_VERSION}"" ""$CURRENT_TAG"" \\\n  | sort -V -C && echo true || echo false) || true)\n# Avoid that the script gets released for alphas or release candidates\nif [[ ""${IS_NEWEST_VERSION}"" == ""true"" && ""$CURRENT_TAG"" =~ ^[0-9.]+$ ]]; then\n  echo ""is_newest_version=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""is_newest_version=false"" >> $GITHUB_OUTPUT\nfi\n', '# Base image\nBASE_IMAGE_HASH=${{ hashFiles(\'docker/Dockerfile.base\') }}\necho ""base_image_hash=${BASE_IMAGE_HASH}"" >> $GITHUB_OUTPUT\n\nBASE_IMAGE_EXISTS=$((docker manifest inspect rasa/rasa:base-${BASE_IMAGE_HASH} &> /dev/null && echo true || echo false) || true)\necho ""base_exists=${BASE_IMAGE_EXISTS}"" >> $GITHUB_OUTPUT\n\n# Base MITIE image\nBASE_MITIE_IMAGE_HASH=${{ hashFiles(\'docker/Dockerfile.base-mitie\') }}\nMAKEFILE_MITIE_HASH=${{ hashFiles(\'Makefile\') }}\necho ""base_mitie_image_hash=${BASE_MITIE_IMAGE_HASH:0:50}-${MAKEFILE_MITIE_HASH:0:50}"" >> $GITHUB_OUTPUT\n\nBASE_IMAGE_MITIE_EXISTS=$((docker manifest inspect rasa/rasa:base-mitie-${BASE_MITIE_IMAGE_HASH:0:50}-${MAKEFILE_MITIE_HASH:0:50} &> /dev/null && echo true || echo false) || true)\necho ""base_mitie_exists=${BASE_IMAGE_MITIE_EXISTS}"" >> $GITHUB_OUTPUT\n\n# Base poetry image\nBASE_IMAGE_POETRY_EXISTS=$((docker manifest inspect rasa/rasa:base-poetry-${{ env.POETRY_VERSION }} &> /dev/null && echo true || echo false) || true)\necho ""base_poetry_exists=${BASE_IMAGE_POETRY_EXISTS}"" >> $GITHUB_OUTPUT\n\n# Base builder image\nBASE_IMAGE_BUILDER_HASH=${{ hashFiles(\'docker/Dockerfile.base-builder\') }}-poetry-${{ env.POETRY_VERSION }}\necho ""base_builder_image_hash=${BASE_IMAGE_BUILDER_HASH}"" >> $GITHUB_OUTPUT\n\nBASE_IMAGE_BUILDER_EXISTS=$((docker manifest inspect rasa/rasa:base-builder-${BASE_IMAGE_BUILDER_HASH} &> /dev/null && echo true || echo false) || true)\necho ""base_builder_exists=${BASE_IMAGE_BUILDER_EXISTS}"" >> $GITHUB_OUTPUT\n', 'export IMAGE_TAG=${{ steps.check_image.outputs.base_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base\n', 'export IMAGE_TAG=${{ steps.check_image.outputs.base_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base --push\n', 'export IMAGE_TAG=${{ steps.check_image.outputs.base_mitie_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base-mitie\n', 'export IMAGE_TAG=${{ steps.check_image.outputs.base_mitie_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base-mitie --push\n', 'export IMAGE_TAG=${{ env.POETRY_VERSION }}\nexport BASE_IMAGE_HASH=${{ steps.check_image.outputs.base_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base-poetry\n', 'export IMAGE_TAG=${{ env.POETRY_VERSION }}\nexport BASE_IMAGE_HASH=${{ steps.check_image.outputs.base_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base-poetry --push\n', 'export IMAGE_TAG=${{ steps.check_image.outputs.base_builder_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base-builder\n', 'export IMAGE_TAG=${{ steps.check_image.outputs.base_builder_image_hash }}\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl base-builder --push\n', 'echo ""IMAGE_TAG=${{ github.event.number }}"" >> $GITHUB_ENV\n', 'TAG_NAME=${GITHUB_REF#refs/tags/}\necho ""IMAGE_TAG=${TAG_NAME}"" >> $GITHUB_ENV\n', 'BRANCH_NAME=${GITHUB_REF#refs/heads/}\nSAFE_BRANCH_NAME=""$(echo ${GITHUB_REF#refs/heads/} | sed \'s/[\\\\*+.$\\#\\-\\/]/-/g\')""\necho ""IMAGE_TAG=${SAFE_BRANCH_NAME}"" >> $GITHUB_ENV\n', 'echo ""image_tag=${{ env.IMAGE_TAG }}"" >> $GITHUB_OUTPUT\n', 'sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'echo ${{ steps.buildx.outputs.platforms }}', 'echo ${{ secrets.DOCKERHUB_PASSWORD }} | docker login -u ${{ env.DOCKERHUB_USERNAME }} --password-stdin || true', './scripts/write_keys_file.sh\n', 'docker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl ${{ matrix.image }}\n', 'docker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl ${{ matrix.image }} --push\n', 'IS_NEWEST_VERSION=${{ needs.build_docker_base_images_and_set_env.outputs.is_newest_version }}\n\ndocker buildx bake --set *.platform=${{ matrix.arch }} -f docker/docker-bake.hcl ${{ matrix.image }} --push\n\n# Tag the image as latest\nif [[ ""${IS_NEWEST_VERSION}"" == ""true"" ]]; then\n  if [[ ""${{ matrix.image }}"" == ""default"" ]]; then\n    RELEASE_TAG=""${IMAGE_TAG}""\n  else\n    RELEASE_TAG=""${IMAGE_TAG}-${{ matrix.image }}""\n  fi\n\n  LATEST_TAG=$(echo $RELEASE_TAG | sed \'s/\'$IMAGE_TAG\'/latest/g\')\n\n  docker tag rasa/rasa:${RELEASE_TAG} rasa/rasa:${LATEST_TAG}\n  docker push rasa/rasa:${LATEST_TAG}\nfi\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', './scripts/write_keys_file.sh\n', 'poetry build', 'curl -sL https://sentry.io/get-cli/ | bash\nGITHUB_TAG=${GITHUB_TAG/refs\\/tags\\//}\nsentry-cli releases new -p rasa-open-source ""rasa-$GITHUB_TAG""\nsentry-cli releases set-commits --auto ""rasa-$GITHUB_TAG""\nsentry-cli releases finalize ""rasa-$GITHUB_TAG""\n', 'GITHUB_TAG=${GITHUB_TAG/refs\\/tags\\//}\npip install -U github3.py pep440-version-utils\npython3 scripts/publish_gh_release_notes.py\n./scripts/ping_slack_about_package_release.sh\n', 'echo ""backend=true"" >> $GITHUB_OUTPUT\necho ""docker=true"" >> $GITHUB_OUTPUT\necho ""docs=true"" >> $GITHUB_OUTPUT\n', 'python3 -m pip install pep440_version_utils\n', 'if [[ ""${IS_MAIN_BRANCH}"" == ""true"" ]]; then\n  echo ""Main branch: setting build_docs to true.""\n  echo ""build_docs=true"" >> $GITHUB_OUTPUT\nelse\n  # Get latest tagged Rasa version\n  git fetch --depth=1 origin ""+refs/tags/*:refs/tags/*""\n  # Fetch branch history\n  TAG_NAME=${GITHUB_REF#refs/tags/}\n  git fetch --prune --unshallow\n  python scripts/evaluate_release_tag.py $TAG_NAME\n  exit_status=$?\n\n  if [[ ${exit_status} -eq 0 ]]; then\n    echo ""Setting build_docs to true.""\n    echo ""build_docs=true"" >> $GITHUB_OUTPUT\n  else\n    echo ""Setting build_docs to false.""\n    echo ""build_docs=false"" >> $GITHUB_OUTPUT\n  fi\nfi\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'make install install-docs', 'make prepare-docs', 'eval ""$(ssh-agent -s)""; touch $TMP_SSH_KEY_PATH; chmod 0600 $TMP_SSH_KEY_PATH\necho ""$GH_DOCS_WRITE_KEY"" > $TMP_SSH_KEY_PATH\nssh-add $TMP_SSH_KEY_PATH\n\ngit config --global user.email ""builds@github-ci.com""\ngit config --global user.name ""GitHub CI""\ngit remote set-url --push origin ""git@github.com:${{github.repository}}""\n\n./scripts/push_docs_to_branch.sh\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'make install install-docs', 'make prepare-docs', 'make preview-docs\nDEPLOY_URL=""https://$PULL_REQUEST_NUMBER--rasahq-docs-rasa-v2.netlify.app${DOCS_SITE_BASE_URL}""\necho ""preview_url=$DEPLOY_URL"" >> $GITHUB_OUTPUT\n', 'make install-docs', 'make publish-docs', 'python3 -m pip install pluggy\npython3 -m pip install ruamel.yaml\n', 'DATE=$(date +\'%Y%m%d\')\n\n# Find latest rasa-oss version\necho ""Trying to find the latest rasa-oss version...""\nLATEST_RASA_MINOR=$(python -c ""import sys; import os; sys.path.append(\'${{ github.workspace }}/rasa\'); from rasa.version import __version__; print(__version__)"")\necho ""Current RASA version: ${LATEST_RASA_MINOR}""\n\nLATEST_NIGHTLY_VERSION=$(echo ${LATEST_RASA_MINOR})\n\necho ""Composing nightly build tag name...""\nGH_TAG=${LATEST_NIGHTLY_VERSION}.dev${DATE}\necho ""New nightly release version: ${GH_TAG}""\necho ""tag_name=${GH_TAG}"" >> $GITHUB_OUTPUT\n', 'git config user.name github-actions\ngit config user.email github-actions@github.com\n\ngit tag -a ${{ steps.set_tagname.outputs.tag_name }} -m ""This is an internal development build""\ngit push origin ${{ steps.set_tagname.outputs.tag_name }} --tags\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', './scripts/write_keys_file.sh\n', 'poetry run pip install toml pep440_version_utils\npoetry run python ./scripts/prepare_nightly_release.py --next_version ""${{ needs.run_script_and_tag_nightly_release.outputs.tag_name }}""\n', 'poetry build\n', ""pip install keyring\npip install keyrings.google-artifactregistry-auth\npip install twine\ngcloud artifacts print-settings python --project=rasa-releases --repository=rasa --location=europe-west3 > ~/.pypirc\ntwine upload --verbose --repository-url https://europe-west3-python.pkg.dev/rasa-releases/rasa/ ${{ format('{0}/dist/*', github.workspace) }} \n"", 'sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', './scripts/write_keys_file.sh\n', 'docker build . -t rasa/rasa:base-localdev -f docker/Dockerfile.base\ndocker build . -t rasa/rasa:base-builder-localdev -f docker/Dockerfile.base-builder --build-arg IMAGE_BASE_NAME=rasa/rasa --build-arg POETRY_VERSION=${{ env.POETRY_VERSION }}\ndocker build . -t rasa/rasa:base-poetry -f docker/Dockerfile.base-poetry --build-arg IMAGE_BASE_NAME=rasa/rasa --build-arg BASE_IMAGE_HASH=localdev\ndocker build . -t rasa/rasa:${IMAGE_TAG} -f Dockerfile --build-arg IMAGE_BASE_NAME=rasa/rasa --build-arg BASE_IMAGE_HASH=localdev --build-arg BASE_BUILDER_IMAGE_HASH=localdev\ndocker tag rasa/rasa:${IMAGE_TAG} ${{env.DEV_REGISTRY}}/rasa:${IMAGE_TAG}\n', '# Set up docker to authenticate via gcloud command-line tool.\ngcloud auth configure-docker europe-west3-docker.pkg.dev\n', 'docker push ${{env.DEV_REGISTRY}}/rasa:${IMAGE_TAG}\n', 'python -m pip install rasa\n', 'python -m pip install rasa[full]\n', 'python -m rasa --version\n# Must create a folder first\nmkdir rasa-demo\n# Next we try to init. \npython -m rasa init --init-dir rasa-demo --no-prompt\n', 'echo ""==========================================================""\necho ""| This build has failed because Trivy detected a secret. |""\necho ""==========================================================""\necho ""1. Check the step \'Run Trivy vulnerability scanner\' for output to help you find the secret.""\necho ""2. If the finding is a false positive, add it as an entry to trivy-secret.yaml in the root of the repo to suppress the finding.""\necho ""3. If the finding is valid, the security team can help advise your next steps.""\nexit 1\n', 'echo ""POETRY_VERSION=$(scripts/poetry-version.sh)"" >> $GITHUB_ENV\n', 'rm -r .venv', 'python -m venv create .venv', 'poetry config virtualenvs.in-project true', 'make install', 'make lint-security', 'semgrep ci']"
"['set -eux\npython ci/scripts/github/github_cc_reviewers.py || echo step failed\n', 'conda build --output-folder=conda/pkg  conda/recipe && conda install tvm -c ./conda/pkg', 'IOS_VERSION=""14.0""\nCMAKE_FLAGS=""-DCMAKE_BUILD_TYPE=Release \\\n             -DCMAKE_SYSTEM_NAME=iOS \\\n             -DCMAKE_SYSTEM_VERSION=${IOS_VERSION} \\\n             -DCMAKE_OSX_SYSROOT=iphonesimulator \\\n             -DCMAKE_OSX_ARCHITECTURES=x86_64 \\\n             -DCMAKE_OSX_DEPLOYMENT_TARGET=14.0 \\\n             -DCMAKE_BUILD_WITH_INSTALL_NAME_DIR=ON \\\n             -DUSE_IOS_RPC=ON""\n\nmkdir build-ios-simulator\ncd build-ios-simulator\ncmake .. ${CMAKE_FLAGS}\ncmake --build . --target ios_rpc\n', 'python -m pytest -v tests/python/all-platform-minimal-test', 'python -m pip install tornado psutil cloudpickle && export PYTHONPATH=tests/python/contrib:${PYTHONPATH} && export BUNDLE_ID=org.apache.tvmrpc && export BUNDLE_PATH=build-ios-simulator/apps/ios_rpc/ios_rpc/src/ios_rpc-build/Release-iphonesimulator/tvmrpc.app && python -m pytest -v tests/python/contrib/test_rpc_server_device.py', 'conda build --output-folder=conda/pkg conda/recipe && conda install tvm -c ./conda/pkg', 'python -m pytest -v tests/python/all-platform-minimal-test', 'tests/scripts/task_config_build_static.sh build\ncd build\ncmake ..\ncmake --build . --config Release --target runtime\n', 'mkdir build\ncd build\n../tests/scripts/task_config_build_jvm.sh .\ncmake ..\nmake\n', 'make jvmpkg\n', 'set -eux\nexport PATH=""${ANDROID_NDK_LATEST_HOME}:$PATH""\ngradle clean build\n', 'set -eux\nexport PATH=""${ANDROID_NDK_LATEST_HOME}:$PATH""\ngradle clean build\n', 'export TVM_HOME=~/work/tvm/tvm\nexport PYTHONPATH=$TVM_HOME/python:${PYTHONPATH}\nset -eux\nmkdir -p app/src/main/assets/models/\nexport TVM_NDK_CC=${ANDROID_NDK_LATEST_HOME}/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android30-clang++\npython3 ${TVM_HOME}/python/gen_requirements.py\npip3 install -r ${TVM_HOME}/python/requirements/core.txt\ncd models\npip3 install -r requirements.txt\npython3 prepare_model.py\ncd ..\nexport PATH=""${ANDROID_NDK_LATEST_HOME}:$PATH""\ngradle clean build\n', 'set -eux\npython ci/scripts/jenkins/open_docker_update_pr.py\n', 'set -eux\npython ci/scripts/github/ping_reviewers.py --wait-time-minutes 10080 || echo failed\n', 'set -eux\npython ci/scripts/github/github_tag_teams.py || echo failed\n', 'set -eux\npython ci/scripts/github/github_tvmbot.py --pr ""$PR_NUMBER"" --run-url ""$RUN_URL"" --trigger-comment-json ""$ISSUE_COMMENT""\n', 'set -eux\npython ci/scripts/github/update_branch.py || echo step failed\n', 'set -eux\ngit checkout -B nightly\ngit log -5\ngit push origin --force nightly\n', 'set -eux\ncurl -L -o downloaded_file ""$URL""\necho ""$SHA256 downloaded_file"" | sha256sum --check\naws s3 cp downloaded_file ""s3://tvm-ci-resources/$UPLOAD_PATH""\necho ""The item is available at https://tvm-ci-resources.s3.us-west-2.amazonaws.com/$UPLOAD_PATH""\necho ""Add this line to tests/scripts/request_hook/request_hook.py""\necho ""    \\""$URL\\"": f\\""{BASE}/$UPLOAD_PATH\\"",""\n']"
"['make init', 'make pr', 'make init\nbin/public_interface.py extract > ""${{ runner.temp }}""/interfaces.new.json\n', '# Keep a copy of bin/public_interface.py\n# So we are using the same bin/public_interface.py to process old/new codebase.\ncp bin/public_interface.py ""${{ runner.temp }}""/public_interface.py\n', 'make init\n# Recover bin/public_interface.py\ncp ""${{ runner.temp }}""/public_interface.py bin/public_interface.py\nbin/public_interface.py extract > ""${{ runner.temp }}""/interfaces.original.json\n', 'bin/public_interface.py check ""${{ runner.temp }}""/interfaces.original.json ""${{ runner.temp }}""/interfaces.new.json\n', 'make init\nmake schema-all\n# Sets condition steps.schema.outputs.changed to true if anything changed\ngit diff --exit-code || echo ""changed=true"" >> $GITHUB_OUTPUT\n', 'git config user.name github-actions\ngit config user.email github-actions@github.com\ngit checkout -b tmp/schema/$GITHUB_RUN_ID/$GITHUB_RUN_ATTEMPT\ngit add -u\ngit commit -m ""chore(schema): update""\ngit push --set-upstream origin tmp/schema/$GITHUB_RUN_ID/$GITHUB_RUN_ATTEMPT\n', ""printf '> **Note**\\n> If checks do not start, close then reopen this pull request.\\n\\nCreated by the [`schema.yml`](https://github.com/aws/serverless-application-model/blob/develop/.github/workflows/schema.yml) workflow.\\n' | gh pr create --fill --base develop --body-file -""]"
""
"['python -m pip install --upgrade pip\npip install -r requirements_dev.txt\npip install tabulate\n', 'python -m pip install -e .\n', 'yes | python examples/benchmark.py\n', 'python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'python -m pip install -e .\n', 'cd doc\nmake clean\nmake html\n', 'sed -i ""s/numpy.*/oldest-supported-numpy/g"" requirements.txt\ncat requirements*.txt\n', 'set -x\npython -m pip install --upgrade pip\npip install -r requirements_dev.txt\npip freeze\n\npython setup.py sdist -v\n', 'set -x\n\npython -m pip install --upgrade pip\npip install scikit-surprise-1.1.3.tar.gz -v\n', 'pip freeze\n', 'pip install pytest pandas\npytest -v\n', 'python -m pip install --upgrade pip\npython -m pip install ""black==22.6.0"" ""flake8==5.0.4"" ""usort==1.0.4""\npip install -r requirements_dev.txt\n', ""python -m pip install -e .  # Not sure it's needed but whatevs\n"", 'chmod +x lint.sh\n./lint.sh\n']"
""
"['pip install -e .\nspacy download en_core_web_sm\npip install pre-commit pytest\n', 'pytest\n']"
"['python -m pip install -U pip wheel setuptools', 'python setup.py bdist_wheel', 'python -m pip install -U pip wheel setuptools', 'python setup.py sdist', 'python -m pip install dist/fs-*.tar.gz', 'rm -rvd fs', 'python -m pip install -r tests/requirements.txt', 'python -m unittest discover -vv', 'python -m pip install dist/fs-*.whl', 'rm -rvd fs', 'python -m pip install -r tests/requirements.txt', 'python -m unittest discover -vv', 'python -m pip install -U pip wheel setuptools', 'python -m pip install tox tox-gh-actions', 'python -m tox', 'python -m pip install -U coverage', 'python -m coverage combine', 'python -m coverage report', 'python -m coverage xml', 'python -m pip install -U pip wheel setuptools', 'python -m pip install tox tox-gh-actions', 'python -m tox -e ${{ matrix.linter }}']"
"['python -m pip install --user --upgrade --progress-bar off pip\npython -m pip install --user --upgrade --progress-bar off -r requirements.txt\npython -m pip install --user --upgrade --progress-bar off -r docs/requirements.txt\npython -m pip install --user --upgrade --progress-bar off ipython ""https://api.github.com/repos/sphinx-gallery/sphinx-gallery/zipball/master"" memory_profiler\npython -m pip install --user -e .\n', 'which python\npython -c ""import ot""\n', 'pip install -e .\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest pytest-cov\n', 'python -m pytest --durations=20 -v test/ ot/ --doctest-modules --color=yes --cov-report=xml\n', 'python -m pip install --upgrade pip\npip install flake8 \n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 examples/ ot/ test/ --count --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install pytest\n', 'pip install -e .\n', 'python -m pytest --durations=20 -v test/ ot/ --ignore ot/gpu/ --color=yes\n', 'pip install -e .\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest ""pytest-cov<2.6""\n', 'python -m pytest --durations=20  -v test/ ot/ --doctest-modules --ignore ot/gpu/ --cov=ot --color=yes\n', 'function Invoke-VSDevEnvironment {\n$vswhere = ""${env:ProgramFiles(x86)}\\Microsoft Visual Studio\\Installer\\vswhere.exe""\n    $installationPath = & $vswhere -prerelease -legacy -latest -property installationPath\n    $Command = Join-Path $installationPath ""Common7\\Tools\\vsdevcmd.bat""\n  & ""${env:COMSPEC}"" /s /c ""`""$Command`"" -no_logo && set"" | Foreach-Object {\n        if ($_ -match \'^([^=]+)=(.*)\') {\n            [System.Environment]::SetEnvironmentVariable($matches[1], $matches[2])\n        }\n    }\n}\nInvoke-VSDevEnvironment\nGet-Command rc.exe | Format-Table -AutoSize\n', 'python -m pip install --upgrade pip setuptools\npython -m pip install cython\n', 'python -m pip install -e .\n', 'python -m pip install -r .github/requirements_test_windows.txt\npython -m pip install torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\npython -m pip install pytest ""pytest-cov<2.6""\n', 'python -m pytest --durations=20  -v test/ ot/ --doctest-modules --ignore ot/gpu/ --cov=ot --color=yes\n', 'python3.10 -m pip install  --ignore-installed -e .\n', 'python3.10 -m pytest --durations=20 -v test/ ot/ --doctest-modules --color=yes --ignore=test/test_dr.py --ignore=ot.dr --ignore=ot.plot\n', 'python -m pip install --upgrade pip\n', 'python -m pip install cibuildwheel==2.2.2\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'python -m pip install --upgrade pip\n', 'python -m pip install cibuildwheel==2.2.2\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'python -m pip install --upgrade pip\n', 'python -m pip install cibuildwheel==2.2.2\n', 'python -m cibuildwheel --output-dir wheelhouse\n']"
"['sudo apt update\nsudo apt install -y \\\n  git python3-dev libsmpeg0 libttspico-utils flac \\\n  libffi-dev libssl-dev portaudio19-dev build-essential \\\n  libatlas3-base mplayer wget vim sudo locales alsa-base alsa-utils \\\n  pulseaudio-utils libasound2-plugins python3-pyaudio libasound-dev \\\n  libportaudio2 libportaudiocpp0 ffmpeg cargo\n', 'pip3 install -r install/files/python_requirements.txt\n', 'python3 -m unittest discover\n']"
"['python -m pip install --upgrade pip\nif [ -f requirements-dev.txt ]; then python -m pip install -r requirements-dev.txt; fi\n', 'tox\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['docker build -t pyupio/safety-v2-beta:latest .', 'echo ""$DOCKER_PASSWORD"" | docker login -u ""$DOCKER_USERNAME"" --password-stdin && docker push pyupio/safety-v2-beta:latest', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install -r test_requirements.txt\n', 'pytest --cov=./ --cov-report=xml\n', 'python binaries.py install', 'python binaries.py test', 'python binaries.py dist', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'TASKS=$(echo $(cat .github/workflows/gh-action-integration-matrix.json) | sed \'s/ //g\' )\necho ""matrix=$TASKS"" >> $GITHUB_OUTPUT\n', 'cp tests/action/requirements.txt-insecure requirements.txt', 'exit 1', 'cp tests/action/poetry.lock-insecure poetry.lock && cp tests/action/pyproject.toml-insecure pyproject.toml', 'exit 1', 'cp tests/action/Pipfile.lock-insecure Pipfile.lock', 'exit 1', 'python -m pip install -r tests/action/requirements.txt-insecure', 'exit 1', 'DOCKER_BUILDKIT=1 docker build -t my-insecure-image tests/action/docker-insecure', 'exit 1', 'TASKS=$(echo $(cat .github/workflows/gh-action-integration-matrix.json) | sed \'s/ //g\' )\necho ""matrix=$TASKS"" >> $GITHUB_OUTPUT\n', 'cp tests/action/requirements.txt-secure requirements.txt', 'cp tests/action/poetry.lock-secure poetry.lock && cp tests/action/pyproject.toml-secure pyproject.toml', 'cp tests/action/Pipfile.lock-secure Pipfile.lock', 'python -m pip install -r tests/action/requirements.txt-secure', 'DOCKER_BUILDKIT=1 docker build -t my-secure-image tests/action/docker-secure']"
[]
"['python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install git+https://github.com/tensorly/tensorly-sphinx-theme.git@main\npython -m pip install -r doc/requirements_doc.txt\n', 'python -m pip install -e .\n', 'cd doc\npython minify.py\nmake html\ncd ..\n', '# Add deploy key and clone through ssh\neval ""$(ssh-agent -s)""\nmkdir ~/.ssh\necho ""${{ secrets.DEPLOY_DOC_KEY }}"" > ~/.ssh/id_rsa\nchmod 600 ~/.ssh/id_rsa\nssh-keyscan -t rsa github.com\necho \'Documentation was successfully built, updating the website.\'\n# See https://github.community/t/github-actions-bot-email-address/17204/5\ngit config --global user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --global user.name ""github-actions""\ngit clone ""git@github.com:tensorly/tensorly.github.io.git"" doc_folder\necho ""-- Updating the content""\ncd doc_folder\ngit rm -r dev/*\nmkdir -p dev\necho ""Copying to folder""\ncp -r ../doc/_build/html/* dev/\necho ""Pushing to git""\ngit add dev\ngit commit -m ""Github action: auto-update.""\ngit push --force origin main\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r doc/requirements_doc.txt\npip install setuptools wheel\n', 'cd doc\npython minify.py\nmake html\ncd ..\n', 'python -m pip install -e .\n', 'python setup.py sdist bdist_wheel\n', '# Add deploy key and clone through ssh\neval ""$(ssh-agent -s)""\nmkdir ~/.ssh\necho ""${{ secrets.DEPLOY_DOC_KEY }}"" > ~/.ssh/id_rsa\nchmod 600 ~/.ssh/id_rsa\nssh-keyscan -t rsa github.com\necho \'Documentation was successfully built, updating the website.\'\n# See https://github.community/t/github-actions-bot-email-address/17204/5\ngit config --global user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --global user.name ""github-actions""\ngit clone ""git@github.com:tensorly/tensorly.github.io.git"" doc_folder\necho ""-- Updating the content""\ncd doc_folder\ncp -r dev/* stable/\necho ""Pushing to git""\ngit add stable\ngit commit -m ""Github action: new release.""\ngit push --force origin main\n', 'python -m pip install --upgrade pip\npip install black\n', 'black --check --diff --color ./tensorly\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r doc/requirements_doc.txt\necho ""Installing dependencies for BACKEND=${{matrix.BACKEND}}""\nif [[ ""${{matrix.BACKEND}}"" == ""numpy"" ]]; then\n  echo ""Installing sparse"";\n  pip install sparse;\nelif [[ ""${{matrix.BACKEND}}"" == ""pytorch"" ]]; then\n  echo ""Installing PyTorch"";\n  pip install torch torchvision;\nelif [[ ""${{matrix.BACKEND}}"" == ""tensorflow"" ]]; then\n  echo ""Installing TensorFlow"";\n  pip install tensorflow;\nelif [[ ""${{matrix.BACKEND}}"" == ""mxnet"" ]]; then\n  echo ""Installing MXNet"";\n  sudo apt install gfortran;\n  echo ""Reverting to older version of numpy 1.23.1 until MXNet fixed bool issue"" \n  python -m pip uninstall numpy -y;\n  python -m pip install numpy==1.23.1;\n  pip install mxnet;\nelif [[ ""${{matrix.BACKEND}}"" == ""jax"" ]]; then\n  echo ""Installing JAX"";\n  pip install jax jaxlib;\nfi\n', 'python -m pip install -e .\n', 'TENSORLY_BACKEND=${{matrix.BACKEND}} pytest -vv --cov tensorly --cov-report xml --durations=10 tensorly\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install .\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npip install pytest-cov\n\npytest --cov=pyntcloud --cov-report=xml --cov-append tests/unit \npytest --cov=pyntcloud --cov-report=xml --cov-append tests/integration \n', 'bash <(curl -s https://codecov.io/bash) -t $TOKEN -B $REF', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python setup.py sdist\npython setup.py bdist_wheel --universal\n']"
""
"['python -m pip install --upgrade pip\npip install requests\n', 'python ./create-pkgbuild.py > ./PKGBUILD\n', 'sudo apt-get install -y libdbus-1-dev libgit2-dev libvirt-dev taskwarrior', ""python -m pip install --upgrade pip\npip install -U coverage pytest pytest-mock freezegun\npip install 'pygit2<1' 'libvirt-python<6.3' 'feedparser<6' || true\npip install $(cat requirements/modules/*.txt | cut -d ' ' -f 1 | sort -u)\n"", 'curl -L https://codeclimate.com/downloads/test-reporter/test-reporter-latest-linux-amd64 > ./cc-test-reporter\nchmod +x ./cc-test-reporter\n./cc-test-reporter before-build\n', 'coverage run --source=. -m pytest tests -v\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['poetry run pre-commit run --all-files', 'wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\\n&& sudo dpkg -i ${{ runner.temp }}/hugo.deb\n', 'sudo snap install dart-sass-embedded', 'poetry run jupyter nbconvert --execute --to notebook --inplace docs/content/*.ipynb', 'poetry run jupyter nbconvert --to markdown docs/content/*.ipynb', ""(for f in docs/content/*.md; do sed -e '/<script/,/<\\/script>/{/^$/d;}' ${f} > ${f}.tmp; mv ${f}.tmp ${f}; done)"", '[[ -f package-lock.json || -f npm-shrinkwrap.json ]] && npm ci || true', 'cd docs && hugo \\\n  --minify \\\n  --baseURL ""${{ steps.pages.outputs.base_url }}/""\n', 'poetry run pytest']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python run_tests']"
""
"['pip install -U https://github.com/platformio/platformio/archive/develop.zip\npio pkg install --global --platform symlink://.\n', 'pio run -d ${{ matrix.example }}\n', 'python -m pip install --upgrade pip\npip install -U https://github.com/platformio/platformio/archive/develop.zip\npio pkg install --global --platform symlink://.\n', 'pio test -d ${{ matrix.example }} --without-uploading --without-testing\n']"
"['docker run -d -p 9092:9092 -e ADV_HOST=127.0.0.1 lensesio/fast-data-dev', 'git fetch --prune --unshallow', 'python -m pip install --upgrade pip setuptools wheel', 'pip install .[dev]', 'black --check .', 'flake8 .', 'isort --check --profile=black .', 'mypy kq', 'py.test --cov=kq --cov-report=xml', 'python -m sphinx -b doctest docs docs/_build', 'python -m sphinx -b html -W docs docs/_build', 'git fetch --prune --unshallow', 'python -m pip install --upgrade pip\npip install setuptools wheel twine setuptools-scm[toml]\n', 'python setup.py sdist bdist_wheel', 'twine upload --repository testpypi dist/*', 'twine upload --repository pypi dist/*']"
"['python3.11 -m pip install --upgrade pip\npython3.11 -m pip install tox\n', 'sudo service postgresql start', 'sudo -u postgres psql -c ""CREATE USER ${USER} WITH SUPERUSER ENCRYPTED PASSWORD \'dummy\'""\necho ""*:*:*:${USER}:dummy"" > ~/.pgpass\nchmod 0600 ~/.pgpass\n', 'tox']"
"['pip install black', 'black --check .', 'echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin\n', 'docker build -t ${IMAGE_NAME}:base -f ./Dockerfile ..\n', 'docker build -t ${IMAGE_NAME}:default-worlds -f ./Dockerfile_default_worlds ..', 'docker build -t ${IMAGE_NAME}:dexterity -f ./Dockerfile_dexterity ..', 'docker push ${IMAGE_NAME}:base\ndocker push ${IMAGE_NAME}:default-worlds\ndocker push ${IMAGE_NAME}:dexterity\n']"
"['pip install tox', 'python -VV', 'tox']"
"['python -m pip install --upgrade pip\npython -m pip install django==${{ matrix.django-version }}\npython -m pip install -e .\n', 'cd ./easyaudit/tests\npython -m manage test\n']"
""
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pytest\n']"
[]
"['(Get-ItemProperty ""HKLM:System\\CurrentControlSet\\Control\\FileSystem"").LongPathsEnabled\n$os_version = (Get-CimInstance Win32_OperatingSystem).version\nEcho ""OS_VERSION=$os_version"" >> $env:GITHUB_ENV\n', 'cd python\npython -m pip install -U --editable "".[dev,parallel]""\npython -m pytest prophet/tests/\n', ""install.packages('remotes')\n"", 'saveRDS(remotes::dev_package_deps(pkgdir = ""R/"", dependencies = TRUE), "".github/depends.Rds"", version = 2)\nwriteLines(sprintf(""R-%i.%i"", getRversion()$major, getRversion()$minor), "".github/R-version"")\n', 'while read -r cmd\ndo\n  eval sudo $cmd\ndone < <(Rscript -e \'writeLines(remotes::system_requirements(os = ""ubuntu"", os_release = ""20.04"", path = ""R/""))\')\n', 'remotes::install_deps(pkgdir = ""R/"", dependencies = NA)\nremotes::install_cran(c(""rcmdcheck"", ""knitr"", ""testthat"", ""readr"", ""rmarkdown""))\ninstall.packages(c(""cmdstanr"", ""posterior""), repos = c(""https://mc-stan.org/r-packages/"", getOption(""repos"")))\n', '(Get-ItemProperty ""HKLM:System\\CurrentControlSet\\Control\\FileSystem"").LongPathsEnabled\n$os_version = (Get-CimInstance Win32_OperatingSystem).version\nEcho ""OS_VERSION=$os_version"" >> $env:GITHUB_ENV\n', 'pipx run build --sdist', ""find . -name 'artifact-*' -exec unzip '{}' \\;\nmkdir -p dist/\nfind . -name '*.tar.gz' -exec mv '{}' dist/ \\;\nfind . -name '*.whl' -exec mv '{}' dist/ \\;\n""]"
""
"['pip install pygithub\ngit config --global user.name ""CuPy Automatic Backport""\ngit config --global user.email ""33715081+chainer-ci@users.noreply.github.com""\ngit clone https://github.com/cupy/backport.git\n', 'cd backport\necho -e ""machine github.com\\nlogin chainer-ci\\npassword ${{secrets.BACKPORT_TOKEN}}"" > ~/.netrc\npython backport.py --repo cupy --pr ${{github.event.number}} --https --bot\n', 'pip install pygithub\n', './.github/workflows/flexci_dispatcher.py \\\n    --event ""${GITHUB_EVENT_NAME}"" \\\n    --webhook ""${GITHUB_EVENT_PATH}"" \\\n    --projects ./.pfnci/config.tags.json \\\n    --external-tag jenkins\n', 'pip install pre-commit\n', 'pre-commit run -a --show-diff-on-failure\n', 'sudo apt-get update -y\nsudo apt-get install -y ccache\necho ""PATH=/usr/lib/ccache:${PATH}"" >> ""${GITHUB_ENV}""\nccache --show-stats -v\n', 'pip install -U pip wheel\npip install cython\nREADTHEDOCS=True pip install -v -e .\nccache --max-size 0.5Gi --cleanup --show-stats\n', '# Test to detect invalid escape sequences in docstrings (#1619)\npython -Werror::DeprecationWarning -m compileall -f -q cupy cupyx examples tests docs\npushd docs\npip install -r requirements.txt\nSPHINXOPTS=""-W --keep-going -j 4"" make html\npopd\n', 'docker run --rm -v ""${PWD}:/src"" -w /src ""rocm/dev-ubuntu-20.04:5.0-complete"" bash .github/workflows/pretest-rocm-test.sh\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python manage.py makemigrations\npython manage.py migrate\npython manage.py test\n', 'sudo swapoff -a\nsudo sysctl -w vm.swappiness=1\nsudo sysctl -w fs.file-max=262144\nsudo sysctl -w vm.max_map_count=262144\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python manage.py makemigrations\npython manage.py migrate\ncoverage run manage.py test\ncoverage xml\n', 'echo ""DOCKER_TAG=test"" >> $GITHUB_ENV\n', 'echo ""DOCKER_TAG=latest"" >> $GITHUB_ENV\n']"
"['pip install . && pip install pyinstaller && make freeze', 'pip install . && pip install pyinstaller && ./make.cmd freeze', 'set -Eeuxo pipefail\nfor BIN_NAME in $(ls)\ndo\n  curl -L \\\n    --no-progress-meter \\\n    -X POST \\\n    -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}""\\\n    -H ""Content-Type: application/octet-stream"" \\\n    ""https://uploads.github.com/repos/${{ github.repository }}/releases/${{ github.event.release.id }}/assets?name=${BIN_NAME}"" \\\n    --data-binary ""@${BIN_NAME}""\ndone\n', 'echo ""ERA_HASH=$( curl -u ""u:${{ github.token }}"" https://api.github.com/repos/matter-labs/era-compiler-tester/git/ref/heads/main | jq .object.sha | tr -d \'""\' )"" >> $GITHUB_ENV\necho ""ERA_VYPER_HASH=$( curl -u ""u:${{ github.token }}"" https://api.github.com/repos/matter-labs/era-compiler-vyper/git/ref/heads/main | jq .object.sha | tr -d \'""\' )"" >> $GITHUB_ENV\n', ""git clone --depth 1 https://github.com/matter-labs/era-compiler-tester.git\ncd era-compiler-tester\nsed -i 's/ssh:\\/\\/git@/https:\\/\\//g' .gitmodules\ngit submodule init\ngit submodule update\nsudo apt install cmake ninja-build clang-13 lld-13 parallel pkg-config lld\ncargo install compiler-llvm-builder\nzkevm-llvm clone && zkevm-llvm build\ncargo build --release\n"", 'set -Eeuxo pipefail\npip install .\necho ""VYPER_VERSION=$(vyper --version | cut -f1 -d\'+\')"" >> $GITHUB_ENV\n', 'mkdir era-compiler-tester/vyper-bin\ncp $(which vyper) era-compiler-tester/vyper-bin/vyper-${{ env.VYPER_VERSION }}\n', 'cd era-compiler-tester\ncargo run --release --bin compiler-tester -- -v --path=tests/vyper/ --mode=""M0B0 ${{ env.VYPER_VERSION }}""\n', 'cd era-compiler-tester\ncargo run --release --bin compiler-tester -- -v --path=tests/vyper/ --mode=""M*B* ${{ env.VYPER_VERSION }}""\n', 'pip install .\necho ""VYPER_VERSION=$(PYTHONPATH=. python vyper/cli/vyper_compile.py --version)"" >> ""$GITHUB_ENV""\n', 'echo ""VERSION_SUFFIX=-dev"" >> ""$GITHUB_ENV""', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel', 'twine upload dist/*', 'pip install .[lint]', 'black --check -C --force-exclude=vyper/version.py ./vyper ./tests ./setup.py', 'flake8 ./vyper ./tests ./setup.py', 'isort --check-only --diff ./vyper ./tests ./setup.py', 'pip install tox', 'TOXENV=docs tox -r', 'pip install tox', 'TOXENV=mypy tox -r', 'pip install tox', 'TOXENV=py${{ matrix.python-version[1] }}-${{ matrix.flag }} tox -r -- --reruns 10 --reruns-delay 1 -r aR tests/', 'exit 1', 'pip install tox', 'curl --location ""https://raw.githubusercontent.com/vyperlang/vyper-test-durations/5982755ee8459f771f2e8622427c36494646e1dd/test_durations"" -o .test_durations', 'TOXENV=fuzzing tox -r -- --splits 60 --group ${{ matrix.group }} --splitting-algorithm least_duration --reruns 10 --reruns-delay 1 -r aR tests/', 'exit 1', 'pip install tox', 'TOXENV=memory tox -r']"
""
"['soil/worker.sh JOB-raw-vm\n', 'soil/github-actions.sh publish-and-exit raw-vm T\n', 'soil/github-actions.sh run-job dummy\n', 'soil/github-actions.sh publish-and-exit dummy T\n', 'soil/github-actions.sh run-job dev-minimal\n', 'soil/github-actions.sh publish-and-exit dev-minimal T\n', 'soil/github-actions.sh run-job interactive\n', 'soil/github-actions.sh publish-and-exit interactive T\n', 'soil/github-actions.sh run-job pea\n', 'soil/github-actions.sh publish-and-exit pea T\n', 'soil/github-actions.sh run-job other-tests\n', 'soil/github-actions.sh publish-and-exit other-tests T\n', 'soil/github-actions.sh run-job ovm-tarball\n', 'soil/github-actions.sh publish-and-exit ovm-tarball T\n', 'soil/github-actions.sh run-job app-tests\n', 'soil/github-actions.sh publish-and-exit app-tests T\n', 'soil/github-actions.sh run-job cpp-coverage\n', 'soil/github-actions.sh publish-and-exit cpp-coverage T\n', 'soil/github-actions.sh run-job benchmarks\n', 'soil/github-actions.sh publish-and-exit benchmarks T\n', 'soil/github-actions.sh run-job benchmarks2\n', 'soil/github-actions.sh publish-and-exit benchmarks2 T\n', 'soil/github-actions.sh run-job cpp-small\n', 'soil/github-actions.sh publish-and-exit cpp-small T\n', 'soil/github-actions.sh run-job cpp-spec\n', 'soil/github-actions.sh publish-and-exit cpp-spec T\n', 'soil/github-actions.sh run-job wild\n', 'soil/github-actions.sh publish-and-exit wild T\n', 'soil/worker.sh JOB-maybe-merge\n', '# NOTE: does not publish to status API\nsoil/github-actions.sh publish-and-exit maybe-merge\n']"
""
['python -m pip install --upgrade pip\npip install setuptools wheel twine\npython setup.py sdist\n']
""
""
['# Install dependencies...\n# python -m venv ./venv\n# venv\\scripts\\activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\npython main.py -git_test\n#venv\\scripts\\deactivate\n']
"[""git lfs ls-files -l | cut -d' ' -f1 | sort > .lfs-assets-id"", 'git lfs pull', 'pip install "".[dev]""\nsudo apt-get update\nsudo apt-get install libssl-dev libcurl4-openssl-dev\n', 'python -m pmlb.update_dataset_files\n', 'BRANCH_REF=${{github.head_ref || github.ref}}\necho ""BRANCH=${BRANCH_REF##*/}"" >> $GITHUB_ENV\ngit config --local user.name \'github-actions[bot]\'\ngit config --local user.email \'github-actions[bot]@users.noreply.github.com\'\n', 'if git diff --quiet ; then\n  echo ""No files changed, skipping commit and deployment.""\nelse\n  echo ""Changed files detected.""\n  git add --all\n  WORKFLOW_URL=""https://github.com/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID""\n  git commit \\\n    --message ""update dataset files"" \\\n    --message ""Created by ${WORKFLOW_URL}\\nfrom ${GITHUB_SHA::7} on $(date --iso --utc)""\n  git checkout $BRANCH\n  git push origin $BRANCH\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', ""git lfs ls-files -l | cut -d' ' -f1 | sort > .lfs-assets-id"", 'git lfs pull', 'pip install "".[dev]""\nsudo apt-get update\nsudo apt-get install libssl-dev libcurl4-openssl-dev\n', 'nosetests -s -v\n', 'python -m pmlb.profiling\n', 'Rscript -e \'install.packages(c(""rmarkdown"", ""plotly"", ""dplyr"", ""htmlwidgets"", ""htmltools"", ""jsonlite"", ""readr"", ""DT"", ""remotes"", ""pmlbr""))\'', 'install.packages(""reticulate"")\nlibrary(reticulate)\nreticulate::install_miniconda()\nreticulate::py_config()\nreticulate::py_discover_config()\n', 'Rscript -e \'print(reticulate::py_discover_config())\'\nRscript -e \'rmarkdown::render_site(input = ""docs_sources"")\'\n', 'ls -lhR docs/']"
"['pip install -r test_requirements.txt', 'pip install tox tox-gh-actions', 'tox']"
""
"[""sed -i.bak 's/-e //g' requirements/base.txt\nsed -i.bak 's/-e //g' requirements/gui.txt\n./install.sh --develop --with-qt\n./jmvenv/bin/pip install -r requirements/testing.txt\n"", './jmvenv/bin/flake8 -v jmclient jmbase jmbitcoin jmdaemon scripts', './test/testrunner/install_bitcoind.sh', 'source ./jmvenv/bin/activate && ./test/run_tests.sh']"
"['sudo apt-get update\nsudo apt-get install -y libzbar0 python-opencv\n', 'brew update\nbrew install zbar\n', 'python -m pip install --upgrade pip==20.3.4\npip install -r requirements-test.txt\n', 'cd pyzbar\ncurl --location --remote-name-all ""https://github.com/NaturalHistoryMuseum/barcode-reader-dlls/releases/download/0.1/{libzbar-32.dll,libiconv-2.dll}""\ncd ..\n', 'cd pyzbar\ncurl --location --remote-name-all ""https://github.com/NaturalHistoryMuseum/barcode-reader-dlls/releases/download/0.1/{libzbar-64.dll,libiconv.dll}""\ncd ..\n', 'pytest --verbose --cov=pyzbar pyzbar', 'pip install coveralls>=3.2.0\ncoveralls --service=github\n']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox\n']"
"['pip install PyYAML colorama\npython tests/test_logsource.py\n', 'pip install sigma-cli~=0.7.1\n', 'sigma check rules\n', 'pip install PyYAML attackcti colorama\npython tests/test_rules.py\n', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/evtx-sigma-checker', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/win7-x86.tgz\ntar xzf win7-x86.tgz\n', 'chmod +x evtx-sigma-checker\n./evtx-sigma-checker --log-source tests/thor.yml --evtx-path win7_x86/ --rule-path rules/windows/ > findings.json\n', './.github/workflows/matchgrep.sh findings.json .github/workflows/known-FPs.csv', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/evtx-sigma-checker', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/win10-client.tgz\ntar xzf win10-client.tgz\n', 'chmod +x evtx-sigma-checker\n./evtx-sigma-checker --log-source tests/thor.yml --evtx-path Logs_Client/ --rule-path rules/windows/ > findings.json\n', './.github/workflows/matchgrep.sh findings.json .github/workflows/known-FPs.csv', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/evtx-sigma-checker', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/win11-client.tgz\ntar xzf win11-client.tgz\n', 'chmod +x evtx-sigma-checker\n./evtx-sigma-checker --log-source tests/thor.yml --evtx-path Logs_Win11/ --rule-path rules/windows/ > findings.json\n', './.github/workflows/matchgrep.sh findings.json .github/workflows/known-FPs.csv', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/evtx-sigma-checker', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/win2022-evtx.tgz\ntar xzf win2022-evtx.tgz\n', 'chmod +x evtx-sigma-checker\n./evtx-sigma-checker --log-source tests/thor.yml --evtx-path win2022-evtx/ --rule-path rules/windows/ > findings.json\n', './.github/workflows/matchgrep.sh findings.json .github/workflows/known-FPs.csv', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/evtx-sigma-checker', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/win2022-ad.tgz\ntar xzf win2022-ad.tgz\n', 'chmod +x evtx-sigma-checker\n./evtx-sigma-checker --log-source tests/thor.yml --evtx-path Win2022-AD/ --rule-path rules/windows/ > findings.json\n', './.github/workflows/matchgrep.sh findings.json .github/workflows/known-FPs.csv', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/evtx-sigma-checker', 'wget --no-verbose https://github.com/NextronSystems/evtx-baseline/releases/download/$EVTX_BASELINE_VERSION/win2022-0-20348-azure.tgz\ntar xzf win2022-0-20348-azure.tgz\n', 'chmod +x evtx-sigma-checker\n./evtx-sigma-checker --log-source tests/thor.yml --evtx-path win2022-0-20348-azure/ --rule-path rules/windows/ > findings.json\n', './.github/workflows/matchgrep.sh findings.json .github/workflows/known-FPs.csv']"
"['python -m pip install --upgrade pip setuptools wheel\n', 'python -m pip install "".[tests]""\n', 'pip install flake8\nflake8 --show-source src/ tests/\n', 'pip install check-manifest\ncheck-manifest\n', 'coverage run --parallel -m pytest\n', 'coverage combine\ncoverage report\ncoverage xml\n', 'python -m pip install "".[docs]""\n', 'sphinx-build -W -b html -d doctrees docs/source docs/_build/html\n', 'python -m pip install --upgrade pip setuptools wheel\npython setup.py sdist bdist_wheel --universal\n']"
""
"['echo Environment file ${{matrix.ENV_FILE}}\nconda info\nconda list\n', 'pip install -e .[pc]', 'pytest']"
"['python -m pip install --upgrade pip\npip install flake8\n', 'flake8 --config .flake8 --format ""$FLAKE8_FORMAT""\n', 'pip install twine requests\n', 'python setup.py sdist\n', 'twine upload dist/* \n', 'pip install requests\n', ""import re\nimport os\nimport json\n\nchangelog_path = os.environ['CHANGELOG_PATH']\nchangelog_regex = os.environ['CHANGELOG_REGEX']\n\nwith open(changelog_path,'rt',encoding='utf-8') as f:\n  textlog = '\\n'.join(f.readlines())\n\nver,log = re.split(changelog_regex,textlog)[1:3]\ndata = json.dumps({'ver':ver,'log':log})\n\nprint('::set-env name=RELEASE_DATA::%s' % data)\n"", 'echo ""::warning:: Not implemented yet! Current release data is $RELEASE_DATA""\n', 'python -m pip install --upgrade pip\npip install -r testing/requirements.txt\n', 'python -m pytest testing/external -s\n', 'python -m pip install --upgrade pip\npip install -r testing/requirements.txt\n', 'python -m pytest testing/internal -vv\n']"
""
"['pip3 install -r docs/requirements.txt\n', 'exit ${{ steps.lc.outputs.exit_code }}', 'pip install wheel; rm -rf dist; python setup.py sdist bdist_wheel', 'pip3 install tox flake8\nmake deps\nmake all\n']"
"['set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npip install -r dev-requirements.txt\npython setup.py sdist bdist_wheel\npip install dist/*.whl\n', 'set -eux\nconda activate test\ncd docs\npip install -r requirements.txt\nmake html\ntouch build/html/.nojekyll\ncd ..\n', 'set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npython setup.py sdist bdist_wheel\npip install dist/*.whl\npip install -r dev-requirements.txt\n', 'set -eux\nconda activate test\npytest tests -vv\n', 'set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npip install --no-build-isolation -e "".[dev]""\n', 'set -eux\nconda activate test\npip install twine\npython setup.py --nightly --append-to-version=${{ github.event.inputs.append_to_version }} sdist bdist_wheel\ntwine upload --username ""$PYPI_USER"" --password ""$PYPI_TOKEN"" dist/* --verbose\n', 'set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npython setup.py sdist bdist_wheel\npip install dist/*.whl\npip install -r dev-requirements.txt\n', 'set -eux\nconda activate test\npytest tests -vv\n', 'set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npip install  --no-build-isolation -e "".[dev]""\n', 'set -eux\nconda activate test\npip install twine\npython setup.py sdist bdist_wheel\ntwine upload --username ""$PYPI_USER"" --password ""$PYPI_TOKEN"" dist/* --verbose\n', 'set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npip install -r dev-requirements.txt\npython setup.py sdist bdist_wheel\npip install dist/*.whl\n', 'set -eux\nconda activate test\ncd docs\npip install -r requirements.txt\nRELEASE_BUILD=1 make html\ntouch build/html/.nojekyll\ncd ..\n', 'set -eux\nconda activate test\nconda install pytorch cpuonly -c pytorch-nightly\npip install -r requirements.txt\npip install -r dev-requirements.txt\npip install  --no-build-isolation -e .\n', 'set -eux\nconda activate test\npytest --cov=. --cov-report xml tests -vv\n']"
""
"['pip install build', 'python -m build', 'pip install -e .[test] coveralls', 'coverage run -m pytest', 'coveralls --service=github', 'pip install coveralls\ncoveralls --service=github --finish\n']"
"['cd external\necho \necho ""$(git log -1 --pretty=%B)"" >> ../body.txt\n', 'echo ""version=$(cat external/VERSION)"" >> $GITHUB_OUTPUT', 'cd main\nset tag_sha=$(git rev-list -n 1 ""${{steps.get-version.outputs.version}}.rc"")\necho $tag_sha\ngit push --delete origin ""${{steps.get-version.outputs.version}}.external"" || true\ngit push --delete origin ""${{steps.get-version.outputs.version}}.rc"" || true\ngit tag -d  ""${{steps.get-version.outputs.version}}.rc"" || true\ngit tag --force ""${{steps.get-version.outputs.version}}.external"" -a -F ../body.txt $tag_sha\ngit push origin ""${{steps.get-version.outputs.version}}.external""\n', 'cd external\ngit push --delete origin ""${{steps.get-version.outputs.version}}"" || true\ngit tag -d  ""${{steps.get-version.outputs.version}}"" || true\ngit tag --force ""${{steps.get-version.outputs.version}}"" -a -F ../body.txt ${{ github.sha }}\ngit push origin ""${{steps.get-version.outputs.version}}""\n', 'cd external\ngit push external HEAD:main\n', 'cd external\ngit push --delete external ""${{steps.get-version.outputs.version}}"" || true\ngit tag --force ""${{steps.get-version.outputs.version}}"" -a -F ../body.txt ${{ github.sha }}\ngit push external ""${{steps.get-version.outputs.version}}""\n', 'cd xed\npython3 .github/scripts/sanity_external.py --compiler=${{matrix.compiler}} --version=${{matrix.ver}}\n', 'echo ""version=$(python3 --version)"" >> $GITHUB_OUTPUT', 'cd xed\npython3 .github/scripts/sanity_external.py\n']"
"['python -m pip install -U pip\npip install pipenv\npipenv install --dev\n', 'pipenv run python setup.py sdist bdist_wheel\n', 'sudo apt-get update\nsudo apt-get install software-properties-common apt-transport-https wget curl\n\nwget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add -\nsudo add-apt-repository ""deb [arch=amd64] https://packages.microsoft.com/repos/edge stable main""\nsudo apt-get update && sudo apt install microsoft-edge-beta\nmicrosoft-edge --version\n\nwget -qO- https://deb.opera.com/archive.key | sudo apt-key add -\nsudo add-apt-repository ""deb [arch=i386,amd64] https://deb.opera.com/opera-stable/ stable non-free""\nsudo apt-get update\nsudo apt-get -y --no-install-recommends install opera-stable\nopera --version\n\nsudo apt-get install chromium-browser\nchromium --version\n\nsudo curl -fsSLo /usr/share/keyrings/brave-browser-archive-keyring.gpg https://brave-browser-apt-release.s3.brave.com/brave-browser-archive-keyring.gpg\necho ""deb [signed-by=/usr/share/keyrings/brave-browser-archive-keyring.gpg arch=amd64] https://brave-browser-apt-release.s3.brave.com/ stable main""|sudo tee /etc/apt/sources.list.d/brave-browser-release.list\nsudo apt-get update\nsudo apt-get install brave-browser\nbrave-browser --version\n', 'choco install chromium opera brave --no-progress -y --force\n', 'brew tap domt4/chromium\nbrew update\nbrew install --cask mac-chromium opera brave-browser\n', 'python -m pip install -U pip wheel\npip install pipenv\npipenv install --dev --skip-lock --python=${{ matrix.python-version }}\npipenv install selenium==${{ matrix.selenium-version }}\n', 'pipenv run py.test -sv --cov-config .coveragerc --cov-report xml --cov-report term:skip-covered --cov=webdriver_manager --tb=short tests/\n', 'pipenv run py.test -sv --cov-config .coveragerc --cov-report xml --cov-report term:skip-covered --cov=webdriver_manager --tb=short tests/\n', 'sudo apt remove firefox google-chrome-stable -y\nsudo apt autoremove && sudo apt autoclean -y\nsudo apt clean\n', 'python -m pip install -U pip wheel\npip install pipenv\npipenv install --dev --skip-lock --python=${{ matrix.python-version }}\npipenv install selenium==${{ matrix.selenium-version }}\n', 'sudo apt-get update\nsudo apt-get install software-properties-common apt-transport-https wget curl\n\nsudo apt-get install chromium-browser\nchromium --version\n', 'python -m pip install -U pip wheel\npip install pipenv\npipenv install --dev --skip-lock --python=${{ matrix.python-version }}\npipenv install selenium==${{ matrix.selenium-version }}\n', 'ls -la ~/.wdm']"
""
[]
"['echo ""${CONDA}/bin"" >> $GITHUB_PATH', 'source ci/deps/${{ matrix.config }}.sh\necho ""CI_PYTHON_VERSION=${CI_PYTHON_VERSION}"" >> $GITHUB_ENV\necho ""CI_PANDAS_VERSION=${CI_PANDAS_VERSION}"" >> $GITHUB_ENV\necho ""CI_NUMPY_VERSION=${CI_NUMPY_VERSION}"" >> $GITHUB_ENV\necho ""CI_SKLEARN_VERSION=${CI_SKLEARN_VERSION}"" >> $GITHUB_ENV\necho ""CI_NO_SLOW=${CI_NO_SLOW}"" >> $GITHUB_ENV\necho ""CONDA_PKGS_DIR=${HOME}/conda_pkgs_dir"" >> $GITHUB_ENV\necho ""py_version=$(cut -d\'.\' -f 1-2 <<< ${CI_PYTHON_VERSION})"" >> $GITHUB_OUTPUT\necho ""requirements=ci/deps/${{ matrix.config }}.sh"" >> $GITHUB_OUTPUT\n', 'sudo chown -R $USER $CONDA\nci/setup_env.sh ${{ runner.os }}\n', 'sudo xcode-select -s ""/Applications/Xcode_13.2.1.app""\n', 'source activate sksurv-test\npython -m build .\npip install --exists-action=w --pre --no-index --find-links dist/ scikit-survival\nrm -fr build dist sksurv\n', 'source activate sksurv-test\nci/run_tests.sh\n', 'if [ ${{ env.CI_NO_SLOW }} = \'false\' ] && [ ${{ runner.os }} = \'Linux\' ] && [ ${{ github.actor }} != \'dependabot[bot]\' ]; then\n  echo ""do=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""do=false"" >> $GITHUB_OUTPUT\nfi\n', 'echo ""${CONDA}/bin"" >> $GITHUB_PATH', 'source ci/deps/${DEPS_CONFIG}.sh\necho ""CI_PYTHON_VERSION=${CI_PYTHON_VERSION}"" >> $GITHUB_ENV\necho ""CI_PANDAS_VERSION=${CI_PANDAS_VERSION}"" >> $GITHUB_ENV\necho ""CI_NUMPY_VERSION=${CI_NUMPY_VERSION}"" >> $GITHUB_ENV\necho ""CI_SKLEARN_VERSION=${CI_SKLEARN_VERSION}"" >> $GITHUB_ENV\necho ""CI_NO_SLOW=${CI_NO_SLOW}"" >> $GITHUB_ENV\necho ""CONDA_PKGS_DIR=${HOME}/conda_pkgs_dir"" >> $GITHUB_ENV\necho ""requirements=ci/deps/${DEPS_CONFIG}.sh"" >> $GITHUB_OUTPUT\n', 'sudo chown -R $USER $CONDA\nci/setup_env.sh ${{ runner.os }}\n', ""mkdir -p ${XDG_CONFIG_HOME}/matplotlib\necho 'figure.figsize: 6.0, 4.0' > ${XDG_CONFIG_HOME}/matplotlib/matplotlibrc\necho 'figure.facecolor: white' >> ${XDG_CONFIG_HOME}/matplotlib/matplotlibrc\necho 'figure.edgecolor: white' >> ${XDG_CONFIG_HOME}/matplotlib/matplotlibrc\necho 'figure.dpi: 72' >> ${XDG_CONFIG_HOME}/matplotlib/matplotlibrc\necho 'font.size: 10' >> ${XDG_CONFIG_HOME}/matplotlib/matplotlibrc\n"", 'source activate sksurv-test\npython -m build .\npip install --exists-action=w --pre --no-index --find-links dist/ scikit-survival\nrm -fr build dist sksurv\n', 'source activate sksurv-test\npytest --nbval doc/user_guide/*.ipynb --nbval-sanitize-with ci/nb_sanitize.cfg\n', 'sudo apt-get install cmake libenchant-2-dev pandoc tox\n', 'tox run -e ${{ matrix.tox_env }}\n', 'cat .tox/${{ matrix.tox_env }}/log/*.log', 'if [ ${{ runner.os }} = \'Linux\' ]; then\n  echo ""build_platform=manylinux"" >> $GITHUB_OUTPUT\nelif [ ${{ runner.os }} = \'macOS\' ]; then\n  echo ""build_platform=macosx"" >> $GITHUB_OUTPUT\nelif [ ${{ runner.os }} = \'Windows\' ]; then\n  echo ""build_platform=win_amd64"" >> $GITHUB_OUTPUT\nfi\n', 'pipx run build --sdist']"
"['git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'echo ""::set-output name=dir::$(python -m pip  cache dir)""\n', 'python setup.py gen_reqfile --include-extras=test,azure-quantum,braket,revkit\n', 'python setup.py gen_reqfile --include-extras=test,azure-quantum,braket\n', 'python -m pip install -U pip setuptools wheel\ncat requirements.txt\npython -m pip install -r requirements.txt --prefer-binary\npython -m pip install coveralls\n', 'python -m pip install pytest-github-actions-annotate-failures', 'python -m pip install -ve .[azure-quantum,braket,revkit,test]', 'python -m pip install -ve .[azure-quantum,braket,test]', ""echo 'backend: Agg' > matplotlibrc\npython -m pytest -p no:warnings --cov=projectq\n"", 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --finish\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'apt-get update && apt-get install -y python3-dev python3-pip python3-setuptools python3-wheel python3-numpy python3-scipy python3-matplotlib python3-requests python3-networkx python3-pytest python3-pytest-cov python3-flaky libomp-dev --no-install-recommends\n', 'python3 -m pip install -U pip setuptools wheel\npython3 setup.py gen_reqfile --include-extras=test,azure-quantum,braket\ncat requirements.txt\npython3 -m pip install -r requirements.txt --prefer-binary\n', 'python3 -m pip install --upgrade pybind11 flaky --prefer-binary', 'python3 -m pip install -ve .[azure-quantum,braket,test]', ""echo 'backend: Agg' > matplotlibrc\npython3 -m pytest -p no:warnings\n"", 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'apt-get update && apt-get install -y python3-dev python3-pip python3-setuptools python3-wheel python3-numpy python3-scipy python3-matplotlib python3-requests python3-networkx python3-pytest python3-pytest-cov python3-flaky --no-install-recommends\n', 'python3 -m pip install -U pip setuptools wheel\npython3 setup.py gen_reqfile --include-extras=test,azure-quantum,braket\ncat requirements.txt\npython3 -m pip install -r requirements.txt --prefer-binary\n', 'python3 -m pip install --upgrade pybind11 flaky --prefer-binary', 'python3 -m pip install -ve .[azure-quantum,braket,test]', ""echo 'backend: Agg' > matplotlibrc\npython3 -m pytest -p no:warnings\n"", ""echo 'keepcache=1' >> /etc/yum.conf"", ""sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*\nsed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*\n"", 'yum update -y && yum install -y python3-devel gcc-c++ make', 'yum -y install https://packages.endpointdev.com/rhel/7/os/x86_64/endpoint-repo.x86_64.rpm', 'yum install -y git\ngit config --global --add safe.directory /__w/ProjectQ/ProjectQ\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'mkdir -p ~/.cache/pip', 'python3 -m pip install --upgrade pip', 'python3 -m pip install -U pip setuptools wheel\npython3 setup.py gen_reqfile --include-extras=test,azure-quantum,braket\ncat requirements.txt\npython3 -m pip install -r requirements.txt --prefer-binary\n', 'python3 -m pip install -ve .[azure-quantum,braket,test]', ""echo 'backend: Agg' > matplotlibrc\npython3 -m pytest -p no:warnings\n"", 'mkdir -p ~/.cache/pip', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'python3 -m pip install .[docs]\n', 'python3 -m sphinx -b html docs docs/.build', 'python3 setup.py sdist', 'sudo apt update && sudo apt install -y git-flow', 'git fetch --tags --depth=1 origin master develop\ngit flow init --default --tag v\n', 'git flow release start ${{ github.event.inputs.tag }}', 'git config user.name ""GitHub actions""\ngit config user.email noreply@github.com\n', 'git add CHANGELOG.md\ngit commit --message ""Preparing release ${{ github.event.inputs.tag }}""\n\necho ""::set-output name=commit::$(git rev-parse HEAD)""\n', 'git flow release publish ${{ github.event.inputs.tag }}', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', ""python3 -m pip install --upgrade pre-commit 'virtualenv!=20.11'"", '# Slow hooks are marked with manual - slow is okay here, run them too\npre-commit run --hook-stage manual --all-files\n', 'apt-get update && apt-get install -y python3-dev python3-pip python3-setuptools python3-wheel --no-install-recommends\n', 'python3 -m pip install --upgrade pybind11 --prefer-binary', 'python3 setup.py clang_tidy --warning-as-errors', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'BRANCH_NAME=""${{ github.event.pull_request.head.ref }}""\nVERSION=${BRANCH_NAME#release/}\necho ""VERSION = ${VERSION}""\ngit tag ${VERSION} master\n', 'BRANCH_NAME=""${{ github.event.pull_request.head.ref }}""\nVERSION=${BRANCH_NAME#hotfix/}\necho ""VERSION = ${VERSION}""\ngit tag ${VERSION} master\n', '$BRANCH_NAME=""${{ github.event.pull_request.head.ref }}""\n$VERSION = $BRANCH_NAME -replace ""release/"",""""\nWrite-Output ""VERSION = ${VERSION}""\ngit tag ${VERSION} master\n', '$BRANCH_NAME=""${{ github.event.pull_request.head.ref }}""\n$VERSION = $BRANCH_NAME -replace ""hotfix/"",""""\nWrite-Output ""VERSION = ${VERSION}""\ngit tag ${VERSION} master\n', ""python -m pip install -U --prefer-binary pip setuptools build wheel twine 'cibuildwheel<3,>=2'"", 'python -m build --sdist\npython -m twine check dist/*\n', 'python -m cibuildwheel --output-dir binary_dist\npython -m twine check binary_dist/*\n', 'python -m cibuildwheel --output-dir failed_dist\n', 'TAG_NAME=$(git describe --tags `git rev-list --tags --max-count=1`)\nVERSION=${TAG_NAME#v}\n\necho ""RELEASE_VERSION=$VERSION"" >> $GITHUB_ENV\n', 'TAG_NAME=""${GITHUB_REF/refs\\/tags\\//}""\nVERSION=${TAG_NAME#v}\n\necho ""RELEASE_VERSION=$VERSION"" >> $GITHUB_ENV\n', 'BRANCH_NAME=""${{ github.event.pull_request.head.ref }}""\nVERSION=${BRANCH_NAME#release/v}\n\necho ""RELEASE_VERSION=$VERSION"" >> $GITHUB_ENV\n', 'BRANCH_NAME=""${{ github.event.pull_request.head.ref }}""\nVERSION=${BRANCH_NAME#hotfix/v}\n\necho ""RELEASE_VERSION=$VERSION"" >> $GITHUB_ENV\n', '# https://github.com/taiki-e/parse-changelog\ncurl -LsSf ""https://github.com/taiki-e/parse-changelog/releases/download/${parse_changelog_tag}/parse-changelog-${target}.tar.gz"" | tar xzf -\nnotes=$(./parse-changelog ""${changelog}"" ""${RELEASE_VERSION}"")\nrm -f ./parse-changelog\n\nif [[ ""${tag}"" =~ ^v?[0-9\\.]+-[a-zA-Z_0-9\\.-]+(\\+[a-zA-Z_0-9\\.-]+)?$ ]]; then\n  prerelease=""--prerelease""\nfi\n\nmkdir -p wheels pypy_wheels\ngh release create ""v${RELEASE_VERSION}"" ${prerelease:-} --title ""ProjectQ v${RELEASE_VERSION}"" --notes ""${notes:-}"" pypy_wheels/* wheels/*\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python manage.py test\n']"
""
""
"['python -m pip install pre-commit\npre-commit run --all-files --verbose --show-diff-on-failure\n', 'python -m pip install --upgrade wheel invoke parver beautifulsoup4 towncrier requests parse hatch-fancy-pypi-readme\npython -m invoke vendoring.update\n', 'echo ""path=$(python -c \'import sys; print(sys.executable)\')"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'git submodule sync\ngit submodule update --init --recursive\npython -m pip install -e . --upgrade\npipenv install --deploy --dev --python=${{ steps.python-path.outputs.path }}\n', 'cmd /c start pipenv run pypi-server run -v --host=0.0.0.0 --port=8080 --hash-algo=sha256 --disable-fallback ./tests/pypi/ ./tests/fixtures\n', 'pipenv run pypi-server run -v --host=0.0.0.0 --port=8080 --hash-algo=sha256 --disable-fallback ./tests/pypi/ ./tests/fixtures &\n', 'pipenv run pypi-server run -v --host=0.0.0.0 --port=8080 --hash-algo=sha256 --disable-fallback ./tests/pypi/ ./tests/fixtures &\n', 'pipenv run pytest -ra -n auto -v --fulltrace tests\n', 'pip install -U build twine', 'python -m build\ntwine check dist/*\n', 'python -m pip install --upgrade --upgrade-strategy=eager pip setuptools wheel twine\npython -m pip install -e . --upgrade\npython -m pipenv install --dev\n', 'python -m pipenv run python setup.py sdist bdist_wheel\n', ""git config --local user.name 'github-actions[bot]'\ngit config --local user.email 'github-actions[bot]@users.noreply.github.com'\npython -m pipenv run inv release.bump-version --dev --commit\n""]"
"['npm install', 'npm run lint', 'echo ""::warning title=\'Python Linter Check failed\'::Please run \\""pip install black; black py\\"" before committing code changes to python files.""', 'git fetch origin $GITHUB_BASE_REF\necho ""Target branch : $GITHUB_BASE_REF""\ngit diff --name-only origin/$GITHUB_BASE_REF --\necho \'jsfileschanged=\'$(git diff --name-only origin/$GITHUB_BASE_REF -- | grep \'^js/*\' | wc -l) >> $GITHUB_OUTPUT\necho \'Num js files changed=\'$(git diff --name-only origin/$GITHUB_BASE_REF -- | grep \'^js/*\' | wc -l)\n', 'npm run build\n', 'git checkout -B PR-HEAD\n', 'git checkout PR-HEAD -- ./cypress\ngit checkout PR-HEAD -- ./example\n', 'git fetch --prune --unshallow --tags\ngit tag --list\n', 'echo ""::set-output name=TAG_NAME::$(cat py/visdom/VERSION)""\necho ""::set-output name=TAG_EXISTS::$(git tag | grep v$(cat py/visdom/VERSION) | wc -l)""\n', 'echo ""Version name: ${{ steps.version.outputs.TAG_NAME }}""\necho ""Existing Matching Tags: ${{ steps.version.outputs.TAG_EXISTS }}""\n', 'python setup.py sdist', 'npm install', 'npm run build', 'git config --local user.name ""github-actions[bot]""\ngit config --local user.email ""github-actions[bot]@users.noreply.github.com""\ngit add -f py/visdom/static/js/main.js py/visdom/static/js/main.js.map\ngit commit -m ""update static/js files""\ngit push\n']"
"['python -m pip install -U pip\npython -m pip install -r test-requirements.txt\npip-compile -U test-requirements.in\npip-compile -U docs-requirements.in\n', '# The new dependencies may contain a new black version.\n# Commit any changes immediately.\npython -m pip install -r test-requirements.txt\nblack setup.py trio\n', '# setup git repo\ngit switch --force-create autodeps/bump_from_${GITHUB_SHA:0:6}\ngit config user.name \'github-actions[bot]\'\ngit config user.email \'41898282+github-actions[bot]@users.noreply.github.com\'\n\nif ! git commit -am ""Dependency updates""; then\n  echo ""No changes to commit!""\n  exit 0\nfi\n\ngit push --force --set-upstream origin autodeps/bump_from_${GITHUB_SHA:0:6}\n\n# git push returns before github is ready for a pr, so we poll until success\nfor BACKOFF in 1 2 4 8 0; do\n  sleep $BACKOFF\n  if gh pr create \\\n    --label dependencies --body """" \\\n    --title ""Bump dependencies from commit ${GITHUB_SHA:0:6}"" \\\n    ; then\n    break\n  fi\ndone\n\nif [ $BACKOFF -eq 0 ]; then\n  echo ""Could not create the PR""\n  exit 1\nfi\n\n# gh pr create returns before the pr is ready, so we again poll until success\n# https://github.com/cli/cli/issues/2619#issuecomment-1240543096\nfor BACKOFF in 1 2 4 8 0; do\n  sleep $BACKOFF\n  if gh pr merge --auto --squash; then\n    break\n  fi\ndone\n\nif [ $BACKOFF -eq 0 ]; then\n  echo ""Could not set automerge""\n  exit 1\nfi\n', './ci.sh', './ci.sh', './ci.sh']"
"['pip install -r requirements.txt', '${{ matrix.task.command }}', '([ -z ""$COVERALLS_REPO_TOKEN"" ] && echo ""coveralls is skipped in forked repo tests"" || coveralls)']"
""
""
"['python --version\npython -m pip install --upgrade --user pip\npip --version\npip install -r requirements.txt --find-links https://download.pytorch.org/whl/cpu/torch_stable.html --upgrade\nsudo apt-get update -y\nsudo apt-get install -y git curl ca-certificates sox libsox-dev libsox-fmt-all\ngit clone --recursive https://github.com/parlance/ctcdecode.git\ncd ctcdecode; pip install .\n', 'pytest tests/ -vv -s', 'python --version\npython -m pip install --upgrade --user pip\npip --version\npip install -r requirements.txt --find-links https://download.pytorch.org/whl/cpu/torch_stable.html --upgrade\nsudo apt-get update -y\nsudo apt-get install -y git curl ca-certificates sox libsox-dev libsox-fmt-all\ngit clone --recursive https://github.com/parlance/ctcdecode.git\ncd ctcdecode; pip install .\n', 'pytest tests/ -vv -s']"
""
"['wget https://github.com/knope-dev/knope/releases/download/$KNOPE_VERSION/knope-x86_64-unknown-linux-musl-$KNOPE_VERSION.tgz\ntar -xvf knope-x86_64-unknown-linux-musl-$KNOPE_VERSION.tgz\ncp knope-x86_64-unknown-linux-musl-$KNOPE_VERSION/knope $HOME/.cargo/bin\n', 'git config --global user.name ""${{ github.triggering_actor }}""\ngit config --global user.email ""${{ github.triggering_actor}}@users.noreply.github.com""\n', 'knope prerelease', 'wget https://github.com/knope-dev/knope/releases/download/$KNOPE_VERSION/knope-x86_64-unknown-linux-musl-$KNOPE_VERSION.tgz\ntar -xvf knope-x86_64-unknown-linux-musl-$KNOPE_VERSION.tgz\ncp knope-x86_64-unknown-linux-musl-$KNOPE_VERSION/knope $HOME/.cargo/bin\n', 'knope release --dry-run', 'wget https://github.com/knope-dev/knope/releases/download/$KNOPE_VERSION/knope-x86_64-unknown-linux-musl-$KNOPE_VERSION.tgz\ntar -xvf knope-x86_64-unknown-linux-musl-$KNOPE_VERSION.tgz\ncp knope-x86_64-unknown-linux-musl-$KNOPE_VERSION/knope $HOME/.cargo/bin\n', 'git config --global user.name ""${{ github.triggering_actor }}""\ngit config --global user.email ""${{ github.triggering_actor}}@users.noreply.github.com""\n', 'knope release', 'sudo apt update\nsudo apt install pandoc\n. scripts/ci_install_deps\npoetry run make docs\n', 'sudo apt update\n. scripts/ci_install_deps\npoetry run make check-format\n', 'sudo apt update\n. scripts/ci_install_deps\npoetry run make check-style\n', 'sudo apt update\n. scripts/ci_install_deps\npoetry run make check-types\n', 'sudo apt update\n. scripts/ci_install_deps\ndocker run --rm -itd -p 5555:5555 rigetti/quilc -R\ndocker run --rm -itd -p 5000:5000 rigetti/qvm -S\npoetry run make test\n', 'sudo apt update\n. scripts/ci_install_deps\ndocker run --rm -itd -p 5555:5555 rigetti/quilc -R\ndocker run --rm -itd -p 5000:5000 rigetti/qvm -S\npoetry run make e2e TEST_QUANTUM_PROCESSOR=2q-qvm\n']"
[]
"['go vet ./...', 'go test -v ./...', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m testing.run_tests']"
"['sudo apt install python3-setuptools python3-wheel\n', 'make --keep-going install-user test lint']"
"['python -m pip install ""cibuildwheel==2.13.0""', 'python -m cibuildwheel --output-dir wheelhouse', 'python setup.py sdist -d wheelhouse/', 'SHA256SUM1=$(cat source/*.c source/bases/* | sha256sum | awk \'{print $1}\')\nSHA256SUM2=$(cat cx_Freeze/bases/__init__.py | awk \'{print $2}\')\nif [ $SHA256SUM1 != $SHA256SUM2 ]; then\n  git config user.name ""Marcelo Duarte""\n  git config user.email marcelotduarte@users.noreply.github.com\n  # Remove any file that match - remove previous versions too\n  git rm --ignore-unmatch \'cx_Freeze/bases/*-win*.exe\' \'cx_Freeze/util.*-win*.pyd\'\n  # Extract base executables and util module\n  for file in wheelhouse/*-win*.whl; do\n    unzip -o $file \'cx_Freeze/bases/*-win*.exe\' \'cx_Freeze/util.*-win*.pyd\'\n  done\n  git add cx_Freeze/bases/*-win*.exe cx_Freeze/util.*-win*.pyd\n  # Save the new SHA256SUM\n  echo ""# $SHA256SUM1"" > cx_Freeze/bases/__init__.py\n  git add cx_Freeze/bases/__init__.py\n  # Update\n  git commit -m ""bases: update base executables and util module""\n  git push --set-upstream origin main\nfi\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\npip install -v -e . --no-deps --no-build-isolation\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\npip install -v -e . --no-deps --no-build-isolation\n', 'python -m pytest', 'python -m pytest --cov=""cx_Freeze"" --cov-report=xml']"
"['python -m pip install --upgrade pip\npip install pre-commit\npip install -r requirements.txt\npip install tox pytest\npython -c ""import nltk; nltk.download(\'stopwords\'); nltk.download(\'punkt\')""\n', 'pre-commit run --all-files', 'tox -e py']"
"['touch docs/README.rst\npython setup.py install\n', 'gem install mdl', 'mdl --git-recurse --rules ~MD013,~MD029 .', 'pip install -r requirements.txt\npip install pylint flake8\nmake lint_python3\n']"
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'make check', 'docker build . --file ""build_image/docker/common/api-engine/Dockerfile.in"" --tag hyperledger/cello-api-engine:$(date +%s)\ndocker build . --file ""build_image/docker/common/nginx/Dockerfile.in"" --tag hyperledger/cello-nginx:$(date +%s)\ndocker build . --file ""build_image/docker/common/dashboard/Dockerfile.in"" --tag hyperledger/cello-dashboard:$(date +%s)\n']"
"['pip3 install -r requirements/release.txt', 'pip3 install -e .', 'make release', 'twine upload dist/*', 'pip3 install -r requirements/check.txt', 'pip3 install -e .', 'make proto', 'flake8', 'mypy grpclib examples', 'pip3 install -r requirements/test.txt', 'pip3 install -e .', 'pytest']"
"['pip3 install .[dev]', 'oj --version', 'pylint --rcfile=setup.cfg onlinejudge_command tests setup.py', 'isort --check-only --diff onlinejudge_command tests setup.py', 'yapf --diff --recursive onlinejudge_command tests setup.py', 'mypy onlinejudge_command tests setup.py', 'pip3 install .\npip3 install setuptools wheel\n', 'python3 setup.py bdist_wheel', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', 'pip3 install --upgrade setuptools\npip3 install .[dev]\n', 'oj -h\npytest -v tests/*.py\n']"
"['python -m pip install --upgrade pip\npip install poetry\npoetry export --with dev > requirements.txt\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install poetry\npoetry export --with dev > requirements.txt\npip install -r requirements.txt\n', 'pytest --cov=pygam\ncodecov\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\nsudo apt update\nsudo apt install -y xvfb\nwget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb\nsudo apt install -y ./wkhtmltox_0.12.6-1.bionic_amd64.deb\npip install six coverage codecov nose\n', 'nosetests --with-coverage --cover-package=src/imgkit --where tests imgkit_test.py', 'codecov']"
""
"['echo ""::set-output name=version::$(grep -Po \'\\d*\\.\\d*\\.\\d*\' src/iris/__init__.py)""', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n', 'twine upload dist/*\n']"
"['pip install prospector[with_everything]==1.7.7 defusedxml requests zxcvbn dominate\n', 'make lint', 'sudo apt-get install gnome-keyring libsodium23 pass\n', 'sudo ln -s /usr/lib/x86_64-linux-gnu/libsodium.so.23 /usr/lib/x86_64-linux-gnu/libequihash.so\nsudo ln -s /usr/lib/x86_64-linux-gnu/libsodium.so.23 /usr/lib/x86_64-linux-gnu/libsphinx.so\nsudo ldconfig\npip3 install -U setuptools\npip3 install green coverage codacy-coverage pyaml defusedxml cryptography pykeepass secretstorage pwdsphinx ${{ matrix.magic }}\n', ""echo '#!/usr/bin/env bash\\necho lpass' | sudo tee /usr/bin/lpass\nsudo chmod 755 /usr/bin/lpass\nmkdir -p ~/.local/share/keyrings/\ncp tests/assets/db/gnome-keyring.keyring ~/.local/share/keyrings/pass-import.keyring\ncp tests/assets/db/sphinx.cfg ~/.sphinxrc\nsphinx init\n"", 'dbus-run-session -- make tests']"
"['sudo apt install gettext git', 'pip3 install dnspython beautifulsoup4 requests \\\n             hatchling hatch-vcs sphinx sphinx_epytext \\\n             sphinx_rtd_theme sphinx-sitemap sphinx-intl\n', 'git config user.email ""linkchecker@linkchecker.github.io""\ngit config user.name ""LinkChecker""\ngit checkout -b man-updates\ngit remote add local ${{ github.server_url }}/${{ github.repository }}\n', 'python3 -m hatchling build -t sdist --hooks-only\nmake -C doc locale\ngit commit -a -m ""Update doc translation catalogs""\nmake -C doc man\ngit commit -a -m ""Update man pages""\n', 'rm -f po/linkchecker.pot\nmake -C po\ngit commit -a -m ""Update application translation catalogs""\n', 'git push --set-upstream local man-updates\n', 'sudo apt-get update\nsudo apt-get install -y clamav-daemon geoip-database libgeoip-dev gettext\n', 'sudo chown -R runner:docker /var/lib/clamav\n', 'sudo chown -R clamav:clamav /var/lib/clamav\n', 'sudo service clamav-daemon restart\n', 'sudo service clamav-freshclam stop\nsudo freshclam\nsudo service clamav-daemon restart\nsudo service clamav-daemon status\n', 'python -m pip install -U pip\npython -m pip install -U hatchling hatch-vcs polib\npython -m pip install -U tox\n', 'while ! test -S /run/clamav/clamd.ctl; do printf "".""; sleep 1; done\n', 'python -m hatchling build -t sdist --hooks-only\npython -m tox -e ${{ matrix.toxenv }}\n', 'sudo apt-get install -y graphviz mandoc\n', 'pip install -U dnspython beautifulsoup4 requests \\\n  hatchling hatch-vcs sphinx sphinx-epytext \\\n  sphinx-intl sphinx-rtd-theme sphinx-sitemap\n', 'python3 -m hatchling build -t sdist --hooks-only\nmake -C doc html\nmake -C doc locale\nmake -C doc man\nmake -C doc check\n', 'python -m pip install -U pip\npython -m pip install -U hatchling hatch-vcs polib\npython -m pip install -U tox\n', 'python -m tox -e ${{ matrix.toxenv }}', 'sudo apt install graphviz', 'pip install dnspython beautifulsoup4 requests \\\n            hatchling hatch-vcs\n# Allow sphinx-rtd-theme to choose the versions of sphinx & docutils\npip install sphinx-rtd-theme\npip install sphinx-epytext sphinx-sitemap\n', 'python3 -m hatchling build -t sdist --hooks-only\nmake -C doc html\n', 'sudo apt install git', 'pip3 install -U hatchling hatch-vcs polib twine\n', 'echo ""SOURCE_DATE_EPOCH=$(git log -n 1 ${{ github.sha }} --format=%ct)"" >> $GITHUB_ENV\n', 'python3 -m hatchling build\n', 'twine check dist/*\n', 'sha256sum dist/*\n', 'gh release upload ${{ github.ref_name }} dist/*\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8', '# stop the build if there are Python syntax errors or undefined names\nflake8 ./ubelt --count --select=E9,F63,F7,F82 --show-source --statistics', 'python -m pip install mypy\nmypy --install-types --non-interactive ./ubelt\nmypy ./ubelt', 'python -m pip install --upgrade pip\npython -m pip install -r requirements/tests.txt\npython -m pip install -r requirements/runtime.txt', 'python -m pip install pip -U\npython -m pip install setuptools>=0.8 wheel build\npython -m build --sdist --outdir wheelhouse', 'ls -al ./wheelhouse\npip install --prefer-binary wheelhouse/ubelt*.tar.gz -v', 'pwd\nls -al\n# Run in a sandboxed directory\nWORKSPACE_DNAME=""testsrcdir_minimal_${CI_PYTHON_VERSION}_${GITHUB_RUN_ID}_${RUNNER_OS}""\nmkdir -p $WORKSPACE_DNAME\ncd $WORKSPACE_DNAME\n# Run the tests\n# Get path to installed package\nMOD_DPATH=$(python -c ""import ubelt, os; print(os.path.dirname(ubelt.__file__))"")\necho ""MOD_DPATH = $MOD_DPATH""\npython -m pytest --cov={self.mod_name} $MOD_DPATH ../tests\ncd ..', 'pwd\nls -al\ntrue\n# Run in a sandboxed directory\nWORKSPACE_DNAME=""testsrcdir_full_${CI_PYTHON_VERSION}_${GITHUB_RUN_ID}_${RUNNER_OS}""\nmkdir -p $WORKSPACE_DNAME\ncd $WORKSPACE_DNAME\n# Run the tests\n# Get path to installed package\nMOD_DPATH=$(python -c ""import ubelt, os; print(os.path.dirname(ubelt.__file__))"")\necho ""MOD_DPATH = $MOD_DPATH""\npython -m pytest --cov={self.mod_name} $MOD_DPATH ../tests\ncd ..', 'python -m pip install setuptools>=0.8 wheel build\npython -m build --wheel --outdir wheelhouse', 'ls -la wheelhouse', 'echo ""Finding the path to the wheel""\nls wheelhouse || echo ""wheelhouse does not exist""\necho ""Installing helpers""\npip install setuptools>=0.8 setuptools_scm wheel build -U\npip install tomli pkginfo\nexport WHEEL_FPATH=$(python -c ""import pathlib; print(str(sorted(pathlib.Path(\'wheelhouse\').glob(\'ubelt*.whl\'))[-1]).replace(chr(92), chr(47)))"")\nexport MOD_VERSION=$(python -c ""from pkginfo import Wheel; print(Wheel(\'$WHEEL_FPATH\').version)"")\npip install --prefer-binary ""ubelt[$INSTALL_EXTRAS]==$MOD_VERSION"" -f wheelhouse\necho ""Install finished.""', 'echo ""Creating test sandbox directory""\nexport WORKSPACE_DNAME=""testdir_${CI_PYTHON_VERSION}_${GITHUB_RUN_ID}_${RUNNER_OS}""\necho ""WORKSPACE_DNAME=$WORKSPACE_DNAME""\nmkdir -p $WORKSPACE_DNAME\necho ""cd-ing into the workspace""\ncd $WORKSPACE_DNAME\npwd\nls -altr\n# Get the path to the installed package and run the tests\nexport MOD_DPATH=$(python -c ""import ubelt, os; print(os.path.dirname(ubelt.__file__))"")\necho ""\n---\nMOD_DPATH = $MOD_DPATH\n---\nrunning the pytest command inside the workspace\n---\n""\npython -m pytest -p pytester -p no:doctest --xdoctest --cov-config ../pyproject.toml --cov-report term --cov=""ubelt"" ""$MOD_DPATH"" ../tests\necho ""pytest command finished, moving the coverage file to the repo root""\nls -al\n# Move coverage file to a new name\nmv .coverage ""../.coverage.$WORKSPACE_DNAME""\necho ""changing directory back to th repo root""\ncd ..\nls -al', ""echo '############ PWD'\npwd\ncp .wheelhouse/.coverage* . || true\nls -al\npython -m pip install coverage[toml]\necho '############ combine'\ncoverage combine . || true\necho '############ XML'\ncoverage xml -o ./coverage.xml || true\necho '### The cwd should now have a coverage.xml'\nls -altr\npwd"", 'ls -la wheelhouse', 'GPG_EXECUTABLE=gpg\n$GPG_EXECUTABLE --version\nopenssl version\n$GPG_EXECUTABLE --list-keys\necho ""Decrypting Keys""\nopenssl enc -aes-256-cbc -pbkdf2 -md SHA512 -pass env:CI_SECRET -d -a -in dev/ci_public_gpg_key.pgp.enc | $GPG_EXECUTABLE --import\nopenssl enc -aes-256-cbc -pbkdf2 -md SHA512 -pass env:CI_SECRET -d -a -in dev/gpg_owner_trust.enc | $GPG_EXECUTABLE --import-ownertrust\nopenssl enc -aes-256-cbc -pbkdf2 -md SHA512 -pass env:CI_SECRET -d -a -in dev/ci_secret_gpg_subkeys.pgp.enc | $GPG_EXECUTABLE --import\necho ""Finish Decrypt Keys""\n$GPG_EXECUTABLE --list-keys || true\n$GPG_EXECUTABLE --list-keys  || echo ""first invocation of gpg creates directories and returns 1""\n$GPG_EXECUTABLE --list-keys\nVERSION=$(python -c ""import setup; print(setup.VERSION)"")\npip install twine\npip install urllib3 requests[security] twine\nGPG_KEYID=$(cat dev/public_gpg_key)\necho ""GPG_KEYID = \'$GPG_KEYID\'""\nDO_GPG=True GPG_KEYID=$GPG_KEYID TWINE_REPOSITORY_URL=${TWINE_REPOSITORY_URL} TWINE_PASSWORD=$TWINE_PASSWORD TWINE_USERNAME=$TWINE_USERNAME GPG_EXECUTABLE=$GPG_EXECUTABLE DO_UPLOAD=True DO_TAG=False ./publish.sh', 'ls -la wheelhouse', 'GPG_EXECUTABLE=gpg\n$GPG_EXECUTABLE --version\nopenssl version\n$GPG_EXECUTABLE --list-keys\necho ""Decrypting Keys""\nopenssl enc -aes-256-cbc -pbkdf2 -md SHA512 -pass env:CI_SECRET -d -a -in dev/ci_public_gpg_key.pgp.enc | $GPG_EXECUTABLE --import\nopenssl enc -aes-256-cbc -pbkdf2 -md SHA512 -pass env:CI_SECRET -d -a -in dev/gpg_owner_trust.enc | $GPG_EXECUTABLE --import-ownertrust\nopenssl enc -aes-256-cbc -pbkdf2 -md SHA512 -pass env:CI_SECRET -d -a -in dev/ci_secret_gpg_subkeys.pgp.enc | $GPG_EXECUTABLE --import\necho ""Finish Decrypt Keys""\n$GPG_EXECUTABLE --list-keys || true\n$GPG_EXECUTABLE --list-keys  || echo ""first invocation of gpg creates directories and returns 1""\n$GPG_EXECUTABLE --list-keys\nVERSION=$(python -c ""import setup; print(setup.VERSION)"")\npip install twine\npip install urllib3 requests[security] twine\nGPG_KEYID=$(cat dev/public_gpg_key)\necho ""GPG_KEYID = \'$GPG_KEYID\'""\nDO_GPG=True GPG_KEYID=$GPG_KEYID TWINE_REPOSITORY_URL=${TWINE_REPOSITORY_URL} TWINE_PASSWORD=$TWINE_PASSWORD TWINE_USERNAME=$TWINE_USERNAME GPG_EXECUTABLE=$GPG_EXECUTABLE DO_UPLOAD=True DO_TAG=False ./publish.sh']"
"['ls -l ./wheelhouse\npip install twine\ntwine upload --skip-existing ./wheelhouse/*.whl\n', 'ls -l ./wheelhouse\npip install twine\ntwine upload --skip-existing ./wheelhouse/*.whl\n', 'ls -l ./wheelhouse\npip install twine\ntwine upload --skip-existing ./wheelhouse/*.whl\n', 'pypy -m pip install wheel\npypy setup.py bdist_wheel\n', 'pip install twine\ntwine upload dist/*\n', 'python setup.py sdist --formats=zip', 'pip install twine\ntwine upload dist/*\n', 'ls -l ./wheelhouse\npip install twine\ntwine upload --skip-existing ./wheelhouse/*.whl\n', 'python -m pip install --upgrade pip\npython -m pip install wheel mypy pyparsing numpy fonttools\n', 'python setup.py install\n', 'python -m mypy --ignore-missing-imports -p ezdxf\n', 'python -m pip install --upgrade pip\npython -m pip install wheel pytest pyparsing fonttools numpy\n', 'python setup.py install\n', 'python -m pytest tests integration_tests\n', 'python -m pip install --upgrade pip\n# Excluding Cython to test the pure Python version.\npython -m pip install wheel pytest pyparsing fonttools numpy\n', 'python setup.py install\n', 'python -m pytest tests integration_tests\n']"
"['python -m pip install --upgrade pip\npython -m pip install pytest pytest-split\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install .[""all""]\nif ${{ matrix.python-version == \'3.9\' }}; then python -m pip install pypower; fi\nif ${{ matrix.python-version != \'3.9\' && matrix.python-version != \'3.11\' }}; then python -m pip install numba; fi\nif ${{ matrix.python-version == \'3.8\' || matrix.python-version == \'3.10\' }}; then python -m pip install lightsim2grid; fi\n', './.install_julia.sh 1.8\npip install julia\npython ./.install_pycall.py\n', 'pip list\n', 'pytest --splits 2 --group ${{ matrix.group }}\n', 'pip install pytest-cov\npytest --cov=./ --cov-report=xml --splits 2 --group ${{ matrix.group }}\n', ""python -m pip install --upgrade pip\npython -m pip install pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install .\npip install matplotlib\nif ${{ matrix.python-version != '3.11' }}; then python -m pip install numba; fi\n"", 'python -m pip install git+https://github.com/e2nIEE/pandapipes@develop#egg=pandapipes\npython -m pip install git+https://github.com/e2nIEE/simbench@develop#egg=simbench\n', 'pip list\n', ""python -c 'from pandapipes import pp_dir; import pytest; import sys; ec = pytest.main([pp_dir]); sys.exit(ec)'\n"", ""python -c 'from simbench import sb_dir; import pytest; import sys; ec = pytest.main([sb_dir]); sys.exit(ec)'\n"", 'python -m pip install --upgrade pip\npython -m pip install flake8\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install .\npip install matplotlib\n', 'pip list\n', '# stop the build if there are Python syntax errors or undefined names (omitted by exit-zero)\nflake8 . --count --exit-zero --select=E9,F63,F7,F82 --show-source --statistics\n', '# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install .[test,fileio]\n', 'pip list\n', 'sudo systemctl start postgresql.service\npg_isready\nsudo -u postgres psql --command=""CREATE USER test_user PASSWORD \'secret\'"" --command=""\\du""\nsudo -u postgres createdb --owner=test_user sandbox\nsudo -u postgres psql --command=""\\c sandbox"" --command=""CREATE SCHEMA test_schema AUTHORIZATION test_user""\nPGPASSWORD=secret psql --username=test_user --host=localhost --list sandbox\n', 'python -c ""import os; import json; from pandapower import pp_dir; conn_data={\'host\': \'localhost\', \'user\': \'test_user\', \'database\': \'sandbox\', \'password\': \'secret\', \'schema\': \'test_schema\'}; fp = open(os.path.join(pp_dir, \'test\', \'test_files\', \'postgresql_connect_data.json\'), \'w\'); json.dump(conn_data, fp); fp.close()""\npython -c \'from pandapower import pp_dir; import pytest; import sys; import os; ec = pytest.main([os.path.join(pp_dir,""test"",""api"",""test_sql_io.py"")]); sys.exit(ec)\'\n', 'python -m pip install --upgrade pip\npython -m pip install pytest nbmake pytest-xdist python-igraph numba seaborn\n./.install_julia.sh 1.8\npip install julia\npython ./.install_pycall.py\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install .[""all""]\n', 'pip list\n', 'pytest --nbmake -n=auto --nbmake-timeout=900 ""./tutorials""\n', '# Upgrade pip\npython3 -m pip install --upgrade pip\n# Install twine\npython3 -m pip install setuptools wheel twine\n', 'python3 setup.py sdist --formats=zip\ntwine check dist/* --strict \npython3 -m twine upload dist/*\n', 'python3 setup.py sdist --formats=zip\ntwine check dist/* --strict \npython3 -m twine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install pytest python-igraph pytest-split seaborn matplotlib plotly geopandas ortools xlsxwriter openpyxl cryptography psycopg2 matpowercaseframes\n', 'pip install --no-cache-dir -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple pandapower\n', 'pip install pandapower\n', ""if ( '${{ matrix.python-version }}'  -eq '3.9' ) { python -m pip install pypower }\nif ( '${{ matrix.python-version }}'  -ne '3.9' -and '${{ matrix.python-version }}' -ne '3.11' ) { python -m pip install numba }\nif ( '${{ matrix.python-version }}'  -eq '3.8' -or '${{ matrix.python-version }}'  -eq '3.10' ) { python -m pip install lightsim2grid }\n"", ""if ${{ matrix.python-version == '3.9' }}; then python -m pip install pypower; fi\nif ${{ matrix.python-version != '3.9' && matrix.python-version != '3.11' }}; then python -m pip install numba; fi\nif ${{ matrix.python-version == '3.8' || matrix.python-version == '3.10' }}; then python -m pip install lightsim2grid; fi\n"", './.install_julia.sh 1.8\npip install julia\npython ./.install_pycall.py\n', 'pip list\n', 'pytest --splits 2 --group ${{ matrix.group }} --pyargs pandapower.test\n']"
"['pip install build && python -m build allure-python-commons --outdir dist/ && python -m build allure-python-commons-test --outdir dist/', 'pip install -r ./requirements/linting.txt', 'poe linter', 'pip install dist/allure-python-commons*.tar.gz && pip install ./${{ matrix.package }} && pip install -r ./requirements/testing.txt && pip install -r ./requirements/testing/${{ matrix.package }}.txt', 'poe tests', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'pushd allure-python-commons-test\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n\npushd allure-python-commons\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n\npushd allure-behave\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n\npushd allure-nose2\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n\npushd allure-pytest\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n\npushd allure-pytest-bdd\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n\npushd allure-robotframework\npython setup.py sdist bdist_wheel\ntwine upload dist/*\npopd\n']"
"['python -m pip install --upgrade pip\npip install -r requirements-test.txt\npip install -r requirements.txt\n', 'pytest tests/unit_tests', 'pip install coveralls==3.0.1\ncoveralls --service=github\n', './flake8.sh']"
"['python -m venv env\nenv/bin/python -m pip install .\nenv/bin/python -m pip install coverage\n', 'env/bin/coverage run -m unittest cvxportfolio/tests/*.py\nenv/bin/coverage lcov\n']"
"['curl -s -L https://github.com/svpcom/wfb-ng/releases/download/wifibroadcast-17.10/qemu-7.2.0-fixed.tar.gz | sudo tar xzv -C /\nmake deb_docker DOCKER_SRC_IMAGE=p2ptech/cross-build:${{ matrix.docker-images }}\n', 'sudo apt update && sudo apt -y install python3-twisted libpcap-dev libsodium-dev python3-pyroute2 python3-future\nmake all_bin\n']"
"['set -ex\n# use that here since the variable are not present before start, so can\'t be in env\nexport LOGIN=$GITHUB_REPOSITORY_OWNER\n\necho $PASSWORD | docker login $REGISTRY -u $LOGIN --password-stdin\n\nfor i in docker/* ; do\n    CONTAINER=$(basename $i)\n    echo ""Building $CONTAINER""\n    export IMAGE=$LOGIN/augur_$CONTAINER\n    DOCKERFILE=${i}/Dockerfile\n\n    docker build . -f $DOCKERFILE --tag $REGISTRY/$IMAGE:latest\n    if [[ $GITHUB_EVENT_NAME == \'release\' ]]; then\n      TAG=$(basename $GITHUB_REF)\n      docker tag $REGISTRY/$IMAGE:latest $REGISTRY/$IMAGE:$TAG\n      docker push $REGISTRY/$IMAGE:latest\n      docker push $REGISTRY/$IMAGE:$TAG\n    elif [[ $GITHUB_EVENT_NAME == \'push\' ]]; then\n      docker tag $REGISTRY/$IMAGE:latest $REGISTRY/$IMAGE:devel-latest\n      docker push $REGISTRY/$IMAGE:devel-latest\n    fi\ndone\n']"
""
""
['./.github/scripts/build.sh']
[]
"['python -m pip install --upgrade pip\npip install --upgrade setuptools\npip install -e .\npip install -r requirements.opt.txt\npip install sacrebleu\npip install flake8\npython -m pip install black==22.* flake8==3.8.*\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'black --check .\n', 'flake8 .\n', 'python -m unittest discover\n', 'python onmt/bin/build_vocab.py \\\n  -config data/data.yaml \\\n  -save_data /tmp/onmt \\\n  -n_sample 5000 \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  && rm -rf /tmp/sample\n', 'python onmt/bin/build_vocab.py \\\n  -config data/features_data.yaml \\\n  -save_data /tmp/onmt_feat \\\n  -src_vocab /tmp/onmt_feat.vocab.src \\\n  -tgt_vocab /tmp/onmt_feat.vocab.tgt \\\n  -n_sample -1 \\\n  && rm -rf /tmp/sample\n', '# The dumped fields are used later when testing tools\npython train.py \\\n  -config data/data.yaml \\\n  -save_data /tmp/onmt.train.check \\\n  -dump_fields \\\n  -dump_transforms \\\n  -n_sample 30 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000\n', 'python train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -hidden_size 2 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -batch_size 10 \\\n  -word_vec_size 5 \\\n  -report_every 5\\\n  -hidden_size 10 \\\n  -train_steps 10 \\\n  -tensorboard ""true"" \\\n  -tensorboard_log_dir /tmp/logs_train\npython onmt/tests/test_events.py --logdir /tmp/logs_train -tensorboard_checks train\n', 'python train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -hidden_size 2 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -batch_size 10 \\\n  -word_vec_size 5 \\\n  -report_every 5 \\\n  -hidden_size 10 \\\n  -train_steps 10 -valid_steps 5 \\\n  -tensorboard ""true"" \\\n  -tensorboard_log_dir /tmp/logs_train_valid \\\n  -copy_attn\npython onmt/tests/test_events.py --logdir /tmp/logs_train_valid -tensorboard_checks train_valid\n', 'python train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -hidden_size 2 -batch_size 10 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -word_vec_size 5 -report_every 5        \\\n  -coverage_attn true -lambda_coverage 0.1 \\\n  -hidden_size 10 -train_steps 10\n', 'python train.py \\\n  -config data/align_data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -encoder_type transformer \\\n  -decoder_type transformer \\\n  -layers 4 \\\n  -word_vec_size 16 \\\n  -hidden_size 16 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -heads 2 \\\n  -transformer_ff 64 \\\n  -lambda_align 0.05 \\\n  -alignment_layer 2 \\\n  -alignment_heads 0 \\\n  -dropout_steps 0 3 7 \\\n  -dropout 0.3 0.2 0.1 \\\n  -attention_dropout 0.2 0.1 0.1 \\\n  -report_every 5 \\\n  -train_steps 10\n', 'python3 train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -encoder_type transformer \\\n  -decoder_type transformer \\\n  -layers 4 \\\n  -word_vec_size 16 \\\n  -hidden_size 16 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -heads 2 \\\n  -transformer_ff 64 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -accum_count 2 4 8 \\\n  -accum_steps 0 15000 30000 \\\n  -save_model /tmp/onmt.model \\\n  -train_steps 20 \\\n  -report_every 5 \\\n  -train_eval_steps 10 \\\n  -train_metrics ""BLEU"" ""TER"" \\\n  -tensorboard ""true"" \\\n  -scoring_debug ""true"" \\\n  -tensorboard_log_dir /tmp/logs_train_metrics \\\n  -dump_preds /tmp/dump_preds\npython onmt/tests/test_events.py --logdir /tmp/logs_train_metrics -tensorboard_checks train_metrics\n', 'python3 train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -encoder_type transformer \\\n  -decoder_type transformer \\\n  -layers 4 \\\n  -word_vec_size 16 \\\n  -hidden_size 16 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -heads 2 \\\n  -transformer_ff 64 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -accum_count 2 4 8 \\\n  -accum_steps 0 15000 30000 \\\n  -save_model /tmp/onmt.model \\\n  -train_steps 10  -valid_steps 5 \\\n  -report_every 2 \\\n  -train_eval_steps 8 \\\n  -train_metrics ""BLEU"" ""TER"" \\\n  -valid_metrics ""BLEU"" ""TER"" \\\n  -tensorboard ""true"" \\\n  -scoring_debug ""true"" \\\n  -tensorboard_log_dir /tmp/logs_train_valid_metrics \\\n  -dump_preds /tmp/dump_preds \\\n  -copy_attn\npython onmt/tests/test_events.py --logdir /tmp/logs_train_valid_metrics -tensorboard_checks train_valid_metrics \n', 'python train.py \\\n    -config data/lm_data.yaml \\\n    -src_vocab /tmp/onmt.vocab.src \\\n    -tgt_vocab /tmp/onmt.vocab.src \\\n    -model_task lm \\\n    -encoder_type transformer_lm \\\n    -decoder_type transformer_lm \\\n    -src_vocab_size 1000 \\\n    -tgt_vocab_size 1000 \\\n    -num_workers 0 -bucket_size 1024 \\\n    -dec_layers 2 -batch_size 10 \\\n    -heads 4 -transformer_ff 64 \\\n    -word_vec_size 16 -report_every 5 \\\n    -hidden_size 16 -train_steps 10\n', 'python train.py \\\n  -config data/lm_data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.src \\\n  -model_task lm \\\n  -encoder_type transformer_lm \\\n  -decoder_type transformer_lm \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -dec_layers 2 -batch_size 10 \\\n  -heads 4 -transformer_ff 64 \\\n  -word_vec_size 16 -report_every 5        \\\n  -hidden_size 16 -train_steps 10 \\\n  -copy_attn\n', 'python train.py \\\n  -config data/ggnn_data.yaml \\\n  -src_seq_length 1000 \\\n  -tgt_seq_length 30 \\\n  -encoder_type ggnn \\\n  -layers 2 \\\n  -decoder_type rnn \\\n  -hidden_size 256 \\\n  -learning_rate 0.1 \\\n  -learning_rate_decay 0.8 \\\n  -global_attention general \\\n  -batch_size 32 \\\n  -word_vec_size 256 \\\n  -bridge \\\n  -num_workers 0 -bucket_size 1024 \\\n  -train_steps 10 \\\n  -n_edge_types 9 \\\n  -state_dim 256 \\\n  -n_steps 10 \\\n  -n_node 64\n', 'python onmt/bin/train.py \\\n  -config data/features_data.yaml \\\n  -src_vocab /tmp/onmt_feat.vocab.src \\\n  -tgt_vocab /tmp/onmt_feat.vocab.tgt \\\n  -src_vocab_size 1000 -tgt_vocab_size 1000 \\\n  -hidden_size 2 -batch_size 10 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -word_vec_size 5 -hidden_size 10 \\\n  -report_every 5 -train_steps 10 \\\n  -save_model /tmp/onmt.model \\\n  -save_checkpoint_steps 10\n', 'python onmt/bin/train.py \\\n  -config data/features_data.yaml \\\n  -src_vocab /tmp/onmt_feat.vocab.src \\\n  -tgt_vocab /tmp/onmt_feat.vocab.tgt \\\n  -src_vocab_size 1000 -tgt_vocab_size 1000 \\\n  -hidden_size 2 -batch_size 10 \\\n  -word_vec_size 5 -hidden_size 10 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -report_every 5 -train_steps 10 \\\n  -train_metrics ""BLEU"" ""TER"" \\\n  -valid_metrics ""BLEU"" ""TER"" \\\n  -save_model /tmp/onmt.model \\\n  -save_checkpoint_steps 10\n', 'python translate.py \\\n    -model /tmp/onmt.model_step_10.pt \\\n    -src data/data_features/src-test-with-feats.txt \\\n    -n_src_feats 1 -verbose\n', 'head data/src-test.txt > /tmp/src-test.txt\npython translate.py \\\n  -model onmt/tests/test_model.pt \\\n  -src /tmp/src-test.txt \\\n  -verbose\n', 'head data/src-test.txt > /tmp/src-test.txt\npython translate.py \\\n  -model onmt/tests/test_model.pt \\\n  onmt/tests/test_model.pt \\\n  -src /tmp/src-test.txt \\\n  -verbose\n', 'python translate.py \\\n  -model onmt/tests/test_model2.pt \\\n  -src data/morph/src.valid \\\n  -verbose \\\n  -batch_size 10 \\\n  -beam_size 10 \\\n  -tgt data/morph/tgt.valid \\\n  -out /tmp/trans\ndiff data/morph/tgt.valid /tmp/trans && rm /tmp/trans\n', 'python translate.py \\\n  -model onmt/tests/test_model2.pt \\\n  -src data/morph/src.valid \\\n  -verbose \\\n  -batch_size 10 \\\n  -beam_size 1 \\\n  -seed 1 \\\n  -random_sampling_topk ""-1"" \\\n  -random_sampling_temp 0.0001 \\\n  -tgt data/morph/tgt.valid \\\n  -out /tmp/trans\ndiff data/morph/tgt.valid /tmp/trans && rm /tmp/trans\n', 'head data/src-test.txt > /tmp/src-test.txt\npython translate.py \\\n  -model onmt/tests/test_model_lm.pt \\\n  -src /tmp/src-test.txt \\\n  -verbose\n', 'python translate.py \\\n  -model onmt/tests/test_model_lm.pt \\\n  -src data/data_lm/src-gen.txt \\\n  -verbose -batch_size 1 \\\n  -beam_size 10 \\\n  -ban_unk_token \\\n  -length_penalty none \\\n  -out /tmp/gen\ndiff data/data_lm/gen-beam-sol.txt /tmp/gen && rm /tmp/gen\n', 'python translate.py -model onmt/tests/test_model_lm.pt \\\n  -src data/data_lm/src-gen.txt \\\n  -verbose -batch_size 1 \\\n  -beam_size 1 \\\n  -seed 1 \\\n  -random_sampling_topk -1 \\\n  -random_sampling_temp 0.0001 \\\n  -ban_unk_token \\\n  -length_penalty none \\\n  -out /tmp/gen\ndiff data/data_lm/gen-sampling-sol.txt /tmp/gen && rm /tmp/gen\n', 'python translate.py -model onmt/tests/test_model_lm.pt \\\n  -src data/data_lm/src-gen.txt \\\n  -verbose -batch_size 1 \\\n  -beam_size 1 \\\n  -seed 3 \\\n  -random_sampling_topk -1 \\\n  -random_sampling_topp 0.95 \\\n  -random_sampling_temp 1 \\\n  -ban_unk_token \\\n  -length_penalty none \\\n  -out /tmp/gen\ndiff data/data_lm/gen-nucleus-sampling-sol.txt /tmp/gen && rm /tmp/gen\n', 'python translate.py -model onmt/tests/test_model_lm.pt \\\n  -src data/data_lm/src-gen.txt \\\n  -verbose -batch_size 1 \\\n  -beam_size 10 \\\n  -seed 2 \\\n  -random_sampling_topk 50 \\\n  -random_sampling_topp 0.95 \\\n  -random_sampling_temp 1 \\\n  -length_penalty avg \\\n  -ban_unk_token \\\n  -min_length 5 \\\n  -out /tmp/gen\ndiff data/data_lm/gen-sampling-beams-sol.txt /tmp/gen && rm /tmp/gen\n', 'python tools/extract_vocabulary.py \\\n  -model onmt/tests/test_model.pt \\\n  -side src \\\n  -out_file /tmp/onmt.vocab.txt\nif ! wc -l /tmp/onmt.vocab.txt | grep -qF  ""1002""\n  then echo ""wrong word count"" && exit 1\nelse\n  echo ""create vocabulary pass""\nfi\n', 'python tools/embeddings_to_torch.py \\\n  -emb_file_enc onmt/tests/sample_glove.txt \\\n  -emb_file_dec onmt/tests/sample_glove.txt \\\n  -dict_file onmt/tests/test_model.pt \\\n  -output_file /tmp/q_gloveembeddings \\\n  && rm /tmp/q_gloveembeddings*\n', 'python tools/extract_embeddings.py \\\n  -model onmt/tests/test_model.pt\n', 'python train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -hidden_size 2 \\\n  -batch_size 10 \\\n  -word_vec_size 5 \\\n  -report_every 5\\\n  -hidden_size 10 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -train_steps 10 \\\n  -save_model /tmp/onmt.model \\\n  -save_checkpoint_steps 10\nsed -i \'1s/^/new_tok\\t100000000\\n/\' /tmp/onmt.vocab.src\npython train.py \\\n  -config data/data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.tgt \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -hidden_size 2 \\\n  -batch_size 10 \\\n  -word_vec_size 5 \\\n  -report_every 5\\\n  -hidden_size 10 \\\n  -train_steps 20 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -update_vocab \\\n  -reset_optim ""states"" \\\n  -train_from /tmp/onmt.model_step_10.pt\n', 'python train.py \\\n  -config data/lm_data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.src \\\n  -model_task lm \\\n  -encoder_type transformer_lm \\\n  -decoder_type transformer_lm \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -dec_layers 2 -batch_size 10 \\\n  -heads 4 -transformer_ff 64 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -word_vec_size 16 -report_every 5 \\\n  -save_model /tmp/lm.onmt.model \\\n  -save_checkpoint_steps 10 \\\n  -hidden_size 16 -train_steps 10\nsed -i \'1s/^/new_tok2\\t100000000\\n/\' /tmp/onmt.vocab.src\npython train.py \\\n  -config data/lm_data.yaml \\\n  -src_vocab /tmp/onmt.vocab.src \\\n  -tgt_vocab /tmp/onmt.vocab.src \\\n  -model_task lm \\\n  -encoder_type transformer_lm \\\n  -decoder_type transformer_lm \\\n  -src_vocab_size 1000 \\\n  -tgt_vocab_size 1000 \\\n  -num_workers 0 -bucket_size 1024 \\\n  -dec_layers 2 -batch_size 10 \\\n  -heads 4 -transformer_ff 64 \\\n  -word_vec_size 16 -report_every 5 \\\n  -hidden_size 16  -train_steps 20 \\\n  -update_vocab -reset_optim ""states"" \\\n  -train_from /tmp/lm.onmt.model_step_10.pt\n', 'python -m pip install --upgrade pip\npip install --upgrade setuptools\npip install -e .\npip install -r docs/requirements.txt\n', 'set -e\n# Check that docs are built without errors\ncd docs/ && make html && cd ..\n', 'python -m pip install --upgrade pip\npip install --upgrade setuptools\npip install -e .\npip install -r docs/requirements.txt\n', 'set -e\n# Check that docs are built without errors\ncd docs/ && make html && cd ..\n', 'python -m pip install --upgrade pip\npip install --upgrade setuptools wheel\n', 'python setup.py sdist bdist_wheel\n']"
"['python -m pip install --upgrade pip\npip install -r test-requirements.txt\n', 'pip install pytest\npip install pytest-cov\npytest --cov=./ --cov-report=xml\n', 'pipx run tox -e type', 'python -m pip install --upgrade pip\npip install -r test-requirements.txt\n', 'python -m tests\n']"
"['pip install -e .[lint]\nblack --version\ngit fetch --depth=1 origin $BASE_SHA\necho ""Files Changed:""\ngit diff --name-only $BASE_SHA... | tee .diff_names.txt\nNAMES=$(cat .diff_names.txt | python scripts/pyfile_exists.py)\nif test -z $NAMES\nthen\n  black --diff --check .\nelse\n  echo $NAMES | xargs black --diff --check\nfi\nmypy --version\nmypy\n', '#install utils\npip install coveralls\npip install -e "".[dev-noks]""\n# Get version info\npip freeze\nz3 --version\n#install cvc4\nsudo wget -O /usr/bin/cvc4 https://github.com/CVC4/CVC4/releases/download/1.7/cvc4-1.7-x86_64-linux-opt\nsudo chmod +x /usr/bin/cvc4\ncvc4 --version\n#install yices\nsudo wget -O yices.tar.gz https://yices.csl.sri.com/releases/2.6.2/yices-2.6.2-x86_64-pc-linux-gnu-static-gmp.tar.gz\nsudo tar -xzf yices.tar.gz\ncd yices-2.6.2\nsudo ./install-yices\nyices --version\n#install boolector\nmkdir -p /tmp/build\ncd /tmp/build\ngit clone https://github.com/boolector/boolector.git\ncd boolector\n# Version 3.2.1\ngit checkout ""f61c0dcf4a76e2f7766a6358bfb9c16ca8217224""\ngit log -1 --oneline > ../boolector.commit\n./contrib/setup-lingeling.sh\n./contrib/setup-btor2tools.sh\n./configure.sh\ncd build\nmake -j4\nmkdir -p /tmp/boolector\nsudo make DESTDIR=/usr install\n# Install solc unconditionally because it only takes a second or two\nsudo wget -O /usr/bin/solc https://github.com/ethereum/solidity/releases/download/v0.4.24/solc-static-linux\nsudo chmod +x /usr/bin/solc\n', 'cp scripts/run_tests.sh .\n./run_tests.sh\n', 'set +e\ncounter=1\nwhile [ ""${counter}"" -le 10 ] ; do\n  echo ""Attempt ${counter} to upload""\n  $(coveralls --service=github)\n  if [ $? -eq 0 ] ; then\n      exit 0\n  fi\n  ((counter++))\n  sleep 1\ndone\nexit 1\n', '# just command runner https://github.com/casey/just#pre-built-binaries\ncurl --proto \'=https\' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to ""${HOME}/.local/bin""\nsudo wget -O /usr/bin/solc https://github.com/ethereum/solidity/releases/download/v0.4.24/solc-static-linux\nsudo chmod +x /usr/bin/solc\n', 'just init\nsource venv/bin/activate\njust lint\n', 'source venv/bin/activate\njust test\n', 'python3 -m pip install wheel\npython3 setup.py --dev_release sdist bdist_wheel\n', 'EXTRAS=""dev-noks""\npip install -e .[$EXTRAS]\n', 'brew install bash\nbrew install wabt\nbrew install SRI-CSL/sri-csl/yices2\nbrew tap cvc4/cvc4\nbrew install cvc4/cvc4/cvc4\npip install solc-select\nsolc-select install 0.4.26\nsolc-select use 0.4.26\n', 'cp scripts/run_tests.sh .\n./run_tests.sh\n', 'python -m pip install --upgrade pip setuptools\npython -m pip install .\n', '#install utils\npip install coveralls\npip install -e "".[dev-noks]""\n#install cvc4\nsudo wget -O /usr/bin/cvc4 https://github.com/CVC4/CVC4/releases/download/1.7/cvc4-1.7-x86_64-linux-opt\nsudo chmod +x /usr/bin/cvc4\n#install yices\nsudo add-apt-repository ppa:sri-csl/formal-methods\nsudo apt-get update\nsudo apt-get install yices2\n#install boolector\nmkdir -p /tmp/build\ncd /tmp/build\ngit clone https://github.com/boolector/boolector.git\ncd boolector\n# Version 3.2.1\ngit checkout ""f61c0dcf4a76e2f7766a6358bfb9c16ca8217224""\ngit log -1 --oneline > ../boolector.commit\n./contrib/setup-lingeling.sh\n./contrib/setup-btor2tools.sh\n./configure.sh\ncd build\nmake -j4\nmkdir -p /tmp/boolector\nsudo make DESTDIR=/usr install\n# Install solc unconditionally because it only takes a second or two\nsudo wget -O /usr/bin/solc https://github.com/ethereum/solidity/releases/download/v0.4.24/solc-static-linux\nsudo chmod +x /usr/bin/solc\n', 'cp scripts/run_tests.sh .\n./run_tests.sh\n', 'python3 -m pip install wheel\npython3 setup.py sdist bdist_wheel\n']"
""
"['./scripts/cibuild', './scripts/cibuild', './scripts/cipublish']"
"['git submodule update --init --recursive\n', 'python -m pip install --upgrade pip\nif [ ""${{ matrix.python-version }}"" = ""2.7"" ]; then\n  pip install numpy==1.16.5\nfi\npip install -r requirements.txt\n', 'pip install .\n', 'pip install flake8\nflake8 .\n', 'pip install pytest\npytest tests\n', 'rm -f dist/*.tar.gz\npython setup.py sdist\npip install dist/*.tar.gz\n']"
"['sudo apt-get install portaudio19-dev python3-dev', 'brew install portaudio', 'python3 -m pip install --upgrade pip wheel setuptools', 'python3 -m pip install pywin32', 'python3 -m pip install -e "".[test]"" pytest-cov codecov', 'git clone https://github.com/ankitects/anki.git anki_upstream\ncd anki_upstream\ngit reset --hard ${{ matrix.anki-version }}\npython3 -m pip install -r requirements.txt\n', 'python3 -m pytest tests -vv --cov genanki --cov-report term-missing:skip-covered --no-cov-on-fail\n', 'codecov']"
"['pdm install -v', 'pdm run tox -v', 'pdm run tox -eextra -v']"
"['docker build .', 'sudo apt update\nsudo apt install -y libpq-dev libffi-dev libssl-dev libkrb5-dev zlib1g-dev latexmk texlive-latex-recommended tex-gyre texlive-latex-extra\n', 'make install-python', '. venv/bin/activate\nmake docs\n', '. venv/bin/activate\nmake docs-epub\n', '. venv/bin/activate\nmake docs-pdf\n', 'cd web\nyarn install\n', 'cd web\nyarn run linter\n', 'sudo apt update\nsudo apt install -y libpq-dev libffi-dev libssl-dev libkrb5-dev zlib1g-dev\n', 'make install-python', '. venv/bin/activate\nmake pip\n', 'sudo pip install --upgrade pip\nsudo pip install pycodestyle\n', 'pycodestyle --config=.pycodestyle docs/', 'pycodestyle --config=.pycodestyle pkg/', 'pycodestyle --config=.pycodestyle web/', 'pycodestyle --config=.pycodestyle tools/', 'sudo apt update\nsudo apt install -y libpq-dev libffi-dev libssl-dev libkrb5-dev zlib1g-dev\n', 'make install-python', '. venv/bin/activate\nmake src\n', 'sudo apt update\nsudo apt install -y python3-babel python3-jinja2\n', 'make msg-extract', 'make msg-update', 'make msg-compile', 'yarn set version berry\n', 'cd web\nyarn install\n', 'cd web\nyarn run test:karma-once\n', 'sudo su -c \'echo ""deb [arch=amd64] https://apt.enterprisedb.com/$(lsb_release -cs)-edb/ $(lsb_release -cs) main"" > /etc/apt/sources.list.d/edb-$(lsb_release -cs).list\'\nsudo su -c \'echo ""machine apt.enterprisedb.com login ${{ secrets.EDB_REPO_USERNAME }} password ${{ secrets.EDB_REPO_PASSWORD }}"" > /etc/apt/auth.conf.d/edb.conf\'\nsudo wget -q -O - https://apt.enterprisedb.com/edb-deb.gpg.key | sudo apt-key add -\n', 'sudo apt update\nsudo apt install -y libpq-dev libffi-dev libssl-dev libkrb5-dev zlib1g-dev edb-as${{ matrix.pgver }}-server edb-as${{ matrix.pgver }}-server-pldebugger edb-as${{ matrix.pgver }}-pgagent\n', 'FOR /f ""delims="" %%F IN (\'python tools\\get_sb_package.py ""${{ secrets.EDB_SBP_URL }}"" ""edb_as${{ matrix.pgver }}_dbserver"" ""windows-x64""\') DO SET INSTALLER_EXE=%%F\nECHO Running %INSTALLER_EXE%...\n%INSTALLER_EXE% --prefix C:\\EPAS\\${{ matrix.pgver }} --datadir C:\\EPAS\\${{ matrix.pgver }}\\data --serverport 58${{ matrix.pgver }} --superpassword enterprisedb --create_samples no --install_runtimes 0 --mode unattended --unattendedmodeui none --disable-components stackbuilderplus${{ matrix.pgver < 15 && \',pgadmin4\' || \'\' }}\n\nchoco install -y mitkerberos\n\nREM Ignore error 3010 (reboot required)\nIF %ERRORLEVEL% EQU 3010 cmd /c ""exit /b 0""\n', 'sudo mkdir -p /var/lib/edb-as/tablespaces/${{ matrix.pgver }}\nsudo chown enterprisedb:enterprisedb /var/lib/edb-as/tablespaces/${{ matrix.pgver }}\n', 'mkdir ""C:\\EPAS\\${{ matrix.pgver }}\\tablespaces""\nicacls ""C:\\EPAS\\${{ matrix.pgver }}\\tablespaces"" /grant ""NETWORK SERVICE"":(OI)(CI)F /T\n', '# Note: we use a custom port for PostgreSQL as the runner may already have a version of PostgreSQL installed\nsudo su -c ""echo local all all trust > /etc/edb-as/${{ matrix.pgver }}/main/pg_hba.conf""\nsudo sed -i ""s/port = 544[0-9]/port = 58${{ matrix.pgver }}/g"" /etc/edb-as/${{ matrix.pgver }}/main/postgresql.conf\nsudo sed -i ""s/shared_preload_libraries = \'/shared_preload_libraries = \'\\$libdir\\/plugin_debugger,/g"" /etc/edb-as/${{ matrix.pgver }}/main/postgresql.conf\nsudo su - enterprisedb -c ""mkdir -p /var/run/edb-as/${{ matrix.pgver }}-main.epas_stat_tmp""\nsudo systemctl restart edb-as@${{ matrix.pgver }}-main\n\nuntil sudo runuser -l enterprisedb -c ""/usr/lib/edb-as/${{ matrix.pgver }}/bin/pg_isready -p 58${{ matrix.pgver }}"" 2>/dev/null; do\n  >&2 echo ""EPAS is unavailable - sleeping for 2 seconds""\n  sleep 2\ndone\n\npsql -U enterprisedb -d postgres -p 58${{ matrix.pgver }} -c \'CREATE EXTENSION IF NOT EXISTS pgagent;\'\n', 'make install-python-testing', 'SET LIB=C:\\EPAS\\${{ matrix.pgver }}\\lib;%LIB%\nSET INCLUDE=C:\\EPAS\\${{ matrix.pgver }}\\include;%INCLUDE%\npython -m venv venv\ncall venv\\Scripts\\activate.bat\npython -m pip install --upgrade pip\npip install wheel sphinx sphinxcontrib-youtube -r web\\regression\\requirements.txt\n', 'cat <<EOF > web/config_local.py\nfrom config import *\n\n# Debug mode\nDEBUG = True\n\n# App mode\nSERVER_MODE = False\n\n# Log\nCONSOLE_LOG_LEVEL = DEBUG\nFILE_LOG_LEVEL = DEBUG\n\nDEFAULT_SERVER = \'127.0.0.1\'\n\nUPGRADE_CHECK_ENABLED = False\n\nLOG_FILE = ""$(pwd)/var/pgadmin4.log""\nSESSION_DB_PATH = ""$(pwd)/var/sessions""\nSTORAGE_DIR = ""$(pwd)/var/storage""\nSQLITE_PATH = ""$(pwd)/var/pgadmin4.db""\nTEST_SQLITE_PATH = ""$(pwd)/var/pgadmin4.db""\nAZURE_CREDENTIAL_CACHE_DIR = ""$(pwd)/var/azurecredentialcache""\nEOF\n\ncat <<EOF > web/regression/test_config.json\n{\n  ""pgAdmin4_login_credentials"": {\n    ""new_password"": ""NEWPASSWORD"",\n    ""login_password"": ""PASSWORD"",\n    ""login_username"": ""USER@EXAMPLE.COM""\n  },\n  ""pgAdmin4_test_user_credentials"": {\n    ""new_password"": ""NEWPASSWORD"",\n    ""login_password"": ""PASSWORD"",\n    ""login_username"": ""USER2@EXAMPLE.COM""\n  },\n  ""pgAdmin4_test_non_admin_credentials"": {\n    ""new_password"": ""NEWPASSWORD"",\n    ""login_password"": ""PASSWORD"",\n    ""login_username"": ""USER@EXAMPLE.COM""\n  },\n  ""server_group"": 1,\n  ""server_credentials"": [\n    {\n      ""name"": ""EPAS ${{ matrix.pgver }}"",\n      ""comment"": ""EPAS ${{ matrix.pgver }} Server"",\n      ""db_username"": ""enterprisedb"",\n      ""host"": ""/var/run/edb-as"",\n      ""db_password"": """",\n      ""db_port"": 58${{ matrix.pgver }},\n      ""maintenance_db"": ""postgres"",\n      ""sslmode"": ""prefer"",\n      ""tablespace_path"": ""/var/lib/edb-as/tablespaces/${{ matrix.pgver }}"",\n      ""enabled"": true,\n      ""default_binary_paths"": {\n        ""pg"": """",\n        ""ppas"": ""/usr/lib/edb-as/${{ matrix.pgver }}/bin""\n      }\n    }\n  ],\n  ""server_update_data"": [\n    {\n      ""comment"": ""This is test update comment""\n    }\n  ]\n}\nEOF\n', 'FOR /f ""delims="" %%D IN (\'python -c ""import os; print(os.getcwd().replace(\'\\\\\', \'\\\\\\\\\'))""\') DO SET WORKING_DIR=%%D\n\n> web\\config_local.py (\n  @echo.from config import *\n  @echo.\n  @echo.# Debug mode\n  @echo.DEBUG = True\n  @echo.\n  @echo.# App mode\n  @echo.SERVER_MODE = False\n  @echo.\n  @echo.# Log\n  @echo.CONSOLE_LOG_LEVEL = DEBUG\n  @echo.FILE_LOG_LEVEL = DEBUG\n  @echo.\n  @echo.DEFAULT_SERVER = \'127.0.0.1\'\n  @echo.\n  @echo.UPGRADE_CHECK_ENABLED = False\n  @echo.\n  @echo.LOG_FILE = ""%WORKING_DIR%\\\\var\\\\pgadmin4.log""\n  @echo.SESSION_DB_PATH = ""%WORKING_DIR%\\\\var\\\\sessions""\n  @echo.STORAGE_DIR = ""%WORKING_DIR%\\\\var\\\\storage""\n  @echo.SQLITE_PATH = ""%WORKING_DIR%\\\\var\\\\pgadmin4.db""\n  @echo.TEST_SQLITE_PATH = ""%WORKING_DIR%\\\\var\\\\test_pgadmin4.db""\n  @echo.AZURE_CREDENTIAL_CACHE_DIR = ""%WORKING_DIR%\\\\var\\\\azurecredentialcache""\n)\n\n> web\\regression\\test_config.json (\n  @echo.{\n  @echo.  ""pgAdmin4_login_credentials"": {\n  @echo.    ""new_password"": ""NEWPASSWORD"",\n  @echo.    ""login_password"": ""PASSWORD"",\n  @echo.    ""login_username"": ""USER@EXAMPLE.COM""\n  @echo.  },\n  @echo.  ""pgAdmin4_test_user_credentials"": {\n  @echo.    ""new_password"": ""NEWPASSWORD"",\n  @echo.    ""login_password"": ""PASSWORD"",\n  @echo.    ""login_username"": ""USER2@EXAMPLE.COM""\n  @echo.  },\n  @echo.  ""pgAdmin4_test_non_admin_credentials"": {\n  @echo.    ""new_password"": ""NEWPASSWORD"",\n  @echo.    ""login_password"": ""PASSWORD"",\n  @echo.    ""login_username"": ""USER@EXAMPLE.COM""\n  @echo.  },\n  @echo.  ""server_group"": 1,\n  @echo.  ""server_credentials"": [\n  @echo.    {\n  @echo.      ""name"": ""EPAS ${{ matrix.pgver }}"",\n  @echo.      ""comment"": ""EPAS ${{ matrix.pgver }} Server"",\n  @echo.      ""db_username"": ""enterprisedb"",\n  @echo.      ""host"": ""127.0.0.1"",\n  @echo.      ""db_password"": ""enterprisedb"",\n  @echo.      ""db_port"": 58${{ matrix.pgver }},\n  @echo.      ""maintenance_db"": ""postgres"",\n  @echo.      ""sslmode"": ""prefer"",\n  @echo.      ""tablespace_path"": ""C:\\\\EPAS\\\\${{ matrix.pgver }}\\\\tablespaces"",\n  @echo.      ""enabled"": true,\n  @echo.      ""default_binary_paths"": {\n  @echo.        ""pg"": """",\n  @echo.        ""ppas"": ""C:\\\\EPAS\\\\${{ matrix.pgver }}\\\\bin""\n  @echo.      }\n  @echo.    }\n  @echo.  ],\n  @echo.  ""server_update_data"": [\n  @echo.    {\n  @echo.      ""comment"": ""This is test update comment""\n  @echo.    }\n  @echo.  ]\n  @echo.}\n)\n', '. venv/bin/activate\nmake check-python\n', 'call venv\\Scripts\\activate.bat\npython web\\regression\\runtests.py --exclude feature_tests\n', 'sudo sh -c \'echo ""deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main"" > /etc/apt/sources.list.d/pgdg.list\'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n', 'sudo apt update\nsudo apt install -y libpq-dev libffi-dev libssl-dev libkrb5-dev zlib1g-dev postgresql-${{ matrix.pgver }} postgresql-${{ matrix.pgver }}-pldebugger pgagent\n', 'brew install postgresql@${{ matrix.pgver }}\n', 'if exist ""C:\\Program Files\\PostgreSQL\\{{ matrix.pgver }}\\uninstall-postgresql.exe"" (\n    ""C:\\Program Files\\PostgreSQL\\{{ matrix.pgver }}\\uninstall-postgresql.exe"" --mode unattended\n)\nreg delete ""HKLM\\SOFTWARE\\PostgreSQL"" /f\n', 'choco install -y postgresql${{ matrix.pgver }} --params ""/Password:postgres /Port:59${{ matrix.pgver }}"" --ia ""--prefix C:\\PostgreSQL\\${{ matrix.pgver }} --datadir C:\\PostgreSQL\\${{ matrix.pgver }}\\data --install_runtimes 0 --disable-components pgAdmin,stackbuilder --enable-components server,commandlinetools""\nchoco install -y mitkerberos\n\nREM Ignore error 3010 (reboot required)\nIF %ERRORLEVEL% EQU 3010 cmd /c ""exit /b 0""\n', 'sudo mkdir -p /var/lib/postgresql/tablespaces/${{ matrix.pgver }}\nsudo chown postgres:postgres /var/lib/postgresql/tablespaces/${{ matrix.pgver }}\n', 'mkdir -p /usr/local/var/tablespaces/${{ matrix.pgver }}\n', 'mkdir ""C:\\PostgreSQL\\${{ matrix.pgver }}\\tablespaces""\nicacls ""C:\\PostgreSQL\\${{ matrix.pgver }}\\tablespaces"" /grant ""NETWORK SERVICE"":(OI)(CI)F /T\n', 'sudo su -c ""echo local all all trust > /etc/postgresql/${{ matrix.pgver }}/main/pg_hba.conf""\nsudo sed -i ""s/port = 543[0-9]/port = 59${{ matrix.pgver }}/g"" /etc/postgresql/${{ matrix.pgver }}/main/postgresql.conf\nsudo sed -i ""s/#shared_preload_libraries = \'\'/shared_preload_libraries = \'\\$libdir\\/plugin_debugger\'/g"" /etc/postgresql/${{ matrix.pgver }}/main/postgresql.conf\nsudo su - postgres -c ""/usr/lib/postgresql/${{ matrix.pgver }}/bin/postgres -D /var/lib/postgresql/${{ matrix.pgver }}/main -c config_file=/etc/postgresql/${{ matrix.pgver }}/main/postgresql.conf &""\n\nuntil sudo runuser -l postgres -c ""pg_isready -p 59${{ matrix.pgver }}"" 2>/dev/null; do\n  >&2 echo ""Postgres is unavailable - sleeping for 2 seconds""\n  sleep 2\ndone\n\npsql -U postgres -p 59${{ matrix.pgver }} -c \'CREATE EXTENSION pgagent;\'\npsql -U postgres -p 59${{ matrix.pgver }} -c \'CREATE EXTENSION pldbgapi;\'\n', 'echo local all all trust > /usr/local/var/postgresql@${{ matrix.pgver }}/pg_hba.conf\nsed -i \'\' ""s/#port = 543[0-9]/port = 59${{ matrix.pgver }}/g"" /usr/local/var/postgresql@${{ matrix.pgver }}/postgresql.conf\nbrew services restart postgresql@${{ matrix.pgver }}\n\nuntil /usr/local/opt/postgresql@${{ matrix.pgver }}/bin/pg_isready -p 59${{ matrix.pgver }} 2>/dev/null; do\n  >&2 echo ""Postgres is unavailable - sleeping for 2 seconds""\n  sleep 2\ndone\n\npsql postgres -p 59${{ matrix.pgver }} -c \'CREATE ROLE postgres SUPERUSER LOGIN;\'\n', 'make install-python-testing', 'SET LIB=C:\\PostgreSQL\\${{ matrix.pgver }}\\lib;%LIB%\nSET INCLUDE=C:\\PostgreSQL\\${{ matrix.pgver }}\\include;%INCLUDE%\npython -m venv venv\ncall venv\\Scripts\\activate.bat\npython -m pip install --upgrade pip\npip install wheel sphinx sphinxcontrib-youtube -r web\\regression\\requirements.txt\n', 'cat <<EOF > web/config_local.py\nfrom config import *\n\n# Debug mode\nDEBUG = True\n\n# App mode\nSERVER_MODE = False\n\n# Log\nCONSOLE_LOG_LEVEL = DEBUG\nFILE_LOG_LEVEL = DEBUG\n\nDEFAULT_SERVER = \'127.0.0.1\'\n\nUPGRADE_CHECK_ENABLED = False\n\nLOG_FILE = ""$(pwd)/var/pgadmin4.log""\nSESSION_DB_PATH = ""$(pwd)/var/sessions""\nSTORAGE_DIR = ""$(pwd)/var/storage""\nSQLITE_PATH = ""$(pwd)/var/pgadmin4.db""\nTEST_SQLITE_PATH = ""$(pwd)/var/pgadmin4.db""\nAZURE_CREDENTIAL_CACHE_DIR = ""$(pwd)/var/azurecredentialcache""\nEOF\n\ncat <<EOF > web/regression/test_config.json\n{\n  ""pgAdmin4_login_credentials"": {\n    ""new_password"": ""NEWPASSWORD"",\n    ""login_password"": ""PASSWORD"",\n    ""login_username"": ""USER@EXAMPLE.COM""\n  },\n  ""pgAdmin4_test_user_credentials"": {\n    ""new_password"": ""NEWPASSWORD"",\n    ""login_password"": ""PASSWORD"",\n    ""login_username"": ""USER2@EXAMPLE.COM""\n  },\n  ""pgAdmin4_test_non_admin_credentials"": {\n    ""new_password"": ""NEWPASSWORD"",\n    ""login_password"": ""PASSWORD"",\n    ""login_username"": ""USER@EXAMPLE.COM""\n  },\n  ""server_group"": 1,\n  ""server_credentials"": [\n    {\n      ""name"": ""PostgreSQL ${{ matrix.pgver }}"",\n      ""comment"": ""PostgreSQL ${{ matrix.pgver }} Server"",\n      ""db_username"": ""postgres"",\n      ""host"": ""${{ matrix.os == \'macos-latest\' && \'/tmp\' || matrix.os == \'ubuntu-latest\' && \'/var/run/postgresql\' || \'127.0.0.1\' }}"",\n      ""db_password"": ""postgres"",\n      ""db_port"": 59${{ matrix.pgver }},\n      ""maintenance_db"": ""postgres"",\n      ""sslmode"": ""prefer"",\n      ""tablespace_path"": ""${{ matrix.os == \'macos-latest\' && format(\'/usr/local/var/tablespaces/{0}\', matrix.pgver) || format(\'/var/lib/postgresql/tablespaces/{0}\', matrix.pgver) }}"",\n      ""enabled"": true,\n      ""default_binary_paths"": {\n        ""pg"": ""${{ matrix.os == \'macos-latest\' && format(\'/usr/local/opt/postgresql@{0}/bin\', matrix.pgver) || format(\'/usr/lib/postgresql/{0}/bin\', matrix.pgver) }}"",\n        ""ppas"": """"\n      }\n    }\n  ],\n  ""server_update_data"": [\n    {\n      ""comment"": ""This is test update comment""\n    }\n  ]\n}\nEOF\n', 'FOR /f ""delims="" %%D IN (\'python -c ""import os; print(os.getcwd().replace(\'\\\\\', \'\\\\\\\\\'))""\') DO SET WORKING_DIR=%%D\n\n> web\\config_local.py (\n  @echo.from config import *\n  @echo.\n  @echo.# Debug mode\n  @echo.DEBUG = True\n  @echo.\n  @echo.# App mode\n  @echo.SERVER_MODE = False\n  @echo.\n  @echo.# Log\n  @echo.CONSOLE_LOG_LEVEL = DEBUG\n  @echo.FILE_LOG_LEVEL = DEBUG\n  @echo.\n  @echo.DEFAULT_SERVER = \'127.0.0.1\'\n  @echo.\n  @echo.UPGRADE_CHECK_ENABLED = False\n  @echo.\n  @echo.LOG_FILE = ""%WORKING_DIR%\\\\var\\\\pgadmin4.log""\n  @echo.SESSION_DB_PATH = ""%WORKING_DIR%\\\\var\\\\sessions""\n  @echo.STORAGE_DIR = ""%WORKING_DIR%\\\\var\\\\storage""\n  @echo.SQLITE_PATH = ""%WORKING_DIR%\\\\var\\\\pgadmin4.db""\n  @echo.TEST_SQLITE_PATH = ""%WORKING_DIR%\\\\var\\\\test_pgadmin4.db""\n  @echo.AZURE_CREDENTIAL_CACHE_DIR = ""%WORKING_DIR%\\\\var\\\\azurecredentialcache""\n)\n\n> web\\regression\\test_config.json (\n  @echo.{\n  @echo.  ""pgAdmin4_login_credentials"": {\n  @echo.    ""new_password"": ""NEWPASSWORD"",\n  @echo.    ""login_password"": ""PASSWORD"",\n  @echo.    ""login_username"": ""USER@EXAMPLE.COM""\n  @echo.  },\n  @echo.  ""pgAdmin4_test_user_credentials"": {\n  @echo.    ""new_password"": ""NEWPASSWORD"",\n  @echo.    ""login_password"": ""PASSWORD"",\n  @echo.    ""login_username"": ""USER2@EXAMPLE.COM""\n  @echo.  },\n  @echo.  ""pgAdmin4_test_non_admin_credentials"": {\n  @echo.    ""new_password"": ""NEWPASSWORD"",\n  @echo.    ""login_password"": ""PASSWORD"",\n  @echo.    ""login_username"": ""USER@EXAMPLE.COM""\n  @echo.  },\n  @echo.  ""server_group"": 1,\n  @echo.  ""server_credentials"": [\n  @echo.    {\n  @echo.      ""name"": ""PostgreSQL ${{ matrix.pgver }}"",\n  @echo.      ""comment"": ""PostgreSQL ${{ matrix.pgver }} Server"",\n  @echo.      ""db_username"": ""postgres"",\n  @echo.      ""host"": ""127.0.0.1"",\n  @echo.      ""db_password"": ""postgres"",\n  @echo.      ""db_port"": 59${{ matrix.pgver }},\n  @echo.      ""maintenance_db"": ""postgres"",\n  @echo.      ""sslmode"": ""prefer"",\n  @echo.      ""tablespace_path"": ""C:\\\\PostgreSQL\\\\${{ matrix.pgver }}\\\\tablespaces"",\n  @echo.      ""enabled"": true,\n  @echo.      ""default_binary_paths"": {\n  @echo.        ""pg"": ""C:\\\\PostgreSQL\\\\${{ matrix.pgver }}\\\\bin"",\n  @echo.        ""ppas"": """"\n  @echo.      }\n  @echo.    }\n  @echo.  ],\n  @echo.  ""server_update_data"": [\n  @echo.    {\n  @echo.      ""comment"": ""This is test update comment""\n  @echo.    }\n  @echo.  ]\n  @echo.}\n)\n', '. venv/bin/activate\nmake check-python\n', 'call venv\\Scripts\\activate.bat\npython web\\regression\\runtests.py --exclude feature_tests\n', 'cat <<EOF > sonar-project.properties\nsonar.projectKey=${{ vars.SONARQUBE_PROJECT_KEY }}\nsonar.projectName=pgAdmin 4\nsonar.projectVersion=%VERSION%\n\n# Ignore templates and SQL scripts as they confuse the scanner\nsonar.exclusions=**/templates/**/*, **/*.sql\n\n# Let SonarQube know where tests can be found\nsonar.test.inclusions=**/tests/**, web/regression\n\n# Python compatibility\nsonar.python.version=3.7, 3.8, 3.9, 3.10, 3.11\nEOF\n\nAPP_RELEASE=`grep ""^APP_RELEASE"" web/config.py | cut -d""="" -f2 | sed \'s/ //g\'`\nAPP_REVISION=`grep ""^APP_REVISION"" web/config.py | cut -d""="" -f2 | sed \'s/ //g\'`\nAPP_LONG_VERSION=${APP_RELEASE}.${APP_REVISION}\nsed -i ""s/%VERSION%/${APP_LONG_VERSION}/g"" sonar-project.properties\n']"
['pip install tox\ntox -- --cov perfplot --cov-report xml --cov-report term\n']
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox', 'pip install wheel', 'python setup.py bdist_wheel']"
"['python -m pip install --upgrade pip\npython -m pip install cibuildwheel==2.3.1\n', 'pwd\nls\npython -m cibuildwheel --output-dir dist/\n', '# OpenMP libraries are not installed by default on macos\nclang --version\nclang --version | grep ""13\\.""\n# hardcoded, not ideal\n# https://mac.r-project.org/openmp/\ncurl -O https://mac.r-project.org/openmp/openmp-13.0.0-darwin21-Release.tar.gz\nsudo tar fvxz openmp-13.0.0-darwin21-Release.tar.gz -C /\nls /usr/local/lib\nls /usr/local/include\n', 'python -m pip install --upgrade pip\npython -m pip install pytest pytest-benchmark build\npython -m pip install numpy scipy matplotlib>=3.0.0\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'export LDFLAGS=""-L/usr/local/lib""\nexport CPPFLAGS=""-I/usr/local/include""\npython -m pip install . -v\nls ${{ github.workspace }}/build || true\n', 'mv dtaidistance dtaidistance_pkg\npython ./util/check_installation.py\nmv dtaidistance_pkg dtaidistance\n', 'export LDFLAGS=""-L/usr/local/lib""\nexport CPPFLAGS=""-I/usr/local/include""\nmv dtaidistance dtaidistance_pkg\npytest --ignore=venv --benchmark-skip\nmv dtaidistance_pkg dtaidistance\n', 'python -m build\n', 'python -m pip install --upgrade pip\npython -m pip install pytest pytest-benchmark pytest-env build cython\npython -m pip install numpy scipy matplotlib>=3.0.0\n', 'python -m pip install . -v\nls ${{ github.workspace }}/build || true\n', 'mv dtaidistance dtaidistance_pkg\npython ./util/check_installation.py\nmv dtaidistance_pkg dtaidistance\n', 'mv dtaidistance dtaidistance_pkg\npytest --ignore=venv --benchmark-skip\nmv dtaidistance_pkg dtaidistance\n', 'python -m build\n', 'python -m pip install build\npython -m build --sdist\n']"
"['python -m pip install cibuildwheel==2.12.2', 'python -m cibuildwheel --output-dir wheelhouse .', 'python -m pip install --upgrade pip wheel setuptools>=61\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip list\n', 'python -m pip install -e . --no-build-isolation -v', 'echo ""PWD: ${PWD}""\npushd doc\nmake html\npopd\necho ""PWD: ${PWD}""\n', 'source ci/push-docs-gh-pages.sh']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get update\nsudo apt-get -y install build-essential doxygen ghp-import\npython3 -m pip install -U pip wheel\n', 'python3 -m pip install -r doc_requirements.txt\n', 'make doc\n', 'touch docs/build/.nojekyll\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit config --global user.email ""dlpack-gh-actions-bot@nomail""\ngit config --global user.name ""dlpack-gh-actions-bot""\nghp-import -m ""Generate DLPack website"" -b gh-pages docs/build\ngit push origin gh-pages -f\n', 'sudo apt-get update\nsudo apt-get -y install build-essential doxygen\npython3 -m pip install -U pip wheel\npython3 -m pip install cmake ninja\n\npython3 --version\npython3 -m pip --version\ndoxygen --version\nmake --version\ncmake --version\nninja --version\n', 'python3 -m pip install -r doc_requirements.txt\n', 'mkdir build\ncd build\ncmake .. -G Ninja -DCMAKE_INSTALL_PREFIX=./install -DBUILD_DOCS=ON\nninja\nninja install\n', 'make doc\n', 'python3 -m pip install cpplint\n', 'sudo apt-get install -y doxygen wget graphviz unzip\n', './tests/scripts/task_lint.sh\n', './tests/scripts/task_build.sh\n']"
"['docker-compose build && docker-compose pull', 'docker-compose run app /venv/bin/python manage.py migrate', 'docker-compose run app /venv/bin/python manage.py load_initial_data', 'docker-compose run app /venv/bin/python manage.py test', 'python -m pip install --upgrade pip\npip install -r requirements/development.txt\n', 'make lint-server\n', 'npm ci', 'make lint-client\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest pytest-cov\npip install -e .[all]\nsudo apt install unrar\n', 'wget http://www.atarimania.com/roms/Roms.rar \nunrar x  -o+  Roms.rar\nale-import-roms ./ROMS\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest --cov=mushroom_rl --cov-report=xml\n', 'python -m pip install --upgrade pip\npip install flake8 pytest pytest-cov\npip install -e .[all]\nsudo apt install unrar\n', 'wget http://www.atarimania.com/roms/Roms.rar \nunrar x  -o+  Roms.rar\nale-import-roms ./ROMS\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['pip install --upgrade pip\npython setup.py develop\npip install pygithub\n', 'base_name=$(uuidgen)\necho ""logfile=${base_name}.html"" >> $GITHUB_ENV\n\nif [ ""${{ github.event_name }}"" = ""issues"" ]; then\n    echo ""source=issue description"" >> $GITHUB_ENV\nelse\n    echo ""source=for comment #${{ github.event.comment.id }}"" >> $GITHUB_ENV\nfi\n\nif [ -e ""body"" ]; then\n    atvlog --output ${base_name}.html --format markdown body\nfi\n', 'mkdir -p ~/.cargo\ncat <<EOF > ~/.cargo/config.toml\n[net]\ngit-fetch-with-cli = true\nEOF\n', '# If not a release branch, add SHA1 to patch version\nif [ ""$(python scripts/version.py is_release)"" = ""false"" ]; then\n  python scripts/version.py sha1\nfi\n\npython -m pip install --upgrade setuptools pip wheel\npython setup.py sdist bdist_wheel\necho ""version=$(python scripts/version.py get)"" >> $GITHUB_OUTPUT\n', 'import os\nimport re\n\nname = None\nversion = None\nnotes = """"\nwith open(""CHANGES.md"", ""r"", encoding=""utf-8"") as handle:\n  for line in handle:\n    if line.startswith(""##""):\n      if name is None:\n        version, name = re.match(r""## ([0-9.]+) ([^(]+) \\(.*"", line).groups()\n      else:\n        break\n    if name is not None:\n      notes += line\n\nprint(f""name={name} >> "" + os.environ[""GITHUB_OUTPUT""])\nprint(f""version={version} >> "" + os.environ[""GITHUB_OUTPUT""])\n\nwith open(""notes.md"", ""w"", encoding=""utf-8"") as handle:\n  handle.write(notes)\n', 'echo ""::set-output name=timestamp::$(timestamp +%s)""', ""cat run_result 2>/dev/null || echo 'default'"", 'python -m pip install --upgrade setuptools pip pyyaml\n', 'python ./scripts/chickn.py -t miniaudio -t ${{ matrix.python-version }} ${{ matrix.chickn-args }}', 'echo ""::set-output name=run_result::success"" > run_result']"
"['sudo apt update\nsudo apt install enchant-2 hunspell aspell-en\n', 'python -m pip install -Ur doc/requirements.txt', 'make spelling', '[ ! -s _build/spelling/output.txt ]', 'pip3 install -Ur doc/requirements.txt', 'make linkcheck', 'python -m pip install -Ue ""src[dev]""', 'isort -c .', 'python -m pip install -Ue ""src[dev]""', 'flake8 .', 'python -m pip install -Ue ""src[dev]""', 'black --check .', 'python -m pip install -Ue ""src[dev]""', 'find -name ""*.html"" | xargs djhtml -c', 'python -m pip install twine wheel -Ue ""src[dev]""', 'sudo apt update\nsudo apt install gettext\n', 'check-manifest .', 'python -m build -n', 'twine check dist/*', 'sudo apt update\nsudo apt install gettext\n', 'sudo apt update && sudo apt install mariadb-client-10.6', 'python -m pip install -Ue ""src[dev]""', 'python -m pip install -Ue ""src[mysql]""', 'python -m pip install -Ue ""src[postgres]""', 'python manage.py check --deploy', 'python manage.py compilemessages', 'PRETALX_CONFIG_FILE=tests/ci_${{ matrix.database }}.cfg py.test -nauto -p no:sugar --cov=./ --cov-report=xml --reruns 3 tests --maxfail=100']"
"['pip install setuptools wheel\npip install .\npip install tox-gh-actions\n', 'python setup.py check', 'tox']"
"['gem install --no-document fpm', 'pip install --upgrade pip wheel setuptools\npip install .[all]\npip install -r scripts/build-requirements.txt\n', 'dvc pull', 'python scripts/build.py ${{ matrix.pkg }}', 'pip install --upgrade pip wheel', ""pip install setuptools_scm\necho version=$(python -m setuptools_scm | awk -F+ '{print $1}' | tail -1) >> $GITHUB_ENV\n"", './scripts/build_package.sh', 'pip install --upgrade pip wheel\npip install ""./dvc[testing]""\npip install -e ""./${{ matrix.plugin }}[tests]""\n', 'pytest -v -n=auto', 'pip install --upgrade pip wheel\npip install -e "".[dev]""\n', 'pytest -n=logical --timeout=300 --durations=100 --cov --cov-report=xml --cov-report=term\n']"
""
[]
"['# Set tag_majorminor\ntag_majorminor=$(git describe --tags --match=\'v*\'|sed \'s/^v//\'|cut -d. -f1,2)\necho ""tag_majorminor=${tag_majorminor}"" >> $GITHUB_OUTPUT\n\n# Set branch_majorminor\ncase ""$GITHUB_REF"" in\n  refs/heads/*) branch=${GITHUB_REF#refs/heads/};;\n  refs/pull/*)  branch=${GITHUB_BASE_REF#refs/heads/};;\nesac\nif [[ $branch == master ]]; then\n   prev_branch_majorminor=$(git for-each-ref --format=\'%(refname:lstrip=4)\' \'refs/remotes/origin/release/v*\' | sort --version-sort | tail -n1 | sed \'s/^v//\')\n   if [[ ""${prev_branch_majorminor%.*}"" == ""${tag_majorminor%.*}"" ]]; then\n     branch_majorminor=""${prev_branch_majorminor%.*}.$((${prev_branch_majorminor##*.}+1))""\n   else\n     branch_majorminor=""$((${prev_branch_majorminor%.*}+1)).0""\n   fi\nelse\n   branch_majorminor=${branch#release/v}\nfi\necho ""branch_majorminor=${branch_majorminor}"" >> $GITHUB_OUTPUT\n\n# Set relnotes_majorminor\nmake tools/bin/yq\nrelnotes_version=$(tools/bin/yq read docs/releaseNotes.yml items[0].version)\nrelnotes_majorminor=$(cut -d. -f1,2 <<<""$relnotes_version"")\necho ""relnotes_majorminor=${relnotes_majorminor}"" >> $GITHUB_OUTPUT\n\ndeclare -p tag_majorminor branch_majorminor relnotes_majorminor\n', 'tag_majorminor=${{ steps.get-versions.outputs.tag_majorminor }}\nbranch_majorminor=${{ steps.get-versions.outputs.branch_majorminor }}\nrelnotes_majorminor=${{ steps.get-versions.outputs.relnotes_majorminor }}\ndeclare -p tag_majorminor branch_majorminor relnotes_majorminor\n\n# Check that those all agree\nif [[ ""$tag_majorminor"" != ""$branch_majorminor"" ]]; then\n  echo ""You seem to be on the Git branch for v${branch_majorminor}.z, but Git tags indicate that this is work for v${tag_majorminor}.z""\n  echo ""Perhaps you need to go yell at the person who set up the release branch for the last .y bump.""\n  exit 1\nfi\nif [[ ""$relnotes_majorminor"" != ""$tag_majorminor"" ]]; then\n  echo ""Your Git tag+branch indicate that you are targeting v${tag_majorminor}.z but your docs/releaseNotes.yml indicate that you are targeting v${relnotes_majorminor}.z""\n  exit 1\nfi\n', 'make lint-deps\n', 'make lint\n', ""if [[ -n '${{ secrets.GHA_SSH_KEY }}' ]]; then\n  install -m700 -d ~/.ssh\n  install -m600 /dev/stdin ~/.ssh/id_rsa <<<'${{ secrets.GHA_SSH_KEY }}'\nfi\n"", 'make generate\n', 'echo ""Dependabot triggered a dependency update. Aborting workflow.""\nexit 1\n', 'make generate\n', ""if [[ -n '${{ secrets.GHA_SSH_KEY }}' ]]; then\n  install -m700 -d ~/.ssh\n  install -m600 /dev/stdin ~/.ssh/id_rsa <<<'${{ secrets.GHA_SSH_KEY }}'\nfi\n"", 'make check-envoy-version', 'make gotest\n', 'sudo sysctl -w fs.file-max=1600000\nsudo sysctl -w fs.inotify.max_user_instances=4096\n\nmake ci/setup-k3d\n', 'export DEV_KUBE_NO_PVC=yes\nexport KAT_REQ_LIMIT=900\nexport DEV_KUBECONFIG=~/.kube/config\nexport DEV_REGISTRY=${{ secrets.DEV_REGISTRY }}\nmake python-integration-test-environment\n', 'export DEV_KUBE_NO_PVC=yes\nexport KAT_REQ_LIMIT=900\nexport DEV_KUBECONFIG=~/.kube/config\nexport DEV_REGISTRY=${{ secrets.DEV_REGISTRY }}\nmake pytest-${{ matrix.test }}\n', 'export DEV_REGISTRY=${{ secrets.DEV_REGISTRY }}\nmake python-virtual-environment\n', ""export PYTEST_ARGS=' --cov-branch --cov=ambassador --cov-report html:/tmp/cov_html '\nmake pytest-unit-tests\n"", 'make ci/setup-k3d\nexport DEV_KUBECONFIG=~/.kube/config\n\nmake test-chart\n', 'make push\n', 'echo ""image-tag=$(build-aux/version.sh)"" >> $GITHUB_OUTPUT\n', 'make push-dev\n', 'echo ${{needs.build.outputs.image-tag}}', 'echo Pass', ""if [[ -n '${{ secrets.GHA_SSH_KEY }}' ]]; then\n  install -m700 -d ~/.ssh\n  install -m600 /dev/stdin ~/.ssh/id_rsa <<<'${{ secrets.GHA_SSH_KEY }}'\nfi\n"", 'make generate\n', 'make generate\n', 'sudo sysctl -w fs.file-max=1600000\nsudo sysctl -w fs.inotify.max_user_instances=4096\n\nmake ci/setup-k3d\n', 'export DEV_KUBE_NO_PVC=yes\nexport KAT_REQ_LIMIT=900\nexport DEV_KUBECONFIG=~/.kube/config\nexport DEV_REGISTRY=${{ secrets.DEV_REGISTRY }}\nmake python-integration-test-environment\n', 'export DEV_KUBE_NO_PVC=yes\nexport KAT_REQ_LIMIT=900\nexport DEV_KUBECONFIG=~/.kube/config\nexport DEV_REGISTRY=${{ secrets.DEV_REGISTRY }}\nmake pytest-${{ matrix.test }}\n', 'make release/promote-oss/to-ga\n', 'echo ""slack_webhook_url=${{secrets.SLACK_WEBHOOK_URL}}"" >> $GITHUB_OUTPUT', ""gh auth login --with-token <<<'${{ secrets.GH_GITHUB_API_KEY }}'\n"", 'make release/ga/create-gh-release\n', 'echo ""slack_webhook_url=${{secrets.SLACK_WEBHOOK_URL}}"" >> $GITHUB_OUTPUT', 'make release/promote-oss/to-rc\necho ""version=$(go run ./tools/src/goversion | sed s/^v//)"" >> $GITHUB_OUTPUT\necho ""chart_version=$(go run ./tools/src/goversion --dir-prefix=chart | sed s/^v//)"" >> $GITHUB_OUTPUT\n', 'echo ""slack_webhook_url=${{secrets.SLACK_WEBHOOK_URL}}"" >> $GITHUB_OUTPUT', 'make release/push-chart\n', 'echo ""slack_webhook_url=${{secrets.SLACK_WEBHOOK_URL}}"" >> $GITHUB_OUTPUT', ""gh auth login --with-token <<<'${{ secrets.GH_GITHUB_API_KEY }}'\n"", 'make release/chart-create-gh-release\n', 'echo ""slack_webhook_url=${{secrets.SLACK_WEBHOOK_URL}}"" >> $GITHUB_OUTPUT']"
"['cargo install grcov\nsudo apt-get install lcov\n', 'python -m pip install --upgrade pip setuptools wheel', 'python -m pip install -e .', ""set -e\npython -m pip install -r requirements-dev.txt qiskit-aer\nstestr run\n# We set the --source-dir to '.' because we want all paths to appear relative to the repo\n# root (we need to combine them with the Python ones), but we only care about `grcov`\n# keeping the `crates/*` files; we don't care about coverage in dependencies.\ngrcov . --binary-path target/debug/ --source-dir . --output-type lcov --output-path rust.info --llvm --branch --parallel --keep-only 'crates/*'\n"", 'set -e\ncoverage combine\ncoverage lcov -o python.info\nlcov --add-tracefile python.info --add-tracefile rust.info --output-file coveralls.info\n', 'python -m pip install -U pip setuptools wheel\npython -m pip install -U -r requirements.txt -c constraints.txt\npython -m pip install -U -r requirements-dev.txt coveralls -c constraints.txt\npython -m pip install -c constraints.txt -e .\npython -m pip install ""qiskit-ibmq-provider"" -c constraints.txt\npython -m pip install ""qiskit-aer""\n', 'make test_randomized', 'python -m pip install -U pip setuptools wheel\npython -m pip install -U -r requirements.txt -c constraints.txt\npython -m pip install -U -r requirements-dev.txt -c constraints.txt\npython -m pip install -c constraints.txt -e .\npython -m pip install ""qiskit-aer"" ""z3-solver"" ""cplex"" -c constraints.txt\n', 'stestr run', 'python -m pip install twine', 'twine upload ./wheelhouse/*.whl', 'python -m pip install twine', 'twine upload ./wheelhouse/*.whl', 'python -m pip install twine', 'twine upload ./wheelhouse/*.whl']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install virtualenv\n', './script/cibuild\n', './script/cibuild-setup-py\n', './script/test-module ${{ matrix.module }}\n']"
"['python -m pip install --upgrade pip poetry\npoetry install --extras docs\n', 'poetry run pre-commit run pyupgrade --all-files\n', 'poetry run pre-commit run black --all-files\n', 'poetry run pre-commit run flake8 --all-files\n', 'poetry run pre-commit run isort --all-files\n', 'poetry run pre-commit run docformatter --all-files\n', 'poetry run pre-commit run bandit --all-files\n', 'poetry run sphinx-build docs/ generated_docs\n', 'poetry run pre-commit run mypy --all-files\n', 'python -m pip install --upgrade pip poetry\npoetry install --extras docs\n', 'poetry run pytest --cov miio --cov-report xml\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['cat ~/.backport/backport.info.log', 'cat ~/.backport/backport.debug.log', 'sudo apt-get update\nsudo apt-get install gettext\npip3 install invoke\ninvoke install\n', 'invoke translate', 'python3 ci/check_migration_files.py', 'pip install requests\npip install pyyaml\npython3 ci/version_check.py\necho ""git_commit_hash=$(git rev-parse --short HEAD)"" >> $GITHUB_ENV\necho ""git_commit_date=$(git show -s --format=%ci)"" >> $GITHUB_ENV\n', 'docker-compose build --no-cache\n', 'docker-compose run inventree-dev-server invoke update\ndocker-compose run inventree-dev-server invoke setup-dev\ndocker-compose up -d\ndocker-compose run inventree-dev-server pip install --upgrade setuptools\ndocker-compose run inventree-dev-server invoke wait\n', 'test -d data\ntest -d data/env\ntest -d data/pgdb\ntest -d data/media\ntest -d data/static\ntest -d data/plugins\ntest -f data/config.yaml\ntest -f data/plugins.txt\ntest -f data/secret_key.txt\n', 'echo ""GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}"" >> docker.dev.env\ndocker-compose run inventree-dev-server invoke test --disable-pty\ndocker-compose down\n', 'cosign sign ${{ steps.meta.outputs.tags }}@${{ steps.build-and-push.outputs.digest }}', 'flake8 InvenTree --extend-ignore=D', 'cd ci\npython3 check_js_templates.py\n', 'python InvenTree/manage.py prerender\nnpx eslint InvenTree/InvenTree/static_i18n/i18n/*.js\n', 'pip install requests\npython3 ci/version_check.py\n', 'pip install pyyaml\npython docs/ci/check_mkdocs_config.py\n', 'pip install linkcheckmd requests\npython -m linkcheckmd docs --recurse\n', 'git clone --depth 1 https://github.com/inventree/${{ env.wrapper_name }} ./${{ env.wrapper_name }}', 'invoke delete-data -f\ninvoke import-fixtures\ninvoke server -a 127.0.0.1:12345 &\ninvoke wait\n', 'cd ${{ env.wrapper_name }}\ninvoke check-server\ncoverage run -m unittest discover -s test/\n', 'flake8 InvenTree --statistics', 'invoke translate', 'python3 ci/check_migration_files.py', 'invoke coverage', 'invoke test', 'invoke test', 'pip install requests\npython3 ci/version_check.py\n', 'sudo apt-get update\nsudo apt-get install -y gettext\npip3 install invoke\ninvoke install\n', 'invoke translate\n', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\ngit checkout -b l10_local\ngit add ""*.po""\ngit commit -m ""updated translation base""\n', 'pip install -r requirements-dev.txt', 'pip-compile --output-file=requirements.txt requirements.in -U', 'pip-compile --generate-hashes --output-file=requirements-dev.txt requirements-dev.in -U']"
""
"['python -m pip install --upgrade pip\npython -m pip install --upgrade build coveralls setuptools tox==4.0.2 wheel\n', 'python -m build', 'python -m pip install --only-binary=:all: --ignore-installed --find-links=dist/ vulture', 'vulture vulture/ tests/\npython -m vulture vulture/ tests/\n', 'python -m tox -e py', 'python -m tox -e style']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine --upgrade\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine Cython --upgrade\n', 'python setup.py bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install twine --upgrade\n', 'twine upload dist/*-manylinux*.whl\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install coveralls flake8 flake8-print mypy setuptools wheel twine Cython\n', '# stop the build if there are Python syntax errors, undefined names or print statements\nflake8 box --count --select=E9,F63,F7,F82,T001,T002,T003,T004 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=20 --max-line-length=120 --statistics --extend-ignore E203\n', 'mypy box', 'python setup.py sdist bdist_wheel\ntwine check dist/*\n', 'pip install dist/*.whl\nrm -rf box\npython -m pytest\n', '$wheel = (Get-ChildItem dist\\*.whl | Sort lastWriteTime | Select-Object -last 1).Name\npip install dist\\${wheel}\nRemove-item box -recurse -force\npython -m pytest\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install coveralls flake8 flake8-print mypy setuptools wheel twine Cython\n', 'pip install dist/*cp310-manylinux*.whl\nrm -rf box\npython -m pytest\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install setuptools wheel Cython\npython setup.py build_ext --inplace\n', 'pytest --cov=box test/\n']"
[]
"['Rscript -e ""install.packages(c(\'reticulate\', \'askpass\'))""', 'python -m pip install jedi==0.17.2\npython -m pip install -e .[test]\npytest -s --coverage\ncoverage combine\ncoverage xml -i -o coverage.xml\n', 'python -m pip install twine', 'python setup.py sdist', 'python -m pip install dist/*', 'python -m twine upload dist/*']"
[]
"['version=$(echo ${RELEASE_TAG} | sed \'s#^.*v##g\')\nsed -i ""s/^pkgver=.*$/pkgver=${version}/g"" packages/arch-dotdrop/PKGBUILD\ncat packages/arch-dotdrop/PKGBUILD\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'cp -r ""${GITHUB_WORKSPACE}/packages/snap"" ""${GITHUB_WORKSPACE}""\nsed -i \'s#source: ../../#source: \\.#g\' ""${GITHUB_WORKSPACE}/snap/snapcraft.yaml""\n', 'python -m pip install --upgrade pip\npip install -r tests-requirements.txt\npip install -r requirements.txt\nnpm install -g remark-cli remark-validate-links\nsudo apt-get install shellcheck\n', './tests.sh\n', './tests.sh\n', 'pip install coveralls\ncoveralls --service=github\n']"
['make check_dead_links\n']
"['pipx run build', 'pipx run twine check dist/*', 'pipx run nox -s check_manifest', 'echo ""::add-matcher::$GITHUB_WORKSPACE/.github/matchers/pylint.json""\npipx run nox -s pylint\n', '. /etc/os-release\necho ""deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /"" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\ncurl -sSfL ""https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/Release.key"" | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install podman\n', 'python -m pip install "".[test]""\n', 'python -m test.test_projects test.test_0_basic.basic_project sample_proj\n', 'python ./bin/run_tests.py --run-podman\n', 'python -m pip install "".[test]""\n', 'pytest --run-emulation test/test_emulation.py\n', 'nox --force-color -s update_constraints', 'nox --force-color -s update_pins', 'nox --force-color -s update_proj -- --auth=${{ secrets.GITHUB_TOKEN }}']"
"['if [[ ""${{ steps.changed_files.outputs.all }}"" =~ (requirements.txt|build.json) ]]; then\n  echo ""::set-output name=changed::true""\nfi\n', '(\n  # Fix out of memory issues with rust\n  echo ""CARGO_NET_GIT_FETCH_WITH_CLI=true""\n) > .env_file\n', 'echo ""BUILD_ARGS=--test"" >> $GITHUB_ENV', 'pip3 install dirhash\ndir_hash=""$(dirhash ""${{ github.workspace }}/supervisor"" -a sha256 --match ""*.py"")""\necho ""::set-output name=dirhash::${dir_hash}""\n', 'docker pull ghcr.io/home-assistant/amd64-hassio-supervisor:${{ needs.init.outputs.version }}\ndocker tag ghcr.io/home-assistant/amd64-hassio-supervisor:${{ needs.init.outputs.version }} homeassistant/amd64-hassio-supervisor:runner\n', 'mkdir -p /tmp/supervisor/data\ndocker create --name hassio_supervisor \\\n  --privileged \\\n  --security-opt seccomp=unconfined \\\n  --security-opt apparmor=unconfined \\\n  -v /run/docker.sock:/run/docker.sock \\\n  -v /run/dbus:/run/dbus \\\n  -v /tmp/supervisor/data:/data \\\n  -v /etc/machine-id:/etc/machine-id:ro \\\n  -e SUPERVISOR_SHARE=""/tmp/supervisor/data"" \\\n  -e SUPERVISOR_NAME=hassio_supervisor \\\n  -e SUPERVISOR_DEV=1 \\\n  -e SUPERVISOR_MACHINE=""qemux86-64"" \\\nhomeassistant/amd64-hassio-supervisor:runner\n', 'docker start hassio_supervisor', 'SUPERVISOR=$(docker inspect --format=\'{{.NetworkSettings.IPAddress}}\' hassio_supervisor)\nping=""error""\nwhile [ ""$ping"" != ""ok"" ]; do\n  ping=$(curl -sSL ""http://$SUPERVISOR/supervisor/ping"" | jq -r \'.result\')\n  sleep 5\ndone\n', 'echo ""Checking supervisor info""\ntest=$(docker exec hassio_cli ha supervisor info --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n\necho ""Checking supervisor network info""\ntest=$(docker exec hassio_cli ha network info --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n', 'echo ""Install Core SSH Add-on""\ntest=$(docker exec hassio_cli ha addons install core_ssh --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n\n# Make sure it actually installed\ntest=$(docker exec hassio_cli ha addons info core_ssh --no-progress --raw-json | jq -r \'.data.version\')\nif [[ ""$test"" == ""null"" ]]; then\n  exit 1\nfi\n\necho ""Start Core SSH Add-on""\ntest=$(docker exec hassio_cli ha addons start core_ssh --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n\n# Make sure its state is started\ntest=""$(docker exec hassio_cli ha addons info core_ssh --no-progress --raw-json | jq -r \'.data.state\')""\nif [ ""$test"" != ""started"" ]; then\n  exit 1\nfi\n', 'echo ""Enable Content-Trust""\ntest=$(docker exec hassio_cli ha security options --content-trust=true --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n\necho ""Run supervisor health check""\ntest=$(docker exec hassio_cli ha resolution healthcheck --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n\necho ""Check supervisor unhealthy""\ntest=$(docker exec hassio_cli ha resolution info --no-progress --raw-json | jq -r \'.data.unhealthy[]\')\nif [ ""$test"" != """" ]; then\n  exit 1\nfi\n\necho ""Check supervisor supported""\ntest=$(docker exec hassio_cli ha resolution info --no-progress --raw-json | jq -r \'.data.unsupported[]\')\nif [[ ""$test"" =~ source_mods ]]; then\n  exit 1\nfi\n', 'test=$(docker exec hassio_cli ha backups new --no-progress --raw-json)\nif [ ""$(echo $test | jq -r \'.result\')"" != ""ok"" ]; then\n  exit 1\nfi\necho ""::set-output name=slug::$(echo $test | jq -r \'.data.slug\')""\n', 'test=$(docker exec hassio_cli ha addons uninstall core_ssh --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n', 'test=$(docker exec hassio_cli ha supervisor restart --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n', 'SUPERVISOR=$(docker inspect --format=\'{{.NetworkSettings.IPAddress}}\' hassio_supervisor)\nping=""error""\nwhile [ ""$ping"" != ""ok"" ]; do\n  ping=$(curl -sSL ""http://$SUPERVISOR/supervisor/ping"" | jq -r \'.result\')\n  sleep 5\ndone\n', 'test=$(docker exec hassio_cli ha backups restore ${{ steps.backup.outputs.slug }} --addons core_ssh --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n\n# Make sure it actually installed\ntest=$(docker exec hassio_cli ha addons info core_ssh --no-progress --raw-json | jq -r \'.data.version\')\nif [[ ""$test"" == ""null"" ]]; then\n  exit 1\nfi\n\n# Make sure its state is started\ntest=""$(docker exec hassio_cli ha addons info core_ssh --no-progress --raw-json | jq -r \'.data.state\')""\nif [ ""$test"" != ""started"" ]; then\n  exit 1\nfi\n', 'test=$(docker exec hassio_cli ha backups restore ${{ steps.backup.outputs.slug }} --folders ssl --no-progress --raw-json | jq -r \'.result\')\nif [ ""$test"" != ""ok"" ]; then\n  exit 1\nfi\n', 'docker logs hassio_supervisor', 'labels=$(jq -r \'.pull_request.labels[] | .name\' ${{github.event_path }})\necho ""$labels""\nif [ ""$labels"" == ""cla-signed"" ]; then\n  exit 1\nfi\n', 'python -m venv venv\n. venv/bin/activate\npip install -U pip setuptools\npip install -r requirements.txt -r requirements_tests.txt\n', '. venv/bin/activate\npre-commit install-hooks\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', '. venv/bin/activate\nblack --target-version py38 --check supervisor tests setup.py\n', 'echo ""::add-matcher::.github/workflows/matchers/hadolint.json""\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""::add-matcher::.github/workflows/matchers/check-executables-have-shebangs.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual check-executables-have-shebangs --all-files\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""::add-matcher::.github/workflows/matchers/flake8.json""\n', '. venv/bin/activate\nflake8 supervisor tests\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual isort --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""::add-matcher::.github/workflows/matchers/check-json.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual check-json --all-files\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""::add-matcher::.github/workflows/matchers/pylint.json""\n', '. venv/bin/activate\npylint supervisor tests\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual pyupgrade --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends libpulse0 libudev1 dbus dbus-x11\n', 'echo ""::add-matcher::.github/workflows/matchers/python.json""\n', "". venv/bin/activate\n# Ideally this should be part of our dependencies\n# However this plugin is fairly new and doesn't run correctly\n# on a non-GitHub environment.\npip install pytest-github-actions-annotate-failures\n"", '. venv/bin/activate\npytest \\\n  -qq \\\n  --timeout=10 \\\n  --durations=10 \\\n  --cov supervisor \\\n  -o console_output_style=count \\\n  tests\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', '. venv/bin/activate\ncoverage combine coverage*/.coverage*\ncoverage report\ncoverage xml\n', 'declare -i newpost\nlatest=$(git describe --tags $(git rev-list --tags --max-count=1))\nlatestpre=$(echo ""$latest"" | awk \'{split($0,a,"".""); print a[1] ""."" a[2]}\')\ndatepre=$(date --utc \'+%Y.%m\')\n\n\nif [[ ""$latestpre"" == ""$datepre"" ]]; then\n    latestpost=$(echo ""$latest"" | awk \'{split($0,a,"".""); print a[3]}\')\n    newpost=$latestpost+1\nelse\n    newpost=0\nfi\n\necho Current version:    $latest\necho New target version: $datepre.$newpost\necho ""::set-output name=version::$datepre.$newpost""\n']"
"['python -m pip install requests', 'python etc/downloads_badges.py', 'make version sdist', 'python -m pip install cibuildwheel==2.11.2', 'python -m cibuildwheel --output-dir dist', 'python -m pip install ""twine>=1.13.0"" readme_renderer\nif python -c ""from twine.commands.check import check; check([\'dist/*\'])"" | grep  ""warning""; then\n    echo ""README will not render properly on PyPI""\n    exit 1\nelse\n    echo ""README rendered appropriately""\nfi\n', 'cd dist\ntar xzf $(find . -name ""*.tar.gz"")\nOUTPUT_DIRECTORY=$(find . -type d -name ""pmdarima-*"")\nif grep -q ""^/"" ${OUTPUT_DIRECTORY}/pmdarima.egg-info/SOURCES.txt; then\n  echo ""SOURCES.txt contains absolute paths in sdist""\n  exit 1\nelse\n  echo ""SOURCES.txt looks good""\n  rm -rf ""$OUTPUT_DIRECTORY""\nfi\n', 'pip install dist/$(ls dist | grep tar)', 'if [ -f ""${GITHUB_WORKSPACE}/pmdarima/VERSION"" ]; then\n  echo ""VERSION file exists""\n  echo ""version_exists=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""VERSION file does not exist""\n  echo ""version_exists=false"" >> $GITHUB_OUTPUT\nfi\n', 'chmod +x build_tools/github/deploy.sh\n./build_tools/github/deploy.sh\n', 'echo ""run_id=$GITHUB_RUN_ID"" >> $GITHUB_OUTPUT', 'dependencies=$(python build_tools/github/get_latest_dependencies.py)\necho ""latest_dependencies=$dependencies"" >> $GITHUB_OUTPUT\n', 'python -m pip install cibuildwheel==2.9.0', 'pip install requests tabulate\ntable=$(python .github/utils/get_dependency_releases.py $DEPENDENCIES)\n\n# This is used in the next job (if necessary) rather than re-running the above\necho ""table=$table"" >> $GITHUB_OUTPUT\n', 'python -m cibuildwheel --output-dir dist', 'chmod +x build_tools/github/test_version_tagging.sh\n./build_tools/github/test_version_tagging.sh\n']"
"['python -m pip install --upgrade pip\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python\n', 'echo ""${HOME}/.poetry/bin"" >> $GITHUB_PATH', 'poetry install\n', 'poetry run maturin develop --release', 'python -m pip install --upgrade pip\npip install flake8 pytest tox\n', 'flake8 .  --max-complexity 10 --ignore E501,W504,W605', 'tox -e py37', 'pip install -r requirements.txt\n', 'python train_bert.py data.batch_size=2 data.samples=10 trainer.epoch=1\n', 'python -m pip install --upgrade pip\npip install pylint\n', 'pylint ./underthesea/utils/vietnamese_ipa.py --fail-under 9', 'cargo --version\n$PYTHON -m poetry --version\n', '$PYTHON -m poetry install\n$PYTHON -m poetry show\n', '$PYTHON -m poetry run maturin build --release --no-sdist --strip --interpreter python${{ matrix.python-version }}', 'find ./target/wheels/', '$PYTHON -m poetry run maturin publish --username __token__ --no-sdist --interpreter python${{ matrix.python-version }}', 'python -m pip install --upgrade pip\nmkdir ""${HOME}/poetry""\ncurl -sSL https://install.python-poetry.org | POETRY_HOME=${HOME}/poetry python -\necho ""${HOME}/poetry/bin"" >> $GITHUB_PATH\n', 'python -m pip install --upgrade pip\nmkdir ""${HOME}/poetry""\ncurl -sSL https://install.python-poetry.org | python -\necho ""C:\\Users\\runneradmin\\AppData\\Roaming\\Python\\Scripts\\"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\n', 'poetry install\n', 'poetry run maturin build --release --no-sdist --strip --interpreter python', 'poetry run maturin build --release --no-sdist --strip --interpreter python${{ matrix.python-version }}', 'find ./target/wheels/', 'dir target\\wheels\\', 'pip install target/wheels/underthesea_core*.whl', 'poetry run maturin publish --username __token__ --no-sdist --interpreter python${{ matrix.python-version }}', 'poetry run maturin publish --username __token__ --no-sdist --interpreter python', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/']"
"['sudo apt-get update --fix-missing\n', 'sudo apt-get install --fix-broken --ignore-missing python3-autopep8\n', 'make pep -C software\n', 'sudo apt-get update --fix-missing\n', 'sudo apt-get install --fix-broken --ignore-missing python3-typed-ast mypy\n', 'find . -type d\nmypy --version\n', 'make type -C software\n', 'sudo apt-get update --fix-missing\n', 'sudo apt-get install --fix-broken --ignore-missing python3-coverage\n', 'sudo apt-get install --fix-broken --ignore-missing python3-pip\n', 'pip3 install unittest-xml-reporting\n', 'make tests -C software || true\n', ""find . -name '*.xml'\n""]"
"['git config --global user.email ""bumpversion-test-git@github.actions""\ngit config --global user.name ""Testing Git on Travis CI""\ngit --version\ngit config --list\n', ""echo -e '[ui]\\nusername = Testing Mercurial on Travis CI <bumpversion-test-hg@travis.ci>' > ~/.hgrc\nhg --version\n"", 'pip install tox tox-gh-actions', 'tox']"
""
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pytest', ""npm install -g firebase-tools\nfirebase emulators:exec --only database --project fake-project-id 'pytest integration/test_db.py'\n"", 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', './lint.sh all', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install setuptools wheel\npip install tensorflow\npip install keras\n', 'pytest', './.github/scripts/run_integration_tests.sh', 'python setup.py bdist_wheel sdist', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install setuptools wheel\npip install tensorflow\npip install keras\n', 'pytest', './.github/scripts/run_integration_tests.sh', 'python setup.py bdist_wheel sdist', './.github/scripts/publish_preflight_check.sh']"
[]
"['echo ""CACHE=${{ secrets.CACHE_DATE }} ${{ runner.os }} $(python -VV | sha256sum | cut -d\' \' -f1) ${{ hashFiles(\'pyproject.toml\') }} ${{ hashFiles(\'poetry.lock\') }}"" >> $GITHUB_ENV', 'pip install poetry', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry install', 'poetry run pytest --prebuild']"
"['nix build .', 'cat > ~/.okta_aws_login_config <<EOF\n[default]\nokta_org_url = https://foobar.okta.com\nokta_auth_server =\nclient_id =\ngimme_creds_server = appurl\naws_appname =\naws_rolename =\nwrite_aws_creds = False\ncred_profile = role\nokta_username = foo@example.com\napp_url = https://foobar.okta.com/home/amazon_aws/00000000000000000000/111\nresolve_aws_alias = False\npreferred_mfa_type =\naws_default_duration = 36000\ndevice_token =\noutput_format = json\nEOF\n', './result/bin/gimme-aws-creds --version', 'python -m pip install --upgrade pip\npython -m pip install -r requirements_dev.txt\n', 'make test', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""', 'pip install -r requirements-dev.txt', 'flake8', 'isort --diff -c .', 'pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install PLY', 'pip install Django==${{ matrix.django }}', 'pip install -e .', 'python test_project/manage.py test core.tests']"
"['echo $JSON', 'pip install --upgrade pip\npip install -r requirements/ubuntu-latest-3.10.txt\n', 'make docs', 'pip install --upgrade pip\npip install -r requirements/${{ matrix.runtime.machine }}-${{ matrix.runtime.python-version }}.txt\n', 'echo ""Build minimal container for docker-in-docker diagnostics""\ndocker build -f Dockerfile.diagnostics -t testcontainers-python .\necho ""Bare metal diagnostics""\npython diagnostics.py\necho ""Container diagnostics with bridge network""\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock --network=bridge testcontainers-python python diagnostics.py\necho ""Container diagnostics with host network""\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock --network=host testcontainers-python python diagnostics.py\n', 'make ${{ matrix.component }}/lint', 'make ${{ matrix.component }}/tests', 'make ${{ matrix.component }}/doctest', 'make ${{ matrix.component }}/dist', 'make ${{ matrix.component }}/upload', 'pip install --upgrade pip pip-tools', 'pip-compile --resolver=backtracking -v --upgrade -o requirements.txt']"
""
"['python -m pip install --upgrade pip\npip install pylint sphinx sphinx_rtd_theme\npip install -r requirements.txt\n', 'pylint --disable=I --disable=W --disable=C --disable=R p5\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\nsudo apt-get install python3-opengl libglfw3 -y\n', 'pytest -svv p5\n', 'sudo apt-get install python3-opengl libglfw3 python3-wheel -y\npython -m pip install --upgrade pip\npip install wheel\npip install .\nsudo apt-get install x11-utils\n']"
"['set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel -r dev-requirements.txt\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ndevpi use https://m.devpi.net/devpi-github\ndevpi login devpi-github --password ${{ secrets.DEVPI_GITHUB_PASSWORD }}\npython .ci/cleanup_devpi_indices.py\n.ci/upload-packages.sh\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nexport POSTGRESQL_BIN_DIR=$(ls -d /usr/lib/postgresql/*/bin)\nls $POSTGRESQL_BIN_DIR\nexport PATH=$PATH:$POSTGRESQL_BIN_DIR\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\nexport POSTGRESQL_BIN_DIR=$(ls -d /usr/lib/postgresql/*/bin)\nls $POSTGRESQL_BIN_DIR\nexport PATH=$PATH:$POSTGRESQL_BIN_DIR\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\nexport POSTGRESQL_BIN_DIR=$(ls -d /usr/lib/postgresql/*/bin)\nls $POSTGRESQL_BIN_DIR\nexport PATH=$PATH:$POSTGRESQL_BIN_DIR\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\nexport POSTGRESQL_BIN_DIR=$(ls -d /usr/lib/postgresql/*/bin)\nls $POSTGRESQL_BIN_DIR\nexport PATH=$PATH:$POSTGRESQL_BIN_DIR\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel tox\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\ncd $PROJECT\npython -m tox -a -vv\nPIP_PRE=1 python -m tox -v -e ${{ matrix.tox-envs }} -- -v --color=yes\n', 'set -xe -o nounset\nsource .ci/set-index-env.sh\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel -r dev-requirements.txt\n', 'set -xe -o nounset\npython -m mypy\n', 'set -xe -o nounset\npython -VV\npython -m site\npython -m pip install --upgrade pip ruff setuptools wheel\n', 'set -xe -o nounset\nruff --format=github .\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -r requirements-dev.txt\n', 'mypy sqlalchemy_mixins', 'nose2  --coverage=sqlalchemy_mixins\nexport PYTHONPATH=.:$PYTHONPATH\npython examples/activerecord.py\npython examples/all_features.py\npython examples/eagerload.py\npython examples/repr.py\npython examples/smartquery.py\npython examples/serialize.py\npython examples/timestamp.py']"
"['python3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel twine\n', 'python3 setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install tox', 'tox -e py']"
"['python -m build --sdist\npython -m pip install dist/*\n', 'python -c ""from pygmt.helpers.testing import download_test_data; download_test_data()""\n', 'echo ""date=$(date +\'%Y-%m-%d\')"" >> $GITHUB_OUTPUT', ""mkdir -p ~/.gmt\nmv .gmt/* ~/.gmt\n# Change modification times of the two files, so GMT won't refresh it\ntouch ~/.gmt/server/gmt_data_server.txt ~/.gmt/server/gmt_hash_server.txt\nls -lhR ~/.gmt\n"", 'python -m build --sdist\npython -m pip install dist/*\n', 'make -C doc clean all', '# Detect if this is a release or from the main branch\nif [[ ""${GITHUB_EVENT_NAME}"" == ""release"" ]]; then\n    # Get the tag name without the ""refs/tags/"" part\n    version=""${GITHUB_REF#refs/*/}""\nelse\n    version=dev\nfi\necho ""Deploying version: $version""\n# Make the new commit message. Needs to happen before cd into deploy\n# to get the right commit hash.\nmessage=""Deploy $version from $(git rev-parse --short HEAD)""\ncd deploy\n# Need to have this file so that Github doesn\'t try to run Jekyll\ntouch .nojekyll\n# Delete all the files and replace with our new  set\necho -e ""\\nRemoving old files from previous builds of ${version}:""\nrm -rvf ${version}\necho -e ""\\nCopying HTML files to ${version}:""\ncp -Rvf ../doc/_build/html/ ${version}/\n# If this is a new release, update the link from /latest to it\nif [[ ""${version}"" != ""dev"" ]]; then\n    echo -e ""\\nSetup link from ${version} to \'latest\'.""\n    rm -f latest\n    ln -sf ${version} latest\nfi\n# Stage the commit\ngit add -A .\necho -e ""\\nChanges to be applied:""\ngit status\n# Configure git to be the GitHub Actions account\ngit config user.email ""github-actions[bot]@users.noreply.github.com""\ngit config user.name ""github-actions[bot]""\n# If this is a dev build and the last commit was from a dev build\n# (detect if ""dev"" was in the previous commit message), reuse the\n# same commit\nif [[ ""${version}"" == ""dev"" && `git log -1 --format=\'%s\'` == *""dev""* ]]; then\n    echo -e ""\\nAmending last commit:""\n    git commit --amend --reset-author -m ""$message""\nelse\n    echo -e ""\\nMaking a new commit:""\n    git commit -m ""$message""\nfi\n# Make the push quiet just in case there is anything that could leak\n# sensitive information.\necho -e ""\\nPushing changes to gh-pages.""\ngit push -fq origin gh-pages 2>&1 >/dev/null\necho -e ""\\nFinished uploading generated files.""\n', ""mkdir -p ~/.gmt\nmv .gmt/* ~/.gmt\n# Change modification times of the two files, so GMT won't refresh it\ntouch ~/.gmt/server/gmt_data_server.txt ~/.gmt/server/gmt_hash_server.txt\nls -lhR ~/.gmt\n"", 'make install', 'make doctest PYTEST_EXTRA=""-r P""', ""mkdir -p ~/.gmt\nmv .gmt/* ~/.gmt\n# Change modification times of the two files, so GMT won't refresh it\ntouch ~/.gmt/server/gmt_data_server.txt ~/.gmt/server/gmt_hash_server.txt\nls -lhR ~/.gmt\n"", 'dvc pull --verbose\nls -lhR pygmt/tests/baseline/\n', 'make install', 'make test PYTEST_EXTRA=""-r P""', ""python -m pip install --pre --prefer-binary \\\n              numpy pandas xarray netCDF4 packaging \\\n              build contextily dvc ipython rioxarray \\\n              'pytest>=6.0' pytest-cov pytest-doctestplus pytest-mpl \\\n              sphinx-gallery\n"", 'micromamba list', 'dvc pull\nls -lhR pygmt/tests/baseline/\n', 'dvc pull\nls -lhR pygmt/tests/baseline/\n', 'curl https://raw.githubusercontent.com/GenericMappingTools/gmt/master/ci/build-gmt.sh | bash', 'micromamba install -c conda-forge/label/dev gmt', ""mkdir -p ~/.gmt\nmv .gmt/* ~/.gmt\n# Change modification times of the two files, so GMT won't refresh it\ntouch ~/.gmt/server/gmt_data_server.txt ~/.gmt/server/gmt_hash_server.txt\nls -lhR ~/.gmt\n"", 'make install', 'echo ${GITHUB_WORKSPACE}/gmt-install-dir/bin >> $GITHUB_PATH', 'make test PYTEST_EXTRA=""-r P""', 'make test PYTEST_EXTRA=""-r P""', ""mkdir -p ~/.gmt\nmv .gmt/* ~/.gmt\n# Change modification times of the two files, so GMT won't refresh it\ntouch ~/.gmt/server/gmt_data_server.txt ~/.gmt/server/gmt_hash_server.txt\nls -lhR ~/.gmt\n"", 'dvc pull pygmt/tests/baseline/test_logo.png --verbose\nls -lhR pygmt/tests/baseline/\n', 'make install', 'make test_no_images PYTEST_EXTRA=""-r P""', 'echo -e ""## Summary of changed images\\n"" > report.md\necho -e ""This is an auto-generated report of images that have changed on the DVC remote\\n"" >> report.md\n\n# Pull image data from cloud storage\ndvc pull --remote upstream\ndvc diff --show-md main HEAD >> report.md\n\n# Get just the filename of the added and modified image from the report\nawk \'NF==5 && NR>=7 && $2==""added"" {print $4}\' report.md > added_files.txt\nawk \'NF==5 && NR>=7 && $2==""modified"" {print $4}\' report.md > modified_files.txt\n\n# Backup new images in the baseline-new directory\nmkdir pygmt/tests/baseline-new\ncp pygmt/tests/baseline/*.png pygmt/tests/baseline-new/\n# Pull images in the main branch from cloud storage\ngit checkout main\ndvc pull --remote upstream\n\n# Append each image to the markdown report\necho -e ""## Image diff(s)\\n"" >> report.md\necho -e ""<details>\\n"" >> report.md\n\n# Added images\necho -e ""### Added images\\n"" >> report.md\nwhile IFS= read -r line; do\n  echo -e ""- $(basename $line) \\n"" >> report.md\n  echo -e ""![](${line/baseline/baseline-new})"" >> report.md\ndone < added_files.txt\n\n# Modified images\necho -e ""### Modified images\\n"" >> report.md\necho -e ""| Path | Old | New |"" >> report.md\necho -e ""|---|---|---|"" >> report.md\nwhile IFS= read -r line; do\n  echo -e ""| $(basename $line) | ![]($line) | ![](${line/baseline/baseline-new}) |"" >> report.md\ndone < modified_files.txt\n\necho -e ""</details>\\n"" >> report.md\n\n# Mention git commit SHA in the report\necho -e ""Report last updated at commit ${{ github.event.pull_request.head.sha }}"" >> report.md\n\n# create/update PR comment\ncml comment update report.md\n', 'python -m pip install black blackdoc docformatter flakeheaven isort\nsudo apt-get install dos2unix\n', 'make format\nfind . -type f -not -path \'*/\\.git/*\' -exec grep -Iq . {} \\; -exec dos2unix {} \\;\nfind . -type f -not -path \'*/\\.git/*\' -exec grep -Iq . {} \\; -exec chmod 644 {} \\;\nif [[ $(git ls-files -m) ]]; then\n  git config --global user.name \'actions-bot\'\n  git config --global user.email \'58130806+actions-bot@users.noreply.github.com\'\n  git commit -am ""[format-command] fixes""\n  git push\nfi\n', 'python -m pip install build', '# Change setuptools-scm local_scheme to ""no-local-version"" so the\n# local part of the version isn\'t included, making the version string\n# compatible with PyPI.\nsed --in-place ""s/node-and-date/no-local-version/g"" pyproject.toml\n', 'make package\necho """"\necho ""Generated files:""\nls -lh dist/\n', 'dvc pull\nls -lhR pygmt/tests/baseline/\n', 'mkdir baseline-images\nmv pygmt/tests/baseline/*.png baseline-images/\nzip -r baseline-images.zip baseline-images\nshasum -a 256 baseline-images.zip\n', 'python -m pip install black blackdoc docformatter flakeheaven pylint isort\nsudo apt-get install dos2unix\n', 'make check', 'make lint', ""find . -type f -not -path '*/\\.git/*' -exec grep -Iq . {} \\; -exec dos2unix --quiet {} \\;\nfind . -type f -not -path '*/\\.git/*' -exec grep -Iq . {} \\; -exec chmod 644 {} \\;\nif [[ $(git ls-files -m) ]]; then git --no-pager diff HEAD; exit 1; fi\n""]"
"['pip install -r requirements.txt', 'python ./check_format.py', 'pip install -r requirements.txt', 'python ./check_format.py']"
[]
""
"['set -eux\ngit fetch origin main\nif [[ ""${{ github.event_name }}"" == ""pull_request"" ]]; then\n  git branch -f main origin/main\nfi\n', ""set -eux\npip install -q flake8 flake8-bugbear flake8-black docformatter==1.3.0 black==22.3.0\npython setup.py develop --no-deps  # get our custom flake8 errors\npython -c 'import parlai'\nflake8 --version\nbash autoformat.sh -c -f | tee ${GITHUB_WORKSPACE}/output-annotations.txt\n"", 'if [ ""$(cat ${GITHUB_WORKSPACE}/output-annotations.txt | wc -l)"" != ""0"" ]\nthen\n  exit 1\nfi\n', 'set -eux\ngit fetch origin main\nif [[ ""${{ github.event_name }}"" == ""pull_request"" ]]; then\n  git branch -f main origin/main\nfi\n', 'HEAD_SHA=""NONE""\nif [[ ""${{ github.event_name }}"" == ""pull_request"" ]]; then\n  HEAD_SHA=""${{ github.event.pull_request.head.sha }}""\nelse\n  HEAD_SHA=""${{ github.event.push.head.sha }}""\nfi\necho $HEAD_SHA\necho ""HEAD_SHA=${HEAD_SHA}"" >> $GITHUB_ENV\n', 'pip install -q -r requirements.txt\npip install -q mypy mypy-extensions\npip install -q git+https://github.com/numpy/numpy-stubs.git\npip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n', ""set -eux\nmypy | sort | grep -v '^Found ' > typeerrors-HEAD.txt\nforkpoint=$(git merge-base main HEAD)\ngit checkout -f $forkpoint\nmypy | sort | grep -v '^Found ' > typeerrors-main.txt\ndiff -u typeerrors-main.txt typeerrors-HEAD.txt | grep -v '^+++' | grep '^+' > typeerrors-CHANGED.txt || true\ncat typeerrors-CHANGED.txt | sed 's/^+//' | sed 's/: error:/:1: mypy:/' | tee ${GITHUB_WORKSPACE}/output-annotations.txt\n"", 'HEAD_SHA=""NONE""\nif [[ ""${{ github.event_name }}"" == ""pull_request"" ]]; then\n  HEAD_SHA=""${{ github.event.pull_request.head.sha }}""\nelse\n  HEAD_SHA=""${{ github.event.push.head.sha }}""\nfi\necho $HEAD_SHA\necho ""HEAD_SHA=${HEAD_SHA}"" >> $GITHUB_ENV\n', 'set -eux\nnpm install\nnpm install --save-dev -g eslint\n', 'set -eux\nnpx eslint --format unix . | \\\n  sed ""s#${GITHUB_WORKSPACE}#.#"" | \\\n  tee ${GITHUB_WORKSPACE}/output-annotations.txt\n']"
""
"['sudo apt-get install git dpkg-dev rpm flex bison ca-certificates wget python3 rsync bc libssl-dev clang llvm libelf-dev libzip-dev git libnetfilter-queue-dev libpcap-dev protobuf-compiler python3-pip dh-golang golang-any golang-golang-x-net-dev golang-google-grpc-dev golang-goprotobuf-dev libmnl-dev golang-github-vishvananda-netlink-dev golang-github-evilsocket-ftrace-dev golang-github-google-gopacket-dev golang-github-fsnotify-fsnotify-dev linux-headers-$(uname -r)\n', 'kernel_version=""${{ matrix.kernel }}""\nif [ ! -d utils/packaging/ ]; then\n  mkdir -p utils/packaging/\nfi\nwget https://raw.githubusercontent.com/evilsocket/opensnitch/master/utils/packaging/build_modules.sh -O utils/packaging/build_modules.sh\nbash utils/packaging/build_modules.sh $kernel_version\nsha1sum ebpf_prog/modules/opensnitch*o > ebpf_prog/modules/checksums.txt\n', 'set -e\nset -x\nsudo apt install desktop-file-utils appstream\n', 'set -e\nset -x\ndesktop-file-validate ui/resources/opensnitch_ui.desktop\nappstreamcli validate ui/resources/io.github.evilsocket.opensnitch.appdata.xml\n', 'sudo apt-get install git libnetfilter-queue-dev libmnl-dev libpcap-dev protobuf-compiler\nexport GOPATH=~/go\nexport PATH=$PATH:$GOPATH/bin\ngo get github.com/golang/protobuf/protoc-gen-go\ngo install google.golang.org/protobuf/cmd/protoc-gen-go\ngo get google.golang.org/grpc/cmd/protoc-gen-go-grpc\ncd proto\nmake ../daemon/ui/protocol/ui.pb.go\ngo get -v -t -d ./...\nif [ -f Gopkg.toml ]; then\n    curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh\n    dep ensure\nfi\n', 'cd daemon\ngo build -v .\n', 'cd daemon\ngo test ./...\n']"
"['pip install "".[extra,test]""\n', 'pip list\n', 'coverage erase\nmake test\n', 'pip install ruff\npip install black\n', 'pip list', 'make lint\n', 'make format\n', 'pip install "".[extra, typing]""\n', 'pip list', 'make typecheck', 'sudo apt-get install pandoc\npip install "".[test,doc]""\npip install -r requirements/examples.txt\n', 'pip list', 'pushd doc\nSPHINXOPTS=-W\nmake html && popd\n', 'pip install --upgrade pip\npip install "".[extra,test]""\npip install --upgrade git+https://github.com/matplotlib/matplotlib\n', 'pip list\n', 'make test\n', 'pip install --upgrade pip\npip install "".[extra,test]""\npip install --upgrade git+https://github.com/pandas-dev/pandas\n', 'pip list\n', 'make test\n', 'echo ""::set-output name=releasetag::$(curl -s https://api.github.com/repos/has2k1/plotnine/releases/latest | jq \'.tag_name\' | sed \'s/\\""//g\')""\n', 'echo ${{ steps.latestrelease.outputs.latestrelease }}\n', 'pip install --upgrade pip\npip install --upgrade git+https://github.com/matplotlib/matplotlib\npip install "".[extra,test]""\n', 'pip list\n', 'make test\n', 'echo ""::set-output name=releasetag::$(curl -s https://api.github.com/repos/has2k1/plotnine/releases/latest | jq \'.tag_name\' | sed \'s/\\""//g\')""\n', 'echo ${{ steps.latestrelease.outputs.latestrelease }}\n', 'pip install --upgrade pip\npip install --upgrade git+https://github.com/pandas-dev/pandas\npip install "".[extra,test]""\n', 'pip list\n', 'make test\n', 'pip install --upgrade pip\npip install "".[extra,typing]""\npip install --upgrade git+https://github.com/matplotlib/matplotlib\n', 'pip list\n', 'make typecheck\n', 'pip install --upgrade pip\npip install "".[extra,typing]""\npip install --upgrade git+https://github.com/pandas-dev/pandas\n', 'pip list\n', 'make test\n']"
"['python -m pip install --upgrade pip\npython setup.py install\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npytest\npython -m s_tui.s_tui -j\n']"
"['helm plugin install https://github.com/databus23/helm-diff\nhelm dependency update ./helm-chart/binderhub\n', 'UPGRADE_FROM_VERSION=$(curl -sSL https://jupyterhub.github.io/helm-chart/info.json | jq -er \'.binderhub.dev\')\n\n# NOTE: We change the directory so binderhub the chart name won\'t be\n#       misunderstood as the local folder name.\n# validation is disabled, because the config is for a different version!\ncd testing\n\nold_config=""../testing/k8s-binder-k8s-hub/binderhub-chart-config-old.yaml""\nif [ -f ""$old_config"" ]; then\n  echo ""using old config""\nelse\n  old_config=""../testing/k8s-binder-k8s-hub/binderhub-chart-config.yaml""\nfi\n\nhelm install binderhub-test binderhub \\\n    --values ""$old_config"" \\\n    --repo https://jupyterhub.github.io/helm-chart/ \\\n    --disable-openapi-validation \\\n    --version=$UPGRADE_FROM_VERSION\n', 'echo ""NOTE: For the helm diff, we have not updated the Chart.yaml""\necho ""      version or image tags using chartpress.""\necho\n\nhelm diff upgrade binderhub-test helm-chart/binderhub \\\n    --values testing/k8s-binder-k8s-hub/binderhub-chart-config.yaml \\\n    --context=3\n', 'npm install', 'npm run lint', '. ./ci/common\nsetup_helm v3.5.4\npip install --no-cache-dir chartpress>=2.1 pyyaml build\n', 'python3 -m build --wheel .', 'mkdir -p ~/.ssh\nssh-keyscan github.com >> ~/.ssh/known_hosts\necho ""${{ secrets.JUPYTERHUB_HELM_CHART_DEPLOY_KEY }}"" > ~/.ssh/id_ed25519\nchmod 600 ~/.ssh/id_ed25519\n', 'docker login -u ""${{ secrets.DOCKER_USERNAME }}"" -p ""${{ secrets.DOCKER_PASSWORD }}""\n', 'git config --global user.email ""github-actions@example.local""\ngit config --global user.name ""GitHub Actions user""\n', './tools/generate-json-schema.py\n./ci/publish\n', 'python -mpip install build', 'python -mbuild .', 'pip install chartpress build', 'python3 -m build --wheel .', 'cd helm-chart\nchartpress \\\n    --builder docker-buildx \\\n    --platform linux/amd64 --platform linux/arm64\n', 'pip install ruamel.yaml', './ci/check_embedded_chart_code.py', 'REGISTRY_HOST=$(hostname -I | awk \'{print $1}\'):5000\necho REGISTRY_HOST=""$REGISTRY_HOST"" >> $GITHUB_ENV\n\n# Allow k3s to pull from private registry\n# https://docs.k3s.io/installation/private-registry\nsudo mkdir -p /etc/rancher/k3s/\ncat << EOF | sudo tee /etc/rancher/k3s/registries.yaml\nmirrors:\n  ""$REGISTRY_HOST"":\n    endpoint:\n      - ""http://$REGISTRY_HOST""\nEOF\n', 'sudo apt-get update\nsudo apt-get install --yes \\\n  build-essential \\\n  curl \\\n  libcurl4-openssl-dev \\\n  libssl-dev\n', 'npm install\nnpm run webpack\n', 'pip install --upgrade pip\npip install --upgrade setuptools wheel\n', 'pip install -r dev-requirements.txt -r helm-chart/images/binderhub/requirements.txt\npip install .\n', './testing/local-binder-k8s-hub/install-jupyterhub-chart\n', './testing/local-binder-k8s-hub/install-jupyterhub-chart --auth\n', 'python3 -m build .\n', 'export DOCKER_BUILDKIT=1\n\nCHARTPRESS_ARGS=\nif [ ""${{ matrix.test-variation }}"" = ""dind"" -o ""${{ matrix.test-variation }}"" = ""pink"" ]; then\n  CHARTPRESS_ARGS=""--image-prefix localhost:5000/binderhub- --push""\n\n  # Allow the pods to push to the non-https GitHub workflow registry\n  envsubst < testing/k8s-binder-k8s-hub/cm-insecure-registries-${{ matrix.test-variation }}.yaml | kubectl apply -f -\nfi\n\n# Use chartpress to create the helm chart and build its images\nhelm dependency update ./helm-chart/binderhub\n(cd helm-chart && chartpress $CHARTPRESS_ARGS)\ngit --no-pager diff --color=always\n', 'tools/generate-json-schema.py\n', 'helm template --validate binderhub-test helm-chart/binderhub \\\n    --values tools/templates/lint-and-validate-values.yaml\n', 'helm template --validate binderhub-test helm-chart/binderhub \\\n    --values tools/templates/lint-and-validate-values.yaml \\\n    --set imageBuilderType=dind\n', 'helm template --validate binderhub-test helm-chart/binderhub \\\n    --values tools/templates/lint-and-validate-values.yaml \\\n    --set imageBuilderType=pink\n', 'helm template --validate binderhub-test helm-chart/binderhub \\\n    --values testing/k8s-binder-k8s-hub/binderhub-chart-config.yaml \\\n    --set config.BinderHub.hub_url=http://localhost:30902 \\\n    --set config.GitHubRepoProvider.access_token=$GITHUB_ACCESS_TOKEN\n', '. ./ci/common\nUPGRADE_FROM_VERSION=$(curl -sSL https://jupyterhub.github.io/helm-chart/info.json | jq -er \'.binderhub.${{ matrix.upgrade-from }}\')\necho ""UPGRADE_FROM_VERSION=$UPGRADE_FROM_VERSION"" >> $GITHUB_ENV\n\necho """"\necho ""Installing already released binderhub version $UPGRADE_FROM_VERSION""\n\n# FIXME: We change the directory so binderhub the chart name won\'t be\n#        misunderstood as the local folder name.\n#\n#        https://github.com/helm/helm/issues/9244\ncd ci\n\nold_config=""../testing/k8s-binder-k8s-hub/binderhub-chart-config-old.yaml""\nif [ -f ""$old_config"" ]; then\n  echo ""using old config""\nelse\n  old_config=""../testing/k8s-binder-k8s-hub/binderhub-chart-config.yaml""\nfi\n\nhelm install binderhub-test \\\n    --repo https://jupyterhub.github.io/helm-chart/ binderhub \\\n    --disable-openapi-validation \\\n    --version=$UPGRADE_FROM_VERSION \\\n    --values ""$old_config"" \\\n    --set config.BinderHub.hub_url=http://localhost:30902 \\\n    --set config.BinderHub.hub_url_local=http://proxy-public \\\n    --set config.GitHubRepoProvider.access_token=$GITHUB_ACCESS_TOKEN \\\n    ${{ matrix.upgrade-from-extra-args }}\n', 'helm plugin install https://github.com/databus23/helm-diff\n', 'helm diff upgrade binderhub-test ./helm-chart/binderhub \\\n    --values testing/k8s-binder-k8s-hub/binderhub-chart-config.yaml \\\n    --set config.BinderHub.hub_url=http://localhost:30902 \\\n    --set config.BinderHub.hub_url_local=http://proxy-public \\\n    --set config.GitHubRepoProvider.access_token=$GITHUB_ACCESS_TOKEN \\\n    ${{ matrix.local-chart-extra-args }}\n', 'helm upgrade --install binderhub-test helm-chart/binderhub \\\n    --values testing/k8s-binder-k8s-hub/binderhub-chart-config.yaml \\\n    --set config.BinderHub.hub_url=http://localhost:30902 \\\n    --set config.BinderHub.hub_url_local=http://proxy-public \\\n    --set config.GitHubRepoProvider.access_token=$GITHUB_ACCESS_TOKEN \\\n    ${{ matrix.local-chart-extra-args }}\n', '. ci/common\nawait_jupyterhub\n\necho curl http://localhost:30902/hub/api/ should print the JupyterHub version\ncurl http://localhost:30902/hub/api/ --max-time 5 --retry 5 --retry-delay 1 --retry-connrefused --fail-with-body --retry-all-errors\n', "". ci/common\nawait_binderhub binderhub-test\n\necho curl http://localhost:30901/health to check BinderHub\\'s health\ncurl http://localhost:30901/health --max-time 5 --retry 5 --retry-delay 1 --retry-connrefused --fail-with-body --retry-all-errors\n"", 'pytest -m ""not auth"" --cov=binderhub', 'pytest -m ""auth"" --cov=binderhub', 'export BINDER_URL=http://localhost:30901\npytest --helm -m ""remote"" --cov=binderhub\n', 'if [ ""${{ matrix.test }}"" = ""helm"" ]; then\n  for endpoint in versions health metrics; do\n    echo -e ""\\n${endpoint}""\n    curl http://localhost:30901/$endpoint --fail-with-body\n  done\nfi\n', 'sudo apt-get update\nsudo apt-get install --yes \\\n  build-essential \\\n  curl \\\n  libcurl4-openssl-dev \\\n  libssl-dev\n', 'pip install --upgrade pip\npip install --upgrade setuptools wheel\n', 'pip install -r dev-requirements.txt -r testing/local-binder-local-hub/requirements.txt\npip install "".[pycurl]""\n', 'npm install -g configurable-http-proxy', 'cd testing/local-binder-local-hub\njupyterhub --config=jupyterhub_config.py > jupyterhub.log 2>&1 &\nsleep 5\n\necho curl http://localhost:8000/hub/api/ should print the JupyterHub version\ncurl http://localhost:8000/hub/api/ --max-time 5 --retry 5 --retry-delay 1 --retry-connrefused --fail-with-body --retry-all-errors\n', 'export BINDER_URL=http://localhost:8000/services/binder/\npytest -m remote -v --color=yes\n', 'cat testing/local-binder-local-hub/jupyterhub.log', 'local_tag=$(cat helm-chart/binderhub/values.yaml | yq e \'.${{ matrix.values_path }}\' -)\necho ""tag=$local_tag"" >> $GITHUB_OUTPUT\n', 'latest_tag=$(\n    docker run --rm quay.io/skopeo/stable list-tags docker://${{ matrix.registry }}/${{ matrix.repository }} \\\n  | jq -r \'[.Tags[] | select(. | match(""^${{ matrix.tag_prefix }}\\\\d+\\\\.\\\\d+\\\\.\\\\d+${{ matrix.tag_suffix }}$"") | .string)] | sort_by(split(""."") | map(ltrimstr(""${{ matrix.tag_prefix }}"") | rtrimstr(""${{ matrix.tag_suffix }}"") | tonumber)) | last\'\n)\necho ""tag=$latest_tag"" >> $GITHUB_OUTPUT\n', 'sed --in-place \'s/tag: ""${{ steps.local.outputs.tag }}""/tag: ""${{ steps.latest.outputs.tag }}""/g\' helm-chart/binderhub/values.yaml\n', 'git --no-pager diff --color=always', 'ci/refreeze', 'git --no-pager diff --color=always']"
""
""
"['docker build --cache-from ${RC_NAME}:cache -t ${RC_NAME} -f docker/Dockerfile .\ndocker tag ${RC_NAME} ${RC_NAME}:${GITHUB_SHA}\ndocker tag ${RC_NAME} ${RC_NAME}:cache\n', 'python3 -m pip install --upgrade pip\npip install poetry\npoetry install\npython3 setup.py install\n', 'wget ""https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz"" -O berrynet/engine/inception_v3_2016_08_28_frozen.pb.tar.gz\ntar -zxvf berrynet/engine/inception_v3_2016_08_28_frozen.pb.tar.gz -C berrynet/engine\n', 'python3 -m unittest\n', ""echo Add other actions to build,\necho test, and deploy your project.\necho GITHUB_SHA is $GITHUB_SHA\t\necho GITHUB_REF is $GITHUB_REF\nsudo apt-get -y update\nsudo apt-get -y install git-buildpackage\nsudo apt-get -y install build-essential devscripts\nsudo apt-get -y install lsb-release\nsudo apt-get -y install debhelper dh-apache2 dh-python git python3 python3-setuptools\nsudo apt-get -y install pristine-tar\nsudo apt-get -y install p7zip-full\nlsb_release -a\ncurl -sL https://raw.githubusercontent.com/DT42/BerryNet-repo/master/setup.sh | sudo -E bash\nsudo apt-get -y install freeboard\ngit remote add github1 https://github.com/grandpaul/BerryNet.git\ngit remote update\ngit branch -a\ngit checkout github1/upstream -b upstream\ngit checkout github1/pristine-tar -b pristine-tar\ngit checkout github1/debian/sid -b debian/sid\necho gbp buildpackage --no-sign\nOLDPWD=`pwd`\necho OLDPWD is $OLDPWD\nsleep 30; uscan --verbose\nVER1=`ls ../berrynet_*.orig.tar.gz | sed 's/.*berrynet_//' | sed 's/.orig.tar.gz//'`\necho VER1 is $VER1\ncd ../berrynet-$VER1; debuild --no-sign; cd $OLDPWD\nls ..\n7z a -snl debpackage.7z ../*.dsc ../*.debian.tar.xz ../*.orig.tar.gz ../*.deb ../*.changes ../*.build ../*.buildinfo ../berrynet-*.tar.gz\n""]"
"['pip install mkdocs-material mkdocs-redirects', 'cd docs\nmkdocs gh-deploy --force\n', 'sudo apt update', 'sudo apt install --no-install-recommends -y x11-xserver-utils', 'pip3 install mypy==0.971 flake8==5.0.4 pyright==1.1.289 yapf==0.31.0 --user', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'flake8 plugin tests', 'pyright plugin']"
"['import os\nrepo = ""${{ github.repository }}""\nevent = ""${{ github.event_name }}""\nref = ""${{ github.event.ref }}""\npublishing = """"\nif (\n    repo == ""jupyterhub/zero-to-jupyterhub-k8s""\n    and event == ""push""\n    and (\n        ref.startswith(""refs/tags/"")\n        or ref == ""refs/heads/main""\n    )\n):\n    publishing = ""true""\n    print(""Publishing chart"")\nwith open(os.environ[""GITHUB_OUTPUT""], ""a"") as f:\n    f.write(f""publishing={publishing}\\n"")\n', 'pip install chartpress pyyaml\npip list\n\nhelm version\n', 'mkdir -p ~/.ssh\nssh-keyscan github.com >> ~/.ssh/known_hosts\necho ""${{ secrets.JUPYTERHUB_HELM_CHART_DEPLOY_KEY }}"" > ~/.ssh/id_ed25519\nchmod 600 ~/.ssh/id_ed25519\n', 'docker login -u ""${{ secrets.DOCKER_USERNAME }}"" -p ""${{ secrets.DOCKER_PASSWORD }}""\n', 'git config --global user.email ""github-actions@example.local""\ngit config --global user.name ""GitHub Actions user""\n', '# Create values.schema.json from values.schema.yaml.\n./tools/generate-json-schema.py\n\n# Append annotations to Chart.yaml with current images so that\n# artifacthub.io can scan and provide vulnerability reports for them.\nchartpress --no-build\n./tools/set-chart-yaml-annotations.py\n', '# Package the Helm chart and publish it to the gh-pages branch of\n# the jupyterhub/helm-chart repo.\n./ci/publish\n', 'helm package jupyterhub', 'pip install pre-commit', 'pre-commit run --all --config .pre-commit-config-shellcheck.yaml', 'pip install chartpress yamllint', 'tools/templates/lint-and-validate.py', 'tools/templates/lint-and-validate.py --strict', '. ci/common\nsetup_helm\npip install pyyaml\n', 'tools/generate-json-schema.py', 'helm lint ./jupyterhub', 'helm lint ./jupyterhub --values tools/templates/lint-and-validate-values.yaml', 'helm lint --strict ./jupyterhub', 'helm lint --strict ./jupyterhub', 'helm install pebble --repo https://jupyterhub.github.io/helm-chart/ pebble --values dev-config-pebble.yaml\n', 'pip3 install -r dev-requirements.txt\nchartpress\n', 'tools/generate-json-schema.py\n', 'helm template --validate jupyterhub ./jupyterhub --values tools/templates/lint-and-validate-values.yaml ${{ matrix.helm-template-validate-extra-args }}\n', '. ./ci/common\nhelm install postgresql postgresql --repo=https://charts.bitnami.com/bitnami ${{ matrix.setup-postgresql-args }}\nawait_kubectl_rollout statefulset/postgresql\n', '. ./ci/common\nif [ ${{ matrix.upgrade-from }} = stable -o ${{ matrix.upgrade-from }} = dev ]; then\n  UPGRADE_FROM_VERSION=$(curl -sSL https://jupyterhub.github.io/helm-chart/info.json | jq -er \'.jupyterhub.${{ matrix.upgrade-from }}\')\nelse\n  UPGRADE_FROM_VERSION=${{ matrix.upgrade-from }}\nfi\necho ""UPGRADE_FROM_VERSION=$UPGRADE_FROM_VERSION"" >> $GITHUB_ENV\n\necho """"\necho ""Installing already released jupyterhub version $UPGRADE_FROM_VERSION""\n\n# FIXME: We change the directory so jupyterhub the chart name won\'t be\n#        misunderstood as the local folder name.\n#\n#        https://github.com/helm/helm/issues/9244\ncd ci\nhelm install jupyterhub --repo https://jupyterhub.github.io/helm-chart/ jupyterhub --values ../dev-config.yaml --version=$UPGRADE_FROM_VERSION ${{ matrix.upgrade-from-extra-args }}\n', 'helm plugin install https://github.com/databus23/helm-diff\n', 'LOCAL_CHART_VERSION=$(cat jupyterhub/Chart.yaml | yq e \'.version\' -)\nexport STRING_REPLACER_A=$LOCAL_CHART_VERSION\nexport STRING_REPLACER_B=$UPGRADE_FROM_VERSION\n\necho ""NOTE: Helm diff upgrade won\'t trigger lookup functions, so it""\necho ""      will look like we seed new passwords all the time.""\necho\necho ""NOTE: For the helm diff only, we have replaced the new chart""\necho ""      version with the old chart version to reduce clutter.""\necho\necho ""      Old version: $UPGRADE_FROM_VERSION""\necho ""      New version: $LOCAL_CHART_VERSION (replaced)""\necho\n\nhelm diff upgrade --install jupyterhub ./jupyterhub --values dev-config.yaml \\\n    --values dev-config-local-chart-extra-config.yaml \\\n    ${{ matrix.local-chart-extra-args }} \\\n    --show-secrets \\\n    --context=3 \\\n    --post-renderer=ci/string-replacer.sh\n', '. ./ci/common\nawait_autohttps_tls_cert_acquisition\nawait_autohttps_tls_cert_save\n', 'kubectl apply -f ci/test-hub-existing-secret.yaml\n', 'helm upgrade --install jupyterhub ./jupyterhub \\\n    --values dev-config.yaml \\\n    --values dev-config-local-chart-extra-config.yaml \\\n    ${{ matrix.local-chart-extra-args }}\n', '. ./ci/common\nawait_autohttps_tls_cert_acquisition\n', "". ./ci/common\n# If you have problems with the tests add '--capture=no' to show stdout\npytest --verbose --maxfail=2 --color=yes ./tests\n"", 'kubectl logs statefulset/postgresql\n', 'pip install chartpress', 'chartpress --builder docker-buildx --platform linux/amd64 --platform linux/arm64', 'pip install -r docs/requirements.txt', ""cd docs\nmake linkcheck SPHINXOPTS='--color -W --keep-going'\n"", './ci/summarise-linkcheck-output ./docs/_build/linkcheck/output.json\n', 'pip install chartpress\n', 'IMAGE_SPEC=$(\n    chartpress --list-images \\\n  | grep ${{ matrix.image_ref }}:\n)\necho ""Identified image: $IMAGE_SPEC""\n\necho ""spec=$IMAGE_SPEC"" >> $GITHUB_OUTPUT\necho ""name=$(echo $IMAGE_SPEC | sed \'s/\\(.*\\):.*/\\1/\')"" >> $GITHUB_OUTPUT\necho ""tag=$(echo $IMAGE_SPEC | sed \'s/.*:\\(.*\\)/\\1/\')"" >> $GITHUB_OUTPUT\n', 'mkdir ./tmp', 'docker build -t rebuilt-image images/${{ matrix.image_ref }}\n', 'echo ""utc_time=$(date --utc +\'%F_%T\')"" >> $GITHUB_OUTPUT\n\njson_to_misc() {\n    # Count vulnerabilities\n    VULNERABILITY_COUNT=""$(cat tmp/scan_$1.json | jq -r \'[.Results[].Vulnerabilities | select(type != null)] | add | select(. != null) | length\')""\n    echo ""VULNERABILITY_COUNT_$1=$VULNERABILITY_COUNT"" >> $GITHUB_ENV\n\n    # Construct a markdown summary\n    if [[ ""$VULNERABILITY_COUNT"" == ""0"" ]]; then\n        echo ""No vulnerabilities! :tada:"" >> tmp/md_summary_$1.md\n    else\n        echo ""Target | Vuln. ID | Package Name | Installed v. | Fixed v."" >> tmp/md_summary_$1.md\n        echo ""-|-|-|-|-"" >> tmp/md_summary_$1.md\n        cat tmp/scan_$1.json | jq -r \'.Results[] | select(.Vulnerabilities != null) | .Type + "" | "" + (.Vulnerabilities[] | .VulnerabilityID + "" | "" + .PkgName + "" | "" + .InstalledVersion + "" | "" + .FixedVersion)\' | sort >> tmp/md_summary_$1.md\n    fi\n\n    # Set a multiline string output with the following technique:\n    # ref: https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n    #\n    eof_marker=EOF_$RANDOM\n    echo ""md_summary_$1<<$eof_marker"" >> $GITHUB_OUTPUT\n    cat tmp/md_summary_$1.md >> $GITHUB_OUTPUT\n    echo ""$eof_marker"" >> $GITHUB_OUTPUT\n\n    # Calculate a hash of the markdown summary\n    HASH=$(cat tmp/md_summary_$1.md | sha1sum)\n    HASH=${HASH:0:10}\n    export HASH_$1=$HASH\n    echo ""hash_$1=$HASH"" >> $GITHUB_OUTPUT\n}\n\njson_to_misc 1\njson_to_misc 2\n\n# Did rebuilding the image change anything?\nif [ ""$HASH_1"" == ""$HASH_2"" ]; then\n    echo ""proceed=no"" >> $GITHUB_OUTPUT\n    echo ""No vulnerabilities were patched by rebuilding the image - won\'t proceed!""\nelse\n    echo ""proceed=yes"" >> $GITHUB_OUTPUT\n    echo ""Vulnerabilities were patched by rebuilding the image - will proceed!""\nfi\n', 'echo ""::warning::None of the $VULNERABILITY_COUNT_1 vulnerabilities got patched by rebuilding the image :(""\n', 'value_to_set=""${{ steps.analyze.outputs.utc_time }}""\nfile_to_update=""images/${{ matrix.image_ref }}/Dockerfile""\nsed --in-place ""s/\\(#.*VULN_SCAN_TIME=\\)\\(.*\\)/\\1${value_to_set}/"" ""$file_to_update""\n\ngit --no-pager diff --color=always\n', 'local_tag=$(cat jupyterhub/values.yaml | yq e \'.${{ matrix.values_path }}\' -)\necho ""tag=$local_tag"" >> $GITHUB_OUTPUT\n', 'latest_tag=$(\n    docker run --rm quay.io/skopeo/stable list-tags docker://${{ matrix.registry }}/${{ matrix.repository }} \\\n  | jq -r \'[.Tags[] | select(. | match(""^v?\\\\d+\\\\.\\\\d+(\\\\.\\\\d+)${{ matrix.version_patch_regexp_group_suffix }}$"") | .string | startswith(""${{ matrix.version_startswith }}""))] | sort_by(split(""."") | map(tonumber? // (.[1:] | tonumber))) | last\'\n)\necho ""tag=$latest_tag"" >> $GITHUB_OUTPUT\n', 'sed --in-place \'s/tag: ""${{ steps.local.outputs.tag }}""/tag: ""${{ steps.latest.outputs.tag }}""/g\' jupyterhub/values.yaml\n', 'git --no-pager diff --color=always', 'pip install packaging requests', 'local_version=$(cat images/hub/requirements.in | grep \'jupyterhub==\' | sed \'s/jupyterhub==//\')\necho ""version=$local_version"" >> $GITHUB_OUTPUT\n', 'import os\nimport packaging.version\nimport requests\n\nrequest = requests.get(""https://pypi.org/pypi/jupyterhub/json"")\ndata = request.json()\nreleases = data[""releases""]\nlatest_version = sorted(releases.keys(), key=packaging.version.Version)[-1]\n\nwith open(os.environ[""GITHUB_OUTPUT""], ""a"") as f:\n    f.write(f""version={latest_version}\\n"")\n', 'for img in hub singleuser-sample; do\n  sed --in-place \'s/jupyterhub==${{ steps.local.outputs.version }}/jupyterhub==${{ steps.latest.outputs.version }}/g\' images/$img/requirements.in\ndone\nsed --in-place \'s/appVersion: ""${{ steps.local.outputs.version }}""/appVersion: ""${{ steps.latest.outputs.version }}""/g\' jupyterhub/Chart.yaml\n', 'ci/refreeze', 'git --no-pager diff --color=always', 'ci/refreeze', 'git --no-pager diff --color=always']"
"['python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'pytest tests/terraform_compliance', 'echo ""Released version is ${{ steps.strip-tag.outputs.tag }}""\ncat terraform_compliance/__init__.py | sed s/{{VERSION}}/${{ steps.strip-tag.outputs.tag }}/g  > terraform_compliance/__init__.py.templated\ncat terraform_compliance/__init__.py.templated\ncp terraform_compliance/__init__.py.templated terraform_compliance/__init__.py\necho ""export RELEASE_VERSION=$(cat terraform_compliance/__init__.py | grep __version__ | cut -d ""\'"" -f2)"" > reqs.sh\nsource reqs.sh\nif [[ ""${{ steps.strip-tag.outputs.tag }}"" != ""$RELEASE_VERSION"" ]]; then echo ""Released version (${{ steps.strip-tag.outputs.tag }}) does not match with RELEASE_VERSION ($RELEASE_VERSION)""; exit 1; fi\npython setup.py install_egg_info\n', 'python setup.py sdist bdist_wheel && \\\nls -al dist/* && \\\npip install --force-reinstall dist/terraform_compliance-${{ steps.strip-tag.outputs.tag }}-*.whl\n', 'python tests/functional/run_functional_tests.py\n', 'twine upload --skip-existing dist/*', 'sleep 120\necho ""Getting the latest terraform version from Hashicorp""\necho ""export LATEST_TERRAFORM_VERSION=$(curl https://checkpoint-api.hashicorp.com/v1/check/terraform | jq -r .current_version)"" > terraform_version.sh\nsource terraform_version.sh\nif [ -z ""$LATEST_TERRAFORM_VERSION"" ]; then echo ""Can not identify latest terraform version!""; travis_terminate 1; fi\ndocker build --compress --no-cache -t ""$IMAGE_NAME"" \\\n             --build-arg VERSION=${{ steps.strip-tag.outputs.tag }} \\\n             --build-arg LATEST_TERRAFORM_VERSION=$LATEST_TERRAFORM_VERSION \\\n             --build-arg HASHICORP_PGP_KEY=""$(cat hashicorp-pgp-key.pub)"" .\ndocker login -u ""$DOCKER_HUB_USER"" -p ""$DOCKER_HUB_PASSWORD""\ndocker tag ""$IMAGE_NAME"" $IMAGE_NAME:latest\ndocker tag ""$IMAGE_NAME"" ""$IMAGE_NAME"":""${{ steps.strip-tag.outputs.tag }}""\ndocker push ""$IMAGE_NAME"":latest\ndocker push ""$IMAGE_NAME"":""${{ steps.strip-tag.outputs.tag }}""\n', 'python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'pytest tests/terraform_compliance\n', 'python setup.py sdist bdist_wheel\npip install dist/terraform_compliance-0.0.0-*.whl\n', 'python tests/functional/run_functional_tests.py\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest pytest-benchmark        \nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install lap scipy ""ortools<9.4"" lapsolver munkres\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['sudo apt-get install liblzo2-dev libsnappy-dev', 'pip install -e . --no-deps\n', 'pytest --verbose --cov=streamz\n', 'coveralls', 'pip install flake8\n', 'flake8 streamz\n', 'python -m pip install --upgrade pip\npip install setuptools setuptools-scm wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['gh pr review --approve ""$PR_URL""', 'gh pr merge --auto --squash ""$PR_URL""', 'pip install --upgrade pip hatch', 'hatch run lint:all', 'hatch run docs:build', 'hatch build', 'pip install --upgrade pip hatch coveralls', 'sudo apt-get install binutils libproj-dev gdal-bin', 'hatch run test-cov', 'coverage combine', 'coveralls --service=github', 'exit 1']"
"['docker run -v $GITHUB_WORKSPACE:/clif -w /clif/build localhost:5000/clif bash -c \'cmake /clif \\\n  -DPYTHON_EXECUTABLE=""/usr/bin/python${{ matrix.python }}"" \\\n  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\n  -DCMAKE_RULE_MESSAGES:BOOL=OFF\'\n', 'docker run -v $GITHUB_WORKSPACE:/clif -w /clif/build localhost:5000/clif make -j$(nproc) --no-print-directory', 'docker run -v $GITHUB_WORKSPACE:/clif -w /clif/build localhost:5000/clif ctest', 'docker run -v $GITHUB_WORKSPACE:/clif -w /clif/build localhost:5000/clif make runPyClifIntegrationTests -j$(nproc) --no-print-directory', ""docker run -v $GITHUB_WORKSPACE:/clif -w /clif localhost:5000/clif bash -c '\n  ./INSTALL.sh $(which python${{ matrix.python }}) && \\\n  cd examples && \\\n  cmake . \\\n      -DPYTHON_EXECUTABLE=$(which python${{ matrix.python }}) \\\n      -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\n      -DCMAKE_RULE_MESSAGES:BOOL=OFF && \\\n  make -j$(nproc) --no-print-directory && \\\n  python${{ matrix.python }} -m pip install .'\n""]"
"['python -m pip install --upgrade pip\npip install -e .\npip install -r requirements-test.txt\n', 'pytest\n', 'vermin --target=3.6 --no-tips .\n', 'flake8\n']"
"['pip install pre-commit', 'pre-commit autoupdate', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['sudo add-apt-repository --yes ppa:kicad/kicad-6.0-releases\nsudo apt-get -qq install --yes --no-install-recommends \\\n  kicad kicad-footprints kicad-symbols kicad-packages3d \\\n  python3 python3-pip \\\n  make git librsvg2-bin xvfb xdotool bats\nsudo python3 -m pip install mypy\npython3 -m pip install -e .\\[dev\\]\n', 'make mypy', 'make test', 'make package']"
"['python -m pip install --upgrade pip\n# pip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\n# pytest\n']"
['git checkout HEAD^2']
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools --upgrade\npip install -r requirements-dev.txt\n', 'flake8 . --exit-zero --max-line-length=100 --ignore=F401,W503 --max-complexity=10 --max-line-length=100\n', 'pytest --cov --cov-report=xml\n']"
"['sudo apt install --yes black clang-format', 'black --check .', 'clang-format --dry-run --Werror pdftotext.cpp', 'sudo apt install --yes libpoppler-cpp-dev pkg-config', 'brew install pkg-config poppler', 'pip install --verbose .', 'python -m unittest discover --verbose --start-directory tests', 'conda config --remove channels defaults', 'conda install c-compiler cxx-compiler pkg-config poppler', 'pip install --verbose .', 'python -m unittest discover --verbose --start-directory tests']"
"['python -m pip install --upgrade pip\n', 'python -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python -m pip install --editable .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
['pip3 install flake8 flake8-docstrings pydocstyle\nflake8 -v\n']
"['python --version\npip --version\nconda --version\n', 'mkdir -p $HOME/.sbt/launchers/1.3.12\ncurl -L -o $HOME/.sbt/launchers/1.3.12/sbt-launch.jar https://repo1.maven.org/maven2/org/scala-sbt/sbt-launch/1.3.12/sbt-launch.jar\n', 'pip install "".[test]""\n', 'pip freeze\n', 'make itest-yarn\n', 'python --version\npip --version\npip list\ndocker logs itest-yarn\n', 'make lint\n', 'pipx run tbump --dry-run --no-tag --no-push 100.100.100rc0\n', 'make docs', 'pytest -vv -W default || pytest -vv -W default --lf\n']"
"['pip install --upgrade -r requirements.txt', 'python lib/fetch.py fetch-all', 'bash tests/run-tests.sh', 'pytest lib/', 'docker-compose build', 'docker images', 'docker-compose -f docker-compose.yml up -d\n# docker-compose -f docker-compose.yml -f docker-compose.debug.yml up -d\ndocker-compose ps\n# wait until the web server is up\nwget --timeout 3 --tries=5 --spider localhost:8002 2>&1 | grep -i http\ndocker-compose logs --no-color\n', 'CHEATSH_TEST_STANDALONE=NO bash tests/run-tests.sh']"
""
"['pdm install -G linting -G email', 'pdm run python3 ./changes/make_history.py', 'pdm install -G docs', ""pdm run python -c 'import docs.plugins.main'"", 'pdm run mkdocs build --verbose', 'pdm install -G testing -G testing-extra -G email -G memray\npdm add pytest-memray\n', 'pdm run pytest --ignore=tests/mypy/ --ignore=tests/test_docs.py --memray', 'pdm install -G testing', 'pdm info && pdm list', 'pdm run python -c ""import pydantic.version; print(pydantic.version.version_info())""', 'mkdir coverage', 'make test', 'pdm install -G testing-extra -G email', 'make test', 'pdm install -G testing -G mypy\n', ""if [ ${{ matrix.mypy-version }} != 'lockfile' ]; then\n  pdm remove mypy && pdm add mypy==${{ matrix.mypy-version }}\nfi\npdm list\n"", 'mkdir coverage', 'pdm run coverage run -m pytest tests/mypy --test-mypy', 'npm install -g pyright@1.1.302', 'make test-pyright', 'pip install coverage[toml]', 'ls -la coverage', 'coverage combine coverage', 'coverage report', 'coverage html --show-contexts --title ""pydantic coverage for ${{ github.sha }}""', 'pdm install -G :all\n', 'make test-fastapi', 'pdm install -G testing -G email\n', ""if [ ${{ matrix.typing-extensions-version }} == 'main' ]; then\n  pdm remove typing-extensions && pdm add 'git+https://github.com/python/typing_extensions.git'\nelse\n  pdm remove typing-extensions && pdm add 'typing-extensions==${{ matrix.typing-extensions-version }}'\nfi\npdm list\n"", 'make test', 'make test-pydantic-settings', 'pdm install -G docs\npdm run pip install https://files.scolvin.com/${MKDOCS_TOKEN}/mkdocs-material/mkdocs_material-9.1.5+insiders.4.32.4-py3-none-any.whl\n', ""pdm run python -c 'import docs.plugins.main'"", 'git config --global user.name ""${{ github.actor }}""\ngit config --global user.email ""${{ github.actor }}@users.noreply.github.com""\n', 'pdm run mike deploy -b docs-site dev-v2 --push', 'mike deploy -b docs-site ${{ steps.check-version.outputs.VERSION_MAJOR_MINOR }} latest --update-aliases --push', 'pip install -U build', 'python -m build', 'pip install tweepy==4.14.0', 'import os\nimport tweepy\n\nclient = tweepy.Client(\n    access_token=os.getenv(""TWITTER_ACCESS_TOKEN""),\n    access_token_secret=os.getenv(""TWITTER_ACCESS_TOKEN_SECRET""),\n    consumer_key=os.getenv(""TWITTER_CONSUMER_KEY""),\n    consumer_secret=os.getenv(""TWITTER_CONSUMER_SECRET""),\n)\ntweet = os.getenv(""TWEET"")\nclient.create_tweet(text=tweet)\n', 'pip install pdm && pdm install --no-isolation', 'pdm run pip install ${{ matrix.PYTHON_DEPENDENCY_CASE }}', 'pdm run pip freeze', 'make test', 'pip install smokeshow', 'smokeshow upload coverage-html']"
""
"['git diff --name-only $(git merge-base origin/main HEAD) | grep \'^allennlp/.*\\.py$\' && echo ""source_files_changed=true"" >> $GITHUB_ENV || echo ""source_files_changed=false"" >> $GITHUB_ENV\n', '# If this step fails, this means you haven\'t updated the CHANGELOG.md\n# file with notes on your contribution.\ngit diff --name-only $(git merge-base origin/main HEAD) | grep \'^CHANGELOG.md$\' && echo ""Thanks for helping keep our CHANGELOG up-to-date!""\n', ""grep -E '^black' dev-requirements.txt | xargs pip install\n"", 'pip freeze\n', 'black --check .\n', '# Get the exact Python version to use in the cache key.\necho ""PYTHON_VERSION=$(python --version)"" >> $GITHUB_ENV\necho ""RUNNER_ARCH=$(uname -m)"" >> $GITHUB_ENV\n# Use week number in cache key so we can refresh the cache weekly.\necho ""WEEK_NUMBER=$(date +%V)"" >> $GITHUB_ENV\n', 'echo ""TORCH_INSTALL=$TORCH_CPU_INSTALL"" >> $GITHUB_ENV\n', 'echo ""TORCH_INSTALL=$TORCH_GPU_INSTALL"" >> $GITHUB_ENV\n', 'python${{ env.DEFAULT_PYTHON_VERSION }} -m venv .venv\nsource .venv/bin/activate\nmake install TORCH_INSTALL=""$TORCH_INSTALL""\n', 'source .venv/bin/activate\npip install --no-deps -e .[all]\nmake download-extras\n', 'source .venv/bin/activate\ngit clone https://github.com/allenai/allennlp-models.git\ncd allennlp-models\n# git checkout dependabot/pip/torch-gte-1.7.0-and-lt-1.13.0\npip install -e .[dev,all]\n', 'source .venv/bin/activate\npip freeze\n', 'source .venv/bin/activate\npython scripts/check_torch_version.py\n', 'source .venv/bin/activate\n${{ matrix.task.run }}\n', 'mkdir coverage\nmv coverage.xml coverage/\n', ""# Could run into issues with the cache if we don't uninstall the editable.\n# See https://github.com/pypa/pip/issues/4537.\nsource .venv/bin/activate\npip uninstall --yes allennlp allennlp-models\n"", 'echo ""PYTHON_VERSION=$(python --version)"" >> $GITHUB_ENV\necho ""RUNNER_ARCH=$(uname -m)"" >> $GITHUB_ENV\necho ""WEEK_NUMBER=$(date +%V)"" >> $GITHUB_ENV\necho ""TORCH_INSTALL=$TORCH_CPU_INSTALL"" >> $GITHUB_ENV\n', 'python${{ env.DEFAULT_PYTHON_VERSION }} -m venv .venv\nsource .venv/bin/activate\nmake install TORCH_INSTALL=""$TORCH_INSTALL""\n', 'source .venv/bin/activate\npip install --no-deps -e .[all]\nmake download-extras\n', 'source .venv/bin/activate\npip freeze\n', '# Verify that current version is ahead of the last release.\nsource .venv/bin/activate\nLATEST=$(scripts/get_version.py latest)\nCURRENT=$(scripts/get_version.py current)\nif [ ""$CURRENT"" == ""$LATEST"" ]; then\n    echo ""Current version needs to be ahead of latest release in order to build nightly release"";\n    exit 1;\nfi\n# This is somewhat bizarre, but you can\'t set env variables to bash\n# commands in the action workflow - so we have to use this odd way of\n# exporting a variable instead.\necho ""ALLENNLP_VERSION_SUFFIX=dev$(date -u +%Y%m%d)"" >> $GITHUB_ENV\n', '# Remove \'refs/tags/\' to get the actual tag from the release.\nsource .venv/bin/activate\nTAG=${GITHUB_REF#refs/tags/};\nVERSION=$(scripts/get_version.py current)\nif [ ""$TAG"" != ""$VERSION"" ]; then\n    echo ""Bad tag or version. Tag $TAG does not match $VERSION"";\n    exit 1;\nfi\n', '# Just print out the version for debugging.\nsource .venv/bin/activate\nmake version\npython setup.py bdist_wheel sdist\n', 'source .venv/bin/activate\npip uninstall --yes allennlp\n', 'pip install --upgrade pip setuptools wheel\n', 'pip install $(ls dist/*.whl)${{ matrix.flavor }} -c constraints.txt\n', 'make download-extras\n', 'rm -rf allennlp/ setup.py tests/ test_fixtures/\n', 'pip freeze\n', 'allennlp test-install\n', 'echo ""DOCKER_TORCH_VERSION=${TORCH_VERSION}-cuda${CUDA}-python3.8"" >> $GITHUB_ENV;\nif [[ $GITHUB_EVENT_NAME == \'release\' ]]; then\n    echo ""DOCKER_IMAGE_NAME=allennlp/allennlp:${GITHUB_REF#refs/tags/}-cuda${CUDA}"" >> $GITHUB_ENV;\nelse\n    echo ""DOCKER_IMAGE_NAME=allennlp/commit:${GITHUB_SHA}-cuda${CUDA}"" >> $GITHUB_ENV;\nfi\n', 'make docker-image DOCKER_IMAGE_NAME=""$DOCKER_IMAGE_NAME"" DOCKER_TORCH_VERSION=""$DOCKER_TORCH_VERSION""\n', 'docker run --rm $DOCKER_IMAGE_NAME test-install\n', 'docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\n', 'docker push $DOCKER_IMAGE_NAME\n', 'docker tag $DOCKER_IMAGE_NAME allennlp/commit:${GITHUB_SHA}\ndocker push allennlp/commit:${GITHUB_SHA}\n', 'docker tag $DOCKER_IMAGE_NAME allennlp/allennlp:latest\ndocker push allennlp/allennlp:latest\n', 'echo ""PYTHON_VERSION=$(python --version)"" >> $GITHUB_ENV\necho ""RUNNER_ARCH=$(uname -m)"" >> $GITHUB_ENV\necho ""WEEK_NUMBER=$(date +%V)"" >> $GITHUB_ENV\necho ""TORCH_INSTALL=$TORCH_CPU_INSTALL"" >> $GITHUB_ENV\n', 'python${{ env.DEFAULT_PYTHON_VERSION }} -m venv .venv\nsource .venv/bin/activate\nmake install TORCH_INSTALL=""$TORCH_INSTALL""\n', 'source .venv/bin/activate\npip install --no-deps -e .[all]\nmake download-extras\n', 'source .venv/bin/activate\npip freeze\n', 'if [[ $GITHUB_EVENT_NAME == \'release\' ]]; then\n    echo ""DOCS_FOLDER=${GITHUB_REF#refs/tags/}"" >> $GITHUB_ENV;\n    echo ""BASE_SOURCE_LINK=https://github.com/allenai/allennlp/blob/${GITHUB_REF#refs/tags/}/allennlp/"" >> $GITHUB_ENV;\nelse\n    echo ""DOCS_FOLDER=main"" >> $GITHUB_ENV;\n    echo ""BASE_SOURCE_LINK=https://github.com/allenai/allennlp/blob/main/allennlp/"" >> $GITHUB_ENV;\nfi\n', 'source .venv/bin/activate\nenv PYTHONPATH=. ./scripts/build_docs.sh\n', 'echo ${{ github.ref }}\n', 'git config --global user.email ""ai2service@allenai.org""\ngit config --global user.name ""ai2service""\ngit config --global push.default simple\n', 'echo ""Staging docs to $DOCS_FOLDER""\n\n# Checkout allennlp-docs to /allennlp-docs\ngit clone git@github.com:allenai/allennlp-docs.git ~/allennlp-docs\n\n# Copy the generated docs to the checked out docs repo\nrm -rf ~/allennlp-docs/$DOCS_FOLDER/\nmkdir -p ~/allennlp-docs/$DOCS_FOLDER\ncp -r site/* ~/allennlp-docs/$DOCS_FOLDER\n', '# Fail immediately if any step fails.\nset -e\nsource .venv/bin/activate\n\nLATEST=$(./scripts/get_version.py latest)\nSTABLE=$(./scripts/get_version.py stable)\n\ncd ~/allennlp-docs/\n\necho ""Updating latest/index.html to point to $LATEST""\nmkdir -p latest\ncat >latest/index.html << EOL\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta http-equiv=""Refresh"" content=""0; url=/${LATEST}/"" />\n  </head>\n  <body>\n    <p>Please follow <a href=""/${LATEST}/"">this link</a>.</p>\n  </body>\n</html>\nEOL\n\necho ""Updating stable/index.html to point to $STABLE""\nmkdir -p stable\ncat >stable/index.html << EOL\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta http-equiv=""Refresh"" content=""0; url=/${STABLE}/"" />\n  </head>\n  <body>\n    <p>Please follow <a href=""/${STABLE}/"">this link</a>.</p>\n  </body>\n</html>\nEOL\n', '# And push them up to GitHub\ncd ~/allennlp-docs/\ngit add -A\ngit commit -m ""automated update of the docs""\ngit push\n', 'cd ~/allennlp-docs/\ngit checkout --orphan latest_branch\ngit add -A\ngit commit -m ""Re-write commit history""\ngit branch -D master  # remove old master branch\ngit branch -m master  # rename clean new branch to master branch\ngit push -f origin master\n', 'source .venv/bin/activate\npip uninstall --yes allennlp\n', 'pip install --upgrade pip setuptools wheel twine\n', 'twine upload -u allennlp -p ${{ secrets.PYPI_PASSWORD }} dist/*\n', 'pip install PyGithub\n', 'python scripts/close_stale_issues.py\n', 'pip install PyGithub\n', 'python scripts/ping_issue_assignees.py\n']"
"['pip install -r requirements.txt', 'pip install mkdocstrings==0.14.0', 'pip install mkdocs-material', 'mkdocs gh-deploy --force', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt update\nsudo apt install ffmpeg\n', 'python -m pip install --upgrade \\\n  ""pip==20.3.4; python_version < \'3.6\'"" \\\n  ""pip==21.3.1; python_version >= \'3.6\'""\npython -m pip install tox==3.24.5 tox-gh-actions==2.9.1\n', 'tox', '# TODO: use standard `psf/black` action after dropping Python 2 support.\npip install black==21.12b0 click==8.0.2  # https://stackoverflow.com/questions/71673404\nblack ffmpeg --check --color --diff\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest==5.3.2 pytest-faulthandler psutil\npip install -e .\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
[]
"['scripts/install', 'scripts/build', 'scripts/publish', 'scripts/install', 'scripts/check', 'scripts/build', 'scripts/test', 'scripts/coverage']"
"['gh pr merge --auto --squash ${{ github.event.pull_request.html_url }}', 'python -m pip install --upgrade build', 'python -m build', 'python -m pip install --upgrade build', 'python -m build backend', 'python scripts/validate_history.py', 'python -m pip install --upgrade pip', 'pip install -e .\npip install -e ./backend\n', ""git config --local user.name 'github-actions[bot]'\ngit config --local user.email 'github-actions[bot]@users.noreply.github.com'\n"", 'hatch run docs:build-check', 'hatch run docs:ci-build dev', 'git archive -o site.zip gh-pages', 'python -m zipfile -e site.zip site', 'python scripts/validate_history.py', 'python -m pip install --upgrade pip', 'pip install -e .\npip install -e ./backend\n', 'hatch version', 'python scripts/set_release_version.py', ""git config --local user.name 'github-actions[bot]'\ngit config --local user.email 'github-actions[bot]@users.noreply.github.com'\n"", 'hatch run docs:build-check', 'hatch run docs:ci-build $HATCH_DOCS_VERSION latest', 'git archive -o site.zip gh-pages', 'python -m zipfile -e site.zip site', 'python -m pip install --upgrade pip', 'pip install -e .\npip install -e ./backend\n', 'hatch run lint:all', 'hatch run full', 'mv .coverage "".coverage.${{ matrix.os }}.${{ matrix.python-version }}""', 'pip install hatch', 'hatch run coverage:combine', 'hatch run coverage:report-xml\nhatch run coverage:report-uncovered-html\n', 'hatch run coverage:generate-summary', 'hatch run coverage:write-summary-report', 'pip install --upgrade -r backend/tests/downstream/requirements.txt', 'python backend/tests/downstream/integrate.py', 'wget https://github.com/sharkdp/hyperfine/releases/download/v${HYPERFINE_VERSION}/hyperfine_${HYPERFINE_VERSION}_amd64.deb\nsudo dpkg -i hyperfine_${HYPERFINE_VERSION}_amd64.deb\n', 'pip install --upgrade flit poetry pipenv', 'pip install .', 'hyperfine -m 100 --warmup 10 -i pipenv\nhyperfine -m 100 --warmup 10 poetry\nhyperfine -m 100 --warmup 10 -i flit\nhyperfine -m 100 --warmup 10 hatch\n']"
"['pip install tox', 'tox -e lint', 'sudo apt-get update -qq && sudo apt-get install -y libmemcached-dev', ""pip install -e '.[dev]'"", 'pytest --benchmark-skip', 'python pytest-gevent.py --benchmark-skip', 'python -m pip install build', 'python -m build .']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'sudo apt-get install -y gettext \npython -m pip install --upgrade pip wheel setuptools\npip install -e .[dev]\n', 'python ../scripts/i18n_updater.py\n', 'echo ""Locale files updated""', 'sudo apt-get install -y gettext \npython -m pip install -U pip\npython -m pip install -U setuptools twine wheel\npip install -e .[dev]\n', 'echo ""Checking if locale files need updating. If they do, cd rest_framework_simplejwt && run python ../scripts/i18n_updater.py""\npython ../scripts/i18n_updater.py\ngit diff --exit-code\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
""
['pip wheel -w wheelhouse/ .\n']
"['python -m pip install --upgrade pip\npip install numpy -I\npip install pytest torch\n', 'pip install -e .\n', 'cd docs/\npip install -r requirements.txt\n', 'cd docs/\nmake html\ntouch build/html/.nojekyll\n', 'python -m pip install --upgrade pip\npip install numpy -I\npip install pytest torch\n', 'pip install -e .\n', 'cd docs/\npip install -r requirements.txt\n', 'cd docs/\nmake html\ntouch build/html/.nojekyll\naws s3 sync build/html/ s3://hangzh/encoding/docs/${{ env.PULL_NUMBER }}/ --acl public-read --follow-symlinks --delete\n', 'python -m pip install --upgrade pip\npip install -e .\n  \n', 'pip install nose\nnosetests -v tests/unit_test/\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install sphinx==4.4.0 pyspark==3.2.1 tensorflow-cpu==2.8.0 pytest\n', 'pushd docs/ && PYTHONPATH=.. make clean html && popd', 'python -m pytest tests/\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8\n', 'flake8 --per-file-ignores=""__init__.py:F401"" .\n', 'python -m pip install --upgrade pip\npip install pylint\n', 'pylint `ls -R|grep .py$|xargs`\n']"
"['python -m pip install --upgrade pip\npython -m pip install tox railroad-diagrams Jinja2\n', 'tox']"
""
"['pip install build\npython -m build --sdist\n', 'pip install -e . --no-deps\npip install -r requirements_docs.txt\n', 'pip uninstall vtk -y\npip install https://github.com/pyvista/pyvista-wheels/raw/main/vtk_osmesa-9.2.5-cp311-cp311-linux_x86_64.whl\n', 'python -c ""import pyvista;print(pyvista.Report())""\necho PYVISTA_EXAMPLE_DATA_PATH=$(python -c ""from pyvista import examples; print(examples.USER_DATA_PATH)"") >> $GITHUB_ENV\npip list\n', 'make -C doc html', 'cp doc/source/ads.txt doc/_build/html/', 'if [ -e doc/sphinx_warnings.txt ]; then cat doc/sphinx_warnings.txt; fi', 'if [ -e doc/errors.txt ]; then cat doc/errors.txt; fi', ""mkdir _notebooks\nfind doc/source/examples -type f -name '*.ipynb' | cpio -p -d -v _notebooks/\n"", 'pip install cookiecutter\n', 'cookiecutter -f --no-input --config-file ./doc/source/pyvista-binder-config.yml https://github.com/pyvista/cookiecutter-pyvista-binder.git;\nrm -rf ./pyvista-examples/notebooks/\ncp -r doc/source/examples/ ./pyvista-examples/\nls -l ./pyvista-examples/\n', 'git clone --depth=1 https://github.com/mne-tools/mne-python.git --branch main --single-branch', './mne-python/tools/setup_xvfb.sh', 'pip install numpy scipy matplotlib nibabel PyQt6 qtpy ipyvtklink ipympl pytest pytest-cov pytest-harvest pytest-timeout sphinx-gallery nbformat nbclient imageio imageio-ffmpeg', 'pip install -ve .', 'pip install -v git+https://github.com/pyvista/pyvistaqt.git', 'pip install -ve .', 'mne sys_info -p', './tools/get_testing_version.sh', './tools/github_actions_download.sh', 'pytest mne/viz/_brain mne/viz/tests/test_3d.py mne/viz/backends', 'git clone https://github.com/pyvista/pyvistaqt.git --single-branch', 'pip install -ve ./pyvistaqt -r ./pyvistaqt/requirements_test.txt PyQt6', 'pip install -ve .', 'pytest ./tests', 'make -C doc update-intersphinx', 'pip install https://github.com/pyvista/pyvista-wheels/raw/main/vtk_osmesa-9.2.5-cp39-cp39-linux_x86_64.whl', 'pip install -e . --no-deps', 'pip install -r requirements_docs.txt', 'make -C doc linkcheck SPHINXOPTS=""--color""', 'pip install -e . --no-deps\npip install -r requirements_test.txt vtk\n', 'python -c ""import pyvista; print(pyvista.Report()); from pyvista import examples; print(\'Examples path:\', examples.USER_DATA_PATH)""\nwhich python\npip list\n', 'make doctest-modules', 'make doctest-modules-local-namespace', 'pip install . --no-deps', 'pip install vtk -r requirements_test.txt', 'python -c ""import pyvista;print(pyvista.Report(gpu=False));from pyvista import examples;print(\'User data path:\', examples.USER_DATA_PATH)""', 'pytest -v --ignore=tests/plotting', 'pytest -v tests/plotting --fail_extra_image_cache', 'source /usr/share/miniconda/etc/profile.d/conda.sh\nconda config --set channel_priority strict\nconda update -n base conda\nconda install -n base conda-libmamba-solver\n', 'rm /usr/share/miniconda/pkgs/cache/*.json', 'source /usr/share/miniconda/etc/profile.d/conda.sh\nconda config --add channels conda-forge\nconda env create --quiet -n ${{ env.conda_env }} --file environment.yml --experimental-solver=libmamba\nconda activate ${{ env.conda_env }}\nconda list\n', 'source /usr/share/miniconda/etc/profile.d/conda.sh\nconda activate ${{ env.conda_env }}\npip install -e .\nwhich python\npython -c ""import pyvista; print(pyvista.Report(gpu=False))""\npython -c ""from pyvista import examples; print(examples.USER_DATA_PATH)""\n', 'source /usr/share/miniconda/etc/profile.d/conda.sh\nconda activate ${{ env.conda_env }}\npip install -r requirements_test.txt\npytest -v --ignore=tests/plotting\n', 'source /usr/share/miniconda/etc/profile.d/conda.sh\nconda activate ${{ env.conda_env }}\npytest -v tests/plotting --cov-report html --fail_extra_image_cache\n', 'pip install build\npython -m build --wheel\npip install dist/pyvista*.whl\n', 'pip install vtk==${{ matrix.vtk-version }}', 'xvfb-run python -c ""import pyvista; print(pyvista.Report());""\n', 'pip install -r requirements_test.txt', ""pip install 'numpy<1.24'"", ""pip install 'matplotlib<3.6'"", 'xvfb-run python -c ""import pyvista; print(pyvista.Report()); from pyvista import examples; print(\'User data path:\', examples.USER_DATA_PATH)""\nwhich python\npip list\n', 'xvfb-run python -m pytest --cov=pyvista --fail_extra_image_cache --cov-report=xml -v', 'pip install build twine\npython -m build\ntwine check --strict dist/*\n', 'twine upload --skip-existing dist/pyvista*\n', 'pip install . --no-deps', 'pip install vtk -r requirements_test.txt', 'python -c ""import pyvista; print(pyvista.Report(gpu=False)); from pyvista import examples; print(\'User data path:\', examples.USER_DATA_PATH)""', 'chcp 65001 #set code page to utf-8\necho (""PYVISTA_EXAMPLE_DATA_PATH="" + $(python -c ""import pyvista; from pyvista import examples; print(examples.USER_DATA_PATH)"")) >> $env:GITHUB_ENV\n', 'python -m pytest -v --ignore=tests/plotting', 'python -m pytest -v tests/plotting --fail_extra_image_cache', 'pip install . --no-deps', 'pip install -r requirements_test.txt', 'pip uninstall vtk -y\npip install vtk_osmesa --pre --no-cache --extra-index-url https://wheels.vtk.org\n', 'python -c ""import pyvista; print(pyvista.Report()); from pyvista import examples; print(\'User data path:\', examples.USER_DATA_PATH)""\nwhich python\npip list\npip show vtk_osmesa\n', 'pytest -v tests/ --fail_extra_image_cache']"
"['docker run jupyter/repo2docker:pr repo2docker --version', 'pip install build\npip freeze\n', 'python -m build --sdist --wheel .\nls -l dist\n', 'if [ ""${{ startsWith(github.ref, \'refs/tags/\') || (github.ref == \'refs/heads/main\') }}"" = ""true"" ]; then\n    REGISTRY=$DEFAULT_REGISTRY\nelse\n    REGISTRY=localhost:5000\nfi\necho ""REGISTRY=$REGISTRY"" >> $GITHUB_ENV\necho ""Publishing to $REGISTRY""\n', 'docker login -u ""${{ secrets.DOCKER_REGISTRY_USERNAME }}"" -p ""${{ secrets.DOCKER_REGISTRY_TOKEN }}"" ""${{ env.REGISTRY }}""\n', 'VERSION=$(python3 -c \'import versioneer; print(versioneer.get_version().replace(""+"", ""-""))\')\nTAGS=""${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:$VERSION""\nif [ ""${{ github.ref }}"" == ""refs/heads/main"" ]; then\n  TAGS=""$TAGS,${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main""\nfi\nif [ ""${{ startsWith(github.ref, \'refs/tags/\') }}"" = ""true"" ]; then\n  TAGS=""$TAGS,${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest""\nfi\n\necho ""TAGS=$TAGS""\necho ""TAGS=$TAGS"" >> $GITHUB_ENV\n', 'pip install -r dev-requirements.txt\npip freeze\n', 'python -m build --wheel .\npip install dist/*.whl\n\n# add for mercurial tests\npip install mercurial hg-evolve\n\npip freeze\n', 'pytest --verbose --color=yes --durations=10 --cov=repo2docker tests/${{ matrix.repo_type }}\n']"
[]
"['python -m pip install --upgrade pip\npip install flake8 pytest_cov coveralls\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'PYTHONPATH=. pytest --cov=./gem\n', 'coveralls', 'pip install build', 'python -m build --sdist --wheel --outdir dist/ .']"
"['sudo apt-get install -y pandoc', '# install pip=>20.1 to use ""pip cache dir""\npython3 -m pip install --upgrade pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'python3 -m pip install -r ./docs/requirements.txt', 'cd docs && make html', '# install pip=>20.1 to use ""pip cache dir""\npython3 -m pip install --upgrade pip\n', 'python3 -m pip install twine', 'python3 setup.py bdist_wheel\npython3 setup.py sdist\n']"
"['npm install\nnpm run lint\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 ../../ --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 ../../ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npytest\n', 'cargo build --verbose', 'cargo test --verbose']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt -r requirements-test.txt\n', 'pre-commit run -a\ncoverage run --source repokid -m py.test\nbandit -r . -ll -ii -x repokid/tests/\n', 'coverage run --source repokid -m py.test\n', 'git checkout HEAD^2', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get -yq install mono-vbnc dos2unix', 'dotnet --info\ntry { msbuild -version } catch { }\ntry { mono --version } catch { }\n', 'pwsh make.ps1', 'pwsh make.ps1 package', './make.ps1 -frameworks net45 test-all', './make.ps1 -frameworks netcoreapp2.1 test-all', './make.ps1 -frameworks netcoreapp3.1 test-all']"
"['pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --max-complexity 20 --max-line-length 127 --statistics    \n', 'pip install tox\n# tox --parallel all\ntox\n']"
"['pip install --upgrade pip setuptools wheel\npip install flake8 flake8-print\n', 'flake8', 'echo ""VERSION=${{ matrix.clickhouse-version }}"" > tests/.env\nif [[ ""${{ matrix.clickhouse-version }}"" > ""21.7"" ]]; then echo ""ORG=clickhouse""; else echo ""ORG=yandex"" ; fi  >> tests/.env\ndocker-compose -f tests/docker-compose.yml up -d\n', '# Faking clickhouse-client real communication with container via docker exec.\necho -e \'#!/bin/bash\\n\\ndocker exec -e ""`env | grep ^TZ=`"" test-clickhouse-client clickhouse client ""$@""\' | sudo tee /usr/local/bin/clickhouse-client > /dev/null\nsudo chmod +x /usr/local/bin/clickhouse-client\n# Overriding setup.cfg. Set host=clickhouse-server\nsed -i \'s/^host=localhost$/host=clickhouse-server/\' setup.cfg\n# Make host think that clickhouse-server is localhost\necho \'127.0.0.1 clickhouse-server\' | sudo tee /etc/hosts > /dev/null\n', 'CYTHON_TRACE=1 python setup.py build_ext --define CYTHON_TRACE', ""# Newer coveralls do not work with github actions.\npip install 'coveralls<3.0.0'\npip install cython\npython testsrequire.py\npython setup.py develop\n# Limit each test time execution.\npip install pytest-timeout\n"", 'coverage run -m pytest --timeout=10 -v', 'coveralls', 'sudo apt-get update && sudo apt-get install -y valgrind', 'echo ""VERSION=$VERSION"" > tests/.env\nif [[ ""$VERSION"" > ""21.7"" ]]; then echo ""ORG=clickhouse""; else echo ""ORG=yandex"" ; fi  >> tests/.env\ndocker-compose -f tests/docker-compose.yml up -d\n', '# Faking clickhouse-client real communication with container via docker exec.\necho -e \'#!/bin/bash\\n\\ndocker exec -e ""`env | grep ^TZ=`"" test-clickhouse-client clickhouse-client ""$@""\' | sudo tee /usr/local/bin/clickhouse-client > /dev/null\nsudo chmod +x /usr/local/bin/clickhouse-client\n# Overriding setup.cfg. Set host=clickhouse-server\nsed -i \'s/^host=localhost$/host=clickhouse-server/\' setup.cfg\n# Make host think that clickhouse-server is localhost\necho \'127.0.0.1 clickhouse-server\' | sudo tee /etc/hosts > /dev/null\n', 'python testsrequire.py\npython setup.py develop\n', 'valgrind --error-exitcode=1 --suppressions=valgrind.supp py.test -v', 'pip install --upgrade pip setuptools\npip install cython cibuildwheel==$VERSION\n', 'cibuildwheel --output-dir wheelhouse', 'pip install --upgrade pip setuptools\npip install cython cibuildwheel==$VERSION\n', 'cibuildwheel --output-dir wheelhouse', 'git config --global core.autocrlf false\n', 'pip install cibuildwheel==$env:VERSION\n', 'cibuildwheel --output-dir wheelhouse', 'pip install --upgrade pip setuptools\npip install cibuildwheel==$VERSION\n', 'cibuildwheel --output-dir wheelhouse', 'pip install --upgrade pip setuptools wheel', 'pip install sphinx', 'pip install -e .', 'cd docs && make html']"
"['echo ""::set-output name=version::$(grep -Po \'\\d*\\.\\d*\\.\\d*\' src/oncall/__init__.py)""', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n', 'twine upload dist/*\n']"
""
"['wget https://github.com/sagiegurari/cargo-make/releases/download/0.36.8/cargo-make-v0.36.8-x86_64-unknown-linux-gnu.zip\nunzip cargo-make-*.zip\nmv cargo-make*/cargo-make $HOME/.cargo/bin\n', 'cargo make crates', 'cargo make install', 'cargo make patch', 'cargo make svd2rust', 'cargo make form', 'cargo make check-patched', 'cargo make clean-all', 'sudo apt-get update\nsudo apt-get install libxml2-utils\n', 'make crates', './scripts/tool_install.sh svdtools\n./scripts/tool_install.sh svd2rust\n', 'make -j2 patch', 'make lint', 'make -j2 svd2rust', 'make -j2 check', './scripts/tool_install.sh svdtools\n./scripts/tool_install.sh svd2html\n', 'make -j2 html', 'cargo init\n', './stm32-rs/scripts/tool_install.sh svdtools\n', 'cd stm32-rs\nCOMMIT=$(git rev-parse HEAD)\nmake -j2 mmaps\nmv mmaps/* ../stm32-rs-mmaps/\ncd ../stm32-rs-mmaps\ngit config user.name ""stm32-rs builder""\ngit config user.email ""action@github.com""\ngit add .\ngit commit --allow-empty -m ""stm32-rs master $COMMIT""\ngit push origin master\n', 'cargo init\n', './master/scripts/tool_install.sh svdtools\n', '# Grab commit ID from checked out PR\ncd pr\nCOMMIT=$(git rev-parse --short HEAD)\nBRANCH=pr-${{ github.event.number }}-$COMMIT\necho ""BRANCH=$BRANCH"" >> $GITHUB_ENV\n\n# Use the PR\'s YAML files to rebuild mmaps\ncd ../master\nrm -rf devices peripherals\nmv ../pr/devices ../pr/peripherals .\nmake -j2 mmaps\n\n# Use the new mmaps to make a commit in the mmaps repo\nmv mmaps/* ../mmaps/\ncd ../mmaps\ngit checkout -b $BRANCH\ngit add .\ngit config user.name ""stm32-rs builder""\ngit config user.email ""action@github.com""\ngit commit --allow-empty -m ""stm32-rs PR #${{ github.event.number }} $COMMIT""\ngit push -f origin $BRANCH\n', 'cargo init\n', './stm32-rs/scripts/tool_install.sh', 'mkdir ~/.ssh\necho ""${{ secrets.NIGHTLIES_KEY }}"" > ~/.ssh/id_rsa\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/id_rsa\nssh-keyscan -t rsa github.com\n', 'mkdir nightly\ncd stm32-rs\nCOMMIT=$(git rev-parse HEAD)\nmake -j2 form\nmv stm32{c,f,g,h,l,mp,wl,wb}* ../nightly\ncp .github/workflows/README-nightlies.md ../nightly/README.md\ncd ../nightly\ngit init\ngit add .\ngit config user.name ""stm32-rs builder""\ngit config user.email ""action@github.com""\ngit commit -m ""stm32-rs master $COMMIT""\ngit remote add origin git@github.com:stm32-rs/stm32-rs-nightlies\ngit push --force origin master\n']"
"['pip install -U .[dev]\n', 'make test', 'make lint']"
"['# fail if curl fails\nset -e\n\necho ""INFO: source branch is: $HEAD_REF""\necho ""INFO: modified files""\ncurl -o- -L ""$DIFF_URL"" 2>/dev/null | grep ""^diff --git""\n\nset +e\n\n# if this PR modifies the CHANGELOG file it needs to come from a branch\n# that follows the pattern \'prepare/vX.Y\' b/c we want to run some extra jobs for such branches!\nif curl -o- -L ""$DIFF_URL"" 2>/dev/null | grep ""^diff --git"" | grep ""CHANGELOG""; then\n    if [[ ! ""$HEAD_REF"" =~ ^prepare/v.+$ ]]; then\n        echo ""FAIL: Modifications to CHANGELOG are only accepted from \'prepate/vX.Y\' branches!""\n        echo ""INFO: Otherwise aarch64 jobs in Circle CI will not be executed.""\n        exit 1\n    fi\nfi\n', 'sudo apt-get update\nsudo apt-get install gettext\n\npip install -r requirements/base.txt\npushd tcms/ && ./npm-install && popd\n', 'echo ""Downloading coverity scan package.""\ncurl -o /tmp/cov-analysis-linux64.tgz https://scan.coverity.com/download/linux64 \\\n  --form project=""$COVERITY_SCAN_PROJECT_NAME"" \\\n  --form token=""$COVERITY_SCAN_TOKEN""\n\npushd /tmp && tar xzvf cov-analysis-linux64.tgz && popd\nmkdir bin\n\n/tmp/cov-analysis-linux64-*/bin/cov-build --dir cov-int --no-command --fs-capture-search ./ --fs-capture-search $(python -c \'from distutils.sysconfig import get_python_lib; print(get_python_lib())\')\ntar cfz cov-int.tar.gz cov-int\n\necho ""Uploading coverity scan result to http://scan.coverity.com""\ncurl https://scan.coverity.com/builds?project=""$COVERITY_SCAN_PROJECT_NAME"" \\\n  --form token=""$COVERITY_SCAN_TOKEN"" \\\n  --form email=""$COVERITY_SCAN_EMAIL"" \\\n  --form file=@cov-int.tar.gz \\\n  --form version=""$(git rev-parse HEAD)"" \\\n  --form description=""$GITHUB_REF / $GITHUB_SHA""\n', 'VERSION=$(python -m tcms)\n\nmake docker-image\ndocker tag kiwitcms/kiwi:latest quay.io/kiwitcms/version:$VERSION-$(uname -m)\n\necho ""+++++ Docker images +++++""\ndocker images\n\necho ""${{ secrets.DOCKER_PUSH_TOKEN }}"" | docker login -u=""${{ secrets.DOCKER_PUSH_USERNAME }}"" --password-stdin\ndocker push kiwitcms/kiwi:latest\ndocker logout\n\necho ""${{ secrets.QUAY_PUSH_TOKEN }}"" | docker login -u=""${{ secrets.QUAY_PUSH_USERNAME }}"" --password-stdin quay.io\ndocker push quay.io/kiwitcms/kiwi:latest\ndocker push quay.io/kiwitcms/version:$VERSION-$(uname -m)\ndocker logout quay.io\n', 'docker-compose -f tests/${{ matrix.tracker}}/docker-compose.yml build\ndocker-compose -f tests/${{ matrix.tracker}}/docker-compose.yml up -d\nsleep 10\n\nIP_ADDR=`docker inspect -f \'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\' bugtracker_kiwitcms_org`\nsudo sh -c ""echo \'$IP_ADDR    bugtracker.kiwitcms.org\' >> /etc/hosts""\n', 'pip install -U pip\npip install -r requirements/devel.txt\n', './tests/${{ matrix.tracker }}/initialize-data\n', 'export GH_BUGTRACKER_INTEGRATION_TEST_API_TOKEN=""${{ secrets.GH_BUGTRACKER_INTEGRATION_TEST_API_TOKEN }}""\nexport GITLAB_INTEGRATION_API_TOKEN=""${{ secrets.GITLAB_INTEGRATION_API_TOKEN }}""\nexport JIRA_BUGTRACKER_INTEGRATION_API_USERNAME=""${{ secrets.JIRA_BUGTRACKER_INTEGRATION_API_USERNAME }}""\nexport JIRA_BUGTRACKER_INTEGRATION_API_TOKEN=""${{ secrets.JIRA_BUGTRACKER_INTEGRATION_API_TOKEN }}""\nexport BITBUCKET_INTEGRATION_API_USERNAME=""${{ secrets.BITBUCKET_INTEGRATION_API_USERNAME }}""\nexport BITBUCKET_INTEGRATION_API_PASSWORD=""${{ secrets.BITBUCKET_INTEGRATION_API_PASSWORD }}""\nexport AZURE_BOARDS_INTEGRATION_API_TOKEN=""${{ secrets.AZURE_BOARDS_INTEGRATION_API_TOKEN }}""\nexport LANG=en-us\nexport TEST_BUGTRACKER_INTEGRATION=1\ncoverage run --source=\'.\' ./manage.py test -v2 --noinput --settings=tcms.settings.test tcms.issuetracker.tests.test_${{ matrix.tracker }}\n', 'pip install doc8\nmake doc8\n', 'pip install flake8\nmake flake8\n', 'pip install -r requirements/devel.txt\n\nexport LANG=en-us\nmake pylint\n', 'pip install -r requirements/mariadb.txt\npip install -r requirements/postgres.txt\npip install -r requirements/devel.txt\n\nmake pylint_site_packages\n', 'pip install -r requirements/devel.txt\n\nexport LANG=en-us\nmake similar_strings\n', 'pip install -r requirements/devel.txt\n\nexport LANG=en-us\n# skip kiwi_lint/__init__.py b/c the import order is used to easily\n# figure out pylint numerical IDs by looking at the last import !!!\nisort --check --color --diff --skip kiwi_lint/__init__.py .\n', 'cd tcms/\n./npm-install && npm install --dev\n\n./node_modules/.bin/eslint .\n', 'sudo apt install python3-pydot graphviz\npip install -r requirements/readthedocs.txt\nmake check-docs-source-in-git\n', 'pip install -r requirements/devel.txt\npushd tcms/ && ./npm-install && popd\n\nexport LANG=en-us\nmake test_for_missing_migrations\n', 'docker-compose -f docker-compose.postgres pull db\ndocker-compose -f docker-compose.postgres run -d -p 5432:5432 --name kiwi_db db\n', 'pip install -r requirements/devel.txt\npip install -r requirements/postgres.txt\npushd tcms/ && ./npm-install && popd\n\nexport LANG=en-us\n# Postgres b/c it has best support for schema altering operations\nexport DJANGO_SETTINGS_MODULE=tcms.settings.test.postgresql\n./tests/migrations-rollback\n', 'pip install checkov\ncheckov --quiet --directory .\n', 'pip install bandit\nmake bandit\n', 'pip install bandit\n\npip install -r requirements/mariadb.txt\npip install -r requirements/postgres.txt\n\nmake bandit_site_packages\n', 'pushd tcms/\n./npm-install\nnpm audit\n', 'echo ""[tcms]"" > ~/.tcms.conf\necho ""url = https://${{ matrix.kiwitcms-url }}/xml-rpc/"" >> ~/.tcms.conf\necho ""username = kiwitcms-bot"" >> ~/.tcms.conf\necho ""password = ${{ secrets.TCMS_PASSWORD }}"" >> ~/.tcms.conf\n', 'sudo apt-get update\nsudo apt-get install gettext\n\nsudo mkdir /Kiwi\nsudo chmod a+w /Kiwi\n\npip install -r requirements/devel.txt\npushd tcms/ && npm install --dev && ./node_modules/.bin/webpack && popd\npushd tcms/ && ./npm-install && popd\n\nexport LANG=en-us\n\n# report to Kiwi TCMS only if we have access to secrets\nif [ -n ""${{ secrets.TCMS_PASSWORD }}"" ]; then\n  pip install kiwitcms-django-plugin\n\n  export DJANGO_TEST_RUNNER=""tcms_django_plugin.TestRunner""\n\n  export TCMS_PRODUCT=$GITHUB_REPOSITORY\n\n  # branch name or pull/123\n  export TCMS_PRODUCT_VERSION=$(echo $GITHUB_REF | sed ""s|refs/heads/||"" | sed ""s|refs/||"" | sed ""s|/merge||"")\n\n  # short commit number\n  export TCMS_BUILD=$(echo $GITHUB_SHA | cut -c1-7)\nfi\n\nmake test\ncoverage report -m\n', 'sudo apt-get update\nsudo apt-get install gettext\npip install -r requirements/devel.txt\npushd tcms/ && npm install --dev && ./node_modules/.bin/webpack && popd\npushd tcms/ && ./npm-install && popd\n', ""export LANG=en-us\nexport TEST_DASHBOARD_CHECK_UNAPPLIED_MIGRATIONS=1\ncoverage run --source='.' ./manage.py test -v2 --noinput --settings=tcms.settings.test tcms.core.tests.test_views.TestDashboardCheckMigrations\ncoverage report -m\n"", 'sudo apt-get update\nsudo apt-get install gettext\n\nsudo mkdir /Kiwi\nsudo chmod a+w /Kiwi\n\npip install -r requirements/devel.txt\npushd tcms/ && npm install --dev && ./node_modules/.bin/webpack && popd\npushd tcms/ && ./npm-install && popd\n\nexport LANG=en-us\nexport KIWI_DISABLE_BUGTRACKER=yes\nmake test\ncoverage report -m\n', ""docker-compose pull db\ndocker-compose run -d -p 3306:3306 --name kiwi_db db\nsleep 20  # wait to initialize\ndocker exec -i kiwi_db mariadb -u root -pkiwi-1s-aw3s0m3 -e 'GRANT ALL PRIVILEGES ON test_kiwi.* TO kiwi;'\n"", 'sudo apt-get update\nsudo apt-get install gettext\n\nsudo mkdir /Kiwi\nsudo chmod a+w /Kiwi\n\npip install -r requirements/devel.txt\npip install -r requirements/mariadb.txt\npushd tcms/ && npm install --dev && ./node_modules/.bin/webpack && popd\npushd tcms/ && ./npm-install && popd\n\nexport LANG=sl-si\nTEST_DB=MariaDB make test\ncoverage report -m\n', 'docker-compose -f docker-compose.postgres pull db\ndocker-compose -f docker-compose.postgres run -d -p 5432:5432 --name kiwi_db db\n', 'sudo apt-get update\nsudo apt-get install gettext\n\nsudo mkdir /Kiwi\nsudo chmod a+w /Kiwi\n\npip install -r requirements/devel.txt\npip install -r requirements/postgres.txt\npushd tcms/ && npm install --dev && ./node_modules/.bin/webpack && popd\npushd tcms/ && ./npm-install && popd\n\nexport LANG=fr-fr\nTEST_DB=Postgres make test\ncoverage report -m\n', 'sudo apt-get update\nsudo apt-get install firefox wrk\n', 'docker --version\ndocker --help\n\ndocker-compose --version\ndocker-compose --help\n', 'pip install -r requirements/devel.txt\nmake test-docker-image\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['sudo apt-get update\nsudo apt-get install libsndfile1-dev\npython -m pip install --upgrade pip\npip install tox flake8\n', 'wget -nv -O ResCNN_triplet_training_checkpoint_265.h5 https://drive.google.com/uc\\?export\\=download\\&id\\=1F9NvdrarWZNktdX9KlRYWWHDwRkip_aP\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --max-complexity 10 --max-line-length 127 --statistics\n', 'tox\n']"
""
"['pip install --upgrade --upgrade-strategy eager -e .[AWS,DEV]', 'mypy sentinelhub', 'pylint sentinelhub', 'pip install --upgrade --upgrade-strategy eager -e .[AWS,DEV]', 'sentinelhub.config \\\n  --sh_client_id ""${{ secrets.SH_CLIENT_ID }}"" \\\n  --sh_client_secret ""${{ secrets.SH_CLIENT_SECRET }}"" \\\n  --instance_id ""${{ secrets.INSTANCE_ID }}"" \\\n  --aws_access_key_id ""${{ secrets.AWS_ACCESS_KEY_ID }}"" \\\n  --aws_secret_access_key ""${{ secrets.AWS_SECRET_ACCESS_KEY }}""\npytest --cov --cov-report=term --cov-report=xml\n', 'pytest -m ""not sh_integration and not aws_integration""\n']"
"[""echo 'Don't worry, everything will be fine."", 'python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 PyPtt/ --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 PyPtt/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['python -m pip install --upgrade pip\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'python setup.py test\n', 'mypy --strict --ignore-missing-imports guesslang/\n', 'flake8 .\n', 'pip install .\n']"
"['pip install .[tests,lint]', 'flake8', 'mypy', 'sudo apt update\nsudo apt install --no-install-recommends ffmpeg\n', 'brew install ffmpeg', 'cinst -y ffmpeg', 'pytest']"
"['pip install --upgrade pip\npip install -r requirements.txt\npip install nose mock coverage\n', 'nosetests test/unit/*.py test/unit/aws/*.py test/unit/oscar/*.py -v --stop --with-xunit --with-coverage --cover-erase --cover-xml --cover-package=scar', 'pip install --upgrade pip\npip install setuptools wheel twine\npip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['set -euxo pipefail\ndocker build -t citusdata/python-all-versions  base_gha_image\nif [[ ${GITHUB_REF##*/} == ""main"" ]]; then\n  docker push citusdata/python-all-versions\nelse\n  docker tag citusdata/python-all-versions citusdata/python-all-versions:${GITHUB_REF##*/}\n  docker push citusdata/python-all-versions:${GITHUB_REF##*/}\nfi\n', 'pip install -r requirements/static-analysis-requirements.txt\n', 'make format-check\n', 'make lint\n', 'cd docs\nsphinx-build -W -b html source builds\n', 'docker-compose --project-name django-multitenant up -d || { docker-compose logs && false ; }\necho ""Running tests for python $PYTHON_VERSION, django $DJANGO_VERSION, citus $CITUS_VERSION""\ndocker run \\\n--entrypoint /build/scripts/test-django-mt.sh \\\n-v $(pwd):/build \\\n--network=host \\\n-e PYTHON_VERSION -e DJANGO_VERSION \\\ncitusdata/python-all-versions\n', 'echo ""Running tests for python $PYTHON_VERSION, django $DJANGO_VERSION, citus $CITUS_VERSION""\ndocker run \\\n--entrypoint /build/scripts/test-django-mt-modules.sh \\\n-v $(pwd):/build \\\n--network=host \\\n-e PYTHON_VERSION -e DJANGO_VERSION \\\ncitusdata/python-all-versions\n', 'docker-compose --project-name django-multitenant up -d || { docker-compose logs && false ; }\necho ""Running tests for python $PYTHON_VERSION, django $DJANGO_VERSION, citus $CITUS_VERSION""\ndocker run \\\n--entrypoint /build/scripts/test-django-mt.sh \\\n-v $(pwd):/build \\\n--network=host \\\n-e PYTHON_VERSION -e DJANGO_VERSION \\\ncitusdata/python-all-versions\n', 'echo ""Running tests for python $PYTHON_VERSION, django $DJANGO_VERSION, citus $CITUS_VERSION""\ndocker run \\\n--entrypoint /build/scripts/test-django-mt-modules.sh \\\n-v $(pwd):/build \\\n--network=host \\\n-e PYTHON_VERSION -e DJANGO_VERSION \\\ncitusdata/python-all-versions\n']"
""
"['python -m pip install --upgrade pip setuptools wheel\npip install .[qa]\n', 'flake8 --extend-ignore F401 parso test/*.py setup.py scripts/', 'mypy parso setup.py', 'python -m pip install --upgrade pip setuptools wheel\npip install .[testing]\n', 'pytest', 'python -m pip install --upgrade pip setuptools wheel\npip install .[testing] coverage coveralls\n', 'coverage run -m pytest\ncoverage report\n', 'coveralls --service=github']"
"['conda create -y -q --name ""${CONDA_ENV_NAME}"" python=""${PYTHON_VERSION}""\n', '. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\npushd ""${TENSORRT_HOME}""\npip install graphsurgeon/graphsurgeon-*.whl\npip install onnx_graphsurgeon/onnx_graphsurgeon-*.whl\npip install uff/uff-*.whl\npip install ""python/tensorrt-${TENSORRT_VERSION}-${TENSORRT_PYTHON_VERSION}-none-linux_x86_64.whl""\n# make sure tensorrt works\npython -c ""import tensorrt""\n', '. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\npushd benchmark\n# Install dependencies\nconda install -y -c pytorch ""${MAGMA_VERSION}""\npip install requests bs4 argparse gitpython boto3 regex\n# Check if nightly builds are available\nNIGHTLIES=$(python torchbenchmark/util/torch_nightly.py --packages torch)\n# If failed, the script will generate empty result\nif [ -z $NIGHTLIES ]; then\n    echo ""Torch nightly build failed. Cancel the workflow.""\n    exit 1\nfi\n# Install PyTorch nightly from pip\npip install --pre torch torchtext torchvision torchaudio \\\n  -f https://download.pytorch.org/whl/nightly/${CUDA_VERSION}/torch_nightly.html\n# make sure pytorch+cuda works\npython -c ""import torch; torch.cuda.init()""\n', '. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\nconda install -y bazel\npushd torch-tensorrt\npushd py\npython setup.py install\npopd\npopd\n# test torch-tensorrt\npython -c ""import torch_tensorrt""\npushd torchdynamo\npython setup.py develop\npopd\n# test torchdynamo\npython -c ""import torchdynamo""\npushd functorch\npython setup.py install\npopd\n', 'set -x\n. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\npushd benchmark\npython install.py\npopd\n', 'set -x\n. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\n# remove the result of previous runs if exist\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\nmkdir -p benchmark-output/\npushd benchmark\n# run the backend options and store the result to the ""benchmark-output"" directory\npython .github/scripts/run-config.py --config ""${{ github.event.inputs.benchmark_config }}"" \\\n                                     --benchmark-repo ""${PWD}"" \\\n                                     --output-dir ""${PWD}/../benchmark-output""\npopd\n', 'conda env remove --name ""${CONDA_ENV_NAME}""\n', ""export TODAY=$(date +'%Y%m%d')\nexport DOCKER_TAG=dev${TODAY}\ncd benchmark/docker\ndocker build . -f torchbench-nightly.dockerfile -t ghcr.io/pytorch/torchbench:${DOCKER_TAG}\ndocker tag ghcr.io/pytorch/torchbench:${DOCKER_TAG} ghcr.io/pytorch/torchbench:latest\n"", ""export TODAY=$(date +'%Y%m%d')\nexport DOCKER_TAG=dev${TODAY}\ndocker push ghcr.io/pytorch/torchbench:${DOCKER_TAG}\ndocker push ghcr.io/pytorch/torchbench:latest\n"", 'container_name=$(docker run \\\n  -e CONDA_ENV \\\n  -e SETUP_SCRIPT \\\n  --tty \\\n  --detach \\\n  --shm-size=32gb \\\n  -v ""${PWD}/benchmark:/benchmark"" \\\n  --gpus all \\\n  -w / \\\n  ""${{ env.DOCKER_IMAGE }}"" \\\n  tail -f /dev/null\n)\necho ""Container name: ${container_name}""\ndocker exec -t -w ""/benchmark"" ""${container_name}"" bash /benchmark/scripts/torchbench_install.sh\ndocker exec -t -w ""/benchmark"" ""${container_name}"" bash /benchmark/scripts/torchbench_test.sh\n', 'sudo nvidia-smi -pm 1\nsudo nvidia-smi -ac 1215,1410\nnvidia-smi\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\nconda create --name ""${CONDA_ENV}"" --clone ""${BASE_CONDA_ENV}""\n', 'bash ./scripts/torchbench_install.sh\n', 'bash ./scripts/torchbench_test.sh\n', '. ""${SETUP_SCRIPT}""\nconda deactivate && conda deactivate\nconda remove -n ""${CONDA_ENV}"" --all\n', 'conda create -y -n ""${CONDA_ENV}"" python=""${PYTHON_VERSION}""\n. activate ""${CONDA_ENV}""\nconda install -y numpy requests=2.22 ninja pyyaml mkl mkl-include setuptools \\\n                 cmake cffi typing_extensions future six dataclasses tabulate gitpython\n# Install pytorch nightly\npip install --pre torch torchtext torchvision torchaudio \\\n-f https://download.pytorch.org/whl/nightly/${CUDA_VERSION}/torch_nightly.html\n# Install torchbench dependencies\npython install.py\n', '. activate ""${CONDA_ENV}""\nmkdir -p ""${TEST_HOME}""\nif [ -z ""${MODEL}"" ] ; then\n   # Load PR to file\n   PR_BODY_FILE=""${TEST_HOME}""/pr-body.txt\n   echo ""${PR_BODY}"" > ""${PR_BODY_FILE}""\n   MODEL=`python ./.github/scripts/test-repeated-runs.py --pr-body ""${PR_BODY_FILE}""`\nfi\n# Setup nvidia gpu frequency\nsudo nvidia-persistenced --user ""${USER}"" || true\nsudo nvidia-smi -pm  ""${GPU_ID}""\nsudo nvidia-smi -ac ""${GPU_FREQ}""\n# Run the tests\nEVAL_LOG=""${TEST_HOME}/eval-${MODEL}.log""\necho -n > ""${EVAL_LOG}""\nfor i in `seq 1 ${REPEAT}`; do\n  python run.py ""${MODEL}"" -t eval -d cuda | tee -a ""${EVAL_LOG}""\ndone\nTRAIN_LOG=""${TEST_HOME}/train-${MODEL}.log""\necho -n > ""${TRAIN_LOG}""\nfor i in `seq 1 ${REPEAT}`; do\n  python run.py ""${MODEL}"" -t train -d cuda | tee -a ""${TRAIN_LOG}""\ndone\n# Check the stability of GPU tests\npython ./.github/scripts/test-repeated-runs.py --log ""${EVAL_LOG}"" && \\\n       echo ""GPU stability test pass for inference!""\npython ./.github/scripts/test-repeated-runs.py --log ""${TRAIN_LOG}"" && \\\n       echo ""GPU stability test pass for train!""\n', 'conda env remove --name ""${CONDA_ENV}""\n', 'sudo nvidia-smi -pm 1\nsudo nvidia-smi -ac 1215,1410\nnvidia-smi\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\npushd benchmark\nTODAY_STR=$(date +\'%Y%m%d\')\npython utils/cuda_utils.py --check-torch-nightly-version --force-date ${TODAY_STR}\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\nconda create --name ""${CONDA_ENV}"" --clone ""${BASE_CONDA_ENV}""\n', 'set -x\n. ""${SETUP_SCRIPT}""\npushd benchmark\npython install.py\n', 'set -x\n. ""${SETUP_SCRIPT}""\n# remove old results\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\npushd benchmark\nif [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi\nMANUAL_WORKFLOW=""${{ github.event.inputs.userbenchmark_name }}""\nif [ -z ""${MANUAL_WORKFLOW}"" ]; then\n  # Figure out what userbenchmarks we should run, and run it\n  python ./.github/scripts/userbenchmark/schedule-benchmarks.py --platform ${PLATFORM_NAME}\n  if [ -d ./.userbenchmark ]; then\n    cp -r ./.userbenchmark ../benchmark-output\n  else\n    mkdir ../benchmark-output\n  fi\nelse\n  python run_benchmark.py ""${{ github.event.inputs.userbenchmark_name }}"" ${{ github.event.inputs.userbenchmark_options }}\n  cp -r ./.userbenchmark/""${{ github.event.inputs.userbenchmark_name }}"" ../benchmark-output\nfi\n', '. ""${SETUP_SCRIPT}""\npushd benchmark\nRESULTS=($(find ${PWD}/../benchmark-output -name ""metrics-*.json"" -maxdepth 2 | sort -r))\necho ""Uploading result jsons: ${RESULTS}""\nfor r in ${RESULTS[@]}; do\n  python ./scripts/userbenchmark/upload_scribe.py --userbenchmark_json ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n  python ./scripts/userbenchmark/upload_s3.py --upload-file ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\ndone\n', '. ""${SETUP_SCRIPT}""\nconda deactivate && conda deactivate\nconda remove -n ""${CONDA_ENV}"" --all\n', 'bash benchmark/scripts/install_conda.sh\n', '. ${HOME}/miniconda3/etc/profile.d/conda.sh\nconda create -y -q --name ""${CONDA_ENV_NAME}"" python=""${PYTHON_VERSION}""\nconda activate ""${CONDA_ENV_NAME}""\nconda install -y pyyaml numpy boto3 requests\n', '. ${HOME}/miniconda3/etc/profile.d/conda.sh && conda activate ""${CONDA_ENV_NAME}""\n# remove old results\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\npushd benchmark\nif [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi\n# Figure out what userbenchmarks we should run, and run it\npython ./.github/scripts/userbenchmark/schedule-benchmarks.py --platform ${PLATFORM_NAME}\ncp -r ./.userbenchmark ../benchmark-output\n', '. ${HOME}/miniconda3/etc/profile.d/conda.sh\nconda env remove --name ""${CONDA_ENV_NAME}""\n', 'cd benchmark\nbash scripts/install_conda.sh\n', 'CONTAINER_ID=$(docker run \\\n  -e CONDA_ENV \\\n  -e SETUP_SCRIPT \\\n  --tty \\\n  --detach \\\n  -v ""${PWD}/benchmark:/benchmark"" \\\n  -w / \\\n  ""${{ env.DOCKER_IMAGE }}"" \\\n  tail -f /dev/null\n)\necho ""Container ID: ${CONTAINER_ID}""\n# Write the CONTAINER_ID to GITHUB_ENV\necho ""CONTAINER_ID=${CONTAINER_ID}"" >> ""${GITHUB_ENV}""\n', 'docker exec -t -w ""/benchmark"" ""${CONTAINER_ID}"" bash /benchmark/scripts/torchbench_install.sh\n', '# remove old results\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\npushd benchmark\nif [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi\n# Run userbenchmark\nMANUAL_WORKFLOW=""${{ github.event.inputs.userbenchmark_name }}""\nif [ -z ""${MANUAL_WORKFLOW}"" ]; then\n  # Figure out what userbenchmarks we should run, and run it\n  docker exec -t -w ""/benchmark"" ""${CONTAINER_ID}"" bash -c "". ${SETUP_SCRIPT} && conda activate ${CONDA_ENV} && \\\n                    python .github/scripts/userbenchmark/schedule-benchmarks.py --platform ${PLATFORM_NAME}""\n  if [ -d ./.userbenchmark ]; then\n    cp -r ./.userbenchmark ../benchmark-output\n  else\n    mkdir ../benchmark-output\n  fi\nelse\n  docker exec -t -w ""/benchmark"" ""${CONTAINER_ID}"" bash -c "". ${SETUP_SCRIPT} && conda activate ${CONDA_ENV} && \\\n                    python run_benchmark.py \\\n                    \\""${{ github.event.inputs.userbenchmark_name }}\\"" ${{ github.event.inputs.userbenchmark_options }}""\n  cp -r ./.userbenchmark/""${{ github.event.inputs.userbenchmark_name }}"" ../benchmark-output\nfi\n', 'pushd benchmark\n. scripts/activate_conda.sh\nRESULTS=($(find ${PWD}/../benchmark-output -name ""metrics-*.json"" -maxdepth 2 | sort -r))\necho ""Uploading result jsons: ${RESULTS}""\nfor r in ${RESULTS[@]}; do\n  python ./scripts/userbenchmark/upload_scribe.py --userbenchmark_json ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n  python ./scripts/userbenchmark/upload_s3.py --upload-file ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\ndone\n', 'sudo nvidia-smi -pm 1\nsudo nvidia-smi -ac 1215,1410\nnvidia-smi\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\nconda create --name ""${CONDA_ENV}"" --clone ""${BASE_CONDA_ENV}""\n', 'set -x\n. ""${SETUP_SCRIPT}""\npushd benchmark\npython install.py\n', 'set -x\n. ""${SETUP_SCRIPT}""\n# remove old results\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\npushd benchmark\nif [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi\n\n# TODO: scale this to run other benchmarks, but let\'s start with optim\npython -m userbenchmark.optim.run_optim_benchmarks -c ${{ github.event.inputs.userbenchmark_options }}\ncp -r ./.userbenchmark/optim ../benchmark-output\n', '. ""${SETUP_SCRIPT}""\npushd benchmark\nRESULTS=($(find ${PWD}/../benchmark-output -name ""metrics-*.json"" -maxdepth 2 | sort -r))\n# TODO: the following assumes only one metrics-*.json is found. It will keep \n# overwriting gh-issue.md if multiple are found. Scaling this up is a potential next step.\nfor r in ${RESULTS[@]}; do\n  python regression_detector.py --platform ""${PLATFORM_NAME}"" --treatment ""${r}"" --owner @janeyx99 \\\n  --gh-issue-path gh-issue.md --errors-path errors.txt\ndone\n', '. ""${SETUP_SCRIPT}""\npushd benchmark\nRESULTS=($(find ${PWD}/../benchmark-output -name ""metrics-*.json"" -maxdepth 2 | sort -r))\necho ""Uploading result jsons: ${RESULTS}""\nfor r in ${RESULTS[@]}; do\n  python ./scripts/userbenchmark/upload_scribe.py --userbenchmark_json ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n  python ./scripts/userbenchmark/upload_s3.py --upload-file ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\ndone\n', '# Do not error earlier as we want all artifacts and regressions to be reported first\n# TODO: potentially move errors.txt to benchmark-output so it gets uploaded to S3\npushd benchmark\nif [ -e errors.txt ]; then cat errors.txt && exit 1; fi\n', '. ""${SETUP_SCRIPT}""\nconda deactivate && conda deactivate\nconda remove -n ""${CONDA_ENV}"" --all\n', 'python benchmark/utils/python_utils.py --create-conda-env ""${CONDA_ENV_NAME}""\nsudo python benchmark/utils/cuda_utils.py --setup-cuda-softlink\n', '. ""${SETUP_SCRIPT}"" && . activate ""${CONDA_ENV_NAME}""\npushd benchmark\n# Install dependencies\npython utils/cuda_utils.py --install-torch-deps\n# check the machine is tuned\npip install -U py-cpuinfo psutil distro boto3\nsudo ${HOME}/miniconda3/envs/${CONDA_ENV_NAME}/bin/python3 torchbenchmark/util/machine_config.py\n# Check if nightly builds are available\nNIGHTLIES=$(python torchbenchmark/util/torch_nightly.py --packages torch)\n# If failed, the script will generate empty result\nif [ -z $NIGHTLIES ]; then\n    echo ""Torch nightly build failed. Cancel the workflow.""\n    exit 1\nfi\n# Install PyTorch and torchvision nightly from pip\npython utils/cuda_utils.py --install-torch-nightly\npython utils/cuda_utils.py --check-torch-nightly-version\n# make sure pytorch+cuda works\npython -c ""import torch; torch.cuda.init()""\n', 'set -x\n. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\npushd benchmark\npython install.py\n', 'set -x\n. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\n# remove old results\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\npushd benchmark\nif [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi\nMANUAL_WORKFLOW=""${{ github.event.inputs.userbenchmark_name }}""\nif [ -z ""${MANUAL_WORKFLOW}"" ]; then\n  # Figure out what userbenchmarks we should run, and run it\n  python ./.github/scripts/userbenchmark/schedule-benchmarks.py --platform ${PLATFORM_NAME}\n  if [ -d ./.userbenchmark ]; then\n    cp -r ./.userbenchmark ../benchmark-output\n  else\n    mkdir ../benchmark-output\n  fi\nelse\n  python run_benchmark.py ""${{ github.event.inputs.userbenchmark_name }}"" ${{ github.event.inputs.userbenchmark_options }}\n  cp -r ./.userbenchmark/""${{ github.event.inputs.userbenchmark_name }}"" ../benchmark-output\nfi\n', '. ""${SETUP_SCRIPT}"" && conda activate ""${CONDA_ENV_NAME}""\npushd benchmark\nRESULTS=($(find ${PWD}/../benchmark-output -name ""metrics-*.json"" -maxdepth 2 | sort -r))\necho ""Uploading result jsons: ${RESULTS}""\nfor r in ${RESULTS[@]}; do\n  python ./scripts/userbenchmark/upload_scribe.py --userbenchmark_json ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n  python ./scripts/userbenchmark/upload_s3.py --upload-file ""${r}"" --userbenchmark_platform ""${PLATFORM_NAME}""\ndone\n', 'conda env remove --name ""${CONDA_ENV_NAME}""\n', 'set -x\nconda create -y -n ""${BISECT_CONDA_ENV}"" python=""${PYTHON_VER}""\n. activate ""${BISECT_CONDA_ENV}""\n. /data/nvme/bin/setup_instance.sh\nconda install -y numpy=""${NUMPY_VER}""  mkl=""${MKL_VER}"" mkl-include=""${MKL_VER}"" \\\n                 requests ninja pyyaml setuptools cffi sympy ffmpeg \\\n                 typing_extensions future six dataclasses tabulate gitpython git-lfs tqdm regex\n# install cmake 3.26 from conda-forge, cmake > 3.24 is required by torchaudio\nconda install -y cmake=""${CMAKE_VER}"" -c conda-forge\n# Install magma\nconda install -y -c pytorch ""${MAGMA_VER}""\n', 'export BISECT_ISSUE=""${{ github.event.inputs.issue_name }}""\nexport BISECT_BASE=""${HOME}/${BISECT_DIR}/${BISECT_ISSUE}""\nexport TORCHBENCH_SRC_DIR=""${PWD}""\n. activate ""$BISECT_CONDA_ENV""\n. /data/nvme/bin/setup_instance.sh\nbash ./.github/scripts/run-bisection.sh\n# Update the result json symbolic link\nln -sf ""${BISECT_BASE}/gh${GITHUB_RUN_ID}/result.json"" ""${BISECT_BASE}/result.json""\n', 'export BISECT_ISSUE=""${{ github.event.inputs.issue_name }}""\nexport BISECT_BASE=""${HOME}/${BISECT_DIR}/${BISECT_ISSUE}""\n. activate ""$BISECT_CONDA_ENV""\n. /data/nvme/bin/setup_instance.sh\npython ./.github/scripts/bmutils/analyze-bisection-result.py --bisection-root ""${BISECT_BASE}"" --gh-workflow-id ""${GITHUB_RUN_ID}""\ncp -r ""${BISECT_BASE}"" ./bisection-result\n', 'conda env remove --name ""$BISECT_CONDA_ENV""\n', 'conda create -y -q --name ""${CONDA_ENV_NAME}"" python=${{ env.PYTHON_VER }}\n', '. activate ""${CONDA_ENV_NAME}""\n. /data/nvme/bin/setup_instance.sh\n# Install dependencies\npip install requests bs4 argparse gitpython boto3 regex\n# Check if nightly builds are available\nNIGHTLIES=$(python torchbenchmark/util/torch_nightly.py --packages torch)\n# If failed, the script will generate empty result\nif [ -z $NIGHTLIES ]; then\n    echo ""Torch nightly build failed. Cancel the workflow.""\n    exit 1\nfi\n# Install magma\nconda install -y -c pytorch ""${MAGMA_VERSION}""\n# Install PyTorch nightly from pip\npip install --no-cache-dir --pre torch torchvision torchaudio torchtext --index-url \\\n  https://download.pytorch.org/whl/nightly/${CUDA_VERSION}\n', '. activate ""${CONDA_ENV_NAME}""\n. /data/nvme/bin/setup_instance.sh\nconda install -y git-lfs\npython install.py\n', '. activate ""${CONDA_ENV_NAME}""\n. /data/nvme/bin/setup_instance.sh\nWORKFLOW_HOME=""${HOME}/${{ env.OUTPUT_DIR }}/gh${GITHUB_RUN_ID}""\nbash ./.github/scripts/run.sh ""${WORKFLOW_HOME}""\n', 'set -x\n. activate ""${CONDA_ENV_NAME}""\nWORKFLOW_HOME=""${HOME}/${{ env.OUTPUT_DIR }}/gh${GITHUB_RUN_ID}""\nmkdir -p benchmark-output/\n# Update the self-hosted pytorch version\npushd ""${HOME}/pytorch""\ngit fetch origin\npopd\npip install gitpython pyyaml dataclasses argparse\n# Compare the result from yesterday and report any perf signals\npython ./.github/scripts/generate-abtest-config.py \\\n       --pytorch-dir ""${HOME}/pytorch"" \\\n       --github-issue ""${WORKFLOW_HOME}/gh-issue.md"" \\\n       --benchmark-dir ""${WORKFLOW_HOME}"" \\\n       --out ""${WORKFLOW_HOME}/bisection.yaml""\n# Include in the GitHub artifact\nif [ -f ""${WORKFLOW_HOME}/gh-issue.md"" ]; then\n  cp ""${WORKFLOW_HOME}/bisection.yaml"" ./benchmark-output/\n  cp ""${WORKFLOW_HOME}/gh-issue.md"" ./benchmark-output/\n  # Setup the bisection environment\n  BISECTION_HOME=""${HOME}/${{ env.BISECTION_ROOT }}/bisection-gh${GITHUB_RUN_ID}""\n  mkdir -p ""${BISECTION_HOME}""\n  mv ./benchmark-output/gh-issue.md ""${BISECTION_HOME}/gh-issue.md""\n  cp ./benchmark-output/bisection.yaml ""${BISECTION_HOME}/config.yaml""\nfi\n', '# Get the workflow ID from\n# https://api.github.com/repos/pytorch/benchmark/actions/workflows\ncurl -u xuzhao9:${{ secrets.TORCHBENCH_ACCESS_TOKEN }} \\\n  -X POST \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  https://api.github.com/repos/pytorch/benchmark/actions/workflows/16176850/dispatches \\\n  -d \'{""ref"": ""main"", ""inputs"": {""issue_name"": ""bisection-gh\'""${GITHUB_RUN_ID}""\'"" } }\'\n', '. activate ""${CONDA_ENV_NAME}""\nTODAY=$(date ""+%Y%m%d%H%M%S"")\nLATEST_RESULT=$(find ${HOME}/${{ env.OUTPUT_DIR }}/gh${GITHUB_RUN_ID} -name ""*.json"" | sort -r | head -1)\necho ""Benchmark result file: ${LATEST_RESULT}""\nmkdir -p benchmark-output/\ncp ""${LATEST_RESULT}"" ./benchmark-output/benchmark-result-${CONFIG_VER}-${TODAY}.json\n# Load environment variables\nCONFIG_DIR=torchbenchmark/score/configs/${CONFIG_VER}\nCONFIG_ENV=${CONFIG_DIR}/config-${CONFIG_VER}.env\n# Load environment variables\nset -a; source ""${CONFIG_ENV}""; set +a\nSCORE_FILE=""./benchmark-result-${CONFIG_VER}-score-${TODAY}.json""\n# Generate score file\npython compute_score.py --score_version ""${CONFIG_VER}"" --benchmark_data_file ""${LATEST_RESULT}"" > ""${SCORE_FILE}""\n# Upload result to Scribe\npython scripts/upload_scribe_${CONFIG_VER}.py --pytest_bench_json ""${LATEST_RESULT}"" --torchbench_score_file ""${SCORE_FILE}""\n', 'conda env remove --name ""${CONDA_ENV_NAME}""\n', 'SWEEP_JOB=${{ github.event.inputs.sweep_name }}\nSWEEP_JOB_ROOT=""${HOME}/${SWEEP_DIR}/${SWEEP_JOB}""\nSWEEP_JOB_OUTPUT=""${SWEEP_JOB_ROOT}/results/gh${GITHUB_RUN_ID}""\nfor CONFIG in ${SWEEP_JOB_ROOT}/configs/*.txt; do\n  echo ""Running config $CONFIG ...""\n  # Create a new conda env\n  CONFIG_BASE=$(basename ${CONFIG})\n  CONDA_ENV_NAME=$(echo ""${CONFIG_BASE}"" | sed \'s/[^-]*-\\(.*\\)\\.txt/\\1/\')\n  conda create -y -q --name ${CONDA_ENV_NAME} python=${PYTHON_VER}\n  . activate ${CONDA_ENV_NAME}\n  . /data/nvme/bin/setup_instance.sh\n  conda install -y git-lfs\n  # Install magma\n  conda install -y -c pytorch ""${MAGMA_VERSION}""\n  pip install -r ""${CONFIG}""\n  python install.py\n  bash .github/scripts/run.sh ""${SWEEP_JOB_OUTPUT}""\n  # Remove the conda env\n  conda deactivate\n  conda env remove --name ${CONDA_ENV_NAME}\ndone\necho ""Finished running tasks""\n', 'UPLOAD_COND=${{ github.event.inputs.upload_result }}\n# Quit if upload is not specified\nif [ ""$UPLOAD_COND"" != ""yes"" ]; then\n  exit 0\nfi\n# Otherwise, continue upload\nSWEEP_JOB=${{ github.event.inputs.sweep_name }}\nSWEEP_JOB_ROOT=""${HOME}/${SWEEP_DIR}/${SWEEP_JOB}""\nSWEEP_JOB_OUTPUT=""${SWEEP_JOB_ROOT}/results/gh${GITHUB_RUN_ID}""\nCONDA_ENV_NAME=sweep-ci\nconda create -y -q --name ${CONDA_ENV_NAME} python=${PYTHON_VER}\n. activate ${CONDA_ENV_NAME}\npip install -r requirements.txt\npip install boto3\nmkdir -p ""${SWEEP_JOB_OUTPUT}/scores""\nfor RESULT in ${SWEEP_JOB_OUTPUT}/*.json; do\n  # Upload when the file is non-empty\n  if [ -s $RESULT ]; then\n    # Generate score file\n    RESULT_BASENAME=$(basename ""${RESULT}"")\n    SCORE_FILE=""${SWEEP_JOB_OUTPUT}/scores/${RESULT_BASENAME}.score.json""\n    python compute_score.py --score_version ""${CONFIG_VER}"" --benchmark_data_file ""${RESULT}"" > ""${SCORE_FILE}""\n    # Upload score\n    python scripts/upload_scribe_${CONFIG_VER}.py --pytest_bench_json ""${RESULT}"" --torchbench_score_file ""${SCORE_FILE}""\n  fi\ndone\nconda deactivate\nconda env remove --name ${CONDA_ENV_NAME}\n', 'sudo nvidia-smi -pm 1\nsudo nvidia-smi -ac 1215,1410\nnvidia-smi\n', 'sudo apt-get -y update && sudo apt -y update\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\ncd benchmark\npython ./utils/python_utils.py --create-conda-env ""${CONDA_ENV}""\n', '. ""${SETUP_SCRIPT}""; cd benchmark\npython utils/cuda_utils.py --install-torch-build-deps\npython utils/cuda_utils.py --install-torchbench-deps\ncc_path=$(conda run -n ""${CONDA_ENV}"" printenv CC)\ncxx_path=$(conda run -n ""${CONDA_ENV}"" printenv CXX)\nln -s ""${cc_path}"" ""$(dirname ""$cc_path"")/cc""\nln -s ""${cc_path}"" ""$(dirname ""$cc_path"")/gcc""\nln -s ""${cxx_path}"" ""$(dirname ""$cxx_path"")/c++""\nln -s ""${cxx_path}"" ""$(dirname ""$cxx_path"")/g++""\n# setup shared library paths\nsudo ln -sf ""${CONDA_PREFIX}/x86_64-conda-linux-gnu/sysroot/lib64/libpthread.so.0"" /lib64/\nsudo ln -sf ""${CONDA_PREFIX}/x86_64-conda-linux-gnu/sysroot/usr/lib64/libpthread_nonshared.a"" /usr/lib64/\nsudo ln -sf ""${CONDA_PREFIX}/x86_64-conda-linux-gnu/sysroot/lib64/libc.so.6"" /lib64/\nsudo ln -sf ""${CONDA_PREFIX}/x86_64-conda-linux-gnu/sysroot/usr/lib64/libc_nonshared.a"" /usr/lib64/\nmkdir -p ""${BISECT_WORKDIR}""\nREGRESSION_DATE=""${{ github.event.inputs.regression_date }}""\npython regression_detector.py --name ""${USERBENCHMARK_NAME}"" --platform ""${PLATFORM_NAME}"" \\\n                              --end-date ""${REGRESSION_DATE}"" --download-from-s3 --output ""${BISECT_WORKDIR}/regression-${REGRESSION_DATE}.yaml""\n', '. ""${SETUP_SCRIPT}""; cd benchmark\nREGRESSION_DATE=""${{ github.event.inputs.regression_date }}""\npython bisection.py --work-dir ""${BISECT_WORKDIR}"" --torch-repos-path ""${PWD}/../srcs"" \\\n      --torchbench-repo-path ""${PWD}"" --config ""${BISECT_WORKDIR}/regression-${REGRESSION_DATE}.yaml"" \\\n      --output ""${BISECT_WORKDIR}/bisect-output-gh${GITHUB_RUN_ID}.json""\ncp -r ""${BISECT_WORKDIR}"" ../bisection-result\n', '. ""${SETUP_SCRIPT}""\nconda deactivate && conda deactivate\nconda remove -n ""${CONDA_ENV}"" --all\n', 'sudo nvidia-smi -pm 1\nsudo nvidia-smi -ac 1215,1410\nnvidia-smi\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\npushd benchmark\nTODAY_STR=$(date +\'%Y%m%d\')\npython utils/cuda_utils.py --check-torch-nightly-version --force-date ${TODAY_STR}\n', 'CONDA_ENV=${BASE_CONDA_ENV} . ""${SETUP_SCRIPT}""\nconda create --name ""${CONDA_ENV}"" --clone ""${BASE_CONDA_ENV}""\n', 'set -x\n. ""${SETUP_SCRIPT}""\npushd benchmark\npython install.py\n', '. ""${SETUP_SCRIPT}""\n# remove old results\nif [ -d benchmark-output ]; then rm -Rf benchmark-output; fi\npushd benchmark\nif [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi\npython run_benchmark.py torch-nightly -c v3-cuda-tests.yaml\ncp -r ./.userbenchmark/torch-nightly ../benchmark-output\n', '. ""${SETUP_SCRIPT}""\npushd benchmark\nRESULTS=($(find ${PWD}/../benchmark-output -name ""metrics-*.json"" -maxdepth 2 | sort -r))\n# TODO: the following assumes only one metrics-*.json is found. It will keep\n# overwriting gh-issue.md if multiple are found.\nfor r in ${RESULTS[@]}; do\n  python regression_detector.py --platform ""${PLATFORM_NAME}"" --treatment ""${r}"" --owner @xuzhao9 \\\n  --gh-issue-path gh-issue.md --errors-path errors.txt\ndone\nrm -r ../benchmark-output || true\ncp -r ./.userbenchmark/torch-nightly ../benchmark-output\n', '. ""${SETUP_SCRIPT}""\npushd benchmark\nLATEST_RESULT=$(find ../benchmark-output/ -name ""metrics-*.json"" | sort -r | head -1)\necho ""Benchmark result file: ${LATEST_RESULT}""\n# Upload the result json to Scribe\npython ./scripts/userbenchmark/upload_scribe.py --userbenchmark_json ""${LATEST_RESULT}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n# Upload the result json to Amazon S3\npython ./scripts/userbenchmark/upload_s3.py --upload-file ""${LATEST_RESULT}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n', '. ""${SETUP_SCRIPT}""\npushd benchmark\nLATEST_REGRESSION_RESULT=$(find ../benchmark-output/ -name ""regression-*.yaml"" | sort -r | head -1)\n# Upload the regression json to Amazon S3\npython ./scripts/userbenchmark/upload_s3.py --upload-file ""${LATEST_REGRESSION_RESULT}"" --userbenchmark_platform ""${PLATFORM_NAME}""\n', '. ""${SETUP_SCRIPT}""\nconda deactivate && conda deactivate\nconda remove -n ""${CONDA_ENV}"" --all\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r test/requirements.txt\n', 'python setup.py install\npython test.py\n', 'codecov\n']"
""
"['curl -X POST ${{ secrets.VERCEL_DOC_DEPLOY_URL_HOOK }}', 'branch=${GITHUB_REF#refs/heads/}\necho branch=""${branch}"" >> $GITHUB_OUTPUT\n', 'edgedb-pkg/integration/macos/build.sh\n', 'edgedb-pkg/integration/macos/build.sh\n', 'edgedb-pkg/integration/macos/test.sh\n', 'edgedb-pkg/integration/macos/test.sh\n', 'set -e\n\nurl=\'https://registry.hub.docker.com/v2/repositories/edgedb/edgedb/tags?page_size=100\'\nrepo_tags=$(\n  while [ -n ""$url"" ]; do\n    resp=$(curl -L -s ""$url"")\n    url=$(echo ""$resp"" | jq -r "".next"")\n    if [ ""$url"" = ""null"" ] || [ -z ""$url"" ]; then\n      break\n    fi\n    echo ""$resp"" | jq -r \'.""results""[][""name""]\'\n  done | grep ""^[[:digit:]]\\+.*"" | grep -v ""alpha\\|beta\\|rc"" || :\n)\n\ntags=( ""$VERSION_CORE"" )\n\ntop=$(printf ""%s\\n%s\\n"" ""$VERSION_CORE"" ""$repo_tags"" \\\n      | grep ""^${VERSION_SLOT}[\\.-]"" \\\n      | sort --version-sort --reverse | head -n 1)\nif [ ""$top"" == ""$VERSION_CORE"" ]; then\n  tags+=( ""$VERSION_SLOT"" )\nfi\n\nif [ -z ""$PKG_SUBDIST"" ]; then\n  top=$(printf ""%s\\n%s\\n"" ""$VERSION_CORE"" ""$repo_tags"" \\\n        | sort --version-sort --reverse | head -n 1)\n  if [ ""$top"" == ""$VERSION_CORE"" ]; then\n    tags+=( ""latest"" )\n  fi\nfi\n\nIFS=,\necho ""tags=${tags[*]}"" >> $GITHUB_OUTPUT\n', 'branch=${GITHUB_REF#refs/heads/}\necho branch=""${branch}"" >> $GITHUB_OUTPUT\n', 'edgedb-pkg/integration/macos/build.sh\n', 'edgedb-pkg/integration/macos/build.sh\n', 'edgedb-pkg/integration/macos/test.sh\n', 'edgedb-pkg/integration/macos/test.sh\n', ""echo 'All build+tests passed, ready to publish now!'"", 'set -e\n\nurl=\'https://registry.hub.docker.com/v2/repositories/edgedb/edgedb/tags?page_size=100\'\nrepo_tags=$(\n  while [ -n ""$url"" ]; do\n    resp=$(curl -L -s ""$url"")\n    url=$(echo ""$resp"" | jq -r "".next"")\n    if [ ""$url"" = ""null"" ] || [ -z ""$url"" ]; then\n      break\n    fi\n    echo ""$resp"" | jq -r \'.""results""[][""name""]\'\n  done | grep ""^[[:digit:]]\\+.*"" | grep -v ""alpha\\|beta\\|rc"" || :\n)\n\ntags=( ""$VERSION_CORE"" )\n\ntop=$(printf ""%s\\n%s\\n"" ""$VERSION_CORE"" ""$repo_tags"" \\\n      | grep ""^${VERSION_SLOT}[\\.-]"" \\\n      | sort --version-sort --reverse | head -n 1)\nif [ ""$top"" == ""$VERSION_CORE"" ]; then\n  tags+=( ""$VERSION_SLOT"" )\nfi\n\nif [ -z ""$PKG_SUBDIST"" ]; then\n  top=$(printf ""%s\\n%s\\n"" ""$VERSION_CORE"" ""$repo_tags"" \\\n        | sort --version-sort --reverse | head -n 1)\n  if [ ""$top"" == ""$VERSION_CORE"" ]; then\n    tags+=( ""latest"" )\n  fi\nfi\n\nIFS=,\necho ""tags=${tags[*]}"" >> $GITHUB_OUTPUT\n', ""echo 'uvloop==0.15.2' > requirements.txt\nmkdir -p pool-simulation/reports\n"", 'pip install -r requirements.txt\n', 'python tests/test_server_pool.py\n', 'branch=${GITHUB_REF#refs/heads/}\necho branch=""${branch}"" >> $GITHUB_OUTPUT\n', 'edgedb-pkg/integration/macos/build.sh\n', 'edgedb-pkg/integration/macos/build.sh\n', 'edgedb-pkg/integration/macos/test.sh\n', 'edgedb-pkg/integration/macos/test.sh\n', ""echo 'All build+tests passed, ready to publish now!'"", 'set -e\n\nurl=\'https://registry.hub.docker.com/v2/repositories/edgedb/edgedb/tags?page_size=100\'\nrepo_tags=$(\n  while [ -n ""$url"" ]; do\n    resp=$(curl -L -s ""$url"")\n    url=$(echo ""$resp"" | jq -r "".next"")\n    if [ ""$url"" = ""null"" ] || [ -z ""$url"" ]; then\n      break\n    fi\n    echo ""$resp"" | jq -r \'.""results""[][""name""]\'\n  done | grep ""^[[:digit:]]\\+.*"" | grep -v ""alpha\\|beta\\|rc"" || :\n)\n\ntags=( ""$VERSION_CORE"" )\n\ntop=$(printf ""%s\\n%s\\n"" ""$VERSION_CORE"" ""$repo_tags"" \\\n      | grep ""^${VERSION_SLOT}[\\.-]"" \\\n      | sort --version-sort --reverse | head -n 1)\nif [ ""$top"" == ""$VERSION_CORE"" ]; then\n  tags+=( ""$VERSION_SLOT"" )\nfi\n\nif [ -z ""$PKG_SUBDIST"" ]; then\n  top=$(printf ""%s\\n%s\\n"" ""$VERSION_CORE"" ""$repo_tags"" \\\n        | sort --version-sort --reverse | head -n 1)\n  if [ ""$top"" == ""$VERSION_CORE"" ]; then\n    tags+=( ""latest"" )\n  fi\nfi\n\nIFS=,\necho ""tags=${tags[*]}"" >> $GITHUB_OUTPUT\n', 'set -e\npython -m pip install -U pip setuptools wheel build\nbdeps_script=""import build; print(\'\\n\'.join(build.ProjectBuilder(\'.\').build_system_requires))""\nreadarray -t build_deps < <(python -c ""${bdeps_script}"")\npython -m pip download --dest=$VIRTUAL_ENV/deps ""${build_deps[@]}"" .[test]\npython -m pip install -U --no-index --find-links=$VIRTUAL_ENV/deps $VIRTUAL_ENV/deps/*\n', ""mkdir -p .tmp\npython setup.py -q ci_helper --type cli > .tmp/edgedbcli_git_rev.txt\npython setup.py -q ci_helper --type rust >.tmp/rust_cache_key.txt\npython setup.py -q ci_helper --type ext >.tmp/ext_cache_key.txt\npython setup.py -q ci_helper --type parsers >.tmp/parsers_cache_key.txt\npython setup.py -q ci_helper --type postgres >.tmp/postgres_git_rev.txt\necho 'v0.17.0' >.tmp/stolon_git_rev.txt\npython setup.py -q ci_helper --type bootstrap >.tmp/bootstrap_cache_key.txt\necho EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n"", 'sudo apt-get update\nsudo apt-get install -y uuid-dev libreadline-dev bison flex\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp -v build/cli/bin/edgedb edb/cli/edgedb\nelse\n  python setup.py -v build_cli\nfi\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p build/rust_extensions\n  rsync -av ./build/rust_extensions/ ${BUILD_LIB}/\n  python setup.py -v build_rust\n  rsync -av ${BUILD_LIB}/ build/rust_extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/rust_extensions/edb/ ./edb/\n', 'python setup.py build_libpg_query\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/extensions\n  rsync -av ./build/extensions/ ${BUILD_LIB}/\n  BUILD_EXT_MODE=py-only python setup.py -v build_ext\n  rsync -av ${BUILD_LIB}/ ./build/extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/extensions/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/lib\n  rsync -av ./build/lib/ ${BUILD_LIB}/\n  python setup.py -v build_parsers\n  rsync -av ${BUILD_LIB}/ ./build/lib/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/lib/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp build/postgres/install/stamp build/postgres/\nelse\n  python setup.py build_postgres\n  cp build/postgres/stamp build/postgres/install/\nfi\n', 'mkdir -p build/stolon/bin/\ncurl -fsSL https://releases.hashicorp.com/consul/1.10.1/consul_1.10.1_linux_amd64.zip | zcat > build/stolon/bin/consul\nchmod +x build/stolon/bin/consul\ncd build/stolon && make\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  rsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\nelse\n  # --no-build-isolation because we have explicitly installed all deps\n  # and don\'t want them to be reinstalled in an ""isolated env"".\n  pip install --no-build-isolation --no-deps -e .[test,docs]\n  rsync -av ./edgedb_server.egg-info/ $VIRTUAL_ENV/edgedb_server.egg-info/\nfi\n', 'edb server --bootstrap-only\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'edb test -j1 -v -k test_ha_\n', 'set -e\npython -m pip install -U pip setuptools wheel build\nbdeps_script=""import build; print(\'\\n\'.join(build.ProjectBuilder(\'.\').build_system_requires))""\nreadarray -t build_deps < <(python -c ""${bdeps_script}"")\npython -m pip download --dest=$VIRTUAL_ENV/deps ""${build_deps[@]}"" .[test]\npython -m pip install -U --no-index --find-links=$VIRTUAL_ENV/deps $VIRTUAL_ENV/deps/*\n', ""mkdir -p .tmp\npython setup.py -q ci_helper --type cli > .tmp/edgedbcli_git_rev.txt\npython setup.py -q ci_helper --type rust >.tmp/rust_cache_key.txt\npython setup.py -q ci_helper --type ext >.tmp/ext_cache_key.txt\npython setup.py -q ci_helper --type parsers >.tmp/parsers_cache_key.txt\npython setup.py -q ci_helper --type postgres >.tmp/postgres_git_rev.txt\necho 'v0.17.0' >.tmp/stolon_git_rev.txt\npython setup.py -q ci_helper --type bootstrap >.tmp/bootstrap_cache_key.txt\necho EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n"", 'sudo apt-get update\nsudo apt-get install -y uuid-dev libreadline-dev bison flex\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp -v build/cli/bin/edgedb edb/cli/edgedb\nelse\n  python setup.py -v build_cli\nfi\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p build/rust_extensions\n  rsync -av ./build/rust_extensions/ ${BUILD_LIB}/\n  python setup.py -v build_rust\n  rsync -av ${BUILD_LIB}/ build/rust_extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/rust_extensions/edb/ ./edb/\n', 'python setup.py build_libpg_query\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/extensions\n  rsync -av ./build/extensions/ ${BUILD_LIB}/\n  BUILD_EXT_MODE=py-only python setup.py -v build_ext\n  rsync -av ${BUILD_LIB}/ ./build/extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/extensions/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/lib\n  rsync -av ./build/lib/ ${BUILD_LIB}/\n  python setup.py -v build_parsers\n  rsync -av ${BUILD_LIB}/ ./build/lib/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/lib/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp build/postgres/install/stamp build/postgres/\nelse\n  python setup.py build_postgres\n  cp build/postgres/stamp build/postgres/install/\nfi\n', 'mkdir -p build/stolon/bin/\ncurl -fsSL https://releases.hashicorp.com/consul/1.10.1/consul_1.10.1_linux_amd64.zip | zcat > build/stolon/bin/consul\nchmod +x build/stolon/bin/consul\ncd build/stolon && make\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  rsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\nelse\n  # --no-build-isolation because we have explicitly installed all deps\n  # and don\'t want them to be reinstalled in an ""isolated env"".\n  pip install --no-build-isolation --no-deps -e .[test,docs]\n  rsync -av ./edgedb_server.egg-info/ $VIRTUAL_ENV/edgedb_server.egg-info/\nfi\n', 'edb server --bootstrap-only\n', 'terraform init', 'terraform apply -auto-approve\n', 'terraform output -raw db_instance_address\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'edb server --bootstrap-only --backend-dsn=$EDGEDB_TEST_BACKEND_DSN --testmode\nedb test -j2 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\n', 'terraform init', 'terraform destroy -auto-approve', 'terraform init', 'terraform apply -auto-approve\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'terraform init', 'terraform output -raw db_instance_address\n', 'terraform output -raw db_instance_port\n', 'terraform output -raw db_instance_user\n', 'terraform output -raw db_instance_password\n', 'terraform output -raw db_instance_database\n', 'edb server --bootstrap-only --backend-dsn=$EDGEDB_TEST_BACKEND_DSN --testmode\nedb test -j2 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\n', 'terraform init', 'terraform destroy -auto-approve', 'terraform init', 'terraform apply -auto-approve\n', 'terraform output -raw db_instance_address\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'edb server --bootstrap-only --backend-dsn=$EDGEDB_TEST_BACKEND_DSN --testmode\nedb test -j2 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\n', 'terraform init', 'terraform destroy -auto-approve', 'terraform init', 'terraform apply -auto-approve\n', 'terraform output -raw rds_cluster_endpoint\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'edb server --bootstrap-only --backend-dsn=$EDGEDB_TEST_BACKEND_DSN --testmode\nedb test -j1 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\n', 'terraform init', 'terraform destroy -auto-approve', 'terraform init', 'terraform apply -auto-approve\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'terraform init', 'terraform output -raw heroku_postgres_dsn\n', 'edb server --bootstrap-only --backend-dsn=$EDGEDB_TEST_BACKEND_DSN --testmode\nedb test -j1 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\n', 'terraform init', 'terraform destroy -auto-approve', 'set -e\npython -m pip install -U pip setuptools wheel build\nbdeps_script=""import build; print(\'\\n\'.join(build.ProjectBuilder(\'.\').build_system_requires))""\nreadarray -t build_deps < <(python -c ""${bdeps_script}"")\npython -m pip download --dest=$VIRTUAL_ENV/deps ""${build_deps[@]}"" .[test]\npython -m pip install -U --no-index --find-links=$VIRTUAL_ENV/deps $VIRTUAL_ENV/deps/*\n', ""mkdir -p .tmp\npython setup.py -q ci_helper --type cli > .tmp/edgedbcli_git_rev.txt\npython setup.py -q ci_helper --type rust >.tmp/rust_cache_key.txt\npython setup.py -q ci_helper --type ext >.tmp/ext_cache_key.txt\npython setup.py -q ci_helper --type parsers >.tmp/parsers_cache_key.txt\npython setup.py -q ci_helper --type postgres >.tmp/postgres_git_rev.txt\necho 'v0.17.0' >.tmp/stolon_git_rev.txt\npython setup.py -q ci_helper --type bootstrap >.tmp/bootstrap_cache_key.txt\necho EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n"", 'sudo apt-get update\nsudo apt-get install -y uuid-dev libreadline-dev bison flex\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp -v build/cli/bin/edgedb edb/cli/edgedb\nelse\n  python setup.py -v build_cli\nfi\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p build/rust_extensions\n  rsync -av ./build/rust_extensions/ ${BUILD_LIB}/\n  python setup.py -v build_rust\n  rsync -av ${BUILD_LIB}/ build/rust_extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/rust_extensions/edb/ ./edb/\n', 'python setup.py build_libpg_query\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/extensions\n  rsync -av ./build/extensions/ ${BUILD_LIB}/\n  BUILD_EXT_MODE=py-only python setup.py -v build_ext\n  rsync -av ${BUILD_LIB}/ ./build/extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/extensions/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/lib\n  rsync -av ./build/lib/ ${BUILD_LIB}/\n  python setup.py -v build_parsers\n  rsync -av ${BUILD_LIB}/ ./build/lib/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/lib/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp build/postgres/install/stamp build/postgres/\nelse\n  python setup.py build_postgres\n  cp build/postgres/stamp build/postgres/install/\nfi\n', 'mkdir -p build/stolon/bin/\ncurl -fsSL https://releases.hashicorp.com/consul/1.10.1/consul_1.10.1_linux_amd64.zip | zcat > build/stolon/bin/consul\nchmod +x build/stolon/bin/consul\ncd build/stolon && make\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  rsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\nelse\n  # --no-build-isolation because we have explicitly installed all deps\n  # and don\'t want them to be reinstalled in an ""isolated env"".\n  pip install --no-build-isolation --no-deps -e .[test,docs]\n  rsync -av ./edgedb_server.egg-info/ $VIRTUAL_ENV/edgedb_server.egg-info/\nfi\n', 'edb server --bootstrap-only\n', 'import json\nimport os\nimport re\nfrom urllib import request\n\nbase = \'https://packages.edgedb.com\'\nu = f\'{base}/archive/.jsonindexes/x86_64-unknown-linux-gnu.json\'\ndata = json.loads(request.urlopen(u).read())\n\nu = f\'{base}/archive/.jsonindexes/x86_64-unknown-linux-gnu.testing.json\'\ndata_testing = json.loads(request.urlopen(u).read())\n\n\nbranch = os.getenv(\'GITHUB_BASE_REF\') or os.getenv(\'GITHUB_REF_NAME\')\nprint(""BRANCH"", branch)\nversion = int(re.findall(r\'\\d+\', branch)[0])\n\nversions = []\nfor obj in data[\'packages\'] + data_testing[\'packages\']:\n    if (\n        obj[\'version_details\'][\'major\'] == version\n        and (\n            not obj[\'version_details\'][\'prerelease\']\n            or obj[\'version_details\'][\'prerelease\'][0][\'phase\'] in (\'beta\', \'rc\')\n        )\n    ):\n        versions.append((obj[\'version\'], base + obj[\'installrefs\'][0][\'ref\']))\n\nmatrix = {\n    ""include"": [\n        {""edgedb-version"": v, ""edgedb-url"": url, ""make-dbs"": mk}\n        for v, url in versions\n        for mk in [True, False]\n    ]\n}\n\nprint(""matrix:"", matrix)\nwith open(os.getenv(\'GITHUB_OUTPUT\'), \'a\') as f:\n    print(f\'matrix={json.dumps(matrix)}\', file=f)\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'wget -q ""$EDGEDB_URL""\ntar xzf edgedb-server-$EDGEDB_VERSION.tar.gz\nedgedb-server-$EDGEDB_VERSION/bin/edgedb-server -D test-dir --bootstrap-only --testmode\n', ""import edgedb\nimport os\nimport subprocess\n\nversion = os.getenv('EDGEDB_VERSION')\ncmd = [\n    f'edgedb-server-{version}/bin/edgedb-server', '-D' 'test-dir',\n    '--testmode', '--security', 'insecure_dev_mode', '--port', '10000',\n]\nproc = subprocess.Popen(cmd)\n\ndb = edgedb.create_client(\n    host='localhost', port=10000, tls_security='insecure'\n)\nfor name in ['json', 'functions', 'expressions', 'casts', 'policies']:\n    db.execute(f'create database {name};')\n\nproc.terminate()\nproc.wait()\n"", '# Should we run *all* the tests?\nedb test -j2 -v --data-dir test-dir tests/test_edgeql_json.py tests/test_edgeql_casts.py tests/test_edgeql_functions.py tests/test_edgeql_expressions.py tests/test_edgeql_policies.py\n', 'import edgedb\nimport os\nimport subprocess\nimport json\n\nversion = os.getenv(\'EDGEDB_VERSION\')\ncmd = [\n    f\'edgedb-server-{version}/bin/edgedb-server\', \'-D\' \'test-dir\',\n    \'--testmode\', \'--security\', \'insecure_dev_mode\', \'--port\', \'10000\',\n]\nproc = subprocess.Popen(cmd)\n\ndb = edgedb.create_client(\n    host=\'localhost\', port=10000, tls_security=\'insecure\',\n    database=\'policies\',\n)\n\ntry:\n    # Test that a basic query works\n    res = json.loads(db.query_json(\'\'\'\n        select Issue { name, number, watchers: {name} }\n        filter .number = ""1""\n    \'\'\'))\n    expected = [{\n        ""name"": ""Release EdgeDB"",\n        ""number"": ""1"",\n        ""watchers"": [{""name"": ""Yury""}],\n    }]\n\n    assert res == expected, res\nfinally:\n    proc.terminate()\n    proc.wait()\n', 'set -e\npython -m pip install -U pip setuptools wheel build\nbdeps_script=""import build; print(\'\\n\'.join(build.ProjectBuilder(\'.\').build_system_requires))""\nreadarray -t build_deps < <(python -c ""${bdeps_script}"")\npython -m pip download --dest=$VIRTUAL_ENV/deps ""${build_deps[@]}"" .[test]\npython -m pip install -U --no-index --find-links=$VIRTUAL_ENV/deps $VIRTUAL_ENV/deps/*\n', ""mkdir -p .tmp\npython setup.py -q ci_helper --type cli > .tmp/edgedbcli_git_rev.txt\npython setup.py -q ci_helper --type rust >.tmp/rust_cache_key.txt\npython setup.py -q ci_helper --type ext >.tmp/ext_cache_key.txt\npython setup.py -q ci_helper --type parsers >.tmp/parsers_cache_key.txt\npython setup.py -q ci_helper --type postgres >.tmp/postgres_git_rev.txt\necho 'v0.17.0' >.tmp/stolon_git_rev.txt\npython setup.py -q ci_helper --type bootstrap >.tmp/bootstrap_cache_key.txt\necho EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n"", 'sudo apt-get update\nsudo apt-get install -y uuid-dev libreadline-dev bison flex\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp -v build/cli/bin/edgedb edb/cli/edgedb\nelse\n  python setup.py -v build_cli\nfi\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p build/rust_extensions\n  rsync -av ./build/rust_extensions/ ${BUILD_LIB}/\n  python setup.py -v build_rust\n  rsync -av ${BUILD_LIB}/ build/rust_extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/rust_extensions/edb/ ./edb/\n', 'python setup.py build_libpg_query\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/extensions\n  rsync -av ./build/extensions/ ${BUILD_LIB}/\n  BUILD_EXT_MODE=py-only python setup.py -v build_ext\n  rsync -av ${BUILD_LIB}/ ./build/extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/extensions/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/lib\n  rsync -av ./build/lib/ ${BUILD_LIB}/\n  python setup.py -v build_parsers\n  rsync -av ${BUILD_LIB}/ ./build/lib/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/lib/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp build/postgres/install/stamp build/postgres/\nelse\n  python setup.py build_postgres\n  cp build/postgres/stamp build/postgres/install/\nfi\n', 'mkdir -p build/stolon/bin/\ncurl -fsSL https://releases.hashicorp.com/consul/1.10.1/consul_1.10.1_linux_amd64.zip | zcat > build/stolon/bin/consul\nchmod +x build/stolon/bin/consul\ncd build/stolon && make\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  rsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\nelse\n  # --no-build-isolation because we have explicitly installed all deps\n  # and don\'t want them to be reinstalled in an ""isolated env"".\n  pip install --no-build-isolation --no-deps -e .[test,docs]\n  rsync -av ./edgedb_server.egg-info/ $VIRTUAL_ENV/edgedb_server.egg-info/\nfi\n', 'edb server --bootstrap-only\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'import asyncio\nimport subprocess\n\nfrom edb.server.pgcluster import get_pg_bin_dir\n\nasync def main():\n    psql = await get_pg_bin_dir() / ""psql""\n    dsn = ""postgres://postgres:postgres@localhost/postgres""\n\n    script = """"""\\\n        CREATE ROLE singles;\n        ALTER ROLE singles WITH LOGIN PASSWORD \'test\' NOSUPERUSER\n          ${{ matrix.single-mode }};\n        CREATE DATABASE singles OWNER singles;\n        REVOKE ALL ON DATABASE singles FROM PUBLIC;\n        GRANT CONNECT ON DATABASE singles TO singles;\n        GRANT ALL ON DATABASE singles TO singles;\n    """"""\n\n    subprocess.run(\n        [str(psql), dsn],\n        check=True,\n        text=True,\n        input=script,\n    )\n\nasyncio.run(main())\n', 'if [[ ""${{ matrix.single-mode }}"" ]]; then\n  export EDGEDB_TEST_BACKEND_DSN=postgres://singles:test@localhost/singles\nelse\n  export EDGEDB_TEST_BACKEND_DSN=postgres://postgres:postgres@localhost/postgres\nfi\nedb server --bootstrap-only --backend-dsn=$EDGEDB_TEST_BACKEND_DSN --testmode\nif [[ ""${{ matrix.single-mode }}"" == *""NOCREATEDB""* ]]; then\n  edb test -j1 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\nelse\n  edb test -j2 -v --backend-dsn=$EDGEDB_TEST_BACKEND_DSN\nfi\n', 'set -e\npython -m pip install -U pip setuptools wheel build\nbdeps_script=""import build; print(\'\\n\'.join(build.ProjectBuilder(\'.\').build_system_requires))""\nreadarray -t build_deps < <(python -c ""${bdeps_script}"")\npython -m pip download --dest=$VIRTUAL_ENV/deps ""${build_deps[@]}"" .[test]\npython -m pip install -U --no-index --find-links=$VIRTUAL_ENV/deps $VIRTUAL_ENV/deps/*\n', 'mkdir -p .tmp\npython setup.py -q ci_helper --type cli > .tmp/edgedbcli_git_rev.txt\npython setup.py -q ci_helper --type rust >.tmp/rust_cache_key.txt\npython setup.py -q ci_helper --type ext >.tmp/ext_cache_key.txt\npython setup.py -q ci_helper --type parsers >.tmp/parsers_cache_key.txt\npython setup.py -q ci_helper --type postgres >.tmp/postgres_git_rev.txt\necho \'v0.17.0\' >.tmp/stolon_git_rev.txt\npython setup.py -q ci_helper --type bootstrap >.tmp/bootstrap_cache_key.txt\necho EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n\ncurl \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  -u edgedb-ci:$GIST_TOKEN \\\n  https://api.github.com/gists/8b722a65397f7c4c0df72f5394efa04c \\\n| jq \'.files.""time_stats.csv"".raw_url\' \\\n| xargs curl > .tmp/time_stats.csv\n', 'sudo apt-get update\nsudo apt-get install -y uuid-dev libreadline-dev bison flex\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp -v build/cli/bin/edgedb edb/cli/edgedb\nelse\n  python setup.py -v build_cli\nfi\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p build/rust_extensions\n  rsync -av ./build/rust_extensions/ ${BUILD_LIB}/\n  python setup.py -v build_rust\n  rsync -av ${BUILD_LIB}/ build/rust_extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/rust_extensions/edb/ ./edb/\n', 'python setup.py build_libpg_query\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/extensions\n  rsync -av ./build/extensions/ ${BUILD_LIB}/\n  BUILD_EXT_MODE=py-only python setup.py -v build_ext\n  rsync -av ${BUILD_LIB}/ ./build/extensions/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/extensions/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" != ""true"" ]]; then\n  rm -rf ${BUILD_LIB}\n  mkdir -p ./build/lib\n  rsync -av ./build/lib/ ${BUILD_LIB}/\n  python setup.py -v build_parsers\n  rsync -av ${BUILD_LIB}/ ./build/lib/\n  rm -rf ${BUILD_LIB}\nfi\nrsync -av ./build/lib/edb/ ./edb/\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  cp build/postgres/install/stamp build/postgres/\nelse\n  python setup.py build_postgres\n  cp build/postgres/stamp build/postgres/install/\nfi\n', 'mkdir -p build/stolon/bin/\ncurl -fsSL https://releases.hashicorp.com/consul/1.10.1/consul_1.10.1_linux_amd64.zip | zcat > build/stolon/bin/consul\nchmod +x build/stolon/bin/consul\ncd build/stolon && make\n', 'if [[ ""$CACHE_HIT"" == ""true"" ]]; then\n  rsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\nelse\n  # --no-build-isolation because we have explicitly installed all deps\n  # and don\'t want them to be reinstalled in an ""isolated env"".\n  pip install --no-build-isolation --no-deps -e .[test,docs]\n  rsync -av ./edgedb_server.egg-info/ $VIRTUAL_ENV/edgedb_server.egg-info/\nfi\n', 'edb server --bootstrap-only\n', 'echo ::error::Cannot retrieve venv cache.\nexit 1\n', 'echo BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'mkdir -p .results/\ncp .tmp/time_stats.csv .results/shard_${SHARD/\\//_}.csv\nedb test -j2 -v -s ${SHARD} --running-times-log=.results/shard_${SHARD/\\//_}.csv\n', 'echo EDGEDBCLI_GIT_REV=$(cat .tmp/edgedbcli_git_rev.txt) >> $GITHUB_ENV\necho POSTGRES_GIT_REV=$(cat .tmp/postgres_git_rev.txt) >> $GITHUB_ENV\necho STOLON_GIT_REV=$(cat .tmp/stolon_git_rev.txt) >> $GITHUB_ENV\necho BUILD_LIB=$(python setup.py -q ci_helper --type build_lib) >> $GITHUB_ENV\necho BUILD_TEMP=$(python setup.py -q ci_helper --type build_temp) >> $GITHUB_ENV\n', 'echo ::error::Cannot retrieve build cache.\nexit 1\n', 'cp -v build/cli/bin/edgedb edb/cli/edgedb\nrsync -av ./build/rust_extensions/edb/ ./edb/\nrsync -av ./build/extensions/edb/ ./edb/\nrsync -av ./build/lib/edb/ ./edb/\ncp build/postgres/install/stamp build/postgres/\nrsync -av $VIRTUAL_ENV/edgedb_server.egg-info/ ./edgedb_server.egg-info/\n', 'edb test --list > .tmp/all_tests.txt\n', 'python -m pip install requests\n', 'import csv\nimport glob\nimport io\nimport os\nimport requests\n\norig = {}\nnew = {}\nall_tests = set()\nwith open("".tmp/time_stats.csv"") as f:\n    for name, t, c in csv.reader(f):\n        assert name not in orig, ""duplicate test name in original stats!""\n        orig[name] = (t, int(c))\n\nwith open("".tmp/all_tests.txt"") as f:\n    for line in f:\n        assert line not in all_tests, ""duplicate test name in this run!""\n        all_tests.add(line.strip())\n\nfor new_file in glob.glob("".results/*.csv""):\n    with open(new_file) as f:\n        for name, t, c in csv.reader(f):\n            if int(c) > orig.get(name, (0, 0))[1]:\n                if name.startswith(""setup::""):\n                    new[name] = (t, c)\n                else:\n                    assert name not in new, f""duplicate test! {name}""\n                    new[name] = (t, c)\n                    all_tests.remove(name)\n\nassert not all_tests, ""Tests not run! \\n"" + ""\\n"".join(all_tests)\n\nif os.environ[""GIT_REF""] == ""refs/heads/master"":\n    buf = io.StringIO()\n    writer = csv.writer(buf)\n    orig.update(new)\n    for k, v in sorted(orig.items()):\n        writer.writerow((k,) + v)\n\n    resp = requests.patch(\n        ""https://api.github.com/gists/8b722a65397f7c4c0df72f5394efa04c"",\n        headers={""Accept"": ""application/vnd.github.v3+json""},\n        auth=(""edgedb-ci"", os.environ[""GIST_TOKEN""]),\n        json={""files"": {""time_stats.csv"": {""content"": buf.getvalue()}}},\n    )\n    resp.raise_for_status()\n']"
"[""python -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\npip install flake8 black isort>=5.0 mypy nbstripout nbformat\n"", 'make lint\n', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build graphviz\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch==1.11.0+cpu torchvision==0.12.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\npip install .[test]\npip install -r docs/requirements.txt\npip freeze\n"", 'make docs\nmake doctest\n', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build graphviz pandoc\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch==1.11.0+cpu torchvision==0.12.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\npip install .[test]\npip install -r docs/requirements.txt\n# requirements for tutorials (from .[dev])\nsudo apt-get install pandoc\npip install nbformat\npip install nbsphinx>=0.3.2\npip install nbstripout\npip install pypandoc\npip install ninja\npip freeze\n"", 'SPHINXOPTS=""-E"" make tutorial\n', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install .[test]\npip install --upgrade coveralls\npip freeze\n"", 'pytest -vs --cov=pyro --cov-config .coveragerc --stage unit --durations 20\n', 'coveralls --service=github || true', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install .[test]\npip install --upgrade coveralls\npip freeze\n"", ""CI=1 pytest -vs --cov=pyro --cov-config .coveragerc --stage test_examples --durations 10\ngrep -l smoke_test tutorial/source/*.ipynb | xargs grep -L 'smoke_test = False' \\\n                  | CI=1 xargs pytest -vx --nbval-lax --current-env\n"", 'coveralls --service=github || true', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install .[test]\npip install --upgrade coveralls\npip freeze\n"", 'pytest -vs --cov=pyro --cov-config .coveragerc --stage integration_batch_1 --durations 10\n', 'coveralls --service=github || true', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install .[test]\npip install --upgrade coveralls\npip freeze\n"", 'pytest -vs --cov=pyro --cov-config .coveragerc --stage integration_batch_2 --durations 10\n', 'coveralls --service=github || true', ""sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-8 g++-8 ninja-build\npython -m pip install --upgrade pip wheel 'setuptools!=58.5.*,<60'\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install .[test]\npip install -e .[funsor]\npip install --upgrade coveralls\npip freeze\n"", 'pytest -vs --cov=pyro --cov-config .coveragerc --stage funsor --durations 10\nCI=1 pytest -vs --cov=pyro --cov-config .coveragerc --stage test_examples --durations 10 -k funsor\n', 'coveralls --service=github || true', 'pip install --upgrade coveralls\ncoveralls --service=github --finish || true\n']"
"['python -m pip install build wheel --user', 'echo `pwd`\necho `git log|head -n 50`\npython setup.py -q sdist bdist_wheel --universal\n', ""echo '${{ toJSON(github) }}'"", 'pip install tensorboardX*.whl', 'pip install torch torchvision', 'python -c ""import tensorboardX; print(tensorboardX.__version__)""', 'cd examples\npython demo.py\n', 'echo ""target version: ${{ github.event.inputs.publish_version }}""\n', 'echo \'${{ toJSON(github) }}\'\necho ""target version: ${{ github.event.inputs.publish_version }}""\n', 'python -m pip install build wheel --user', 'echo `pwd`\npython setup.py -q sdist bdist_wheel --universal ${{ github.event.inputs.publish_version }}\n', 'pip install dist/tensorboardX*.whl', 'python -c ""import tensorboardX; print(tensorboardX.__version__)""', 'sudo apt install libsndfile1\npython -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f test-requirements.txt ]; then pip install -r test-requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 tensorboardX --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 tensorboardX --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'visdom &\npytest --cov=tensorboardX tests/\n', 'pip install six tensorboard pytest matplotlib torchvision protobuf==4.22.3 moviepy==1.0.3 imageio==2.27\npython examples/demo.py\npython examples/demo_graph.py\npython examples/demo_embedding.py\npython examples/demo_custom_scalars.py\npython examples/demo_multiple_embedding.py\npython examples/demo_purge.py\npython examples/demo_matplotlib.py\n']"
['git checkout HEAD^2']
""
"['poetry install --no-root', 'poetry build --format wheel', 'pip install dist/gerapy-*.whl', 'gerapy -v\ngerapy init\ncd gerapy\ngerapy migrate\ngerapy initadmin\n', 'docker login -u germey -p ${{ secrets.DOCKERHUB_LOGIN_PASSWORD }}', 'cd gerapy/client\nnpm install -g yarn\nyarn \nyarn build\n', 'docker build -t germey/gerapy:master -f ./docker/Dockerfile .\n', 'docker push germey/gerapy:master', 'docker login -u germey -p ${{ secrets.DOCKERHUB_LOGIN_PASSWORD }}', 'cd gerapy/client\nnpm install -g yarn\nyarn \nyarn build\n', 'tag=${GITHUB_REF:11}\necho ""Build Tag \'$tag\'""\ndocker build -t germey/gerapy:$tag -f ./docker/Dockerfile .\ndocker push germey/gerapy:$tag\nregex=\'^([0-9]+\\.){0,2}(\\*|[0-9]+)$\'\nif [[ $tag =~ $regex ]]; then\n  echo ""Build Stable Version \'$tag\'""\n  docker tag germey/gerapy:$tag germey/gerapy:latest\n  docker push germey/gerapy:latest\nfi\n', 'mkdir -p ~/.ssh/\necho ""$DOCS_DEPLOY_KEY"" > ~/.ssh/id_rsa\nchmod 600 ~/.ssh/id_rsa\nssh-keyscan github.com > ~/.ssh/known_hosts\nchmod 700 ~/.ssh && chmod 600 ~/.ssh/*\ngit config --global user.email ""cqc@cuiqingcai.com""\ngit config --global user.name ""Germey""\n', 'cd /tmp\ngit clone git@github.com:Gerapy/Docs.git docs\ncd docs\ngit branch -D docs || true\ngit push origin --delete docs || true\ngit clone https://github.com/Gerapy/Gerapy.git gerapy\ncd gerapy\ngit subtree split --prefix=docs --squash -b docs\ngit checkout docs\ngit push /tmp/docs docs:docs\ncd /tmp/docs\ngit checkout docs\ngit checkout -b master || git checkout master || true\ngit reset --hard docs\ngit push origin master --force\n']"
"['pip install -e .\n', 'pip install -r requirements.txt\n', 'python -m pip install --upgrade pip setuptools\npip install -r docs/requirements.txt\n', 'cd docs && make html\n', 'sudo apt update\nsudo apt install yarn\nsudo yarn global add vuepress\n', 'sudo vuepress build\n', ""sudo git init\nsudo git add -A\nsudo git commit -m 'deploy'\nsudo git push -f https://x-access-token:${GITHUB_TOKEN}@github.com/bethgelab/foolbox.git master:gh-pages\n"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'function retry-with-backoff() {\n  for BACKOFF in 0 1 2 4 8 16 32 64; do\n    sleep $BACKOFF\n    if ""$@""; then\n      return 0\n    fi\n  done\n  return 1\n}\n\npython -m pip install --upgrade pip setuptools wheel\nretry-with-backoff pip install -r requirements.txt\n', 'flake8 . --count --show-source --statistics\n', 'black --check --verbose .\n', 'pip install -e .\n', 'function retry-with-backoff() {\n  for BACKOFF in 0 1 2 4 8 16 32 64; do\n    sleep $BACKOFF\n    if ""$@""; then\n      return 0\n    fi\n  done\n  return 1\n}\n\nretry-with-backoff pip install -r tests/requirements.txt\n', 'mypy --install-types --non-interactive foolbox/\nmypy -p foolbox\n', 'mypy tests/\n', 'pytest --durations=0 --cov-report term-missing --cov=foolbox --verbose --backend ${{ matrix.backend }}\n', 'codecov\n', 'coveralls\n']"
"['curl -sSL https://install.python-poetry.org | python3 - --version ${{ matrix.poetry-version }}', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry install', 'poetry run pytest --cov=./']"
""
"['', 'python -m pip install --upgrade pip', 'pip install -r requirements/requirements.txt', 'pip install -r requirements/requirements.deepspeed.txt', 'if [ ""$RUNNER_OS"" == ""Linux"" ]; then\n  pip install -r requirements/requirements.faiss-cpu.txt\nfi\n', 'pip install -r requirements/requirements.dev.txt', 'pytest --version\npytest\n', 'pylint --version\npylint --rcfile=.pylintrc sockeye\npylint --rcfile=.pylintrc test --disable=E1102\n', 'mypy --version\nmypy --ignore-missing-imports --follow-imports=silent @typechecked-files --no-strict-optional\n', 'check-manifest --ignore sockeye/git_version.py', 'pytest test/system \n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'pip install -r requirements/requirements.txt', 'pip install -r requirements/requirements.deepspeed.txt', 'pip install -r requirements/requirements.dev.txt', 'pytest --version\npytest\n', 'check-manifest --ignore sockeye/git_version.py', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements/requirements.txt\npip install -r requirements/requirements.dev.txt\npip3 install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --upgrade\n', 'pip list | grep torch', 'pytest', 'pytest --maxfail=1 test/system']"
"['echo ""PY_CACHE_KEY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_ENV', 'python -m site\npython -m pip --version\npython -m pip list\nif [[ ""${{ matrix.python }}"" == ""3.4"" ]]; then\n  # Install latest available pip.\n  # 7.1.2 (installed) is too old to not install too new packages,\n  # including pip itself.  19.2 dropped support for Python 3.4.\n  python -m pip install -U pip==19.1.1\nfi\npython -m pip install -U setuptools==42.0.2\npython -m pip install -U virtualenv==20.4.3\n', ""python -m pip install ${{ matrix.tox || 'git+https://github.com/blueyed/tox@master' }}"", 'python -m pip list', 'python -m tox --notest -v $TOX_ARGS --force-dep=""$PYTEST""', 'python -m tox -v $TOX_ARGS --force-dep=""$PYTEST""', 'python -m pip install -r .github/workflows/requirements/release.txt', 'python setup.py sdist bdist_wheel', 'check-manifest', 'twine check dist/*', 'twine upload dist/*']"
"['python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'make docs', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'make lint', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install --upgrade pip\npip install -e .\npip install -r requirements_dev.txt\n', 'make test', 'python examples/demo.py']"
"['python -m pip install --upgrade pip\npip install mkdocs\n', 'python redditdownloader/tools/doc_builder.py\nmkdocs build\n', 'echo ""Scanned: ${{ steps.badge.outputs.counted_files }}"";\necho ""Line Count: ${{ steps.badge.outputs.total_lines }}"";\n', 'echo ""Artifact Name: ${{ matrix.artifact }}""\necho ""Latest Release Tag: ${{ steps.update.outputs.release_tag }}""\necho ""Update Packages: ${{ steps.update.outputs.updated_packages }}""\necho ""Needs Update: ${{ steps.update.outputs.update_available }}""\n', ""pip install -r requirements.txt\npip install -r requirements-dev.txt\npip install --upgrade 'setuptools>=49.1.1'\n"", 'pyinstaller build-rmd.spec\n', '${{matrix.run_test}}\n', 'echo ""::set-output name=path::$(python -c \'import site; print(site.getusersitepackages())\')""\n', 'python -m pip install --upgrade pip setuptools wheel\npip install --upgrade -r requirements.txt --user --no-warn-script-location\npip install --upgrade -r requirements-dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'pytest --cov=./ --cov-report=xml --verbose redditdownloader/\n', 'pip install codecov\ncodecov -t ${{secrets.CODECOV_TOKEN}} --name ""${{ matrix.os }}, ${{ matrix.python-version }}""\n']"
"['python -m pip install --upgrade pip\npython -m pip install virtualenv\npython -m virtualenv pykaldi_env\npykaldi_env/bin/python3 -m pip install numpy pyparsing ninja wheel\n# Kaldi dependencies\nsudo apt-get install sox\n', 'source pykaldi_env/bin/activate\ncd tools\n./check_dependencies.sh\n', 'source pykaldi_env/bin/activate\ncd tools\n./install_protobuf.sh\n', 'source pykaldi_env/bin/activate\ncd tools\n./install_clif.sh\n', 'source pykaldi_env/bin/activate\ncd tools\ngit clone -b pykaldi_02 https://github.com/pykaldi/kaldi.git\ncd kaldi/tools\ngit pull\ncd extras\nsudo ./install_mkl.sh\ncd ../../..\n./install_kaldi.sh\n', 'source pykaldi_env/bin/activate\npython3 setup.py install\npython3 setup.py bdist_wheel\n', 'source pykaldi_env/bin/activate\npython3 setup.py test\n', 'find dist -type f -exec curl --netrc --user $FTP_USER:$FTP_PW --ftp-create-dirs -T {} ftp://$FTP_DOMAIN:990/public/ -k --ftp-ssl \\;\n']"
"['python -m pip install -e .', 'cd docs\nmake html\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*', 'source ci/install.sh', 'pytest -v', 'source ci/install.sh', 'pytest -v']"
"['make install', 'make test TEST_FLAGS=""-n 5""', 'python -m scripts.statcast_timing', 'make validate-cache', 'make mypy ONLY_MODIFIED=0', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install -U pip pipenv\npipenv sync --system --dev\n', 'make test', 'make pytest', 'python -m pip install -U pip pipenv wheel\npipenv sync --system --dev\n', 'python setup.py sdist bdist_wheel', ""sed -i 's/chinesecalendar/chinese_calendar/g' setup.py\npython setup.py sdist bdist_wheel\n""]"
"['gh pr merge --auto --squash ""$PR_URL""', 'make setup', 'make -C docs html SPHINXOPTS=""-W -E""', 'make setup', 'make lint', 'python -m pip install -r requirements/ci.txt', 'pytest --cov-branch --cov=aiojobs --cov-report=term tests', 'python -m pip install -U pip wheel setuptools build twine\n', 'python -m build\n']"
"['make dockerize', 'make docker-push VERSION=""$(echo $GITHUB_REF | sed \'s!^refs/\\(heads\\|tags\\)/!!\')""', 'pip install -r requirements_frozen.txt', 'pytest']"
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npip install pytest-cov\npytest --cov=pysbd tests/ --color yes --cov-report=xml --cov-report=html\n']"
"['matrix_str=$(python utils/info2json.py deploy_matrix)\necho ""matrix=$matrix_str"" >> $GITHUB_OUTPUT\n', 'sudo npm install -g json2yaml\n', ""matrix='${{ needs.prepare_matrix.outputs.matrix }}'\necho $matrix\necho $matrix | jq .\necho $matrix | json2yaml\n"", 'pip install --upgrade pip\npip install .[test]\n', 'pytest --cov-report term --cov-report xml:cobertura.xml --cov=pydmd\ncurl -s https://coverage.codacy.com/get.sh -o CodacyCoverageReporter.sh\nchmod +x CodacyCoverageReporter.sh\n./CodacyCoverageReporter.sh report -r cobertura.xml  -t $CODACY_API_TOKEN\n', 'matrix_str=$(python utils/info2json.py deploy_matrix)\necho ""matrix=$matrix_str"" >> $GITHUB_OUTPUT\n', 'sudo npm install -g json2yaml\n', ""matrix='${{ needs.prepare_matrix.outputs.matrix }}'\necho $matrix\necho $matrix | jq .\necho $matrix | json2yaml\n"", 'pip install --upgrade pip\npip install build\n', 'python -m build --sdist --wheel --outdir dist/ .\n', 'cp docs/source/_tutorials/* docs/build/html\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install .[test]\npython3 -m pip install .\n# Dependencies for tutorials\npython3 -m pip install jupyter pandas ezyrb opencv-python ffmpeg-python black[jupyter]\n', 'curl https://web.engr.oregonstate.edu/~lif/SegTrack2/SegTrackv2.zip --output SegTrackv2.zip\nunzip -qq SegTrackv2.zip\nmv SegTrackv2 tutorials/tutorial12\n', 'git config user.name ""github-actions[bot]""\ngit config user.email 41898282+github-actions[bot]@users.noreply.github.com\n', 'for file in ${{ steps.files.outputs.all }}; do\n  if [[ $file == *tutorial-5* ]]; then\n    echo ""Skipped $file""\n  elif [[ $file == *.ipynb ]]; then\n    filename=$(basename $file)\n\n    pyfilename=$(echo ${filename%?????})py\n    jupyter nbconvert --execute $file --to python --output $pyfilename\n\n    htmlfilename=$(echo ${filename%?????} | sed -e \'s/-//g\')html\n    jupyter nbconvert --execute $file --to html --output $htmlfilename --output-dir=docs/source/_tutorials\n  fi\ndone\n', 'black tutorials/', 'rm -rf build/\nrm -rf tutorials/tutorial12/SegTrackv2/\n', 'pip install --upgrade pip\npip install .[test]\n', 'pytest\n', 'python utils/mathlab_versioning.py set --only-date ""post$(date +%y%m)""\ncat pydmd/meta.py\nVERS=$(python utils/mathlab_versioning.py get)\ngit config --global user.name \'Monthly Tag bot\'\ngit config --global user.email \'mtbot@noreply.github.com\'\ngit add pydmd/meta.py\ngit commit -m ""monthly version $VERS""\ngit tag -a ""v$VERS"" -m ""Monthly version $VERS""\ngit push origin ""v$VERS""\n', 'matrix_str=$(python utils/info2json.py testing_matrix)\necho ""matrix1=$matrix_str"" >> $GITHUB_OUTPUT\nmatrix_str=$(python utils/info2json.py tutorial_testing_matrix)\necho ""matrix2=$matrix_str"" >> $GITHUB_OUTPUT\n', 'sudo npm install -g json2yaml\n', ""matrix='${{ needs.prepare_matrix.outputs.matrix_unit_test }}'\necho $matrix\necho $matrix | jq .\necho $matrix | json2yaml\n"", 'pip install --upgrade pip\npip install .[test]\n', 'pytest\n', 'pip install black[jupyter]\n', 'black --check pydmd/\nif [[ $? -ne ""0"" ]]; then\n  echo ""Source code needs re-formatting""\n  exit 1\nfi\n', 'black --check tutorials/\nif [[ $? -ne ""0"" ]]; then\n  echo ""Tutorials need re-formatting""\n  exit 1\nfi\n', 'pip install --upgrade pip\npip install .[test]\n# Dependencies for tutorials\npip install jupyter pandas opencv-python ffmpeg-python\n', 'curl https://web.engr.oregonstate.edu/~lif/SegTrack2/SegTrackv2.zip --output SegTrackv2.zip\nunzip -qq SegTrackv2.zip\nmv SegTrackv2 tutorials/tutorial12\n', 'cd tutorials/\nfor dir in $(ls -d tutorial*/); do\n  if grep -q ""$dir"" ""README.md""\n    then\n    echo ""$dir is referenced""\n  else\n    echo ""$dir not referenced""\n    exit 1\n  fi\ndone\n', 'cd tutorials/\nfor dir in $(ls -d tutorial*/); do\n  if [[ $dir != tutorial5* ]]\n    then\n    cd $dir\n    jupyter nbconvert --to notebook --execute *.ipynb\n    cd ..\n  fi\ndone\n']"
""
"['python -m pip install --upgrade pip\npip install .\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 --ignore E501,E722\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install coverage\npip install coveralls\npip install pytest\ncoverage run --source=aws_google_auth/ --omit=aws_google_auth/tests/* setup.py test\ncoverage report\ncoveralls', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install Pygments restructuredtext_lint\n', 'rst-lint README.rst\n']"
""
"['gh pr merge --auto --squash ""$PR_URL""', '# black = ""^21.7b0""\nexport BLACK_VERSION=$(grep black pyproject.toml | egrep -o \'\\^[0-9a-z.]+\' | sed \'s/\\^//g\')\n\nset -x\npip install black==${BLACK_VERSION}\n', 'black --check .\n', 'pip install wheel', 'pip install black', 'poetry --version\npoetry config --list\n', 'poetry install --no-root\npoetry run python --version\npoetry show\n', 'make coverage', 'poetry run coveralls --service=github || true', 'make lint', 'poetry build -vvv', 'poetry build -vvv\npoetry publish -u ${{ secrets.PYPI_USERNAME }} -p ${{ secrets.PYPI_PASSWORD }}\n']"
"['set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox', 'python -m coverage xml', 'set -xe\npython -m pip install virtualenv tox\n', 'python -m tox']"
"['echo ""::set-output name=date::$(date +\'%Y%m%d\')""', 'kubectl get nodes\nkubectl get svc -n scrape\n', 'echo $BUILD_NUMBER\n', 'docker-compose -f build.yaml build\ndocker tag germey/proxypool germey/proxypool:$BUILD_NUMBER\ndocker push germey/proxypool\ndocker push germey/proxypool:$BUILD_NUMBER\ncat deployment.yml | sed \'s/\\${TAG}/\'""$BUILD_NUMBER""\'/g\' | kubectl apply -f -\n']"
"['pip install pipenv\npipenv install -d\n', 'pipenv run flake8\n', 'pipenv run black --check --diff monkeytype\n', 'pipenv run isort --check --diff monkeytype\n', 'pipenv run mypy monkeytype\n', 'pipenv run pytest\n', 'cd doc\npipenv run make html\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npytest\n']"
[]
"['pip install pip==22.0.3 setuptools==60.8.2\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nsource $HOME/.poetry/env\npoetry install --no-interaction -E pg -E mysql\n', 'pip list', 'sphinx-intl create-transifexrc\nmake -C docs -e LOC=""${{ matrix.language }}"" pull\n', 'make -C docs -e SPHINXOPTS=""-D language=\'${{ matrix.language }}\' -A GAID=\'${{ secrets.GAID }}\' -A VERSION=\'${{ github.ref }}\'"" html\n', ""import os, json\ntry:\n    with open('docs/versions.json') as f:\n        versions = json.load(f)\nexcept Exception:\n    versions = {}\nby_loc = versions.setdefault(os.environ['LOC'], [])\nby_loc.append(os.environ['GITHUB_REF'].split('/')[-1])\nby_loc.sort()\nwith open('docs/versions.json', 'w') as f:\n    json.dump(versions, f)\nprint(versions)\n"", 'pip install pip==22.0.3 setuptools==60.8.2\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nsource $HOME/.poetry/env\npoetry build\npoetry publish --username __token__ --password ${{ secrets.PYPI_TOKEN }}\n', 'pip install pip==21.3.1 setuptools==59.6.0\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nsource $HOME/.poetry/env\npoetry install --no-interaction -E pg -E mysql\n', 'pip list', 'black --check --diff src', 'pytest --cov=src --cov-fail-under=95 --cov-report xml\n', 'pip install codacy-coverage\npython-codacy-coverage -r coverage.xml\n', 'pytest', 'pytest tests/', 'pytest mysql_tests/', 'pip install asyncpg==0.18 SQLAlchemy==1.3 pytest', 'pytest tests/ mysql_tests/', 'sphinx-intl create-transifexrc\nmake -C docs gettext push\n']"
"['pip install flake8 mypy types-dataclasses .\n', 'flake8 ib_insync', 'mypy -p ib_insync\n']"
""
"['pip install bandit codespell flake8', 'bandit --recursive  --skip B102,B110,B307,B311,B404,B603,B602 .', 'codespell --ignore-words-list=""crate,nd,te"" .', 'flake8 . --count --select=E9,F63,F7 --show-source --statistics']"
"['pip install --upgrade pip\npip install --user .[dev]\ntox -e pre-commit,mypy\n', 'SOURCE_DATE_EPOCH=$(git log -1 --pretty=%ct) pipx run build --sdist --wheel\n', 'pipx run --python $(which python${{ matrix.python }}) --spec dist/*.whl ${GITHUB_REPOSITORY##*/} --version', 'touch requirements.txt requirements_dev.txt\npip install -r requirements.txt -e .\npip freeze --exclude-editable > requirements.txt\npip install -r requirements_dev.txt -e .[dev]\npip freeze --exclude-editable > requirements_dev.txt\n', 'pip install -e .[dev]', 'pytest tests', 'pip freeze --exclude-editable > requirements_dev.txt\n', 'pipx run twine upload artifacts/dist/*', 'sleep 60', 'sudo apt-get install graphviz', 'touch requirements_dev.txt\npip install -r requirements_dev.txt -e .[dev]\n', 'tox -e docs', 'mv build/html "".github/pages/${GITHUB_REF##*/}""', 'sphinx_rtd_theme_github_versions .github/pages', 'sleep 60', 'touch requirements_dev.txt\npip install -r requirements_dev.txt -e .[dev]\n', 'tox -e docs -- -b linkcheck']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['pip3 install -r requirements.txt\npip3 install pyinstaller wheel pyinstaller-hooks-contrib\npython setup.py sdist bdist_wheel\npython pack.py\n', 'python --version\nexport QT_DEBUG_PLUGINS=1\nsudo apt-get update\nDEBIAN_FRONTEND=noninteractive sudo apt-get install -y --no-install-recommends \\\n  xvfb \\\n  x11-utils \\\n  libxkbcommon-x11-0 \\\n  libxcb-icccm4 \\\n  libxcb-image0 \\\n  libxcb-keysyms1 \\\n  libxcb-randr0 \\\n  libxcb-render-util0 \\\n  libxcb-xkb1 \\\n  libegl1-mesa \\\n  libxcb-xinerama0 \\\n  libglib2.0-0 \\\n  libopengl0\npip3 install -r requirements.txt\npip3 install -U pyinstaller wheel pyinstaller-hooks-contrib\npython setup.py sdist bdist_wheel\nxvfb-run python pack.py\n', 'python --version\nexport QT_DEBUG_PLUGINS=1\nsudo apt-get update\nDEBIAN_FRONTEND=noninteractive sudo apt-get install -y --no-install-recommends \\\n  xvfb \\\n  x11-utils \\\n  libxkbcommon-x11-0 \\\n  libxcb-icccm4 \\\n  libxcb-image0 \\\n  libxcb-keysyms1 \\\n  libxcb-randr0 \\\n  libxcb-render-util0 \\\n  libxcb-xkb1 \\\n  libegl1-mesa \\\n  libxcb-xinerama0 \\\n  libglib2.0-0 \\\n  libopengl0\npip3 install -r requirements.txt\npip3 install pyinstaller wheel\npython setup.py sdist bdist_wheel\nxvfb-run python pack.py\nrelease_path=`python pack.py ${{ matrix.os }}`\necho $release_path\nrelease_name=`echo $release_path | awk -F""/"" \'{print $NF}\'`\necho ::set-output name=release_path::$release_path\necho ::set-output name=release_name::$release_name\n', 'python --version\npip3 install -r requirements.txt\npip3 install pyinstaller wheel pyinstaller-hooks-contrib\npython setup.py sdist bdist_wheel\npython pack.py\nrelease_path=`python pack.py ${{ matrix.os }}`\necho $release_path\nrelease_name=`echo $release_path | awk -F""/"" \'{print $NF}\'`\necho ::set-output name=release_path::$release_path\necho ::set-output name=release_name::$release_name\n', 'python --version\npip3 install -r requirements.txt\npip3 install pyinstaller wheel pyinstaller-hooks-contrib\npython setup.py sdist bdist_wheel\npython pack.py\n$release_path = python pack.py ${{ matrix.os }}\necho ""::set-output name=release_path::$release_path""\n']"
"['sudo apt-get update\nsudo apt-get install libnode-dev node-gyp libssl-dev\nsudo apt-get install npm\nsudo npm install -g json\n', 'curl -s https://api.github.com/repos/geigi/cozy/releases/latest | json tag_name > /tmp/VERSION\necho https://github.com/geigi/cozy/archive/$(cat /tmp/VERSION).tar.gz > /tmp/RELEASE_URL\nwget -O /tmp/cozy.tar.gz $(cat /tmp/RELEASE_URL)\nsha256sum /tmp/cozy.tar.gz | cut -d "" "" -f 1 > /tmp/SHA256SUM\n', 'mkdir -p ~/.ssh\nssh-keyscan aur.archlinux.org >> ~/.ssh/known_hosts\nssh-agent -a $SSH_AUTH_SOCK > /dev/null\nssh-add - <<< ""${{ secrets.AUR_PRIVATE }}""\n', 'git clone ssh://aur@aur.archlinux.org/cozy-audiobooks.git /tmp/aur', 'ls /tmp/aur\ncd /tmp/aur\nsed -i ""s/^pkgver.*\\$/pkgver=$(cat /tmp/VERSION)/"" PKGBUILD\nsed -i ""s/^sha256sum.*\\$/sha256sums=(\'$(cat /tmp/SHA256SUM)\')/"" PKGBUILD\nsed -i ""s/.*pkgver.*\\$/        pkgver = $(cat /tmp/VERSION)/"" .SRCINFO\nsed -i ""s/.*source.*\\$/       source = https\\:\\/\\/github.com\\/geigi\\/cozy\\/archive\\/$(cat /tmp/VERSION)\\.tar\\.gz/"" .SRCINFO\nsed -i ""s/.*sha256sums.*\\$/        sha256sums = $(cat /tmp/SHA256SUM)/"" .SRCINFO\n', 'git config --global user.email ""github@geigi.de""\ngit config --global user.name ""Github Actions""\ncd /tmp/aur\ngit commit -am ""Bump version to $(cat /tmp/VERSION)""\ngit push', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --builtins=""_""\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --builtins=""_""\n', 'pytest\n', 'meson --prefix=/usr ./build\nninja -C build install\n', 'sudo apt-get update\nsudo apt-get install libnode-dev node-gyp libssl-dev\nsudo apt-get install npm\nsudo npm install -g json\n', 'curl -s https://api.github.com/repos/geigi/cozy/releases/latest | json tag_name > /tmp/VERSION\necho https://github.com/geigi/cozy/archive/$(cat /tmp/VERSION).tar.gz > /tmp/RELEASE_URL\nwget -O /tmp/cozy.tar.gz $(cat /tmp/RELEASE_URL)\nsha256sum /tmp/cozy.tar.gz | cut -d "" "" -f 1 > /tmp/SHA256SUM\n', 'git clone https://geigi:${{ secrets.FLATHUB_TOKEN }}@github.com/flathub/com.github.geigi.cozy.git /tmp/flathub', 'ls /tmp/flathub\njson -I -f /tmp/flathub/com.github.geigi.cozy.json -e ""this.modules[this.modules.length - 1].sources[0].url=\'https://github.com/geigi/cozy/archive/$(cat /tmp/VERSION).tar.gz\'""\njson -I -f /tmp/flathub/com.github.geigi.cozy.json -e ""this.modules[this.modules.length - 1].sources[0].sha256=\'$(cat /tmp/SHA256SUM)\'""\n', 'git config --global user.email ""github@geigi.de""\ngit config --global user.name ""Github Actions""\ncd /tmp/flathub\ngit commit -am ""Bump version to $(cat /tmp/VERSION)""\ngit push\n', 'cd $GITHUB_WORKSPACE/.ci\nchmod +x *.sh\n./flathub_wait_for_build.sh', 'dnf -y install docker\n', 'mkdir -p ~/.config/osc\nsudo apt-get update\nsudo apt-get install libnode-dev node-gyp libssl-dev\nsudo apt-get install osc npm python3-m2crypto\nsudo npm install -g json\n', 'cat >~/.config/osc/oscrc <<EOL\n${{ secrets.OSCRC }}\nEOL\n', 'osc co X11:Pantheon:Apps/cozy', 'curl -s https://api.github.com/repos/geigi/cozy/releases/latest | json tag_name > /tmp/VERSION\ncurl -s https://api.github.com/repos/geigi/cozy/releases/latest | json body > /tmp/CHANGES\necho https://github.com/geigi/cozy/archive/$(cat /tmp/VERSION).tar.gz > /tmp/RELEASE_URL\ncd X11:Pantheon:Apps/cozy\nrm cozy-*.tar.gz\nwget -O cozy-$(cat /tmp/VERSION).tar.gz $(cat /tmp/RELEASE_URL)\n', 'cd X11:Pantheon:Apps/cozy\nsed -i -e \'s/- /  * /g\' /tmp/CHANGES\nsed -i ""1s/^/Update to $(cat /tmp/VERSION)\\n/"" /tmp/CHANGES\nosc vc -m ""$(cat /tmp/CHANGES)""\nsed -i -e ""s/Version:.*/Version:        $(cat /tmp/VERSION)/g"" cozy.spec\n', 'cd X11:Pantheon:Apps/cozy\nosc addremove\nosc ci --noservice -m ""Update to version $(cat /tmp/VERSION).""\n', 'chmod +x $GITHUB_WORKSPACE/.ci/*.sh\ncd X11:Pantheon:Apps/cozy\n$GITHUB_WORKSPACE/.ci/obs_wait_for_build.sh\n', 'cd X11:Pantheon:Apps/cozy\nosc submitrequest -m ""Update to version $(cat /tmp/VERSION).""']"
['curl https://raw.githubusercontent.com/ekalinin/github-markdown-toc/master/gh-md-toc -o gh-md-toc\nchmod a+x gh-md-toc\n./gh-md-toc --insert --no-backup README.md\nrm ./gh-md-toc\n']
"['python -m pip install --upgrade pip\npip install -e .[dev]\n', 'tox', 'python -m pip install --upgrade pip\npip install -e .[docs]\n', 'sphinx-build -W -b html src build', 'python -m pip install --upgrade pip\npip install -e .[checks]\n', 'tox -e lint']"
"['sudo apt-get update && sudo apt-get install libgl1-mesa-glx xvfb -y\nsudo apt-get install -y xvfb\nXvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &\nsleep 3\n', 'conda list gempy\nconda list gdal\nconda list pyvista\nconda list vtk\nconda list pandas\nconda list numpy\nconda install gdal\npython -m pip install --upgrade pip\npip install -r dev-requirements.txt\n', 'pytest\n', 'sudo apt-get update && sudo apt-get install libgl1-mesa-glx xvfb -y\nsudo apt-get install -y xvfb\nXvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &\nsleep 3\n', 'python -m pip install --upgrade pip\npip install -r dev-requirements.txt\n', 'pytest']"
"['python -m pip install --upgrade pip setuptools==65.6.3 wheel', 'make ci-info', 'make bootstrap-dev-plugins', 'make bundle', 'if ! which gh; then\n  echo ""Downloading \'gh\' utility""\n  if [ ""$(uname -s)"" = ""Linux"" ]; then\n    curl -L -o gh.tar.gz https://github.com/cli/cli/releases/download/v2.28.0/gh_2.28.0_linux_amd64.tar.gz\n    tar xzf gh.tar.gz\n    mv ./gh_2.28.0_linux_amd64/bin/gh /usr/local/bin/gh\n  else\n    curl -L -o gh.zip https://github.com/cli/cli/releases/download/v2.28.0/gh_2.28.0_macOS_amd64.zip\n    unzip xzf gh.zip\n    mv ./gh_2.28.0_macOS_amd64/bin/gh /usr/local/bin/gh\n  fi\n  which gh\nfi\n', 'scriv github-release --repo=overhangio/tutor', 'export FILENAME=""tutor-$(uname -s)_$(uname -m)""\nmv ./dist/tutor $FILENAME\ngh release upload --clobber v$(make version) $FILENAME\n', 'git remote add overhangio https://${{ secrets.GIT_USERNAME }}:${{ secrets.GIT_PASSWORD }}@git.overhang.io/core/tutor.git', 'git push overhangio $GITHUB_REF', 'python -m pip install --upgrade pip setuptools==44.0.0', 'pip install -r requirements/dev.txt', 'make test-lint', 'make test-unit', 'make test-types', 'make test-format', 'make test-pythonpackage', 'pip install -r requirements/docs.txt', 'make docs']"
"['cd programming_fundamentals/python_part_1\npip install -r requirements.txt\nVALUE=$(echo $((1 + RANDOM % 10)))\npython example1.py $VALUE\n', 'cd programming_fundamentals/python_part_2\npip install -r requirements.txt\npython example1.py\n', 'cd programming_fundamentals/python_part_3\npip install -r requirements.txt\npython api_ncclient_example.py\n', 'cd programming_fundamentals/python_part_3\npip install -r requirements.txt\npython api_netmiko_example.py\n', 'cd programming_fundamentals/python_part_3\npip install -r requirements.txt\npython api_pysnmp_example.py\n', 'cd programming_fundamentals/python_part_3\npip install -r requirements.txt\npython api_requests_example.py\n', 'cd programming_fundamentals/python_part_3 \npip install -r requirements.txt\npython data_library_exercises.py\n']"
"['pip install -U tox==3.27.1 tox-factor', 'tox -f ${{ matrix.python-version }}']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'python -m test.specification --ignore-known\n', 'python -m pip install --upgrade pip\npython -m pip install pytest\n# note: the following also installs ""coverage""\npython -m pip install coveralls\nif [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi\n', 'coverage run --source=${package_name} --append -m pytest\ncoverage run --source=${package_name} --append -m test.specification --ignore-known\n# quick local report output to console:\ncoverage report\n', 'coveralls --service=github\n']"
"['scripts/install', 'scripts/build', 'scripts/publish', 'scripts/install', 'scripts/check', 'scripts/build', 'scripts/test', 'scripts/coverage']"
"['./scripts/install_ubuntu_ci_core_dependencies.sh\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pipenv install --dev --deploy --system\npip install -e .\n', 'pipenv run ./scripts/lint.sh ${{ matrix.lint-flags }}\n', 'pipenv run typing_copilot tighten --error-if-can-tighten\n', './scripts/install_ubuntu_ci_core_dependencies.sh\n', 'docker-compose up -d\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pipenv install --dev --deploy --system\npip install -e .\n', ""pytest --cov=graphql_compiler graphql_compiler/tests -m '${{ matrix.markers }}'\ncodecov\n""]"
"['cosign sign ${{ steps.meta.outputs.tags }}@${{ steps.build-and-push.outputs.digest }}', 'pipx install poetry', 'pushd ..\ngit clone https://github.com/antirez/redis.git\npushd redis\ngit checkout 7.0\nmake\npopd\npopd\n', 'sudo apt install libfuzzy-dev\n# playwright required deps.\nsudo apt install libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 libcups2 libxkbcommon0 libxdamage1 libgbm1 libpango-1.0-0 libcairo2 libatspi2.0-0\n', 'echo LOOKYLOO_HOME=""\'`pwd`\'"" > .env\ncp config/generic.json.sample config/generic.json\ncp config/modules.json.sample config/modules.json\npoetry install -vvv\npoetry run playwright install\npoetry run tools/validate_config_files.py --check\npoetry run tools/validate_config_files.py --update\npoetry run tools/3rdparty.py\npoetry run start\n', 'git clone https://github.com/Lookyloo/PyLookyloo.git\npushd PyLookyloo\npoetry install\npoetry run python tests/testing_github.py\npopd\n', 'poetry run stop\n', 'pipx install poetry', 'sudo apt install libfuzzy-dev\npoetry install\necho LOOKYLOO_HOME=""`pwd`"" >> .env\npoetry run tools/3rdparty.py\n', 'poetry run tools/generate_sri.py\ngit diff website/web/sri.txt\ngit diff --quiet website/web/sri.txt\n', 'poetry run mypy .\n']"
""
"['pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html', 'python -m pip install --upgrade pip\ngit submodule update --init --recursive\npython -m pip install .\n', 'python $GITHUB_WORKSPACE/scripts/check_installation.py', ""python -m pip install '.[dev,docs]'\npython -m pip install iopath transformers pyarrow\npython -m pip install git+https://github.com/facebookresearch/fairscale.git@main\npython -m pip install pygit2 pgzip\n"", 'brew install llvm libomp\nCC=/usr/local/opt/llvm/bin/clang CXX=clang++ pip install git+https://github.com/facebookresearch/xformers.git@main\n', 'python -m pip install --progress-bar off git+https://github.com/facebookresearch/xformers.git@main\n', 'black --check --diff .', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'make singlehtml', 'pytest --import-mode=append -vvv tests/', 'output=$(python3 release_utils.py --release-type ${{ github.event.inputs.name }}) \necho $output\nnew_version=$(echo $output | awk \'{print $1}\')\nnew_tag=$(echo $output | awk \'{print $2}\')\necho ""new version is $new_version""\necho ""new tag is $new_tag""\necho ::set-output name=version::$new_version\necho ::set-output name=tag::$new_tag\necho ::set-output name=branch_name::$new_version-release\necho ""NEW_TAG=$new_tag"" >> $GITHUB_ENV\necho ""NEW_BRANCH=$new_version-release"" >> $GITHUB_ENV\n', 'echo ""current folder = $PWD""\necho ""current branch = $(git branch --show-current)""\noutput=$(python3 release_utils.py --release-type ${{ github.event.inputs.name }} --update-version)\n', 'python3 -m pip install --upgrade pip\n', 'python3 -m pip install setuptools wheel twine torch\npython3 setup.py sdist\n', 'python3 -m pip install --upgrade pip\n', 'python3 -m pip install cibuildwheel\n', 'python3 -m cibuildwheel --output-dir dist\n', 'pip install setuptools wheel twine\npython3 -m twine upload --repository pypi dist/*\n']"
"['sudo apt-get install -y zip', 'make venv', 'VERSION=${{ github.ref }}\nVERSION=${VERSION:11}  # refs/tags/v1.2.3 => 1.2.3\necho ""inter_version=$VERSION"" >> $GITHUB_ENV\n', 'GITSHA=${{ github.sha }}\nVERSION=$(cat version.txt)\necho ""inter_version=${VERSION}-${GITSHA:0:10}"" >> $GITHUB_ENV\n', 'ZIP=Inter-${{ env.inter_version }}.zip\necho ""inter_zip=$ZIP"" >> $GITHUB_ENV\nmake -j zip\nmv build/release/Inter*.zip ""$ZIP""\n']"
"['python -m pip install --upgrade pip\npython -m pip install tox\n', 'tox -e docs\n', 'python -m pip install --upgrade pip\npython -m pip install tox\n', 'tox -e lint\n', 'python -m pip install --upgrade pip\npython -m pip install tox\n', 'tox -e tests\n']"
"['conclusion=$(gh api --paginate ""${{ github.event.workflow_run.jobs_url }}"" -q \'.jobs[] | select(.name | startswith(""Build and Test ("")) | .conclusion\' | sort | uniq | paste -sd "","" -)\necho ""build-and-test conclusion: ${conclusion}""\necho ""build-and-test=${conclusion}"" >> $GITHUB_OUTPUT\n', 'artifacts_url=${{ github.event.workflow_run.artifacts_url }}\ngh api --paginate ""$artifacts_url"" -q \'.artifacts[] | select(.name == ""PR Meta"") .archive_download_url\' | while read url\ndo\n  gh api ""$url"" > ""pr.zip""\n  unzip -o ""pr.zip""\n  echo ""json=$(cat pr.json)"" >> $GITHUB_OUTPUT\n  cat pr.json\n  echo\ndone\n\nif [[ ! -e ""pr.json"" ]]\nthen\n  echo ""::error title=Artifact \'PR Meta\' missing::Expected artifact \'PR Meta\' does not exist for pull_request event.""\n  exit 1\nfi\n', 'curl --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.event.workflow_run.head_commit.id }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data ""{\n  \\""state\\"": \\""pending\\"",\n  \\""context\\"": \\""${GITHUB_WORKFLOW} / Build and Test GPU (on Builtkite)\\"",\n  \\""target_url\\"": \\""https://github.com/horovod/horovod/actions/runs/${GITHUB_RUN_ID}\\""\n}""\n', 'echo ""::warning::Buildkite pipeline did not pass: ${{ needs.buildkite-trigger.outputs.url }}""\nexit 1\n', 'curl --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.event.workflow_run.head_commit.id }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data ""{\n  \\""state\\"": \\""${{ job.status }}\\"",\n  \\""context\\"": \\""${GITHUB_WORKFLOW} / Build and Test GPU (on Builtkite)\\"",\n  \\""target_url\\"": \\""https://github.com/horovod/horovod/actions/runs/${GITHUB_RUN_ID}\\""\n}""\n', 'curl --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.event.workflow_run.head_commit.id }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data ""{\n  \\""state\\"": \\""pending\\"",\n  \\""context\\"": \\""${GITHUB_WORKFLOW} / Build and Test GPU heads (on Builtkite)\\"",\n  \\""target_url\\"": \\""https://github.com/horovod/horovod/actions/runs/${GITHUB_RUN_ID}\\""\n}""\n', 'echo ""::warning::Buildkite pipeline did not pass: ${{ needs.buildkite-heads-trigger.outputs.url }}""\nexit 1\n', 'curl --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.event.workflow_run.head_commit.id }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data ""{\n  \\""state\\"": \\""${{ job.status }}\\"",\n  \\""context\\"": \\""${GITHUB_WORKFLOW} / Build and Test GPU heads (on Builtkite)\\"",\n  \\""target_url\\"": \\""https://github.com/horovod/horovod/actions/runs/${GITHUB_RUN_ID}\\""\n}""\n', 'mkdir -p artifacts && cd artifacts\n\nartifacts_url=${{ github.event.workflow_run.artifacts_url }}\n\ngh api ""$artifacts_url"" -q \'.artifacts[] | [.name, .archive_download_url] | @tsv\' | while read artifact\ndo\n  IFS=$\'\\t\' read name url <<< ""$artifact""\n  gh api $url > ""$name.zip""\n  unzip -d ""$name"" ""$name.zip""\ndone\n', 'declare -A last_runs\nls -d artifacts/Unit\\ Test\\ Results\\ */* | sort > runs.txt\nwhile read run\ndo\n  test=${run/%[_-]run[_-][0123456789]/}\n  last_runs[$test]=$run\ndone < runs.txt\n\necho ""LAST_RUNS<<EOF"" >> $GITHUB_ENV\nfor test in ""${!last_runs[@]}""\ndo\n  echo ""${last_runs[$test]}"" >&2\n  echo ""${last_runs[$test]}/**/*.xml"" >> $GITHUB_ENV\ndone\necho ""EOF"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip\npython -m pip install setuptools wheel\npython setup.py sdist\npip -v install dist/horovod-*.tar.gz\n', 'pip install -r .github/requirements.txt', 'python .github/gen-workflow-ci.py\nif [[ $(git diff .github/workflows/ci.yaml | wc -l) -gt 0 ]]\nthen\n  echo ""::error::Workflow file .github/workflows/ci.yaml is out-dated, please run .github/gen-workflow-ci.py and commit changes""\n  exit 1\nfi\n', 'if [[ ""${{ github.event_name }}"" == ""pull_request"" ]]\nthen\n  changes=""$(python .github/get-changed-code-files.py)""\n  if [[ -z ""$changes"" ]]\n  then\n    echo ""No code changes, no need to build and test""\n    echo ""needed=false"" >> $GITHUB_OUTPUT\n  else\n    echo ""Code changes, we need to build and test:""\n    echo ""$changes""\n    echo ""needed=true"" >> $GITHUB_OUTPUT\n  fi\nelse\n  echo ""This is not part of a pull request, we need to build and test""\n  echo ""needed=true"" >> $GITHUB_OUTPUT\nfi\n', 'branch=""${{ github.event.pull_request.head.ref || github.ref }}""\nbranch=""${branch#""refs/heads/""}""\nbranch=""${branch#""refs/tags/""}""\n\nbranch_label=""${branch}""\nif [[ ""${{ github.event_name }}"" == ""schedule"" ]]\nthen\n  # we add this label to the branch used by Buildkite to avoid it cancelling one of concurrent schedule and push builds on master\n  branch_label=""${branch} (schedule)""\nfi\necho ""branch-label=${branch_label}"" >> $GITHUB_OUTPUT\n\nif [[ ""${{ github.event_name }}"" == ""pull_request"" ]]\nthen\n  head_sha=""${{ github.event.pull_request.head.sha }}""\n  message=""$(gh api https://api.github.com/repos/horovod/horovod/commits/${head_sha} -q .commit.message | head -n1)""\n  echo ""message=${message}"" >> $GITHUB_OUTPUT\nfi\n', 'rm -f pr.json\necho -n ""{"" >> pr.json\necho -n "" \\""merge_sha\\"": \\""${{ github.sha }}\\"","" >> pr.json\necho -n "" \\""base_sha\\"": \\""${{ github.event.pull_request.base.sha }}\\"","" >> pr.json\necho -n "" \\""head_sha\\"": \\""${{ github.event.pull_request.head.sha }}\\"" "" >> pr.json\necho -n ""}"" >> pr.json\ncat pr.json\n', 'echo ::group::Disk space before clean up\ndf -h\necho ::endgroup::\n\nfor dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\n           /usr/share/dotnet/shared \\\n           /usr/local/lib/android/sdk/ndk \\\n           /usr/local/lib/android/sdk/build-tools \\\n           /opt/ghc\ndo\n  echo ::group::Deleting ""$dir""\n  sudo du -hsc $dir | tail -n1 || true\n  sudo rm -rf $dir\n  echo ::endgroup::\ndone\n\necho ::group::Disk space after clean up\ndf -h\necho ::endgroup::\n', 'pip install docker-compose', '.github/timeout-and-retry.sh ${{ matrix.build_timeout }}m 3 10 docker-compose -f docker-compose.test.yml build ${{ matrix.image }}\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.torch.xml test_elastic_spark_torch.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.torch.xml test_elastic_spark_torch.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.torch.xml test_elastic_spark_torch.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_1_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_1_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_1_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_1_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_1_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_1_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_2_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_2_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow.py test_elastic_tensorflow_keras.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_2_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_2_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow.py test_elastic_tensorflow_keras.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_2_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_2_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow.py test_elastic_tensorflow_keras.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras/keras_mnist_advanced.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras/keras_mnist_advanced.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras/keras_mnist_advanced.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo /bin/bash /pytest.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo /bin/bash /pytest.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo /bin/bash /pytest.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""horovodrun -np 2 python -m horovod.tensorflow.data.compute_worker /tmp/compute.json & horovodrun -np 2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""horovodrun -np 2 python -m horovod.tensorflow.data.compute_worker /tmp/compute.json & horovodrun -np 2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""horovodrun -np 2 python -m horovod.tensorflow.data.compute_worker /tmp/compute.json & horovodrun -np 2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py 2 2 2 localhost:2,127.0.0.1:2\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py 2 2 2 localhost:2,127.0.0.1:2\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py 2 2 2 localhost:2,127.0.0.1:2\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 --min-np 2 --max-np 2 -H localhost:2,127.0.0.1:2 --gloo python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 --min-np 2 --max-np 2 -H localhost:2,127.0.0.1:2 --gloo python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 --min-np 2 --max-np 2 -H localhost:2,127.0.0.1:2 --gloo python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow/tensorflow_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow/tensorflow_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow/tensorflow_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/test && pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.integration.xml integration/test_interactiverun.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/test && pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.integration.xml integration/test_interactiverun.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/test && pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.integration.xml integration/test_interactiverun.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/keras/keras_mnist_advanced.py --epochs 3 --batch-size 64""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/keras/keras_mnist_advanced.py --epochs 3 --batch-size 64""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/keras/keras_mnist_advanced.py --epochs 3 --batch-size 64""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_lightning_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_lightning_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_lightning_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && (ls -1 test_spark*.py | xargs -n 1 /bin/bash /pytest_standalone.sh spark)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && (ls -1 test_spark*.py | xargs -n 1 /bin/bash /pytest_standalone.sh spark)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && (ls -1 test_spark*.py | xargs -n 1 /bin/bash /pytest_standalone.sh spark)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/examples/spark/tensorflow2; spark-submit --master \\""local[2]\\"" \\""/horovod/horovod/spark/tensorflow/compute_worker.py\\"" /tmp/compute.json & OMP_NUM_THREADS=1 /spark_env.sh spark-submit --master \\""local[2]\\"" --py-files tensorflow2_mnist_data_service_train_fn_compute_side_dispatcher.py,tensorflow2_mnist_data_service_train_fn_training_side_dispatcher.py tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/examples/spark/tensorflow2; spark-submit --master \\""local[2]\\"" \\""/horovod/horovod/spark/tensorflow/compute_worker.py\\"" /tmp/compute.json & OMP_NUM_THREADS=1 /spark_env.sh spark-submit --master \\""local[2]\\"" --py-files tensorflow2_mnist_data_service_train_fn_compute_side_dispatcher.py,tensorflow2_mnist_data_service_train_fn_training_side_dispatcher.py tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/examples/spark/tensorflow2; spark-submit --master \\""local[2]\\"" \\""/horovod/horovod/spark/tensorflow/compute_worker.py\\"" /tmp/compute.json & OMP_NUM_THREADS=1 /spark_env.sh spark-submit --master \\""local[2]\\"" --py-files tensorflow2_mnist_data_service_train_fn_compute_side_dispatcher.py,tensorflow2_mnist_data_service_train_fn_training_side_dispatcher.py tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'echo ::group::Disk space before clean up\ndf -h\necho ::endgroup::\n\nfor dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\n           /usr/share/dotnet/shared \\\n           /usr/local/lib/android/sdk/ndk \\\n           /usr/local/lib/android/sdk/build-tools \\\n           /opt/ghc\ndo\n  echo ::group::Deleting ""$dir""\n  sudo du -hsc $dir | tail -n1 || true\n  sudo rm -rf $dir\n  echo ::endgroup::\ndone\n\necho ::group::Disk space after clean up\ndf -h\necho ::endgroup::\n', 'pip install docker-compose', '.github/timeout-and-retry.sh ${{ matrix.build_timeout }}m 3 10 docker-compose -f docker-compose.test.yml build ${{ matrix.image }}\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_1_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_TensorFlow_Tests_2_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.tf.xml test_elastic_spark_tensorflow.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.torch.xml test_elastic_spark_torch.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.torch.xml test_elastic_spark_torch.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Spark_Torch_Tests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && /spark_env.sh HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.spark.torch.xml test_elastic_spark_torch.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_1_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_1_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_1_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_1_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_1_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_1_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow2.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_2_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_2_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow.py test_elastic_tensorflow_keras.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_2_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_2_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow.py test_elastic_tensorflow_keras.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Elastic_Tests_2_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Elastic_Tests_2_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format \'[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s\' --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.elastic.xml test_elastic_torch.py test_elastic_tensorflow.py test_elastic_tensorflow_keras.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Cluster_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.gloo.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras/keras_mnist_advanced.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras/keras_mnist_advanced.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Keras_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras/keras_mnist_advanced.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet2_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_MXNet_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet/mxnet_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo /bin/bash /pytest.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo /bin/bash /pytest.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Parallel_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo /bin/bash /pytest.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_PyTorch_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_Single_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c "" cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh gloo)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""horovodrun -np 2 python -m horovod.tensorflow.data.compute_worker /tmp/compute.json & horovodrun -np 2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""horovodrun -np 2 python -m horovod.tensorflow.data.compute_worker /tmp/compute.json & horovodrun -np 2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Data_Service_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""horovodrun -np 2 python -m horovod.tensorflow.data.compute_worker /tmp/compute.json & horovodrun -np 2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py 2 2 2 localhost:2,127.0.0.1:2\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py 2 2 2 localhost:2,127.0.0.1:2\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py 2 2 2 localhost:2,127.0.0.1:2\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 --min-np 2 --max-np 2 -H localhost:2,127.0.0.1:2 --gloo python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 --min-np 2 --max-np 2 -H localhost:2,127.0.0.1:2 --gloo python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_Elastic_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 --min-np 2 --max-np 2 -H localhost:2,127.0.0.1:2 --gloo python /horovod/examples/elastic/tensorflow2/tensorflow2_mnist_elastic.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 gloo\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_2_0_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2/tensorflow2_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow/tensorflow_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow/tensorflow_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Gloo_TensorFlow_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow/tensorflow_mnist.py\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Cluster_PyTests_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.static.xml test_static_run.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet2_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_MXNet_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && OMP_NUM_THREADS=1 \\$(cat /mpirun_command) python /horovod/examples/mxnet/mxnet_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Parallel_PyTests_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/parallel && (ls -1 test_*.py | xargs -n 1 \\$(cat /mpirun_command) /bin/bash /pytest.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_api_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_PyTorch_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_Single_PyTests_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 15m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command &&  cd /horovod/test/single && (ls -1 test_*.py | xargs -n 1 /bin/bash /pytest_standalone.sh mpi)""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_api_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_Keras_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_api_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/tensorflow2/tensorflow2_mnist.py 2 localhost:2 mpi""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/MPI_TensorFlow_2_0_MNIST_horovodrun_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && \\$(cat /mpirun_command) python /horovod/examples/tensorflow2/tensorflow2_mnist.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/test && pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.integration.xml integration/test_interactiverun.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/test && pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.integration.xml integration/test_interactiverun.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Run_PyTests_test_interactiverun_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/test && pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.mpi.integration.xml integration/test_interactiverun.py""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/keras/keras_mnist_advanced.py --epochs 3 --batch-size 64""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/keras/keras_mnist_advanced.py --epochs 3 --batch-size 64""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_Keras_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/keras/keras_mnist_advanced.py --epochs 3 --batch-size 64""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet2_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet2_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_MXNet_MNIST_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/mxnet/mxnet_mnist.py --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c "" python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_MPI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_mpi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Single_PyTorch_MNIST_ONECCL_OFI_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""\\$(cat /oneccl_env) && echo \'/mpirun_command_ofi\' > /mpirun_command && python /horovod/examples/pytorch/pytorch_mnist.py --epochs 3 --data-dir /data/pytorch_datasets""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Estimator_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Keras_Rossmann_Run_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/keras/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.1""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_lightning_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_lightning_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Lightning_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_lightning_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_PyTests_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_PyTests_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && (ls -1 test_spark*.py | xargs -n 1 /bin/bash /pytest_standalone.sh spark)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_PyTests_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_PyTests_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && (ls -1 test_spark*.py | xargs -n 1 /bin/bash /pytest_standalone.sh spark)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_PyTests_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_PyTests_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 30m bash -c ""cd /horovod/test/integration && (ls -1 test_spark*.py | xargs -n 1 /bin/bash /pytest_standalone.sh spark)""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/examples/spark/tensorflow2; spark-submit --master \\""local[2]\\"" \\""/horovod/horovod/spark/tensorflow/compute_worker.py\\"" /tmp/compute.json & OMP_NUM_THREADS=1 /spark_env.sh spark-submit --master \\""local[2]\\"" --py-files tensorflow2_mnist_data_service_train_fn_compute_side_dispatcher.py,tensorflow2_mnist_data_service_train_fn_training_side_dispatcher.py tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/examples/spark/tensorflow2; spark-submit --master \\""local[2]\\"" \\""/horovod/horovod/spark/tensorflow/compute_worker.py\\"" /tmp/compute.json & OMP_NUM_THREADS=1 /spark_env.sh spark-submit --master \\""local[2]\\"" --py-files tensorflow2_mnist_data_service_train_fn_compute_side_dispatcher.py,tensorflow2_mnist_data_service_train_fn_training_side_dispatcher.py tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_TensorFlow_2_0_MNIST_Data_Service_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""cd /horovod/examples/spark/tensorflow2; spark-submit --master \\""local[2]\\"" \\""/horovod/horovod/spark/tensorflow/compute_worker.py\\"" /tmp/compute.json & OMP_NUM_THREADS=1 /spark_env.sh spark-submit --master \\""local[2]\\"" --py-files tensorflow2_mnist_data_service_train_fn_compute_side_dispatcher.py,tensorflow2_mnist_data_service_train_fn_training_side_dispatcher.py tensorflow2_mnist_data_service.py /tmp/compute.json""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_1\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_1:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_2\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_2:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'mkdir -p artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_3\ndocker-compose -f docker-compose.test.yml run -e GITHUB_ACTIONS --rm --volume ""$(pwd)/artifacts/${{ matrix.image }}/Spark_Torch_MNIST_run_3:/artifacts"" ${{ matrix.image }} /usr/bin/timeout 10m bash -c ""OMP_NUM_THREADS=1 /spark_env.sh python /horovod/examples/spark/pytorch/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""\n', 'echo ::group::Disk space before clean up\ndf -h\necho ::endgroup::\n\nfor dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\n           /usr/share/dotnet/shared \\\n           /usr/local/lib/android/sdk/ndk \\\n           /usr/local/lib/android/sdk/build-tools \\\n           /opt/ghc\ndo\n  echo ::group::Deleting ""$dir""\n  sudo du -hsc $dir | tail -n1 || true\n  sudo rm -rf $dir\n  echo ::endgroup::\ndone\n\necho ::group::Disk space after clean up\ndf -h\necho ::endgroup::\n', 'pip install docker-compose', '.github/timeout-and-retry.sh ${{ matrix.build_timeout }}m 3 10 docker-compose -f docker-compose.test.yml build ${{ matrix.image }}\n', 'brew reinstall -f zlib bzip2\nbrew install -f openmpi cmake libuv pyenv coreutils curl\nexport PATH=$(pyenv root)/shims:$PATH\npyenv uninstall -f 3.7.7\nCFLAGS=""-I$(brew --prefix bzip2)/include -I$(brew --prefix zlib)/include"" LDFLAGS=""-L$(brew --prefix zlib)/lib -L$(brew --prefix bzip2)/lib"" pyenv install --patch 3.7.7 < <(curl -sSL https://github.com/python/cpython/commit/8ea6353.patch)\npyenv global 3.7.7\npython --version\n\npython -m pip install -U pip\npip install tensorflow==${TENSORFLOW} keras==${KERAS}\nif [[ ${TENSORFLOW} == 1.* ]] || [[ ${TENSORFLOW} == 2.[012345].* ]]; then pip install ""h5py<3"" ""protobuf~=3.20""; fi\npip install torch==${PYTORCH} pytorch_lightning==${PYTORCH_LIGHTNING} torchvision==${TORCHVISION}\npip install mxnet==${MXNET}\nHOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITH_MXNET=1 pip install --no-cache-dir .[test]\nhorovodrun --check-build\n', 'export PATH=$(pyenv root)/shims:$PATH\npyenv global 3.7.7\npython --version\n\nartifacts_path=""$(pwd)/artifacts/${{ matrix.image }}-macos-run-1""\nmkdir -p ""$artifacts_path""\necho ""artifacts-path=$artifacts_path"" >> $GITHUB_OUTPUT\necho pytest -v --capture=no --continue-on-collection-errors --junit-xml=$artifacts_path/junit.\\$1.\\${HOROVOD_RANK:-\\${OMPI_COMM_WORLD_RANK:-\\${PMI_RANK}}}.\\$2.xml \\${@:2} > pytest.sh\nchmod u+x pytest.sh\n\ncd test/parallel\nls test_*.py | gtimeout 10m xargs -n 1 horovodrun -np 2 /bin/bash ../../pytest.sh macos\n', 'export PATH=$(pyenv root)/shims:$PATH\npyenv global 3.7.7\npython --version\n\nartifacts_path=""$(pwd)/artifacts/${{ matrix.image }}-macos-run-2""\nmkdir -p ""$artifacts_path""\necho ""artifacts-path=$artifacts_path"" >> $GITHUB_OUTPUT\necho pytest -v --capture=no --continue-on-collection-errors --junit-xml=$artifacts_path/junit.\\$1.\\${HOROVOD_RANK:-\\${OMPI_COMM_WORLD_RANK:-\\${PMI_RANK}}}.\\$2.xml \\${@:2} > pytest.sh\nchmod u+x pytest.sh\n\ncd test/parallel\nls test_*.py | gtimeout 10m xargs -n 1 horovodrun -np 2 /bin/bash ../../pytest.sh macos\n', 'export PATH=$(pyenv root)/shims:$PATH\npyenv global 3.7.7\npython --version\n\nartifacts_path=""$(pwd)/artifacts/${{ matrix.image }}-macos-run-3""\nmkdir -p ""$artifacts_path""\necho ""artifacts-path=$artifacts_path"" >> $GITHUB_OUTPUT\necho pytest -v --capture=no --continue-on-collection-errors --junit-xml=$artifacts_path/junit.\\$1.\\${HOROVOD_RANK:-\\${OMPI_COMM_WORLD_RANK:-\\${PMI_RANK}}}.\\$2.xml \\${@:2} > pytest.sh\nchmod u+x pytest.sh\n\ncd test/parallel\nls test_*.py | gtimeout 10m xargs -n 1 horovodrun -np 2 /bin/bash ../../pytest.sh macos\n', 'echo ""::warning::Buildkite pipeline did not pass: ${{ needs.buildkite-trigger.outputs.url }}""\nexit 1\n', 'echo ""::warning::Buildkite pipeline did not pass: ${{ needs.buildkite-heads-trigger.outputs.url }}""\nexit 1\n', 'echo Repository: ${{ github.repository }}\necho Event: ${{ github.event_name }}\necho Run: $run\necho ""run=$run"" >> $GITHUB_OUTPUT\necho Push: $push\necho ""push=$push"" >> $GITHUB_OUTPUT\n', 'echo ::group::Disk space before clean up\ndf -h\necho ::endgroup::\n\nfor dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\n           /usr/share/dotnet/shared \\\n           /usr/local/lib/android/sdk/ndk \\\n           /usr/local/lib/android/sdk/build-tools \\\n           /opt/ghc\ndo\n  echo ::group::Deleting ""$dir""\n  sudo du -hsc $dir | tail -n1 || true\n  sudo rm -rf $dir\n  echo ::endgroup::\ndone\n\necho ::group::Disk space after clean up\ndf -h\necho ::endgroup::\n', 'docker image ls horovod-test\n', 'grep ""RUN sed"" Dockerfile.test.cpu | sed ""s/^RUN //"" | docker run -i --name horovod-test horovod-test:latest /bin/bash\n', 'docker start -ai horovod-test <<<""python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication gloo""\n', 'docker start -ai horovod-test <<<""python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 gloo""\n', 'docker start -ai horovod-test <<<""python /horovod/examples/pytorch/pytorch_mnist.py --data-dir /data/pytorch_datasets --num-proc 2 --hosts localhost:2 --communication mpi""\n', 'docker start -ai horovod-test <<<""python /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py 2 localhost:2 mpi""\n', 'echo ::group::Disk Space\ndf -h\necho ::endgroup::\n\necho ::group::Docker Space\ndocker system df\necho ::endgroup::\n\necho ::group::Docker Images\ndocker images -a\necho ::endgroup::\n\necho ::group::Docker Container\ndocker container list -a\necho ::endgroup::\n', '$INIT\n\npatch --quiet -p0 $LEFT ${RIGHT}.patch -o ${LEFT}.expected\nif ! diff -q ${LEFT}.expected --label $LEFT $RIGHT\nthen\n  echo\n  echo ""::error::Files are out-of-sync: $LEFT vs. $RIGHT""\n  echo ""Unexpected differences are:""\n  diff ${LEFT}.expected --label $LEFT $RIGHT || true\n\n  echo\n  echo ""Use the following as ${RIGHT}.patch to accept those changes:""\n  diff $LEFT $RIGHT || true\n\n  false\nfi\n', 'python setup.py egg_info\npip install $(head -n $(( $(grep -m 1 -n -e ""^\\["" horovod.egg-info/requires.txt | cut -d : -f 1) - 1 )) horovod.egg-info/requires.txt)\n', 'echo ::group::Disk space before clean up\ndf -h\necho ::endgroup::\n\nfor dir in /usr/share/dotnet/sdk/\\*/nuGetPackagesArchive.lzma \\\n           /usr/share/dotnet/shared \\\n           /usr/local/lib/android/sdk/ndk \\\n           /usr/local/lib/android/sdk/build-tools \\\n           /opt/ghc\ndo\n  echo ::group::Deleting ""$dir""\n  sudo du -hsc $dir | tail -n1 || true\n  sudo rm -rf $dir\n  echo ::endgroup::\ndone\n\necho ::group::Disk space after clean up\ndf -h\necho ::endgroup::\n', '# copy CodeQL distribution into docker context\ncp -rl ""${{ env.CODEQL_DIST }}"" .codeql\n\ncommand=$(cat <<EOF\n# Install CodeQL\nCOPY .codeql ${{ env.CODEQL_DIST }}\nRUN ${{ env.CODEQL_DIST }}/codeql version --format=terse\nRUN ${{ env.CODEQL_DIST }}/codeql version --format=json\nRUN ${{ env.CODEQL_DIST }}/codeql resolve queries cpp-code-scanning.qls --format=bylanguage\nRUN ${{ env.CODEQL_DIST }}/codeql resolve languages\nRUN ${{ env.CODEQL_DIST }}/codeql resolve qlpacks\nEOF\n)\nsed -i -e ""s%^# setup ssh service$%${command//$\'\\n\'/\\\\n}\\n\\n# setup ssh service%"" Dockerfile.test.?pu\n\ncommand=$(cat <<EOF\n# Setup CodeQL tracing\nRUN mkdir -p /home/runner/work/horovod\nRUN ln -s /horovod /home/runner/work/horovod/horovod\nRUN mkdir -p /home/runner/work/_temp\nRUN ${{ env.CODEQL_DIST }}/codeql database init --db-cluster /home/runner/work/_temp/codeql_databases --source-root=/home/runner/work/horovod/horovod --language=cpp --begin-tracing --trace-process-name=Runner.Worker.exe\n\n# Build Horovod (C++)\nRUN mkdir -p build/temp\nRUN cd build/temp; source /home/runner/work/_temp/codeql_databases/temp/tracingEnvironment/start-tracing.sh; cmake /home/runner/work/horovod/horovod -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELWITHDEBINFO=/home/runner/work/horovod/horovod/build/temp/lib.linux-x86_64-3.8 -DPYTHON_EXECUTABLE:FILEPATH=/usr/bin/python\nRUN cd build/temp; source /home/runner/work/_temp/codeql_databases/temp/tracingEnvironment/start-tracing.sh; cmake --build . --config RelWithDebInfo -- VERBOSE=1\nEOF\n)\nsed -i -e ""s%^# Install Horovod.$%${command//$\'\\n\'/\\\\n}\\n\\n# Install Horovod.%"" Dockerfile.test.?pu\n\n# Truncate Dockerfile.test.?pu, after CMake we are done here\nfor file in Dockerfile.test.?pu\ndo\n  head -n $(( $(grep -m 1 -n ""# Install Horovod."" $file | cut -d : -f 1) - 1 )) $file > tmp\n  mv tmp $file\ndone\n\n# Print out changes\ngit diff\n', 'pip install docker-compose\nimage=$(grep mixed docker-compose.test.yml | sed -e ""s/[ :]//g"")\ndocker-compose -f docker-compose.test.yml build ${image}\ndocker run --name horovod horovod_${image} ls -lah /home/runner/work/_temp/codeql_databases\nrm -rf /home/runner/work/_temp/codeql_databases\ndocker cp horovod:/home/runner/work/_temp/codeql_databases /home/runner/work/_temp/\n', 'python -m pip install --upgrade pip\npython -m pip install setuptools wheel twine\n', 'python setup.py sdist\npython -m twine upload dist/*\n']"
[]
"['pip install -r requirements.txt', 'pip install flake8']"
[]
['script/test']
""
"['python -m pip install --upgrade pip pipenv\npipenv install --deploy --dev --system\n', 'pytest --verbose --cov=./ --cov-report xml\n', 'python -m pip install codacy-coverage\npython-codacy-coverage\n', 'python -m pip install black', 'black .\ngit config --global user.name \'Auto Black Formatter\'\ngit config --global user.email \'claudiugeorgiu@users.noreply.github.com\'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit checkout ${GITHUB_REF#refs/heads/}\ngit diff --quiet || git commit -am ""Format code with Black [auto]"" && git push\n', 'python -m pip install --upgrade pip pipenv\npipenv install --deploy --dev --system\n', 'pytest --verbose --cov=./ --cov-report xml\n', 'bash <(curl -Ls https://coverage.codacy.com/get.sh) report -l Python -r ./coverage.xml\n', 'python -m pip install --upgrade pip pipenv\npipenv install --deploy --dev --system\n', 'pytest --verbose --cov=./ --cov-report xml\n', 'python -m pip install codacy-coverage\npython-codacy-coverage\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest pytest-cov yapf\nmake && make install\n', 'flake8 .\n', 'yapf --diff --recursive --verbose blocksatcli/\n', 'python -m pytest --cov=blocksatcli']"
"['pip install . --no-deps\n', 'pytest -v\n', 'python -m pip install --upgrade pip\npip install setuptools setuptools-scm wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python3 -m pip install --upgrade pip\npip3 install -e .[dev]\n', 'cd doc\nsphinx-build source build\n', 'python3 setup.py sdist', 'pipx install --verbose dist/*.tar.gz', 'python3 -m pip install --upgrade pip\n# Vanilla install and smoke test, just to make sure we do\n# not accidentally add an import for a [dev] dependency:\npip3 install -e .\ncrosshair -h\n# Now add the dev dependencies, so we can use them for testing.\npip3 install -e .[dev]\n', 'pre-commit run --all-files --hook-stage manual\n']"
""
['ls -l ${{ github.workspace }}/newsfragments/${{ github.event.number }}.*.rst\n']
"['pip install -r requirements/linting.txt -r requirements/pyproject.txt', 'mypy devtools', 'pip install -r requirements/docs.txt -r requirements/pyproject.txt', 'pip install .', 'make docs', 'pip install -r requirements/testing.txt -r requirements/pyproject.txt', 'pip freeze', 'make test', 'coverage xml', 'pip uninstall -y multidict numpy pydantic asyncpg sqlalchemy', 'make test', 'coverage xml', 'pip install build twine', 'python -m build', 'twine check dist/*', 'twine upload dist/*']"
"['pip install -r requirements.txt\npip install -r requirements-testing.txt\n', 'nosetests --with-coverage --cover-package=threatingestor --cover-xml']"
"['python -m pip install --upgrade pip\npip install tox-gh-actions\n', 'tox', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip\n', 'source workflow_scripts/protobuf/build_protobuf_unix.sh $(nproc)\npython -m pip install --quiet -r requirements-release.txt\ngit submodule update --init --recursive\nexport ONNX_ML=1\nsudo python setup.py install\n', 'source workflow_scripts/protobuf/build_protobuf_unix.sh $(nproc)\n\npython -m pip install --quiet --upgrade pip setuptools wheel\npython -m pip install --quiet -r requirements-release.txt\n\ngit submodule update --init --recursive\nexport ONNX_BUILD_TESTS=0\nexport ONNX_ML=1\nexport CMAKE_ARGS=""-DONNXIFI_DUMMY_BACKEND=ON -DONNX_WERROR=ON""\nexport ONNX_NAMESPACE=ONNX_NAMESPACE_FOO_BAR_FOR_CI\n\npython setup.py install\n', 'python -m pip install -r requirements-dev.txt\nlintrunner init\n', 'set +e\nif ! lintrunner --force-color --all-files --tee-json=lint.json -v; then\n    echo """"\n    echo -e ""\\e[1m\\e[36mYou can reproduce these results locally by using \\`lintrunner\\`.\\e[0m""\n    echo -e ""\\e[1m\\e[36mSee https://github.com/onnx/onnx/blob/main/docs/CONTRIBUTING.md#code-style for setup instructions.\\e[0m""\n    exit 1\nfi\n', 'python -m lintrunner_adapters to-sarif lint.json lintrunner.sarif\n', 'echo -e ""\\n::group:: ===> check auto-gen files are up-to-date...""\npython onnx/defs/gen_doc.py\npython onnx/gen_proto.py -l\npython onnx/gen_proto.py -l --ml\npython onnx/backend/test/stat_coverage.py\n\ngit status\ngit diff --exit-code -- . \':(exclude)onnx/onnx-data.proto\' \':(exclude)onnx/onnx-data.proto3\'\nif [ $? -ne 0 ]; then\n  echo ""git diff returned failures""\n  exit 1\nfi\necho -e ""::endgroup::""\n', 'python -m pip install --quiet --upgrade pip setuptools wheel\npython -m pip install -r docs/docsgen/source/requirements.txt\npython -m pip install protobuf==3.20.2\n', 'python -m pip uninstall -y onnx', 'sudo apt-get install libprotobuf-dev protobuf-compiler\ngit submodule update --init --recursive\nexport CMAKE_ARGS=""-DONNX_USE_PROTOBUF_SHARED_LIBS=ON""\nexport ONNX_ML=1\npython setup.py install\n', 'cd docs/docsgen\nmake html\n', 'auth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n  ${{ env.img }} \\\n  bash -exc '${{ env.py }} -m pip install -q virtualenv && ${{ env.py }} -m venv .env && \\\n  source .env/bin/activate && \\\n  ${{ env.py }} -m pip install -q -r requirements-release.txt && \\\n  yum install -y protobuf-compiler protobuf-devel\n  deactivate'\n"", ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n  ${{ env.img }} \\\n  bash -exc '\\\n  source .env/bin/activate && \\\n  yum install -y sudo && \\\n  sudo chmod +x .github/workflows/manylinux/entrypoint.sh && \\\n  sudo .github/workflows/manylinux/entrypoint.sh ${{ env.py }} manylinux2014_aarch64 ${{ github.event_name }}\n  deactivate'\n"", ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n  ${{ env.img }} \\\n  bash -exc '\\\n  source .env/bin/activate && \\\n  python -m pip install -q --upgrade pip && \\\n  python -m pip install -q -r requirements-release.txt && \\\n  pip install dist/*manylinux2014_aarch64.whl && \\\n  pytest && \\\n  deactivate'\n"", 'python -m pip install -q twine\ntwine upload --verbose dist/*.whl --repository-url https://upload.pypi.org/legacy/ -u ${{ secrets.ONNXWEEKLY_USERNAME }} -p ${{ secrets.ONNXWEEKLY_TOKEN }}\n', ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n ${{ env.img }} \\\n bash -exc '\\\n source .env/bin/activate && \\\n python -m pip uninstall -y numpy onnx protobuf && python -m pip install numpy protobuf && \\\n python -m pip install dist/*manylinux2014_aarch64.whl && \\\n pytest && \\\n deactivate'\n"", '# only difference is numpy version\nif [[ ""${{ matrix.python-version }}"" == ""cp38-cp38"" || ""${{ matrix.python-version }}"" == ""cp39-cp39"" ]]; then\n  docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n    ${{ env.img }} \\\n    bash -exc \'\\\n    source .env/bin/activate && \\\n    python -m pip uninstall -y onnx numpy && python -m pip install numpy==1.16.6 && \\\n    python -m pip install dist/*manylinux2014_aarch64.whl && \\\n    pytest && \\\n    deactivate\'\nelse\n  docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n  ${{ env.img }} \\\n  bash -exc \'\\\n  source .env/bin/activate && \\\n  python -m pip uninstall -y onnx numpy && python -m pip install numpy==1.23.2 && \\\n  python -m pip install dist/*manylinux2014_aarch64.whl && \\\n  pytest && \\\n  deactivate\'\nfi\n', ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n ${{ env.img }} \\\n bash -exc '\\\n source .env/bin/activate && \\\n python -m pip uninstall -y onnx && python -m pip install protobuf==3.20.2 && \\\n python -m pip install dist/*manylinux2014_aarch64.whl && \\\n pytest && \\\n deactivate'\n"", ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n ${{ env.img }} \\\n bash -exc '\\\n source .env/bin/activate && \\\n python -m pip uninstall -y protobuf numpy && python -m pip install -q -r requirements-release.txt && \\\n python -m pip install -q onnxruntime && \\\n export ORT_MAX_IR_SUPPORTED_VERSION=8 \\\n export ORT_MAX_ML_OPSET_SUPPORTED_VERSION=3 \\\n export ORT_MAX_ONNX_OPSET_SUPPORTED_VERSION=18 \\\n pytest && \\\n deactivate'\n"", 'auth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', 'python -m pip install -q --upgrade pip\npython -m pip install -q -r requirements-release.txt\n', 'source workflow_scripts/protobuf/build_protobuf_unix.sh $(nproc)\n', 'python -m pip install dist/*manylinux2014_x86_64.whl\npytest\n', 'twine upload --verbose dist/*.whl --repository-url https://upload.pypi.org/legacy/ -u ${{ secrets.ONNXWEEKLY_USERNAME }} -p ${{ secrets.ONNXWEEKLY_TOKEN }}\nTEST_HUB=1 pytest\n', 'python -m pip uninstall -y numpy onnx && python -m pip install numpy\npython -m pip install dist/*manylinux2014_x86_64.whl\npytest\n', 'if [[ ""${{ matrix.python-version }}"" == ""3.8"" || ""${{ matrix.python-version }}"" == ""3.9"" ]]; then\n  export minimum_numpy_version=1.16.6\nelse\n  export minimum_numpy_version=1.23.2\nfi\npython -m pip uninstall -y numpy onnx && python -m pip install numpy==$minimum_numpy_version\npython -m pip install dist/*manylinux2014_x86_64.whl\npytest\n', 'python -m pip uninstall -y protobuf onnx && python -m pip install protobuf\npython -m pip install dist/*manylinux2014_x86_64.whl\npytest\n', 'python -m pip uninstall -y protobuf onnx && python -m pip install protobuf==3.20.2\npython -m pip install dist/*manylinux2014_x86_64.whl\npytest\n', 'python -m pip uninstall -y protobuf numpy && python -m pip install -q -r requirements-release.txt\npython -m pip install -q onnxruntime\nexport ORT_MAX_IR_SUPPORTED_VERSION=8\nexport ORT_MAX_ML_OPSET_SUPPORTED_VERSION=3\nexport ORT_MAX_ONNX_OPSET_SUPPORTED_VERSION=18\npytest\n', 'auth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', 'python -m pip install -q --upgrade pip\npython -m pip install -q -r requirements-release.txt\n', '# Install Protobuf from source\nexport NUM_CORES=`sysctl -n hw.logicalcpu`\nif [ \'${{ matrix.target-architecture }}\' == \'x86_64\' ]; then\n  export CMAKE_OSX_ARCHITECTURES=x86_64\nelif [ \'${{ matrix.target-architecture }}\' == \'universal2\' ]; then\n  export CMAKE_OSX_ARCHITECTURES=\'arm64;x86_64\'\nfi\nsource workflow_scripts/protobuf/build_protobuf_unix.sh $NUM_CORES $(pwd)/protobuf/protobuf_install\nexport CMAKE_ARGS=""-DONNX_USE_LITE_PROTO=ON""\n# Currently GitHub Action agent is using macos-11, use -p to force change the final MACOSX_DEPLOYMENT_TARGET\n# Change -p if MACOSX_DEPLOYMENT_TARGET is different\nif [ \'${{ github.event_name }}\' == \'schedule\' ]; then\n  python setup.py bdist_wheel -p macosx_10_12_${{ matrix.target-architecture }} --weekly_build\nelse\n  python setup.py bdist_wheel -p macosx_10_12_${{ matrix.target-architecture }}\nfi\nfor file in dist/*.whl; do python -m pip install --upgrade $file; done\n', 'pytest\n', 'twine upload --verbose dist/*.whl --repository-url https://upload.pypi.org/legacy/ -u ${{ secrets.ONNXWEEKLY_USERNAME }} -p ${{ secrets.ONNXWEEKLY_TOKEN }}\n', 'python -m pip uninstall -y numpy onnx && python -m pip install numpy\nfor file in dist/*.whl; do python -m pip install --upgrade $file; done\npytest\n', 'if [[ ""${{ matrix.python-version }}"" == ""3.8"" || ""${{ matrix.python-version }}"" == ""3.9"" ]]; then\n  export minimum_numpy_version=1.16.6\nelse\n  export minimum_numpy_version=1.23.2\nfi\npython -m pip uninstall -y numpy onnx && python -m pip install numpy==$minimum_numpy_version\nfor file in dist/*.whl; do python -m pip install --upgrade $file; done\npytest\n', 'python -m pip uninstall -y protobuf onnx && python -m pip install protobuf\nfor file in dist/*.whl; do python -m pip install --upgrade $file; done\npytest\n', 'python -m pip uninstall -y protobuf onnx && python -m pip install protobuf==3.20.2\nfor file in dist/*.whl; do python -m pip install --upgrade $file; done\npytest\n', '# Build and upload source distribution to PyPI\ngit clean -xdf\npython setup.py sdist --weekly_build\ntwine upload dist/* --repository-url https://upload.pypi.org/legacy/ -u ${{ secrets.ONNXWEEKLY_USERNAME }} -p ${{ secrets.ONNXWEEKLY_TOKEN }}\n\n# Test weekly source distribution from PyPI\npython -m pip uninstall -y onnx\npython -m pip install setuptools\npython -m pip install --index-url https://upload.pypi.org/legacy/ --use-deprecated=legacy-resolver --no-use-pep517 --no-binary onnx-weekly onnx-weekly\npytest\n', 'python -m pip uninstall -y protobuf numpy && python -m pip install -q -r requirements-release.txt\npython -m pip install -q onnxruntime\nexport ORT_MAX_IR_SUPPORTED_VERSION=8\nexport ORT_MAX_ML_OPSET_SUPPORTED_VERSION=3\nexport ORT_MAX_ONNX_OPSET_SUPPORTED_VERSION=18\npytest\n', 'cd onnx\nauth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', 'python -m pip install -q --upgrade pip\ncd onnx\npython -m pip install -q -r requirements-release.txt\n', '$arch = \'x64\'\nif (\'${{ matrix.architecture }}\' -eq \'x86\') {\n  $arch = \'Win32\'\n}\n\n. .\\onnx\\workflow_scripts\\protobuf\\build_protobuf_win.ps1 -arch $arch\ncd onnx\n\necho ""Install ONNX""\n$Env:ONNX_ML=1\n$Env:CMAKE_ARGS=""-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF -DONNX_USE_LITE_PROTO=ON""\n\nif (\'${{ github.event_name }}\' -eq \'schedule\') {\n  echo ""Build weekly TestPyPI package""\n  python setup.py bdist_wheel --weekly_build\n} else {\n  python setup.py bdist_wheel\n}\nGet-ChildItem -Path dist/*.whl | foreach {python -m pip install --upgrade $_.fullname}\n', 'cd onnx\npytest\n', 'twine upload --verbose onnx/dist/*.whl --repository-url https://upload.pypi.org/legacy/ -u ${{ secrets.ONNXWEEKLY_USERNAME }} -p ${{ secrets.ONNXWEEKLY_TOKEN }}\n', 'cd onnx\npython -m pip uninstall -y numpy onnx && python -m pip install numpy\nGet-ChildItem -Path dist/*.whl | foreach {python -m pip install --upgrade $_.fullname}\npytest\n', 'cd onnx\nif (\'${{ matrix.python-version }}\' -eq \'3.8\' -Or \'${{ matrix.python-version }}\' -eq \'3.9\') {\n  $minimum_numpy_version=""1.16.6""\n} else {\n  $minimum_numpy_version=""1.23.2""\n}\npython -m pip uninstall -y numpy onnx && python -m pip install numpy==$minimum_numpy_version\nGet-ChildItem -Path dist/*.whl | foreach {python -m pip install --upgrade $_.fullname}\npytest\n', 'cd onnx\npython -m pip uninstall -y protobuf onnx && python -m pip install protobuf\nGet-ChildItem -Path dist/*.whl | foreach {python -m pip install --upgrade $_.fullname}\npytest\n', 'cd onnx\npython -m pip uninstall -y protobuf onnx && python -m pip install protobuf==3.20.2\nGet-ChildItem -Path dist/*.whl | foreach {python -m pip install --upgrade $_.fullname}\npytest\n', 'cd onnx\npython -m pip uninstall -y protobuf numpy && python -m pip install -q -r requirements-release.txt\npython -m pip install -q onnxruntime\n$Env:ORT_MAX_IR_SUPPORTED_VERSION=8\n$Env:ORT_MAX_ML_OPSET_SUPPORTED_VERSION=3\n$Env:ORT_MAX_ONNX_OPSET_SUPPORTED_VERSION=18\npytest\n', 'auth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', 'set -e\npython -m pip install -q --upgrade pip\npython -m pip install -q -r requirements-release.txt\n', '# Install protobuf from source\nexport NUM_CORES=`sysctl -n hw.logicalcpu`\nsource workflow_scripts/protobuf/build_protobuf_unix.sh $NUM_CORES $(pwd)/protobuf/protobuf_install\n# Build ONNX\nexport CC=clang\nexport CXX=clang++\nexport ONNX_ML=1\npip install -e . -v\n', 'python workflow_scripts/test_model_zoo.py\n', 'cd onnx\nauth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\n', '. .\\onnx\\workflow_scripts\\protobuf\\build_protobuf_win.ps1 -arch ${{ matrix.architecture }}\n\ncd onnx\necho ""Build ONNX""\ncmake -G ""Visual Studio 17 2022"" -A ${{ matrix.architecture }} -DONNX_USE_PROTOBUF_SHARED_LIBS=OFF -DONNX_USE_LITE_PROTO=ON -DONNX_WERROR=ON -DONNX_DISABLE_EXCEPTIONS=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_BUILD_TYPE=Release -DONNX_USE_MSVC_STATIC_RUNTIME=OFF -DONNX_ML=1 -DONNX_BUILD_TESTS=ON -S . -B .setuptools-cmake-build\\\ncd .setuptools-cmake-build\\\nmsbuild onnx.sln /m /p:Configuration=Release\n\necho ""Run gtests""\nRelease\\onnx_gtests.exe\nif($lastexitcode -ne 0) {\n  EXIT 1\n}\n\ncd ..\ngit clean -xdf\necho ""Build ONNX with non-static registration for testing selective ONNX schema loading""\ncmake -G ""Visual Studio 17 2022"" -A ${{ matrix.architecture }} -DONNX_USE_PROTOBUF_SHARED_LIBS=OFF -DONNX_USE_LITE_PROTO=ON -DONNX_WERROR=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_BUILD_TYPE=Release -DONNX_USE_MSVC_STATIC_RUNTIME=OFF -DONNX_ML=1 -DONNX_BUILD_TESTS=ON -DONNX_DISABLE_STATIC_REGISTRATION=ON -S . -B .setuptools-cmake-build\\\n\ncd .setuptools-cmake-build\\\nmsbuild onnx.sln /m /p:Configuration=Release\n\necho ""Only test selective ONNX schema loading""\nRelease\\onnx_gtests.exe --gtest_filter=""SchemaRegistrationTest*""\nif($lastexitcode -ne 0) {\n  EXIT 1\n}\n']"
""
"['gh auth status\ngh pr list --repo ""${{ github.repository }}"" --assignee ""machineFL"" --base main --state open --search ""status:success review:required"" --limit 1 --json number > dep_PRs_waiting_approval.json\ndep_pull_request=$(cat dep_PRs_waiting_approval.json | grep -Eo ""[0-9]*"")\necho ::set-output name=dep_pull_request::${dep_pull_request}\n', 'gh pr review --repo ""${{ github.repository }}"" --comment --body ""auto approve"" ${{ steps.find_prs.outputs.dep_pull_request }}\ngh pr review --repo ""${{ github.repository }}"" --approve ${{ steps.find_prs.outputs.dep_pull_request }}\ngh pr merge --repo ""${{ github.repository }}"" --auto --squash --delete-branch ${{ steps.find_prs.outputs.dep_pull_request }}\n', 'echo ""${{steps.link-report.outputs.result}}"" >> $GITHUB_STEP_SUMMARY', 'make package\n', 'python -m pip install ""unpacked_sdist/[dev]""\n', 'python -m pip install ""unpacked_sdist/[dev]"" --no-deps\n', 'sudo apt update\nsudo apt install -y pandoc\nsudo apt install -y graphviz\nsudo apt install -y openjdk-11-jre-headless\npython -m pip check\n', 'make -C docs/ -e ""SPHINXOPTS=-W -j auto"" clean html', 'gh auth status\ngh repo sync alteryx/featuretools-feedstock --branch main --source conda-forge/featuretools-feedstock --force\n', 'cat featuretools-feedstock/recipe/meta.yaml', 'cd featuretools-feedstock\ngit config --unset-all http.https://github.com/.extraheader\ngit config --global user.email ""machineOSS@alteryx.com""\ngit config --global user.name ""machineAYX Bot""\ngit remote set-url origin https://${{ secrets.AUTO_APPROVE_TOKEN }}@github.com/alteryx/featuretools-feedstock\ngit checkout -b ${{ github.event.inputs.version }}\ngit add recipe/meta.yaml\ngit commit -m ""${{ github.event.inputs.version }}""\ngit push origin ${{ github.event.inputs.version }}\n', 'echo ""Conda Feedstock Pull Request: https://github.com/alteryx/featuretools-feedstock/pull/new/${{ github.event.inputs.version }}"" >> $GITHUB_STEP_SUMMARY\n', 'make package\n', 'python -m pip install ""unpacked_sdist/[complete]""\n', 'python -m pip install ""unpacked_sdist/[complete]"" --no-deps\n', 'python -c ""import alteryx_open_src_update_checker""\npython -c ""import premium_primitives""\npython -c ""from premium_primitives import PolarityScore""\npython -c ""from featuretools.primitives import PolarityScore""\npython -c ""from featuretools.primitives import CountryCodeToContinent""\npython -c ""from featuretools_sql import DBConnector""\n', 'python -m pip check\n', 'python -m pip install ""unpacked_sdist/[nlp,spark,updater,sql,premium]""\n', 'gh workflow run unit_tests_with_featuretools_main_branch.yaml --repo ""alteryx/evalml""', 'python -m pip install --upgrade pip\npython -m pip install -e "".[spark,test]""\nmake checkdeps OUTPUT_PATH=featuretools/tests/requirement_files/latest_requirements.txt\n', 'python -m pip install -e .[dev]\n', 'python -m pip install -e .[dev] --no-deps\n', 'make lint', 'if [ -z ""${{ inputs.current_hash_input }}"" ]\nthen\n  current_hash=$(git rev-parse --short HEAD)\nelse\n  current_hash=${{ inputs.current_hash_input }}\nfi\necho ""Latest commit hash: $current_hash""\necho ""::set-output name=current_hash::$current_hash""\n', 'if [ -z ""${{ inputs.previous_hash_input }}"" ]\nthen\n  previous_hash=$(git rev-parse --short HEAD~1)\nelse\n  previous_hash=${{ inputs.previous_hash_input }}\nfi\necho ""Previous commit hash: $previous_hash""\necho ""::set-output name=previous_hash::$previous_hash""\n', 'make installdeps\nmake installdeps-test\n', 'response=$( looking-glass run-featuretools-entitysets --username featuretools-github-lg-user --scenarios-yaml entityset_scenarios_${{ matrix.test_type }}.yaml --featuretools-branch ${{ steps.get_current_hash.outputs.current_hash }} --job-desc ${{ steps.get_current_hash.outputs.current_hash }} )\necho ""::set-output name=job_id::$(echo $response | sed \'s/.*Job ID: \\([^ ]*\\).*/\\1/\')""\n', 'response=$( looking-glass run-featuretools-entitysets --username featuretools-github-lg-user --scenarios-yaml entityset_scenarios_${{ matrix.test_type }}.yaml --featuretools-branch ${{ steps.get_previous_hash.outputs.previous_hash }} --job-desc ${{ steps.get_previous_hash.outputs.previous_hash }} )\necho ""::set-output name=job_id::$(echo $response | sed \'s/.*Job ID: \\([^ ]*\\).*/\\1/\')""\n', 'for id in ${{ steps.current.outputs.job_id }} ${{ steps.previous.outputs.job_id }}; do\n  echo ""Waiting for job id: $id""\n  result=\'\'\n  sleep_time=0\n  while [ -z ""$result""  ]\n  do\n    sleep $sleep_time\n    result=$(looking-glass get-job --job-id $id | grep -n \'COMPLETED\' || :;)\n    sleep_time=60\n  done\n  looking-glass get-results --job-id $id --file-path $id\ndone\n', 'filename_base=${{ steps.get_current_hash.outputs.current_hash }}_v_${{ steps.get_previous_hash.outputs.previous_hash }}_${{ matrix.test_type }}\nlooking-glass run-local-report --previous-path ${{ steps.previous.outputs.job_id }} -n ${{ steps.current.outputs.job_id }} --output-name ${filename_base} --output-type html\n', 'filename_base=${{ steps.get_current_hash.outputs.current_hash }}_v_${{ steps.get_previous_hash.outputs.previous_hash }}_${{ matrix.test_type }}\nurl=s3://featuretools-performance-results/${filename_base}.html\naws s3 cp ./${filename_base}.html $url\necho ""::set-output name=report_url::$url""\necho ""Featuretools Report: $url""\n', 'filename_base=${{ steps.get_current_hash.outputs.current_hash }}_v_${{ steps.get_previous_hash.outputs.previous_hash }}_${{ matrix.test_type }}\npresigned_url=$( aws s3 presign ${{ steps.s3_upload.outputs.report_url }} --expires-in 604800 )\nresponse=$(\ncurl -X POST https://slack.com/api/chat.postMessage -H \'Content-type: application/json;charset=UTF-8\' -H \'Authorization: Bearer ${{ secrets.LG_SLACK_TOKEN }}\' \\\n--data-binary @- << EOF\n{\n  ""channel"": ""CSCENDGLQ"",\n  ""blocks"": [\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""${{ matrix.test_type }} performance tests for commit ${{ steps.get_current_hash.outputs.current_hash }} on Featuretools are complete""\n      }\n    },\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""*Author*: ${{ github.event.head_commit.author.name }}""\n      }\n    },\n    {\n      ""type"": ""section"",\n      ""fields"": [\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Current Commit*: <https://github.com/alteryx/featuretools/commit/${{ steps.get_current_hash.outputs.current_hash }}|${{ steps.get_current_hash.outputs.current_hash }}>""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Previous Commit*: <https://github.com/alteryx/featuretools/commit/${{ steps.get_previous_hash.outputs.previous_hash }}|${{ steps.get_previous_hash.outputs.previous_hash }}>""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Current Job*: ${{ steps.current.outputs.job_id }}""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Previous Job*: ${{ steps.previous.outputs.job_id }}""\n        },\n      ]\n    },\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""*Report*: <$presigned_url|${{ steps.s3_upload.outputs.report_url }}>""\n      }\n    },\n  ]\n}\nEOF\n)\necho $response\nts=$(echo $response | jq .ts)\ncurl -F ""file=@${filename_base}.html"" -F channels=CSCENDGLQ -F \'thread_ts=$ts\' -H \'Authorization: Bearer ${{ secrets.LG_SLACK_TOKEN }}\'  https://slack.com/api/files.upload\n', 'echo ""{\\""TestCommit\\"": \\""$GITHUB_SHA\\"", \\""Flags\\"": \\""--upload-slack\\""}"" | base64 > payload.b64\naws lambda invoke --function-name $lambda_function --payload file://payload.b64 --invocation-type Event /dev/stdout 1>/dev/null\n', 'rm -rf docs/', 'gh workflow run create_feedstock_pr.yaml --repo ""alteryx/featuretools"" -f version=${{ github.event.release.tag_name }}\n', 'from re import compile\nmain = \'^main$\'\nrelease = \'^release_v\\d+\\.\\d+\\.\\d+$\'\nbackport = \'^backport_v\\d+\\.\\d+\\.\\d+$\'\ndep_update = \'^latest-dep-update-[a-f0-9]{7}$\'\nmin_dep_update = \'^min-dep-update-[a-f0-9]{7}$\'\nregex = main, release, backport, dep_update, min_dep_update\npatterns = list(map(compile, regex))\nref = ""${{ github.event.pull_request.head.ref }}""\nis_dev = not any(pattern.match(ref) for pattern in patterns)\nprint(\'::set-output name=is_dev::\' + str(is_dev))\n', 'cat docs/source/release_notes.rst | grep "":pr:\\`${{ github.event.number }}\\`""', 'make package\npython -m pip install unpacked_sdist/\n', 'import featuretools as ft\nes = ft.demo.load_mock_customer(return_entityset=True)\nft.dfs(\n    entityset=es,\n    target_dataframe_name=""customers"",\n    agg_primitives=[""count""],\n    trans_primitives=[""month""],\n    max_depth=1,\n)\nfrom featuretools.primitives import IsFreeEmailDomain\nis_free_email_domain = IsFreeEmailDomain()\nis_free_email_domain([\'name@gmail.com\', \'name@featuretools.com\']).tolist()\n', 'make package', 'pip config --site set global.progress_bar off\npython -m pip install --upgrade pip\nsudo apt update && sudo apt install -y graphviz\n', 'python -m pip install -e unpacked_sdist/\npython -m pip install -e unpacked_sdist/[test]\n', 'sudo apt install -y openjdk-11-jre-headless\nJAVA_HOME=""/usr/lib/jvm/java-11-openjdk-amd64""\npython -m pip install -e unpacked_sdist/[dask]\npython -m pip install -e unpacked_sdist/[spark]\npython -m pip install -e unpacked_sdist/[test]\n', 'echo ""coverage_args=--cov=featuretools --cov-config=../pyproject.toml --cov-report=xml:../coverage.xml"" >> $GITHUB_ENV', 'cd unpacked_sdist\ncoverage erase\n', 'cd unpacked_sdist\npytest featuretools/ -n auto\n', 'cd unpacked_sdist\npytest featuretools/ -n auto --ignore=featuretools/tests/computational_backend --ignore=featuretools/tests/entityset_tests --ignore=featuretools/tests/primitive_tests ${{ env.coverage_args }}\n', 'cd unpacked_sdist\npytest featuretools/tests/computational_backend/ -n auto ${{ env.coverage_args }}\n', 'cd unpacked_sdist\npytest featuretools/tests/entityset_tests -n auto --ignore=featuretools/tests/entityset_tests/test_es.py --ignore=featuretools/tests/entityset_tests/test_ww_es.py ${{ env.coverage_args }}\n', 'cd unpacked_sdist\npytest featuretools/tests/entityset_tests/test_es.py featuretools/tests/entityset_tests/test_ww_es.py ${{ env.coverage_args }}\n', 'cd unpacked_sdist\npytest featuretools/tests/primitive_tests -n auto ${{ env.coverage_args }}\n', '$File = ""Miniconda3-latest-Windows-x86_64.exe""\n$Uri = ""https://repo.anaconda.com/miniconda/$File""\n$ProgressPreference = ""silentlyContinue""\nInvoke-WebRequest -Uri $Uri -Outfile ""$env:USERPROFILE/$File""\n$hashFromFile = Get-FileHash ""$env:USERPROFILE/$File"" -Algorithm SHA256\n$hashFromUrl = ""307194e1f12bbeb52b083634e89cc67db4f7980bd542254b43d3309eaf7cb358""\nif ($hashFromFile.Hash -ne ""$hashFromUrl"") {\n  Throw ""File hashes did not match!""\n}\n', 'start /wait """" %UserProfile%\\Miniconda3-latest-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\\Miniconda3', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda create -n featuretools python=${{ matrix.python_version }}\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate featuretools\nconda config --add channels conda-forge\nconda install -q -y -c conda-forge python-graphviz graphviz\npython -m pip install --upgrade pip\npython -m pip install .[test]\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate featuretools\npytest featuretools\\ -n auto\n', 'sudo apt update\nsudo apt install -y graphviz\npip config --site set global.progress_bar off\npython -m pip install --upgrade pip\npython -m pip install wheel\n', 'python -m pip install -e . --no-dependencies\n', 'NUMPY_VERSION=$(cat featuretools/tests/requirement_files/minimum_spark_requirements.txt | grep numpy)\npython -m pip uninstall numpy -y\npython -m pip install $NUMPY_VERSION --no-build-isolation\n', 'NUMPY_VERSION=$(cat featuretools/tests/requirement_files/minimum_core_requirements.txt | grep numpy)\npython -m pip uninstall numpy -y\npython -m pip install $NUMPY_VERSION --no-build-isolation\n', 'python -m pip install -r featuretools/tests/requirement_files/minimum_test_requirements.txt\n', 'sudo apt install -y openjdk-11-jre-headless\nJAVA_HOME=""/usr/lib/jvm/java-11-openjdk-amd64""\npython -m pip install -r featuretools/tests/requirement_files/minimum_spark_requirements.txt\n', 'python -m pip install -r featuretools/tests/requirement_files/minimum_core_requirements.txt\n', 'python -m pytest -x -n auto featuretools/tests/', 'pytest featuretools/ -n auto --ignore=featuretools/tests/computational_backend --ignore=featuretools/tests/entityset_tests --ignore=featuretools/tests/primitive_tests', 'pytest featuretools/tests/computational_backend/ -n auto', 'pytest featuretools/tests/entityset_tests -n auto --ignore=featuretools/tests/entityset_tests/test_es.py --ignore=featuretools/tests/entityset_tests/test_ww_es.py', 'pytest featuretools/tests/entityset_tests/test_es.py featuretools/tests/entityset_tests/test_ww_es.py', 'pytest featuretools/tests/primitive_tests -n auto', 'make package', 'pip config --site set global.progress_bar off\npython -m pip install -U pip\nsudo apt update && sudo apt install -y graphviz\n', 'sudo apt install -y openjdk-11-jre-headless\nJAVA_HOME=""/usr/lib/jvm/java-11-openjdk-amd64""\nPYSPARK_SUBMIT_ARGS=""--master local[2] pyspark-shell""\npython -m pip install -e unpacked_sdist/[spark]\n', 'python -m pip install -e unpacked_sdist/[test]\npython -m pip uninstall -y woodwork\npython -m pip install https://github.com/alteryx/woodwork/archive/main.zip\n', 'echo ""Run unit tests without code coverage for ${{ matrix.python_version }} and ${{ matrix.libraries }}""\necho ""Testing with woodwork version:"" `python -c ""import woodwork; print(woodwork.__version__)""`\n', 'pytest featuretools/ -n auto', 'pytest featuretools/ -n auto --ignore=featuretools/tests/computational_backend --ignore=featuretools/tests/entityset_tests --ignore=featuretools/tests/primitive_tests', 'pytest featuretools/tests/computational_backend/ -n auto', 'pytest featuretools/tests/entityset_tests -n auto --ignore=featuretools/tests/entityset_tests/test_es.py --ignore=featuretools/tests/entityset_tests/test_ww_es.py', 'pytest featuretools/tests/entityset_tests/test_es.py featuretools/tests/entityset_tests/test_ww_es.py', 'pytest featuretools/tests/primitive_tests -n auto']"
"['echo ""ðŸŽ‰ The job was automatically triggered by a ${{ github.event_name }} event.""', 'echo ""ðŸ§ This job is now running on a ${{ runner.os }} server hosted by GitHub!""', 'echo ""ðŸ”Ž The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.""', 'echo ""ðŸ’¡ The ${{ github.repository }} repository has been cloned to the runner.""', 'echo ""ðŸ–¥ï¸ The workflow is now ready to test your code on the runner.""', '# set up environment\necho ""Setting up environment...""\nbash\n. $CONDA_PREFIX/etc/profile.d/conda.sh\nconda activate stanza\nexport STANZA_TEST_HOME=/scr/stanza_test\nexport CORENLP_HOME=$STANZA_TEST_HOME/corenlp_dir\nexport CLASSPATH=$CORENLP_HOME/*:\necho CORENLP_HOME=$CORENLP_HOME\necho CLASSPATH=$CLASSPATH\n# install from stanza repo being evaluated\npwd\npip install -e .\npip install -e .[test]\n# set up for tests\necho ""Running stanza test set up...""\nrm -rf $STANZA_TEST_HOME\npython stanza/tests/setup.py\n# run tests\necho ""Running tests...""\nexport CUDA_VISIBLE_DEVICES=2\npytest stanza/tests\n', 'echo ""ðŸ This job\'s status is ${{ job.status }}.""']"
"['echo $CONDA/bin >> $GITHUB_PATH\n', 'conda create -q -n test-environment python=${{ matrix.python-version }}\nsource activate test-environment\nif [[ ""${{ matrix.pytorch-version }}"" == ""nightly"" ]]; then\n  conda install pytorch cpuonly -c pytorch-nightly \nelse\n  conda install pytorch==${{ matrix.pytorch-version }} cpuonly -c pytorch\nfi\nconda install flake8\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython setup.py install\n', 'source activate test-environment\nconda install pytest\npytest test/\n', 'source activate test-environment\nflake8 . --exit-zero']"
"['./uiautomator2/assets/sync.sh\npython -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install pytest\npip install -e "".[image]""\npytest tests/test_utils.py\n', 'python3 -m pip install wheel', 'python3 setup.py sdist bdist_wheel']"
""
"['python -m pip install --upgrade pip setuptools wheel\n', 'make build\n', 'make install-dev\n', 'make lint\n', 'make install-dev\n', 'make tests\n', 'for changed_file in ${{ steps.files.outputs.all }}; do\n  if [[ ${changed_file} =~ ^tests\\/.*test_.*\\.py$ ]];\n  then\n    echo ""::set-output name=MODIFIED::TRUE""; # Tests were modified\n    exit 0\n  fi\ndone\necho ""::set-output name=MODIFIED::FALSE"" # Tests were not modified\nexit 0\n', 'echo ""::set-output name=git_latest_tag::$(git describe --tags --abbrev=0)""', 'python -m pip install --upgrade pip\npip install -U setuptools wheel twine\n', 'python3 setup.py release\ntwine upload dist/*\n', 'echo ""RELEASE_VERSION=${GITHUB_REF:10}"" >> $GITHUB_ENV', 'echo ""Release new docker image with tag ${{ env.RELEASE_VERSION }}""', 'docker login -u ${DOCKER_USER_NAME} -p ${DOCKER_PWD}\ndocker push chaostoolkit/chaostoolkit:$DOCKER_TAG\ndocker push chaostoolkit/chaostoolkit:latest\n']"
"['curl -sSL ""https://install.python-poetry.org"" | python -\n\n# Adding `poetry` to `$PATH`:\necho ""$HOME/.local/bin"" >> $GITHUB_PATH\n', 'poetry install\npoetry run pip install -U pip\n', 'poetry run flake8 .\npoetry run pytest\npoetry run bash tests/test_github.sh\npoetry run poetry check\npoetry run pip check\n# TODO: remove once `py` / `pytest` package are updated\npoetry run safety check --full-report --ignore=51457\n']"
"['pipx run --python /usr/bin/python3 --spec cibuildwheel==2.12.0 cibuildwheel . --output-dir wheelhouse 2>&1\n', 'python -m pip install tomli\npython build-scripts/environ-from-pyproject.py >> $env:GITHUB_ENV\ndir env:\n', 'build-scripts/win-download-qpdf.ps1 $env:QPDF_VERSION ${{ matrix.platform }}\n', 'sudo apt-get update\nsudo apt-get install -y cmake jbig2dec lcov libexempi8 poppler-utils\n', 'python -m pip install tomli\npython build-scripts/environ-from-pyproject.py >> $GITHUB_ENV\n', 'build-scripts/linux-download-qpdf.bash $QPDF_MIN_VERSION', 'echo ""/usr/local/lib"" | sudo tee /etc/ld.so.conf.d/local-lib.conf', 'build-scripts/linux-build-sdist-deps.bash', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel build\npython -m build --sdist\n', 'python -m venv v\nsource v/bin/activate\nSDIST=dist/*.tar.gz\nSDIST_EXPANDED=$(echo -n $SDIST)\npython -m pip install ${SDIST_EXPANDED}[test]  # yes this works\npython -m pytest -nauto\ndeactivate\n', '# pip install with --coverage is broken so we have to do it this horrible hacky way\n# https://github.com/pypa/setuptools/issues/3025\n# Install it with pip, or else dependencies won\'t be resolved (*sigh*)\npython -m pip install -e .[test]\n# Nuke the C++ extension but leave the Python install intact\nrm -rf build/ src/pikepdf/_core*.so\n# Rebuild the extension inplace with coverage\nenv CFLAGS=""--coverage"" CXXFLAGS=""--coverage"" python setup.py build_ext --inplace\npython -m pytest -nauto --cov-report xml --cov=src\n', ""lcov --no-external --capture --directory . --output-file cpp_coverage_all.info\nlcov --remove cpp_coverage_all.info '*/pybind11/*' -o cpp_coverage.info\nrm cpp_coverage_all.info\n"", 'if [[ ""${{ secrets.RTDS_WEBHOOK_URL }}"" != """" && \\\n      ""${{ secrets.RTDS_WEBHOOK_TOKEN }}"" != """" ]]; \\\nthen\n  echo ""Secrets to use trigger ReadTheDocs were configured""\n  echo ""::set-output name=have_secrets::true""\nelse\n  echo ""Secrets to use trigger ReadTheDocs were not configured""\n  echo ""::set-output name=have_secrets::false""\nfi\n', 'echo ""::set-output name=branch::${GITHUB_REF##refs/heads/}""\n', 'curl -X POST -d ""branches=${{ steps.get_branch.outputs.branch }}"" \\\n             -d ""token=${{ secrets.RTDS_WEBHOOK_TOKEN }}"" \\\n             ""${{ secrets.RTDS_WEBHOOK_URL }}""\n', 'python -m pip install tomli\npython build-scripts/environ-from-pyproject.py >> $GITHUB_ENV\n', 'build-scripts/linux-download-qpdf.bash $QPDF_VERSION\necho ""/usr/local/lib"" | sudo tee /etc/ld.so.conf.d/local-lib.conf\nbuild-scripts/linux-build-sdist-deps.bash\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel build\npython -m build --wheel\n']"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install ninja psutil\n# sudo apt-get update\n', 'python runtest.py test/import.py test/ninja\n', 'python runtest.py testing/framework\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -r requirements-dev.txt\n# sudo apt-get update\n', 'python runtest.py --all --time --jobs=2\n', 'python -m pip install --upgrade pip setuptools wheel\n#python -m pip install flake8 pytest\nif [ -f requirements-pkg.txt ]; then pip install -r requirements-pkg.txt; elif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nsudo apt-get update\nsudo apt-get -y install docbook-xml docbook-xsl xsltproc fop docbook-xsl-doc-pdf\n# try to keep the texlive install as small as we can to save some time/space\nsudo apt-get -y --no-install-recommends install texlive biber texmaker ghostscript texlive-latex-base texlive-latex-extra texlive-bibtex-extra texlive-font-utils latexmk\n', 'python bin/docs-update-generated.py\npython bin/docs-validate.py\npython bin/docs-create-example-outputs.py\n', 'python scripts/scons.py\n', 'ls -l build/dist\npython build/scons-local/scons.py --version\n']"
[]
"['python -m pip install --upgrade pip\npip install -U codecov coveralls flake8 mypy pylint pytest tox vulture\n', 'tox\n']"
"['python3 -m pip install --upgrade pip setuptools\npython3 -m pip install poetry\npoetry run python -m pip install -U pip setuptools\npoetry install -E textract\n', 'poetry run python main.py --version\n', 'poetry run pytest\n', 'poetry run mypy organize main.py\n']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel numpy cython\n', 'sudo rm dist/*linux_x86_64.whl\n', 'pip install numpy Cython scipy\npip install -r tests/requirements.txt\n', 'python setup.py install\n', 'pip install .\nrm -rf pymoo\n', 'python -c ""from pymoo.util.function_loader import is_compiled;print(\'Compiled Extensions: \', is_compiled())""\n', 'pytest -v --maxfail 1 --no-header -m ""not long""\n']"
""
"['pip install -r dev_tools/requirements/format.env.txt', 'check/format-incremental', 'pip install -r dev_tools/requirements/mypy.env.txt', 'check/mypy', 'pip install -r dev_tools/requirements/pylint.env.txt', 'check/pylint', 'pip install -r dev_tools/requirements/pytest.env.txt\npip install cirq-core${{matrix.cirq-version}}\n', 'check/pytest', 'pip install -r dev_tools/requirements/pytest.env.txt', 'check/pytest-and-incremental-coverage', 'pip install -r dev_tools/requirements/pytest.env.txt\npip install -U cirq-core --pre\n', 'check/pytest']"
"['pip install flake8', 'python -m pip install --upgrade setuptools\npython -m pip install --upgrade pip\npython -m pip install flake8 pytest safety\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -m pip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'safety check', 'pytest', 'pip install ruff\n', 'ruff check .\n']"
""
"['echo ""::add-mask::${{ secrets.NPM_REGISTRY_URL }}""\necho ""::add-mask::${{ secrets.NPM_REGISTRY_AUTH }}""\necho ""::add-mask::${{ secrets.CODECOV_TOKEN }}""\necho ""::add-mask::${{ secrets.NETWORK_MASK_1 }}""\necho ""::add-mask::${{ secrets.NETWORK_MASK_2 }}""\necho ""::add-mask::${{ secrets.NETWORK_MASK_3 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_1 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_2 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_3 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_4 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_5 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_6 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_7 }}""\necho ""::add-mask::${{ secrets.INTERNAL_NODE_8 }}""\necho ""::add-mask::${{ secrets.SECRET_STRING_1 }}""\necho ""::add-mask::${{ secrets.SECRET_STRING_2 }}""\necho ""::add-mask::${{ secrets.SECRET_STRING_3 }}""\n', 'ulimit -n 9999', 'sudo apt install -y python3-venv\npython3 -m venv venv\n. venv/bin/activate\necho PATH=$PATH >> $GITHUB_ENV\necho venv $VIRTUAL_ENV\n', './src/install/pre_install.sh', './src/install.py -U -R -N -L DEBUG\n', 'python3 -m pip install codecov\npytest --cov=.\n', 'python3 -m pip install --upgrade pip wheel setuptools\npython3 -m pip install -r ./docsrc/doc_dependencies.txt\n', 'make -C docsrc html\ntouch ./docsrc/_build/html/.nojekyll\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install -r ./docsrc/doc_dependencies.txt\n', 'make -C docsrc html', 'pip install black flake8']"
"['python -m pip install --upgrade pip wheel twine build', 'python -m build', 'twine check --strict dist/*.whl', 'pip install hatch\n', 'hatch build', 'python -m pip install --upgrade pip wheel\n', 'pip install ${{ matrix.pip-flags }} "".[dev,pymde,autotune,hub]""\n', 'pytest -v --cov --color=yes\n']"
"['python -m pip install --upgrade pip wheel setuptools\npython setup.py bdist_wheel\n', 'python -m venv test-env\ntest-env/bin/pip install --upgrade pip\ntest-env/bin/pip install dist/*.whl\n', 'test-env/bin/pip install -r guild/tests/requirements.txt\n', 'git config --global user.name unused\ngit config --global user.email unused@localhost\ngit config --global init.defaultBranch main\n', 'test-env/bin/guild check -T -c8 --fast\n', 'test-env/bin/guild check --force-uat\n', 'python -m pip install --upgrade pip wheel setuptools\npython setup.py bdist_wheel\n', ""python -m venv test-env\ntest-env\\Scripts\\python -m pip install --upgrade pip\npwsh -Command 'test-env\\Scripts\\pip install $(ls dist)'\n"", 'test-env\\Scripts\\pip install setuptools==58\ntest-env\\Scripts\\pip install -r guild\\tests\\requirements.txt\n', 'git config --global user.name unused\ngit config --global user.email unused@localhost\ngit config --global init.defaultBranch main\nmkdir \\Tmp -Force\n', 'test-env\\Scripts\\guild check -T\n', 'python -m pip install --upgrade pip wheel setuptools\npython setup.py bdist_wheel\n', 'python -m pip install --upgrade pip\npip install pylint yapf\npip install -r requirements.txt\npip install -r guild/tests/requirements.txt\n', 'python -m pylint setup.py guild --reports y\n', 'python -m yapf --diff -r setup.py tools.py guild examples\n', 'touch continue', 'python -m pip install --upgrade pip wheel setuptools\npython setup.py bdist_wheel\n', 'python -m venv test-env\ntest-env/bin/pip install --upgrade pip\ntest-env/bin/pip install dist/*.whl\n', 'git config --global user.name unused\ngit config --global user.email unused@localhost\ngit config --global init.defaultBranch main\n', 'test-env/bin/pip install -r guild/tests/requirements.txt\ntest-env/bin/guild check -T -c8 --fast\n', 'sleep 7200', 'touch continue', 'python -m pip install --upgrade pip wheel setuptools\npython setup.py bdist_wheel\n', ""python -m venv test-env\ntest-env\\Scripts\\python -m pip install --upgrade pip\npwsh -Command 'test-env\\Scripts\\pip install $(ls dist)'\n"", 'test-env\\Scripts\\pip install setuptools==58\ntest-env\\Scripts\\pip install -r guild\\tests\\requirements.txt\n', 'test-env\\Scripts\\guild check -T\n', 'sleep 7200', 'touch continue', 'python --version', 'python -c ""import platform; print(platform.uname())""', 'python -c ""import os; print(os.cpu_count())""', 'python -c ""import time; t0 = time.time(); [x+1 for x in range(10000000)]; print(time.time() - t0)""']"
"['brew install bash', 'choco install wget unzip', 'python -m pip install --upgrade pip\npip install pytest\npip install .[ja]\npip install .[ko]\n', 'python3 -m pytest', './test.sh']"
"['poetry build', 'pip install twine', 'twine check dist/*', ""flake8 --format='::error file=%(path)s,line=%(row)d,col=%(col)d::[%(code)s]: %(text)s'"", 'if [[ ""${{ matrix.os }}"" == ""windows"" ]]\nthen\n    v=""true""\nelse\n    v=""false""\nfi\nmypy --show-column-numbers ""--always-$v=IS_WINDOWS"" bgmi\n', 'pre-commit run --all-files --show-diff-on-failure && pre-commit gc', 'poetry publish --build', 'echo ""TAG=${GITHUB_REF##*/}"" >> $GITHUB_ENV', 'echo ""$(go env GOPATH)/bin"" >> $GITHUB_PATH\ngo install github.com/git-chglog/git-chglog/cmd/git-chglog@latest\n', 'git-chglog ""$TAG"" | node -p \'require(""fs"").readFileSync(0).toString().trim()\n  .split(""\\n"").slice(4).join(""\\n"").trim()\' | tee chglog.md\n', 'gh release create ""$TAG"" -F chglog.md ./dist/*', 'docker-compose -f tests/downloader/docker-compose.yaml up -d', 'sleep 10', 'set -ex\ncoverage run -a -m bgmi install\ncoverage run -a tests/downloader/setup.py\ncoverage run -a -m pytest tests/downloader -s\n', 'coverage xml', 'set -ex\ncoverage run -a -m bgmi install\nbgmi --help\ncoverage run -a -m pytest tests --cache-requests --ignore=tests/downloader\n', 'coverage xml --omit templates/download_xml.generated.py']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'poetry install --with docs', 'poetry run mkdocs gh-deploy --force']"
"['echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'git fetch --prune --tags --unshallow -f', 'echo ""tag=${GITHUB_REF#refs/*/}"" >> $GITHUB_OUTPUT', 'conda activate base\nconda install conda-libmamba-solver\nconda config --set solver libmamba\nconda create -n test-environment -c pyviz/label/dev -c conda-forge python=3.9 anaconda-project\n', 'conda activate test-environment\nconda list\ncd examples\nanaconda-project prepare --env-spec doc\ncd ..\nconda activate examples/envs/doc\npip install --no-deps --no-build-isolation .\n', 'conda activate examples/envs/doc\nnbsite generate-rst --org holoviz --project-name holoviz --offset 1 --skip envs\n', 'conda activate examples/envs/doc\nnbsite build --what=html --output=builtdocs --org holoviz --project-name holoviz\n', 'conda activate examples/envs/doc\nconda info\nconda list\n', 'git status\ngit diff\n', 'git fetch --prune --tags --unshallow', 'conda activate base\nconda install conda-libmamba-solver\nconda config --set solver libmamba\nconda install -c pyviz ""pyctdev>=0.5""\nconda create -c pyviz --name test-environment python=${{ matrix.python-version }} pyctdev anaconda-project>=0.10.1\n', 'conda activate test-environment\ndoit env_capture\n', 'conda activate test-environment\ndoit test_lint\n', 'conda activate test-environment\ndoit test_examples\n']"
['pip install tox\ntox -e py\n']
"['sudo rm -rf /usr/share/dotnet\n', 'conda install conda-build conda-verify --yes\n', './conda/pyg/build_conda.sh ${{ matrix.python-version }} ${{ matrix.torch-version }} ${{ matrix.cuda-version }}\n', 'conda install anaconda-client --yes\nanaconda upload --force --label main $HOME/conda-bld/*/*.tar.bz2\n', 'sudo rm -rf /usr/share/dotnet\n', 'conda install conda-build conda-verify --yes\n', './conda/pytorch-geometric/build_conda.sh ${{ matrix.python-version }} ${{ matrix.torch-version }} ${{ matrix.cuda-version }}\n', 'conda install anaconda-client --yes\nanaconda upload --force --label main $HOME/conda-bld/*/*.tar.bz2\n', 'pip install git+https://github.com/pyg-team/pyg_sphinx_theme.git\npip install -e .\n', 'cd docs && make clean && make html SPHINXOPTS=""-W""  # Fail on warning.\n', 'pip install .[benchmark]\n', 'python examples/gcn.py --wandb\n', 'python examples/gat.py --wandb\n', 'python examples/mutag_gin.py --wandb\n', 'python examples/explain/gnn_explainer.py\n', 'sudo apt-get install graphviz\n', 'pip install -e .[full,test]\n', 'FULL_TEST=1 pytest --cov --cov-report=xml\n', 'pip install -e .[full,test]\n', 'pytest\n', 'pip install pyright\n', 'pyright\n', 'pip install pylint', 'pylint **/*.py', 'pip install mypy', 'mypy --install-types --non-interactive --ignore-missing-imports -m torch_geometric\n', 'pip install -e .[test]\n', 'pytest\n', 'echo ""VERSION=$(sed -n ""s/^__version__ = \'\\(.*\\)\'/\\1/p"" torch_geometric/__init__.py)"" >> ${GITHUB_ENV}', 'echo ""TODAY=$(date +\'%Y%m%d\')"" >> ${GITHUB_ENV}', 'sed -i ""s/$VERSION/$VERSION.dev$TODAY/"" torch_geometric/__init__.py\nsed -i \'0,/name=""torch_geometric""/s//name=""pyg-nightly""/\' pyproject.toml # Only change first occurence\nsed -i ""s/version=\\""$VERSION\\""/version=\\""$VERSION.dev$TODAY\\""/"" pyproject.toml\n', 'pip install --upgrade build\npython -m build\n', 'pip install -e .[full,test]\n', 'pytest\n', 'pip install -e .[full,test]\n', 'pytest --cov --cov-report=xml\n']"
"['pip install datasette-publish-vercel\n', 'export BRANCH=""${{ github.event.inputs.branch }}""\nwget https://latest.datasette.io/fixtures.db\ndatasette publish vercel fixtures.db \\\n  --branch $BRANCH \\\n  --project ""datasette-preview-$BRANCH"" \\\n  --token $VERCEL_TOKEN \\\n  --scope datasette \\\n  --about ""Preview of $BRANCH"" \\\n  --about_url ""https://github.com/simonw/datasette/tree/$BRANCH""\n', 'python -m pip install --upgrade pip\npython -m pip install -e .[test]\npython -m pip install -e .[docs]\npython -m pip install sphinx-to-sqlite==0.1a1\n', 'pytest -n auto -m ""not serial""\npytest -m ""serial""\n', 'python tests/fixtures.py fixtures.db fixtures.json plugins --extra-db-filename extra_database.db', 'cd docs\nsphinx-build -b xml . _build\nsphinx-to-sqlite ../docs.db _build\ncd ..', 'echo \'\nfrom datasette import hookimpl\n\n@hookimpl\ndef startup(datasette):\n    db = datasette.get_database(""fixtures2"")\n    db.route = ""alternative-route""\n\' > plugins/alternative_route.py\ncp fixtures.db fixtures2.db\n', 'cat fixtures.json | \\\n  jq \'.databases |= . + {""ephemeral"": {""allow"": {""id"": ""*""}}}\' | \\\n  jq \'.plugins |= . + {""datasette-ephemeral-tables"": {""table_ttl"": 900}}\' \\\n  > metadata.json\ncat metadata.json\n', 'gcloud config set run/region us-central1\ngcloud config set project datasette-222320\nexport SUFFIX=""-${GITHUB_REF#refs/heads/}""\nexport SUFFIX=${SUFFIX#-main}\n# Replace 1.0 with one-dot-zero in SUFFIX\nexport SUFFIX=${SUFFIX//1.0/one-dot-zero}\ndatasette publish cloudrun fixtures.db fixtures2.db extra_database.db \\\n    -m metadata.json \\\n    --plugins-dir=plugins \\\n    --branch=$GITHUB_SHA \\\n    --version-note=$GITHUB_SHA \\\n    --extra-options=""--setting template_debug 1 --setting trace_debug 1 --crossdb"" \\\n    --install \'datasette-ephemeral-tables>=0.2.2\' \\\n    --service ""datasette-latest$SUFFIX"" \\\n    --secret $LATEST_DATASETTE_SECRET', '# Deploy docs.db to a different service\ndatasette publish cloudrun docs.db \\\n    --branch=$GITHUB_SHA \\\n    --version-note=$GITHUB_SHA \\\n    --extra-options=""--setting template_debug 1"" \\\n    --service=datasette-docs-latest', 'npm ci', 'npm run prettier -- --check', ""pip install -e '.[test]'\n"", 'pytest\n', 'pip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install -e .[docs]\npython -m pip install sphinx-to-sqlite==0.1a1\n', 'cd docs\nsphinx-build -b xml . _build\nsphinx-to-sqlite ../docs.db _build\ncd ..', 'gcloud config set run/region us-central1\ngcloud config set project datasette-222320\ndatasette publish cloudrun docs.db \\\n    --service=datasette-docs-stable', 'sleep 60 # Give PyPI time to make the new release available\ndocker login -u $DOCKER_USER -p $DOCKER_PASS\nexport REPO=datasetteproject/datasette\ndocker build -f Dockerfile \\\n  -t $REPO:${GITHUB_REF#refs/tags/} \\\n  --build-arg VERSION=${GITHUB_REF#refs/tags/} .\ndocker tag $REPO:${GITHUB_REF#refs/tags/} $REPO:latest\ndocker push $REPO:${GITHUB_REF#refs/tags/}\ndocker push $REPO:latest', 'docker login -u $DOCKER_USER -p $DOCKER_PASS\nexport REPO=datasetteproject/datasette\ndocker build -f Dockerfile \\\n  -t $REPO:${VERSION_TAG} \\\n  --build-arg VERSION=${VERSION_TAG} .\ndocker push $REPO:${VERSION_TAG}', ""pip install -e '.[docs]'\n"", 'codespell docs/*.rst --ignore-words docs/codespell-ignore-words.txt\ncodespell datasette -S datasette/static --ignore-words docs/codespell-ignore-words.txt\n', 'python -m pip install --upgrade pip\npython -m pip install -e .[test]\npython -m pip install pytest-cov\n', 'ls -lah\ncat .coveragerc\npytest --cov=datasette --cov-config=.coveragerc --cov-report xml:coverage.xml --cov-report term\nls -lah', 'pip install shot-scraper build\nshot-scraper install\n', './test-in-pyodide-with-shot-scraper.sh\n', '(cd tests && gcc ext.c -fPIC -shared -o ext.so)', ""pip install -e '.[test]'\npip freeze\n"", 'pytest -n auto -m ""not serial""\npytest -m ""serial""\n# And the test that exceeds a localhost HTTPS server\ntests/test_datasette_https_server.sh\n', 'cog --check docs/*.rst\n', '# This fails on syntax errors, or a diff was applied\nblacken-docs -l 60 docs/*.rst\n']"
"['python -m pip install --upgrade pip\npip install -r requirements_ci.txt\npip install pytest\npip install coverage\npip install coveralls\n', 'coverage run --source=pyod -m pytest\n', 'coveralls --service=github\n', 'python -m pip install --upgrade pip\npip install -r requirements_ci.txt\npip install pytest\npip install coverage\npip install coveralls\n', 'coverage run --source=pyod -m pytest\n', 'coveralls --service=github\n']"
"['pip3 install -q tensorflow==${{ matrix.tf-version }}\npip install -q protobuf==3.19.0\npip install -q requests\npip install -e .\n', 'pip install -q pytest\npip install -q pytest-cov\npip install -q python-coveralls\npytest --cov=deepctr --cov-report=xml\n', 'pip3 install -q tensorflow==${{ matrix.tf-version }}\npip install -q protobuf==3.19.0\npip install -q requests\npip install -e .\n', 'pip install -q pytest\npip install -q pytest-cov\npip install -q python-coveralls\npytest --cov=deepctr --cov-report=xml\n']"
"['df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:mxnet-py38-cu113 -t ${{secrets.DOCKER_REPO}}:mxnet-py38 -t ${{secrets.DOCKER_REPO}}:mxnet-cu113 -t ${{secrets.DOCKER_REPO}}:mxnet -f docker/Dockerfile.mxnet-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:mxnet-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:mxnet-py38\ndocker push ${{secrets.DOCKER_REPO}}:mxnet-cu113\ndocker push ${{secrets.DOCKER_REPO}}:mxnet\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:darknet-cpu -f docker/Dockerfile.darknet-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:darknet-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:pytorch-py38-cpu -t ${{secrets.DOCKER_REPO}}:pytorch-cpu -f docker/Dockerfile.pytorch-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:pytorch-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:pytorch-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:paddle-py38-cpu -t ${{secrets.DOCKER_REPO}}:paddle-cpu -f docker/Dockerfile.paddle-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:paddle-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:paddle-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:paddle-py38-cu113 -t ${{secrets.DOCKER_REPO}}:paddle-py38 -t ${{secrets.DOCKER_REPO}}:paddle-cu113 -t ${{secrets.DOCKER_REPO}}:paddle -f docker/Dockerfile.paddle-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:paddle-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:paddle-py38\ndocker push ${{secrets.DOCKER_REPO}}:paddle-cu113\ndocker push ${{secrets.DOCKER_REPO}}:paddle\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:darknet-cu113 -t ${{secrets.DOCKER_REPO}}:darknet -f docker/Dockerfile.darknet-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:darknet-cu113\ndocker push ${{secrets.DOCKER_REPO}}:darknet\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:chainer-py38-cpu -t ${{secrets.DOCKER_REPO}}:chainer-cpu -f docker/Dockerfile.chainer-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:chainer-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:chainer-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:keras-py38-cpu -t ${{secrets.DOCKER_REPO}}:keras-cpu -f docker/Dockerfile.keras-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:keras-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:keras-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:chainer-py38-cu113 -t ${{secrets.DOCKER_REPO}}:chainer-py38 -t ${{secrets.DOCKER_REPO}}:chainer-cu113 -t ${{secrets.DOCKER_REPO}}:chainer -f docker/Dockerfile.chainer-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:chainer-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:chainer-py38\ndocker push ${{secrets.DOCKER_REPO}}:chainer-cu113\ndocker push ${{secrets.DOCKER_REPO}}:chainer\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:keras-py38-cu113 -t ${{secrets.DOCKER_REPO}}:keras-py38 -t ${{secrets.DOCKER_REPO}}:keras-cu113 -t ${{secrets.DOCKER_REPO}}:keras -f docker/Dockerfile.keras-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:keras-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:keras-py38\ndocker push ${{secrets.DOCKER_REPO}}:keras-cu113\ndocker push ${{secrets.DOCKER_REPO}}:keras\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:all-py38-cpu -t ${{secrets.DOCKER_REPO}}:all-cpu -t ${{secrets.DOCKER_REPO}}:py38-cpu -t ${{secrets.DOCKER_REPO}}:cpu -t ${{secrets.DOCKER_REPO}}:all-jupyter-py38-cpu -t ${{secrets.DOCKER_REPO}}:all-jupyter-cpu -f docker/Dockerfile.all-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:all-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:all-cpu\ndocker push ${{secrets.DOCKER_REPO}}:py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:cpu\ndocker push ${{secrets.DOCKER_REPO}}:all-jupyter-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:all-jupyter-cpu\n', 'docker run ${{secrets.DOCKER_REPO}}:all-py38-cpu  python -c ""import tensorflow as m; print(m.__name__, \':\', m.__version__);import mxnet as m; print(m.__name__, \':\', m.__version__);from tensorflow import keras as m; print(m.__name__, \':\', m.__version__);import torch as m; print(m.__name__, \':\', m.__version__);import chainer as m; print(m.__name__, \':\', m.__version__);import paddle as m; print(m.__name__, \':\', m.__version__);""', 'docker run ${{secrets.DOCKER_REPO}}:all-py38-cpu  darknet', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:all-py38-cu113 -t ${{secrets.DOCKER_REPO}}:all-py38 -t ${{secrets.DOCKER_REPO}}:all-cu113 -t ${{secrets.DOCKER_REPO}}:py38-cu113 -t ${{secrets.DOCKER_REPO}}:all -t ${{secrets.DOCKER_REPO}}:cu113 -t ${{secrets.DOCKER_REPO}}:py38 -t ${{secrets.DOCKER_REPO}}:latest -t ${{secrets.DOCKER_REPO}}:all-jupyter-py38-cu113 -t ${{secrets.DOCKER_REPO}}:all-jupyter-py38 -t ${{secrets.DOCKER_REPO}}:all-jupyter-cu113 -t ${{secrets.DOCKER_REPO}}:all-jupyter -f docker/Dockerfile.all-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:all-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:all-py38\ndocker push ${{secrets.DOCKER_REPO}}:all-cu113\ndocker push ${{secrets.DOCKER_REPO}}:py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:all\ndocker push ${{secrets.DOCKER_REPO}}:cu113\ndocker push ${{secrets.DOCKER_REPO}}:py38\ndocker push ${{secrets.DOCKER_REPO}}:latest\ndocker push ${{secrets.DOCKER_REPO}}:all-jupyter-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:all-jupyter-py38\ndocker push ${{secrets.DOCKER_REPO}}:all-jupyter-cu113\ndocker push ${{secrets.DOCKER_REPO}}:all-jupyter\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:mxnet-py38-cpu -t ${{secrets.DOCKER_REPO}}:mxnet-cpu -f docker/Dockerfile.mxnet-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:mxnet-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:mxnet-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:pytorch-py38-cu113 -t ${{secrets.DOCKER_REPO}}:pytorch-py38 -t ${{secrets.DOCKER_REPO}}:pytorch-cu113 -t ${{secrets.DOCKER_REPO}}:pytorch -f docker/Dockerfile.pytorch-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:pytorch-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:pytorch-py38\ndocker push ${{secrets.DOCKER_REPO}}:pytorch-cu113\ndocker push ${{secrets.DOCKER_REPO}}:pytorch\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:tensorflow-py38-cpu -t ${{secrets.DOCKER_REPO}}:tensorflow-cpu -f docker/Dockerfile.tensorflow-py38-cpu .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:tensorflow-py38-cpu\ndocker push ${{secrets.DOCKER_REPO}}:tensorflow-cpu\n', 'df -h\nsudo apt-get autoremove -y\nsudo apt-get clean\nsudo swapoff -a\nsudo rm -f /swapfile\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'docker build -t ${{secrets.DOCKER_REPO}}:tensorflow-py38-cu113 -t ${{secrets.DOCKER_REPO}}:tensorflow-py38 -t ${{secrets.DOCKER_REPO}}:tensorflow-cu113 -t ${{secrets.DOCKER_REPO}}:tensorflow -f docker/Dockerfile.tensorflow-py38-cu113 .', 'docker login -u ${{secrets.DOCKER_USER}} -p ${{secrets.DOCKER_PASS}}\ndocker push ${{secrets.DOCKER_REPO}}:tensorflow-py38-cu113\ndocker push ${{secrets.DOCKER_REPO}}:tensorflow-py38\ndocker push ${{secrets.DOCKER_REPO}}:tensorflow-cu113\ndocker push ${{secrets.DOCKER_REPO}}:tensorflow\n']"
"['sudo apt update\nsudo apt install -y libgirepository1.0-dev gcc libcairo2-dev pkg-config python3-dev gir1.2-gtk-3.0\nsudo apt install -y libcurl4-openssl-dev libssl-dev\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements.dev.txt\n', 'mypy --version\nmypy bottles\n', 'sudo apt update\nsudo apt install -y libgirepository1.0-dev gcc libcairo2-dev pkg-config python3-dev gir1.2-gtk-3.0\nsudo apt install -y libcurl4-openssl-dev libssl-dev\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements.dev.txt\n', 'pylint --version\nmkdir -p output\npython3 utils/pylint-parser.py > output/pylint-result\ncat output/pylint-result\necho ${{ github.event.number }} > output/pr-number\n', 'unzip pylint-result.zip\necho ""PYLINT_RES<<EOF"" >> $GITHUB_ENV\ncat pylint-result >> $GITHUB_ENV\necho ""EOF"" >> $GITHUB_ENV\n\necho ""PR_NUMBER=$(cat pr-number)"" >> $GITHUB_ENV\n', 'sudo apt update\nsudo apt install -y libgirepository1.0-dev gcc libcairo2-dev pkg-config python3-dev gir1.2-gtk-3.0\nsudo apt install -y libcurl4-openssl-dev libssl-dev\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements.dev.txt\n', 'python -m pytest --version\npython -m pytest bottles\n']"
"['python -m pip install --upgrade pip\npip install tox coverage\n', 'tox', 'coverage combine\ncoverage report\ncoverage xml\n']"
"['tools/install-workflow-deps.sh', 'yarn install --check-files', 'npx projen build', 'git add .\ngit diff --staged --patch --exit-code > .repo.patch || echo ""self_mutation_happened=true"" >> $GITHUB_OUTPUT', 'echo ""::error::Files were changed during build (see build log). If this was triggered from a fork, you will need to update your branch.""\ncat .repo.patch\nexit 1', '[ -s ${{ runner.temp }}/.repo.patch ] && git apply ${{ runner.temp }}/.repo.patch || echo ""Empty patch. Skipping.""', 'git config user.name ""github-actions""\ngit config user.email ""github-actions@github.com""', 'git add .\ngit commit -s -m ""chore: self mutation""\ngit push origin HEAD:$PULL_REQUEST_REF', 'pip install pipenv', 'yarn install --frozen-lockfile', 'yarn run integ', 'tools/install-workflow-deps.sh', 'yarn install --check-files --frozen-lockfile', 'npx projen upgrade-configuration', 'git add .\ngit diff --staged --patch --exit-code > .repo.patch || echo ""patch_created=true"" >> $GITHUB_OUTPUT', '[ -s ${{ runner.temp }}/.repo.patch ] && git apply ${{ runner.temp }}/.repo.patch || echo ""Empty patch. Skipping.""', 'git config user.name ""github-actions""\ngit config user.email ""github-actions@github.com""', 'tools/install-workflow-deps.sh', 'yarn install --check-files --frozen-lockfile', 'npx projen upgrade-dependencies', 'git add .\ngit diff --staged --patch --exit-code > .repo.patch || echo ""patch_created=true"" >> $GITHUB_OUTPUT', '[ -s ${{ runner.temp }}/.repo.patch ] && git apply ${{ runner.temp }}/.repo.patch || echo ""Empty patch. Skipping.""', 'git config user.name ""github-actions""\ngit config user.email ""github-actions@github.com""', 'yarn install --frozen-lockfile', 'cd website\n./build.sh', './docs/build.sh website/public/docs']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*.tar.gz\ntwine upload dist/*.whl\n', 'python setup.py install\nart\nart test\nart test2\n', 'python -m pip install --upgrade pip\npip install --upgrade --upgrade-strategy=only-if-needed -r dev-requirements.txt\n', 'python otherfile/version_check.py\n', 'python otherfile/font_check.py\n', 'python otherfile/art_decor_check.py\n', 'pip install notebook>=5.2.2\npython otherfile/notebook_check.py\n', ""python -m vulture art/ otherfile/ setup.py art_profile.py --min-confidence 65 --exclude=__init__.py --sort-by-size\npython -m bandit -r art -s B311\npython -m pydocstyle --match='(?!test).*\\.py' -v\n"", 'coverage run -m art test2\ncodecov\n', 'python -m cProfile -s cumtime art_profile.py\n']"
"['echo ""TAG_VERSION=${GITHUB_REF#refs/*/v}"" >> $GITHUB_ENV\necho ""REF_NAME=${GITHUB_REF##*/}"" >> $GITHUB_ENV\n', 'echo ""MAJOR_VERSION=${TAG_VERSION:0:4}"" >> $GITHUB_ENV\n', 'echo ${{ env.TAG_VERSION }}\necho ${{ env.MAJOR_VERSION }}\necho ${{ env.REF_NAME }}\n', '[ ${{ env.REF_NAME }} == ""master"" ] && tagname=""latest"" || tagname=${{ env.REF_NAME }}\ndocker run -t --rm kapicorp/kapitan:${tagname} --version\n', 'echo ""TAG_VERSION=${GITHUB_REF#refs/*/v}"" >> $GITHUB_ENV\necho ""REF_NAME=${GITHUB_REF##*/}"" >> $GITHUB_ENV\n', 'echo ""MAJOR_VERSION=${TAG_VERSION:0:4}"" >> $GITHUB_ENV\n', 'echo ${{ env.TAG_VERSION }}\necho ${{ env.MAJOR_VERSION }}\necho ${{ env.REF_NAME }}\n', 'python -m pip install --upgrade pip\npip install pex\n', ""mkdir -p dist\npex .[gojsonnet] -r requirements.txt --python-shebang='#!/usr/bin/env python3' --python=python3.7 --python=python3.8 --python=python3.9 -m kapitan -o dist/kapitan.linux-x86_64.pex"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine build\n', 'python -m build\ntwine check dist/*', 'twine upload --repository testpypi dist/*', 'twine upload dist/*', 'sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 762E3157\nsudo apt-get -qq update\nsudo apt-get install -y gnupg2 git curl\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\nchmod 700 get_helm.sh\nsudo ./get_helm.sh\npip3 install --editable "".[test]""\npip3 install coverage black\n', 'make test && make test_coverage']"
"['npm install --location=global gulp-cli', 'npm install', 'gulp lint', 'gulp coverage']"
""
"['python -m pip install -U mkdocs mkdocs-material\n', 'python -m mkdocs build\n', 'python -m pip install -U commitizen\n', 'echo ""project_version=$(cz version --project)"" >> $GITHUB_ENV\n', 'python -m pip install -U pip poetry\npoetry --version\npoetry install\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""GitHub Action""\n./scripts/test\n', 'python -m pip install -U pip poetry mkdocs mkdocs-material\npoetry --version\npoetry install\n', './scripts/publish\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest pytest-cov codecov\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest --cov=./\ncodecov\n']"
[]
"['pip install pytest tabulate ${{ matrix.tensorflow-version }}\nONNX_TF_PATH=$(pwd)\n#build onnx \nauth_header=""$(git config --local --get http.https://github.com/.extraheader)""\ntmp_dir=$(mktemp -d)\ngit clone https://github.com/onnx/onnx.git $tmp_dir\ncd $tmp_dir\ngit submodule sync --recursive\ngit -c ""http.extraheader=$auth_header"" -c protocol.version=2 submodule update --init --force --recursive --depth=1\ndocker run --rm -v $(pwd):/github/workspace --workdir /github/workspace --entrypoint bash \\\nquay.io/pypa/manylinux2010_x86_64 .github/workflows/manylinux/entrypoint.sh ${{ matrix.python-version }} manylinux2010_x86_64 pull_request\n#install onnx\npip install dist/*manylinux2010_x86_64.whl\ncd $ONNX_TF_PATH\npip install -e .\n', 'pip install pytest tabulate ${{ matrix.tensorflow-version }}\npip install ${{ matrix.onnx-version }}\npip install -e .\n', 'python -m unittest discover test -v\n', 'pip install onnx==1.10.2 tensorflow==2.8.0 tensorflow-addons==0.16.1 tensorflow_probability==0.16.0\npip install -e .\n', '# Make sure model and wiki match the checkout paths above\npython test/test_modelzoo.py -m models -o wiki -v\n']"
"['pip install flake8\n# exit-zero treats all errors as warnings.\nflake8 . --ignore=E501 --exit-zero --statistics\n', 'conda info\nconda list\n', 'python setup.py test\n']"
"['pip install -r requirements/pyproject.txt -r requirements/testing.txt', 'pip install -e .', 'pip freeze', 'make test', 'coverage xml', 'pip install -r requirements/pyproject.txt -r requirements/linting.txt', 'pip install -e .', 'pip install -r requirements/docs.txt', 'mkdocs build', 'python .github/set_version.py', 'cargo update -p watchfiles_rust_notify', ""${{ matrix.ls || 'ls -lh' }} dist/"", 'ls -lh dist/\necho ""`ls dist | wc -l` files""\n', 'mkdir sdist-files\ntar -xvf dist/*.tar.gz -C sdist-files\ntree -a sdist-files\n', 'ls dist/*cp37-abi3-manylinux*x86_64.whl | head -n 1\npython -m zipfile --list `ls dist/*cp37-abi3-manylinux*x86_64.whl | head -n 1`\n', 'pip install twine', 'twine check dist/*', 'pip install twine', 'twine check dist/*', 'twine upload dist/*', 'pip install click==8.0.4', 'pip install smokeshow', 'smokeshow upload docs']"
"['python -m pip install --upgrade pip\npython -m pip install black flake8\n', '# Black code style\nblack --check --diff pytorch_widedeep tests examples setup.py\n# Stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E901,E999,F821,F822,F823 --ignore=E266 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --ignore=E203,E266,E501,E722,F401,F403,F405,F811,W503,C901 --statistics\n', 'python -m pip install --upgrade pip\npython -m pip install pytest-cov codecov .\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest --doctest-modules pytorch_widedeep --cov-report xml --cov-report term --disable-pytest-warnings --cov=pytorch_widedeep tests/\n', 'python -m pip install --upgrade pip\npython -m pip install coverage\n', 'coverage combine coverage*/.coverage*\ncoverage report --fail-under=95\ncoverage xml\n']"
"['echo ""MODULE_VERSION=$(date -u +%Y%m%d).dev$(date -u +%H%M%S)"" >> $GITHUB_ENV', 'echo ""MODULE_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', 'echo ""MODULE_VERSION=${MODULE_VERSION}+${GITHUB_SHA::8}"" >> $GITHUB_ENV', 'echo ::set-output name=module_version::${MODULE_VERSION}', 'echo ::set-output name=file_version::$(date -u +%Y%m%dT%H%M%SZ)', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'bash tests/run_pre_tests.sh src', 'sudo apt update\nsudo apt install -y libglu1-mesa\n', 'sudo apt install -y pandoc', 'bash tools/utils/download_blender.sh ${{ matrix.blender_version }} blender-bin', 'bash tools/pip_package/build_pip_package.sh release ${{ matrix.blender_version }} ./blender ./blender-bin/blender-v${{ matrix.blender_version }}-bin ${{ matrix.blender_version }}', 'bash tests/run_tests.sh raw_modules', 'bash tests/pylint_cycles.sh ${{ matrix.blender_version }} ./blender/ ./release/${{ matrix.blender_version }}/fake_bpy_module_${{ matrix.blender_version }}-*-py3-none-any.whl', 'bash tools/collect_failure_state/collect_failure_state.sh /tmp/failure_state_${{ matrix.blender_version}}', 'export FAKE_BPY_MODULE_FILEPATH=$(ls -1 dist/fake_bpy_modules_${{ matrix.blender_version }}_raw_*/*.zip | head -n 1) && echo ${FAKE_BPY_MODULE_FILEPATH}\necho ""FAKE_BPY_MODULE_FILEPATH=${FAKE_BPY_MODULE_FILEPATH}"" >> $GITHUB_ENV\necho ""FAKE_BPY_MODULE_FILENAME=$(basename ""${FAKE_BPY_MODULE_FILEPATH}"")"" >> $GITHUB_ENV\necho ""MIME_TYPE=$(file --mime-type -b ""${FAKE_BPY_MODULE_FILEPATH}"")"" >> $GITHUB_ENV\n', 'echo ""MODULE_VERSION=$(date -u +%Y%m%d).dev$(date -u +%H%M%S)"" >> $GITHUB_ENV', 'echo ""MODULE_VERSION=$(date -u +%Y%m%d)"" >> $GITHUB_ENV', 'echo ::set-output name=module_version::${MODULE_VERSION}', 'echo ::set-output name=file_version::$(date -u +%Y%m%dT%H%M%SZ)', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'bash tests/run_pre_tests.sh src', 'sudo apt update\nsudo apt install -y libglu1-mesa libegl1\n', 'sudo apt install -y pandoc', 'bash tools/utils/download_latest_blender.sh nutti blender-daily-build ""Blender daily build"" ./blender-bin ${{ secrets.TOKEN_FOR_ACTION_BLENDER_DAILY_BUILD }}', 'bash tools/pip_package/build_pip_package.sh release latest ./blender ./blender-bin/blender-latest-bin', 'bash tests/run_tests.sh raw_modules', 'bash tests/pylint_cycles.sh latest ./blender/ ./release/latest/fake_bpy_module_latest-*-py3-none-any.whl', 'bash tools/collect_failure_state/collect_failure_state.sh /tmp/failure_state', 'sudo apt-get update -qq\nsudo apt-get install -y python3 python3-pip\n', 'pip3 install pycodestyle==2.9.1', 'bash tests/lint/pycodestyle/run.sh .', 'sudo apt-get update -qq\nsudo apt-get install -y python3 python3-pip\n', 'pip3 install pylint==2.15.0\npip3 install -r requirements.txt\n', 'bash tests/lint/pylint/run.sh .', 'npm install -g markdownlint-cli', 'bash tests/lint/markdownlint/run.sh .', 'sudo apt-get update -qq\nsudo apt-get install -y python3 python3-pip\n', 'pip3 install yamllint==1.27.1', 'bash tests/lint/yamllint/run.sh .', 'npm install -g jsonlint', 'bash tests/lint/jsonlint/run.sh .', 'sudo apt-get update -qq\nsudo apt-get install -y shellcheck\n', 'bash tests/lint/shellcheck/run.sh .']"
"['python -m pip install --upgrade pip\npip install pyflakes mypy\n', 'find ./bin -name \'*.py\' -exec pyflakes ""{}"" \\;\nfind ./tests -name \'*.py\' -exec pyflakes ""{}"" \\;\nmypy --strict-optional --config-file mypy.ini bin/deepstate\n', 'sudo apt-get update\nsudo apt-get install -y build-essential gcc-multilib cmake libffi-dev\npython -m pip install --upgrade pip\npip install z3-solver angr nose\npip install git+git://github.com/trailofbits/manticore.git\n', 'mkdir build && cd build\ncmake ..\nsudo make install\n', 'nosetests tests/test_${{ matrix.env.TEST }}.py\n']"
"['python -m pip install --upgrade pip setuptools wheel\npip install pytest\npython setup.py install\n', 'pytest']"
[]
"['python -m ensurepip\npip install wheel setuptools\npip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel', 'sudo apt update\nsudo apt install -y libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0\npython -m ensurepip\npip install -r docs/requirements.txt\npip install -r requirements.txt\n', 'cd docs\nmake html\n', 'python -m ensurepip\npip install black\n', 'black twitchio --line-length 120 --verbose --check', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python edit_version.py --latest ${{ steps.latest_version.outputs.latest_tag }}\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python3 -m build --sdist --wheel --no-isolation', 'python3 -m mnamer --version', 'twine upload --username __token__ --password ${{secrets.pypi_password}} dist/*']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest pylint pytest-cov\npython -m pip install opencv-python\npython -m pip install -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 pibooth --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 pibooth --count --exit-zero --max-complexity=10 --max-line-length=160 --statistics\n', 'pylint $(git ls-files \'*.py\') || echo ""pylint exited with $?""\n', 'pytest --cov pibooth\n']"
[]
"['echo ""dir=$(yarn cache dir)"" >> $GITHUB_OUTPUT', 'python -m pip install -U jupyter_packaging~=0.12.3 jupyterlab~=4.0.0 pip wheel', 'set -eux\npython -m pip install .\n', 'jupyter server extension list 2>&1 | grep -ie ""jupyterlab_latex.*OK""\njupyter labextension list 2>&1 | grep -ie ""@jupyterlab/latex.*OK""\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'TOXENV=lint tox', 'TOXENV=pylint tox', 'TOXENV=docs tox', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'TOXENV=codecov tox', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'TOXENV=bandit tox']"
"['python3 -m pip install build --user', 'python3 -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install tensorflow\npip install -e "".[tests]"" --progress-bar off\n', 'pytest --cov=autokeras --cov-report xml:coverage.xml\n', 'bash shell/pre-commit.sh', 'python -m pip install --upgrade pip setuptools\npip install tensorflow\npip install -e .\npip install -r docs/requirements.txt\n', 'cd docs\npython autogen.py\nmkdocs build\n', 'python -m pip install --upgrade pip setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -e "".[tests]"" --progress-bar off\npip uninstall keras-tuner -y\npip install git+https://github.com/keras-team/keras-tuner.git@master\npip uninstall keras tensorflow tensorflow-text tensorflow-datasets -y\npip install tf-nightly tensorflow-text-nightly tfds-nightly --progress-bar off --upgrade\n', 'pytest\n']"
"['docker compose run --user root --rm ${{ matrix.target }} build-rpm.sh\n', 'ls -alF dist/rpm/noarch/*.noarch.rpm\n', 'sudo apt-get update\nsudo apt-get install libdbus-1-dev\n', 'pip install wheel\npip install -r requirements.txt -r dev-requirements.txt\n', 'pip install -r all-plugin-requirements.txt\n\n# Installing `dbus-python` will only work on Linux/CPython.\n[[ $RUNNER_OS = ""Linux"" && $PYTHON != \'pypy\'* ]] && pip install dbus-python || true\n', ""[[ $PYTHON != 'pypy'* ]] && pip install -r win-requirements.txt || true\n"", 'pip install --editable=.\npython setup.py compile_catalog\n', 'flake8 . --count --show-source --statistics\n', 'coverage run -m pytest\n', 'coverage combine\ncoverage xml\ncoverage report\n']"
""
"['sed -i ""s/__version__ = \\""\\(.*\\)\\""/__version__ = \\""\\1.dev$(date -u +%Y%m%d)\\""/g"" ignite/__init__.py\ncat ignite/__init__.py\n', 'conda install -y pytorch torchvision cpuonly -c pytorch-nightly\npip install -r requirements-dev.txt\n', 'chmod +x ./conda.recipe/build_and_upload.sh\n./conda.recipe/build_and_upload.sh\n', '# workaround to fix https://github.com/pytorch/ignite/issues/2373\npip uninstall -y twine pkginfo\npip install --upgrade --no-cache-dir twine \'pkginfo>=1.8.2\'\npython setup.py sdist bdist_wheel\ntwine --version\ntwine check dist/*\nTWINE_USERNAME=""${{ secrets.PYPI_USER }}"" TWINE_PASSWORD=""${{ secrets.PYPI_TOKEN }}"" twine upload --verbose dist/*\n', 'bash ./tests/run_code_style.sh install\nbash ./tests/run_code_style.sh fmt\n', 'echo ""modified=${{ steps.files.outputs.files_created }} ${{ steps.files.outputs.files_updated }}"" >> $GITHUB_ENV', 'echo ""modified=${{ env.modified }}"" >> $GITHUB_OUTPUT', 'echo ""pth_version=$(python -c ""import configparser; cfg=configparser.ConfigParser(); cfg.read(\'docker.cfg\'); print(cfg.get(\'DEFAULT\', \'build_docker_image_pytorch_version\'))"")"" >> $GITHUB_OUTPUT\necho ""hvd_version=$(python -c ""import configparser; cfg=configparser.ConfigParser(); cfg.read(\'docker.cfg\'); print(cfg.get(\'DEFAULT\', \'build_docker_image_hvd_version\'))"")"" >> $GITHUB_OUTPUT\necho ""msdp_version=$(python -c ""import configparser; cfg=configparser.ConfigParser(); cfg.read(\'docker.cfg\'); print(cfg.get(\'DEFAULT\', \'build_docker_image_msdp_version\'))"")"" >> $GITHUB_OUTPUT\n', 'sudo rm -rf ""/usr/local/share/boost""\nsudo rm -rf ""$AGENT_TOOLSDIRECTORY""\n', 'pip install docker\nexport PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport HVD_VERSION=${{ needs.setup.outputs.hvd_version }}\ndocker system prune -a -f\nbash build.sh hvd hvd-base\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport HVD_VERSION=${{ needs.setup.outputs.hvd_version }}\nbash build.sh hvd hvd-vision\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport HVD_VERSION=${{ needs.setup.outputs.hvd_version }}\nbash build.sh hvd hvd-nlp\n', 'docker images | grep pytorchignite\n', 'sudo rm -rf ""/usr/local/share/boost""\nsudo rm -rf ""$AGENT_TOOLSDIRECTORY""\n', 'pip install docker\nexport PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport HVD_VERSION=${{ needs.setup.outputs.hvd_version }}\ndocker system prune -a -f\nbash build.sh hvd hvd-apex\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport HVD_VERSION=${{ needs.setup.outputs.hvd_version }}\nbash build.sh hvd hvd-apex-vision\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport HVD_VERSION=${{ needs.setup.outputs.hvd_version }}\nbash build.sh hvd hvd-apex-nlp\n', 'docker images | grep pytorchignite\n', 'sudo rm -rf ""/usr/local/share/boost""\nsudo rm -rf ""$AGENT_TOOLSDIRECTORY""\n', 'pip install docker\nexport PTH_VERSION=${{ needs.setup.outputs.pth_version }}\ndocker system prune -a -f\nbash build.sh main base\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nbash build.sh main vision\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nbash build.sh main nlp\n', 'docker images | grep pytorchignite\n', 'sudo rm -rf ""/usr/local/share/boost""\nsudo rm -rf ""$AGENT_TOOLSDIRECTORY""\n', 'pip install docker\nexport PTH_VERSION=${{ needs.setup.outputs.pth_version }}\ndocker system prune -a -f\nbash build.sh main apex\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nbash build.sh main apex-vision\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nbash build.sh main apex-nlp\n', 'docker images | grep pytorchignite\n', 'sudo rm -rf ""/usr/local/share/boost""\nsudo rm -rf ""$AGENT_TOOLSDIRECTORY""\n', 'pip install docker\nexport PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport MSDP_VERSION=${{ needs.setup.outputs.msdp_version }}\ndocker system prune -a -f\nbash build.sh msdp msdp-apex\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport MSDP_VERSION=${{ needs.setup.outputs.msdp_version }}\nbash build.sh msdp msdp-apex-vision\n', 'export PTH_VERSION=${{ needs.setup.outputs.pth_version }}\nexport MSDP_VERSION=${{ needs.setup.outputs.msdp_version }}\nbash build.sh msdp msdp-apex-nlp\n', 'docker images | grep pytorchignite\n', ""pip install requests\n\nif [ $GITHUB_EVENT_NAME == 'pull_request' ]; then should_publish_docker_images=false; else should_publish_docker_images=true; fi\nbranch=$GITHUB_REF\n\npython -u .github/workflows/trigger_circle_ci.py $should_publish_docker_images $branch\n"", 'sudo npm install katex -g', 'bash .github/workflows/install_docs_deps.sh', 'bash .github/workflows/build_docs.sh', 'bash .github/workflows/install_docs_deps.sh', 'make linkcheck', 'sudo npm install katex -g', 'bash .github/workflows/install_docs_deps.sh', 'make html\nmake doctest\nmake coverage\n', 'echo ""::group::Cleanup debug output""\nsudo rm -rfv ""${GITHUB_WORKSPACE}""\nmkdir -p ""${GITHUB_WORKSPACE}""\necho ""::endgroup::""\n', 'docker run --name pthd --gpus=all --rm \\\n  --cap-add=SYS_PTRACE \\\n  --detach \\\n  --ipc=host \\\n  --security-opt seccomp=unconfined \\\n  --shm-size=2g \\\n  --tty \\\n  --ulimit stack=10485760:83886080 \\\n  -v $PWD:/work \\\n  -w /work \\\n  ${DOCKER_IMAGE}\n\nscript=$(cat << EOF\n\n  set -x\n\n  nvidia-smi\n  ls -alh\n\n  conda --version\n  python --version\n\nEOF\n)\ndocker exec -t pthd /bin/bash -c ""${script}""\n', '\nscript=$(cat << EOF\n\nset -x\n\n# Install PyTorch\nif [ ""${{ matrix.pytorch-channel }}"" == ""pytorch"" ]; then\n  pip install --upgrade torch torchvision --extra-index-url https://download.pytorch.org/whl/cu117\nelse\n  pip install --upgrade --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cu117\nfi\n\nnvidia-smi\npython -c ""import torch; print(torch.__version__, \', CUDA is available: \', torch.cuda.is_available()); exit(not torch.cuda.is_available())""\npip list\n\n# Install dependencies\npip install -r requirements-dev.txt\npip install -e .\n\nEOF\n)\n\ndocker exec -t pthd /bin/bash -c ""${script}""\n', '\nscript=$(cat << EOF\n\nset -x\n\nbash tests/run_gpu_tests.sh 2\n\nEOF\n)\n\ndocker exec -t pthd /bin/bash -c ""${script}""\n', 'SCRIPT=$(cat << EOF\n\nset -x\n\n# Install additional example dependencies\npip install fire\n\n# Check training on cifar10, run without backend\n## initial run\nCI=1 python examples/contrib/cifar10/main.py run --checkpoint_every=200 --stop_iteration=500\n## resume\nCI=1 python examples/contrib/cifar10/main.py run --checkpoint_every=200 --num_epochs=7 --resume-from=/tmp/output-cifar10/resnet18_backend-None-1_stop-on-500/training_checkpoint_400.pt\n\n# Check training on cifar10, run with NCCL backend using torchrun\n## initial run\nCI=1 torchrun --nproc_per_node=2 examples/contrib/cifar10/main.py run --backend=nccl --checkpoint_every=200 --stop_iteration=500\n## resume\nCI=1 torchrun --nproc_per_node=2 examples/contrib/cifar10/main.py run --backend=nccl --checkpoint_every=200 --num_epochs=7 --resume-from=/tmp/output-cifar10/resnet18_backend-nccl-2_stop-on-500/training_checkpoint_400.pt\n\n# Check training on cifar10, run with NCCL backend using spawn\n## initial run\nCI=1 python -u examples/contrib/cifar10/main.py run --backend=nccl --nproc_per_node=2 --checkpoint_every=200 --stop_iteration=500\n## resume\nCI=1 python -u examples/contrib/cifar10/main.py run --backend=nccl --nproc_per_node=2 --checkpoint_every=200 --num_epochs=7 --resume-from=/tmp/output-cifar10/resnet18_backend-nccl-2_stop-on-500/training_checkpoint_400.pt\n\nEOF\n)\n\ndocker exec -t pthd /bin/bash -c ""${script}""\n', 'echo ""date=$(/bin/date ""+%Y-%U"")"" >> $GITHUB_OUTPUT', 'python3 -m pip install -U pip\necho ""pip_cache=$(python3 -m pip cache dir)"" >> $GITHUB_OUTPUT\n', '\n#install other dependencies\npip install torch torchvision -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install -r requirements-dev.txt\npip install horovod\npython setup.py install\n', 'bash tests/run_cpu_tests.sh\n', 'echo ""date=$(/bin/date ""+%Y-%U"")"" >> $GITHUB_OUTPUT', 'python3 -m pip install -U pip\necho ""pip_cache=$(python3 -m pip cache dir)"" >> $GITHUB_OUTPUT\n', 'conda install pytorch=${{ matrix.pytorch-version }} torchvision cpuonly python=${{ matrix.python-version }} -c pytorch\npip install -r requirements-dev.txt\npython setup.py install\n', 'conda install pytorch=${{ matrix.pytorch-version }} cpuonly python=${{ matrix.python-version }} -c pytorch\npip install torchvision==0.5.0\npip install -r requirements-dev.txt\npython setup.py install\n', 'bash tests/run_cpu_tests.sh ""not test_time_profilers""\n', 'conda install -y pytorch torchvision cpuonly -c pytorch\npython setup.py install\n', 'chmod +x ./conda.recipe/build_and_upload.sh\n./conda.recipe/build_and_upload.sh\n', 'conda install -y pytorch torchvision cpuonly -c pytorch\npip install -r requirements-dev.txt\npip install --upgrade --no-cache-dir twine\npython setup.py install\n', 'python setup.py sdist bdist_wheel\ntwine check dist/*\nTWINE_USERNAME=""${{ secrets.PYPI_USER }}"" TWINE_PASSWORD=""${{ secrets.PYPI_TOKEN }}"" twine upload --verbose dist/*\n', ""pip install requests\n\nif [ $GITHUB_EVENT_NAME == 'pull_request' ]; then should_publish_docker_images=false; else should_publish_docker_images=true; fi\nbranch=$GITHUB_REF\n\npython -u .github/workflows/trigger_circle_ci.py $should_publish_docker_images $branch\n"", 'echo ""date=$(/bin/date ""+%Y-%U"")"" >> $GITHUB_OUTPUT', 'pip3 install -U pip\necho ""pip_cache=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', '\n## Install openblas, mkl, gsutil\nsudo apt-get update && sudo apt-get install -y libopenblas-dev libomp5\n# mkl version fixed due to https://github.com/pytorch/ignite/issues/2350\npip install mkl==2021.4.0 requests gsutil\n\n## Install torch & xla and torchvision\npip install --pre  https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch-nightly-cp38-cp38-linux_x86_64.whl\npip install --pre  https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-nightly-cp38-cp38-linux_x86_64.whl\npip install --pre  https://storage.googleapis.com/tpu-pytorch/wheels/colab/torchvision-nightly-cp38-cp38-linux_x86_64.whl\n\n## Install test deps and Ignite\npip install -r requirements-dev.txt\npython setup.py install\n', 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${Python_ROOT_DIR}/lib\nexport XRT_DEVICE_MAP=""CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0""\nexport XRT_WORKERS=""localservice:0;grpc://localhost:40934""\n\npython -c ""import torch_xla; print(\'torch xla version:\', torch_xla.__version__)""\nbash tests/run_tpu_tests.sh\n', 'echo ""date=$(/bin/date ""+%Y-%U"")"" >> $GITHUB_OUTPUT\n', 'pip install -U pip || python -m pip install -U pip\necho ""pip_cache=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install pip wheel setuptools -Uqq', 'pip install torch torchvision -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre', 'pip install -r requirements-dev.txt\npython setup.py install\n', 'bash ./tests/run_code_style.sh install\nbash ./tests/run_code_style.sh lint\n', 'bash ./tests/run_code_style.sh mypy\n', 'cp -R /tmp/MNIST .\n', 'SKIP_DISTRIB_TESTS=${{ matrix.skip-distrib-tests }} bash tests/run_cpu_tests.sh\n', '# MNIST\n# 1) mnist.py\npython examples/mnist/mnist.py --epochs=1\n', '# 2) mnist_with_visdom.py\npython -c ""from visdom.server.build import download_scripts; download_scripts()"" # download scripts : https://github.com/facebookresearch/visdom/blob/master/py/server.py#L929\npython -m visdom.server &\nsleep 10\npython examples/mnist/mnist_with_visdom.py --epochs=1\nkill %1\n# 3.1) mnist_with_tensorboard.py with tbX\npython examples/mnist/mnist_with_tensorboard.py --epochs=1\n# 3.2) mnist_with_tensorboard.py with native torch tb\npip uninstall -y tensorboardX\npython examples/mnist/mnist_with_tensorboard.py --epochs=1\n', '# 4) mnist_save_resume_engine.py\npython examples/mnist/mnist_save_resume_engine.py --epochs=2 --crash_iteration 1100\n', 'python examples/mnist/mnist_save_resume_engine.py --epochs=2 --resume_from=/tmp/mnist_save_resume/checkpoint_1.pt\n', '# DCGAN\npython examples/gan/dcgan.py --dataset fake --dataroot /tmp/fakedata --output-dir /tmp/outputs-dcgan --batch-size 2 --epochs 2  --workers 0\n', '# RL\n# 1) Actor-Critic\npython examples/reinforcement_learning/actor_critic.py --max-episodes=2\n# 2) Reinforce\npython examples/reinforcement_learning/reinforce.py --max-episodes=2\n', '#fast-neural-style\n#train\nmkdir -p ~/.cache/torch/checkpoints/ && wget ""https://download.pytorch.org/models/vgg16-397923af.pth"" -O ~/.cache/torch/checkpoints/vgg16-397923af.pth\npython examples/fast_neural_style/neural_style.py train --epochs 1 --cuda 0 --dataset test --dataroot . --image_size 32 --style_image examples/fast_neural_style/images/style_images/mosaic.jpg --style_size 32\n', '# Super-Resolution\npython examples/super_resolution/main.py --upscale_factor 3 --crop_size 180 --batch_size 4 --test_batch_size 100 --n_epochs 1 --lr 0.001 --threads 2 --debug\n']"
"['pip install --upgrade virtualenv\npip install tox\n', 'tox', 'sed -i ""s/<source>\\/home\\/runner\\/work\\/caldera\\/caldera/<source>\\/github\\/workspace/g"" /home/runner/work/caldera/caldera/coverage.xml', 'pip install --upgrade virtualenv\npip install tox\n', 'tox']"
"['pip install --user ruff', 'ruff --format=github --ignore=F401 --target-version=py38 .', 'pip install pytest pytest-cov -r requirements.txt', 'pytest --cov=webssh', 'mkdir -p coverage']"
[]
"['pip install -r requirements/publish.txt', 'python setup.py sdist bdist_wheel', 'pip install -r requirements/ci.txt', 'tox']"
""
""
""
""
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
""
"['python -m pip install --upgrade pip\npip install wheel\npip install pillow==${{ matrix.pillow-version }}\npip install pytest\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install wheel\npip install pillow==${{ matrix.pillow-version }}\npip install pytest\n', 'pytest\n']"
"['python -m pip install -e .[tests,tensorflow]\n', 'black --check .\n', 'isort --check-only .\n', 'flake8 .\n', 'python -m pip install tensorflow==${{ matrix.tensorflow }}.* tensorflow-text==${{ matrix.tensorflow }}.*\npython -m pip install -e .[tests]\n', 'wget https://s3.amazonaws.com/opennmt-models/transliteration-aren-v2.tar.gz\nmkdir -p testdata\ntar xf transliteration-aren-v2.tar.gz -C testdata\nwget -P testdata https://opennmt-trainingdata.s3.amazonaws.com/wmtende.model\n', 'pytest --cov=opennmt --cov-report xml opennmt/tests\n', 'python -m pip install wheel\n', 'python setup.py sdist bdist_wheel\n', 'python -m pip install -e .[tensorflow,docs]\n', 'python docs/generate-apidoc.py docs/package\nsphinx-build docs docs/build\n']"
['pip install pre-commit\npre-commit run -av trailing-whitespace\npre-commit run -av end-of-file-fixer\npre-commit run -av check-yaml\npre-commit run -av check-added-large-files\n']
"['pip install tox-travis -c constraints.txt\n', 'tox -c ${TOXCFG}\n']"
"['python -m pip install --upgrade pip\npython -m pip install torch==${{ matrix.pytorch-version }}\npython -m pip install --upgrade numpy\npython -m pip install -r requirements-test.txt\npython -m pip install -e .\n', 'pytest', 'python -m pip install coveralls\ncoveralls --service=github\n']"
"['sudo apt-get install -y attr\npip install --upgrade pip\npip install .\n', 'pip install prospector>=1.5.1\nprospector --version\nprospector\n', 'sudo apt-get install -y attr\npip install --upgrade pip\npip install .\n', 'python ci/test_commit_message.py', 'sudo apt-get install -y attr\npip install --upgrade pip\npip install .\n', 'pip install bandit>=1.6\nc=`python ci/evaluate_docs.py`; if [ -z $c ]; then echo ""No .py files to lint""; else echo $c | xargs bandit; fi\n', 'sudo apt-get update && sudo apt-get install -y attr openjdk-8-jdk-headless maven\npip install --upgrade pip\npip install .\ndocker pull photon:3.0 && docker save photon:3.0 > photon.tar\n# build SPDX validation tool from source\ngit clone https://github.com/spdx/tools-java.git && cd tools-java\nexport JAVA_HOME=$(readlink -f /usr/bin/javac | sed ""s:/bin/javac::"")\nmvn clean install && cd ..\n', 'python ci/test_files_touched.py', 'sudo apt-get install -y attr\npip install --upgrade pip\npip install .\n', 'pip install coverage\npip install .\ncoverage run -m unittest discover -s tests\ncoverage report\n']"
"['poetry install', 'poetry run sphinx-build -b html docs docs/_build', 'poetry build', 'poetry publish', 'poetry add django==${{ matrix.django-version }}\npoetry install\n', 'poetry run scripts/test.sh']"
"[""pip install -e '.[test]'\n"", 'pytest\n', 'pip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', ""pip install -e '.[test]'\n"", 'pytest\n']"
"['./go setup', './go check', './go test', './go test-setuptools', './go test-docker-build', 'gh pr merge --repo ${GITHUB_REPOSITORY} --auto --rebase ""${PR_BRANCH}""']"
"['poetry install\n', 'make check-fmt\n', 'make flake\nmake lint\n', 'poetry install\n', 'make test\n', 'pip install termcolor==2.2.0 pytest==7.2.1 pytest-xdist==3.2.0\n', 'py.test -n auto -v\n', 'make coverage\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m unittest\n', 'python -m mypy webauthn\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip\npip install coverage coveralls\npip install -r requirements.txt\n', 'python -m coverage run --branch --source=prometheus_flask_exporter -m unittest discover -s tests -v\n', 'coveralls --service=github\n', 'sh examples/${{ matrix.testname }}/run_tests.sh\n', 'pip install build\n', 'python -m build\n', 'pip install prometheus-flask-exporter\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['python -m pip install --upgrade pip\npip install flake8 pylint mypy pytest pytest-mock pytest-xvfb wheel\npip install types-attrs types-cryptography types-pyOpenSSL types-PyYAML types-setuptools\npip install -r ./requirements/requirements_${{ matrix.backend }}.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --select=E9,F63,F7,F82 --show-source\n# exit-zero treats all errors as warnings.\nflake8 . --exit-zero\n', 'mypy .\n', 'if [ ""${{ matrix.backend }}"" == ""amd"" ] ; then echo ""{\\""PLAIDML_DEVICE_IDS\\"":[\\""llvm_cpu.0\\""],\\""PLAIDML_EXPERIMENTAL\\"":true}"" > ~/.plaidml; fi ;\necho ""{\\""PLAIDML_DEVICE_IDS\\"":[\\""llvm_cpu.0\\""],\\""PLAIDML_EXPERIMENTAL\\"":true}"" > ~/.plaidml;\nFACESWAP_BACKEND=""${{ matrix.backend }}"" KERAS_BACKEND=""${{ matrix.kbackend }}"" py.test -v tests/;\n', 'FACESWAP_BACKEND=""${{ matrix.backend }}"" KERAS_BACKEND=""${{ matrix.kbackend }}"" python tests/simple_tests.py;\nif [ ""${{ matrix.backend }}"" == ""amd"" ] ; then rm -f ~/.plaidml; fi ;\n', 'python -m pip install --upgrade pip\npip install flake8 pylint mypy pytest pytest-mock wheel\npip install types-attrs types-cryptography types-pyOpenSSL types-PyYAML types-setuptools\npip install -r ./requirements/requirements_${{ matrix.backend }}.txt\n', 'echo ""FACESWAP_BACKEND=${{ matrix.backend }}"" | Out-File -FilePath $env:GITHUB_ENV -Append', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --select=E9,F63,F7,F82 --show-source\n# exit-zero treats all errors as warnings.\nflake8 . --exit-zero\n', 'mypy .\n', 'py.test -v tests', 'python tests/simple_tests.py']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f web-gui/requirements.txt ]; then pip install -r web-gui/requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['cat /etc/os-release', '# NOTE(kamo): cmake sndfile will be download using anacond:\nyum install -y git centos-release-scl bzip2 wget which unzip bc patch\nyum-config-manager --enable rhel-server-rhscl-7-rpms\nyum install -y devtoolset-7-gcc-c++ devtoolset-7-make sox ncurses-devel libtool automake autoconf\nlocaledef -f UTF-8 -i en_US en_US\n', '# NOTE(karita) this line exited 1\n# source scl_source enable devtoolset-7\nPATH=""/opt/rh/devtoolset-7/root/usr/bin:${PATH:-}""\n./ci/install.sh\n', 'PATH=""/opt/rh/devtoolset-7/root/usr/bin:${PATH:-}""\n./ci/test_shell.sh\n', 'PATH=""/opt/rh/devtoolset-7/root/usr/bin:${PATH:-}""\n./ci/test_python.sh\n', 'ci/check_kaldi_symlinks.sh', ""sudo apt-get update -qq\n# NOTE(kamo): g++-7 doesn't exist in ubuntu-latest\nsudo apt-get install -qq -y cmake libsndfile1-dev bc sox\n"", './ci/install.sh\n', './ci/test_shell.sh\n', './ci/test_python.sh', 'source tools/activate_python.sh\ncoverage erase\n', './ci/install_kaldi.sh\n', './ci/test_utils.sh', 'source tools/activate_python.sh\ncoverage erase\n', './ci/test_integration_espnet1.sh', 'source tools/activate_python.sh\ncoverage erase\n', './ci/test_integration_espnet2.sh', 'cat /etc/os-release', 'apt-get update -qq\n# NOTE(kamo): cmake sndfile will be download using anacond:\napt-get install -qq -y \\\n  build-essential git unzip bzip2 wget curl bc locales make sox \\\n  libncurses5-dev automake libtool pkg-config\nlocaledef -f UTF-8 -i en_US en_US\n', './ci/install.sh', './ci/test_shell.sh', './ci/test_python.sh', './ci/install_kaldi.sh', './ci/test_utils.sh', './ci/test_integration_espnet1.sh', './ci/test_integration_espnet2.sh', 'cat /etc/os-release', 'sudo apt-get update -qq\nsudo apt-get install -qq -y cmake python3-dev git pandoc ffmpeg bc\n', './ci/install.sh', './ci/doc.sh', 'cd docker\ndocker build --build-arg FROM_TAG=runtime-latest \\\n  -f prebuilt/devel.dockerfile \\\n  --target devel \\\n  -t espnet/espnet:cpu-latest .\ndocker push espnet/espnet:cpu-latest\n', 'cd docker\ndocker build --build-arg FROM_TAG=cuda-latest \\\n  --build-arg CUDA_VER=11.1 \\\n  -f prebuilt/devel.dockerfile \\\n  --target devel \\\n  -t espnet/espnet:gpu-latest .\ndocker push espnet/espnet:gpu-latest\n', './ci/install.sh\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo apt-get install -qq -y libsndfile1-dev\npython3 -m pip install --upgrade pip setuptools wheel\n', 'python3 -m pip install -U numba\n./tools/installers/install_torch.sh false ${TH_VERSION} CPU\n./tools/installers/install_chainer.sh CPU\npython3 setup.py bdist_wheel\npython3 -m pip install dist/espnet-*.whl\n# log\npython3 -m pip freeze\n', 'python3 ./ci/test_import_all.py\n', 'python3 -m pip install ""$(ls dist/espnet-*.whl)[all]""\n# log\npython3 -m pip freeze\n', 'python3 ./ci/test_import_all.py\n', 'choco install -y wget\n', './ci/install.sh\n']"
"['git checkout HEAD^2', 'python setup.py sdist', 'pip3 install -r requirements-meta.txt', 'tox -- --durations=0 --timeout=30', 'pip3 install -r requirements-meta.txt', 'tox -e typecheck']"
"['GO111MODULE=on go get k8s.io/release/cmd/release-notes', 'echo ORG=$(echo \'${{ github.repository }}\' | awk -F \'/\' \'{print $1}\') >> $GITHUB_ENV\necho REPO=$(echo \'${{ github.repository }}\' | awk -F \'/\' \'{print $2}\') >> $GITHUB_ENV\n\necho ""BRANCH=${{ github.event.inputs.branch }}"" >> $GITHUB_ENV\n\nif [[ ""${{ github.event.inputs.start-sha }}"" != """" ]]; then\n  echo ""START_SHA=${{ github.event.inputs.start-sha }}"" >> $GITHUB_ENV\nelse\n  # Default Starting SHA (thanks @dprotaso)\n  export semver=$(git describe --match ""v[0-9]*"" --abbrev=0)\n  echo ""Using ${semver} tag for starting sha.""\n  echo START_SHA=$(git rev-list -n 1 ""${semver}"") >> $GITHUB_ENV\nfi\n\nif [[ ""${{ github.event.inputs.end-sha }}"" != """" ]]; then\n  echo ""END_SHA=${{ github.event.inputs.end-sha }}"" >> $GITHUB_ENV\nelse\n  # Default Ending SHA (thanks @dprotaso)\n  echo ""END_SHA=$(git rev-list -n 1 HEAD)"" >> $GITHUB_ENV\nfi\n', '# See  https://github.com/kubernetes/release/tree/master/cmd/release-notes for options.\n# Note: we are setting env vars in the Defaults step.\nrelease-notes \\\n  --required-author """" \\\n  --repo-path       ""$(pwd)"" \\\n  --output          release-notes.md\n', 'cat release-notes.md\n', 'python -m pip install -r tools/requirements.txt', 'make -f tools/Makefile test_tools', 'make -f tools/Makefile verify']"
""
"['pip install -r dev_tools/requirements/isolated-base.env.txt', 'check/pytest -n auto -m weekly dev_tools/notebooks/isolated_notebook_test.py -k ${{matrix.partition}}', 'check/misc', 'pip install -r dev_tools/requirements/deps/packaging.txt', './dev_tools/packaging/packaging_test.sh', 'pip install -r dev_tools/requirements/deps/format.txt', 'check/format-incremental', 'pip install -r dev_tools/requirements/mypy.env.txt', 'check/mypy', 'dev_tools/conf/pip-install-minimal-for-pytest-changed-files.sh', 'check/pytest-changed-files -n auto', 'pip install -r dev_tools/requirements/deps/pylint.txt', 'check/pylint --version', 'check/pylint -v', 'pip install -r dev_tools/requirements/dev.env.txt', 'find . -type f -name ""*.rst"" | xargs rstcheck --report warning --ignore-directives autoclass,automodule', 'check/doctest -q', 'pip install -r dev_tools/requirements/deps/tensorflow-docs.txt', 'check/nbformat', 'check/shellcheck', 'pip install -r dev_tools/requirements/isolated-base.env.txt', 'pytest -n auto -m slow dev_tools/packaging/isolated_packages_test.py', 'pip install wheel\npip install --upgrade --upgrade-strategy eager -r dev_tools/requirements/dev.env.txt\n', 'docker-compose -f cirq-rigetti/docker-compose.test.yaml up -d', 'check/pytest -n auto --ignore=cirq-core/cirq/contrib --rigetti-integration', 'docker-compose -f cirq-rigetti/docker-compose.test.yaml down', 'pip install pip-tools', 'pip-compile --resolver=backtracking dev_tools/requirements/deps/cirq-all.txt -o-\n', 'pip install -r rtd_docs/requirements.txt\n', 'dev_tools/docs/build-rtd-docs.sh', 'pip install -r dev_tools/requirements/deps/protos.txt\n', 'check/build-changed-protos', 'pip install wheel\npip install --upgrade --upgrade-strategy eager -r dev_tools/requirements/dev.env.txt\n', 'docker-compose -f cirq-rigetti/docker-compose.test.yaml up -d', 'check/pytest-and-incremental-coverage -n auto --rigetti-integration', 'docker-compose -f cirq-rigetti/docker-compose.test.yaml down', 'pip install wheel\npip install --upgrade --upgrade-strategy eager -r dev_tools/requirements/no-contrib.env.txt\n', 'source dev_tools/pypath\ncheck/pytest -n auto --ignore=cirq-core/cirq/contrib\n', 'pip install wheel\npip install --upgrade --upgrade-strategy eager -r dev_tools/requirements/no-contrib.env.txt\n', 'check/pytest -n auto --ignore=cirq-core/cirq/contrib', 'pip install -r dev_tools/requirements/isolated-base.env.txt', 'check/pytest -n auto -m slow dev_tools/notebooks/isolated_notebook_test.py -k ${{matrix.partition}}', 'pip install -r dev_tools/requirements/notebooks.env.txt', 'check/pytest -n auto -m slow dev_tools/notebooks/notebook_test.py', 'check/npm ci', 'check/ts-build-current', 'check/npm ci', 'check/ts-lint', 'check/npm ci', 'check/ts-test', 'check/ts-test-e2e', 'check/npm ci', 'check/ts-coverage', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'export CIRQ_PRE_RELEASE_VERSION=$(dev_tools/packaging/generate-dev-version-id.sh)\n[[ ""$CIRQ_PRE_RELEASE_VERSION"" =~ .*dev.* ]] && echo ""Deploying dev version \'$CIRQ_PRE_RELEASE_VERSION\'"" || (echo ""not dev version""; exit 1)\nout_dir=""${PWD}/dist""\ndev_tools/packaging/produce-package.sh ${out_dir} $CIRQ_PRE_RELEASE_VERSION\ntwine upload ""${out_dir}/*""\n']"
[]
"['python -m pip install --upgrade pip\npip install tox\n', 'make venv\nmake api\n', 'tox']"
"['python -m pip install --upgrade setuptools pip tox virtualenv', 'tox -e py', 'tox -e mypy', 'python -m pip install --upgrade setuptools pip tox virtualenv', 'tox -e py', 'tox -e mypy', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
"['./setup.sh\n', 'git submodule update --init --recursive\n. venv/bin/activate && python manage.py test\n', 'bash integration-test.sh || exit 1\n', './setup.sh\n', 'docker run \\\n --env SECURE_LOG_LEVEL=${SECURE_LOG_LEVEL} \\\n -v ""$PWD:/code"" \\\n -v /var/run/docker.sock:/var/run/docker.sock \\\n registry.gitlab.com/gitlab-org/security-products/dependency-scanning:latest /code\n ls\n\n sudo apt-get install jq\n sudo pip install archerysec-cli\n\n DATE=`date +%Y-%m-%d`\n export PROJECT_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} --createproject \\\n--project_name=devsecops --project_disc=""devsecops project"" --project_start=${DATE} \\\n--project_end=${DATE} --project_owner=dev | tail -n1 | jq \'.project_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n\n # Upload Scan report in archerysec\n\n export SCAN_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} \\\n --upload --file_type=JSON --file=gl-dependency-scanning-report.json --TARGET=${GITHUB_SHA} \\\n --scanner=gitlabsca --project_id=\'\'$PROJECT_ID\'\' | tail -n1 | jq \'.scan_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n\n echo ""Scan Report Uploaded Successfully, Scan Id:"" $SCAN_ID\n', 'docker run \\\n --volume ""$PWD:/code"" \\\n --volume /var/run/docker.sock:/var/run/docker.sock \\\n registry.gitlab.com/gitlab-org/security-products/sast:latest /app/bin/run /code\n ls\n\n     sudo apt-get install jq\n     sudo pip install archerysec-cli\n\n   DATE=`date +%Y-%m-%d`\n    export PROJECT_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} --createproject \\\n    --project_name=devsecops --project_disc=""devsecops project"" --project_start=${DATE} \\\n    --project_end=${DATE} --project_owner=dev | tail -n1 | jq \'.project_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n\n   export SCAN_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} \\\n   --upload --file_type=JSON --file=gl-sast-report.json --TARGET=${GITHUB_SHA} \\\n   --scanner=gitlabsast --project_id=\'\'$PROJECT_ID\'\' | tail -n1 | jq \'.scan_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n   echo ""Scan Report Uploaded Successfully, Scan Id:"" $SCAN_ID\n', '# create wrk folder\nmkdir wrk\nchmod 777 wrk\n\ndocker run \\\n --volume $(pwd)/wrk:/output:rw \\\n --volume $(pwd)/wrk:/zap/wrk:rw \\\n registry.gitlab.com/gitlab-org/security-products/dast:latest /analyze -t ${ARCHERYSEC_HOST} \\\n--auth-url ${ARCHERYSEC_AUTH_HOST} \\\n--auth-username ${ARCHERYSEC_USER} \\\n--auth-password ${ARCHERYSEC_PASS} \\\n--auth-username-field \'username\' \\\n--auth-password-field \'password\' \\\n--auth-exclude-urls ${ARCHERYSEC_EXCLUDE_HOST} \\\n--full-scan True \\\n -x report.xml\n\nsudo apt-get install jq\nsudo pip install archerysec-cli\n\nDATE=`date +%Y-%m-%d`\nexport PROJECT_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} --createproject \\\n--project_name=devsecops --project_disc=""devsecops project"" --project_start=${DATE} \\\n--project_end=${DATE} --project_owner=dev | tail -n1 | jq \'.project_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n\n\nexport SCAN_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} \\\n--upload --file_type=XML --file=wrk/report.xml --TARGET=${GITHUB_SHA} \\\n--scanner=zap_scan --project_id=\'\'$PROJECT_ID\'\' | tail -n1 | jq \'.scan_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n\necho ""Scan Report Uploaded Successfully, Scan Id:"" $SCAN_ID\n', 'sudo apt-get install jq\nsudo pip install archerysec-cli\n\nDATE=`date +%Y-%m-%d`\nexport PROJECT_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} --createproject \\\n--project_name=devsecops --project_disc=""devsecops project"" --project_start=${DATE} \\\n--project_end=${DATE} --project_owner=dev | tail -n1 | jq \'.project_id\' | sed -e \'s/^""//\' -e \'s/""$//\'`\n\n\nexport SCAN_ID=`archerysec-cli -s ${ARCHERYSEC_HOST} -u ${ARCHERYSEC_USER} -p ${ARCHERYSEC_PASS} \\\n--openvasscan --target_ip=${ARCHERYSEC_TARGET_HOST} --project_id=${PROJECT_ID}`\n']"
"['make init\n', 'make ci\n', 'pip install flake8\n', 'make lint', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nmake init\nmake ci\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install .\n', 'make validate-examples\n']"
"['pip install -r docs/requirements.txt', 'pip install -e .', 'python docs/scripts/gen_mds.py', 'python docs/scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'pip install -r docs/requirements.txt', 'pip install -e .', 'python docs/scripts/gen_mds.py', 'python docs/scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'python -m pip install -U build', 'python -m build --sdist --wheel --outdir dist/ .', 'docker build -f py.Dockerfile \\\n  --build-arg PYTHON_VERSION=${{ matrix.python-version }} \\\n  --tag minigrid-docker .      \n', 'docker run minigrid-docker pytest', 'docker run minigrid-docker pytest --doctest-modules minigrid/', 'pip install -r docs/requirements.txt', 'pip install -e .', 'python docs/scripts/gen_mds.py', 'python docs/scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'pip install pre-commit', 'pre-commit --version', 'pre-commit install', 'pre-commit run --all-files']"
"['python -m pip install --upgrade pip\npip install flake8 pytest cython\npip install aiohttp==3.8.1\npip install pyyaml\npython setup.py develop\n', 'pytest tests/\n']"
"['sudo rm -rf /usr/share/dotnet\n', 'conda install conda-build conda-verify --yes\n', 'bash .github/workflows/cuda/${{ matrix.cuda-version }}-${{ runner.os }}.sh\n', 'FORCE_CUDA=0 TORCH_CUDA_ARCH_LIST=0 ./conda/pytorch-scatter/build_conda.sh ${{ matrix.python-version }} ${{ matrix.torch-version }} ${{ matrix.cuda-version }}\n', 'source .github/workflows/cuda/${{ matrix.cuda-version }}-${{ runner.os }}-env.sh\n./conda/pytorch-scatter/build_conda.sh ${{ matrix.python-version }} ${{ matrix.torch-version }} ${{ matrix.cuda-version }}\n', 'conda install anaconda-client --yes\nanaconda upload --force --label main $HOME/conda-bld/*/*.tar.bz2\n', 'conda install anaconda-client --yes\nanaconda upload --force --label main $HOME/conda-bld/*/*.tar.bz2\n', 'pip install --upgrade setuptools\n', 'sudo rm -rf /usr/share/dotnet\n', 'bash .github/workflows/cuda/${{ matrix.cuda-version }}-${{ runner.os }}.sh\n', 'pip install torch==${{ matrix.torch-version }} --extra-index-url https://download.pytorch.org/whl/${{ matrix.cuda-version }}\npython -c ""import torch; print(\'PyTorch:\', torch.__version__)""\npython -c ""import torch; print(\'CUDA:\', torch.version.cuda)""\n', ""Torch_DIR=`python -c 'import os; import torch; print(os.path.dirname(torch.__file__))'`\nsed -i '31,38c\\\nTORCH_API void lazy_init_num_threads();' ${Torch_DIR}/include/ATen/Parallel.h\n"", 'VERSION=`sed -n ""s/^__version__ = \'\\(.*\\)\'/\\1/p"" torch_scatter/__init__.py`\nTORCH_VERSION=`echo ""pt${{ matrix.torch-version }}"" | sed ""s/..$//"" | sed ""s/\\.//g""`\nCUDA_VERSION=`echo ${{ matrix.cuda-version }}`\necho ""New version name: $VERSION+$TORCH_VERSION$CUDA_VERSION""\nsed -i ""s/$VERSION/$VERSION+$TORCH_VERSION$CUDA_VERSION/"" setup.py\nsed -i ""s/$VERSION/$VERSION+$TORCH_VERSION$CUDA_VERSION/"" torch_scatter/__init__.py\n', 'FORCE_ONLY_CPU=1 pip install -e .\n', 'source .github/workflows/cuda/${{ matrix.cuda-version }}-${{ runner.os }}-env.sh\npip install -e .\n', 'python -c ""import torch_scatter; print(\'torch-scatter:\', torch_scatter.__version__)""\n', 'pip install wheel\npython setup.py bdist_wheel --dist-dir=dist\n', 'aws s3 sync dist s3://data.pyg.org/whl/torch-${{ matrix.torch-version }}+${{ matrix.cuda-version }} --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers\n', 'pip install flake8\n', 'flake8 .\n', 'pip install torch==${{ matrix.torch-version }} --extra-index-url https://download.pytorch.org/whl/cpu\n', ""Torch_DIR=`python -c 'import os; import torch; print(os.path.dirname(torch.__file__))'`\nsed -i '31,38c\\\nTORCH_API void lazy_init_num_threads();' ${Torch_DIR}/include/ATen/Parallel.h\n"", 'pip install -e .[test]\n', 'pytest --cov --cov-report=xml\n']"
"['python -m pip install black isort', 'black --check .', 'isort --check .', 'poetry install --no-interaction', 'source .venv/bin/activate\npytest ./tests -m fast \n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run pytest ./tests -m fast --cov=./src/innvestigate --cov-report=xml']"
['docker build . --file Dockerfile --tag my-image-name:$(date +%s)']
"['curl -qL https://github.com/jedisct1/dnscrypt-proxy/releases/download/2.1.3/dnscrypt-proxy-linux_x86_64-2.1.3.tar.gz | tar xzvf -\ncp v3/* linux-x86_64\ncd linux-x86_64\ncp example-dnscrypt-proxy.toml dnscrypt-proxy.toml\nsed -i -e \'s/listen_addresses.*/listen_addresses = [""127.0.0.1:5300""]/\' dnscrypt-proxy.toml\nsed -i -e \'s/require_dnssec.*/require_dnssec = false/\' dnscrypt-proxy.toml\nsed -i -e \'s/require_nolog.*/require_nolog = false/\' dnscrypt-proxy.toml\nsed -i -e \'s/http3.*/http3 = true/\' dnscrypt-proxy.toml\nsed -i -e \'s/require_nofilter.*/require_nofilter = false/\' dnscrypt-proxy.toml\n./dnscrypt-proxy -list-all\n./dnscrypt-proxy -show-certs\n', '.github/workflows/prcheck.sh', '.github/workflows/resolverscheck.sh']"
"['gem install github_changelog_generator', 'github_changelog_generator \\\n  -u ${{ github.repository_owner }} \\\n  -p ${{ github.event.repository.name }} \\\n  --token ${{ secrets.GITHUB_TOKEN }} \\\n  --output CHANGELOG.md\n', 'git add CHANGELOG.md\ngit config --local user.email ""github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\nif git diff --quiet && git diff --staged --quiet; then\n  echo ""No changes in CHANGELOG.md, skipping commit.""\n  echo ""commit_status=skipped"" >> $GITHUB_ENV\nelse\n  git commit -m ""Update CHANGELOG.md""\n  echo ""commit_status=committed"" >> $GITHUB_ENV\nfi\n', 'cat CHANGELOG.md', 'pip install nox', 'nox -e coverage', 'echo ""::add-matcher::.github/workflows/matchers/pytest.json""', 'pip install nox pytest-github-actions-annotate-failures', 'nox -e ""pytest-${{ matrix.python-version }}(all_deps=False)""', 'nox -e ""pytest-${{ matrix.python-version }}(all_deps=True)""', 'python -m pip install --upgrade pip\npip install setuptools wheel twine build\n', 'python -m build\ntwine upload dist/*\n', 'pip install nox', 'nox -e pytest_typeguard']"
""
"['python -m pip install --upgrade pip\npip install build setuptools wheel twine\n', 'python -m build\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -e .[testing]\n', 'pytest --cov=kneed --cov-report=xml:coverage1.xml tests/test_sample.py\npip uninstall -y matplotlib\npytest --cov=kneed --cov-report=xml:coverage2.xml tests/test_no_matplotlib.py\n']"
"['echo ::set-output name=PROTOBUF_VERSION::$(grep ""^protobuf>="" test_requirements.txt | cut -f2 -d=)\necho ::set-output name=PYRIGHT_VERSION::$(grep \'""pyright""\' .github/package.json | cut -d\\"" -f4)\n', 'if [ ! -e ~/.pyenv/bin/pyenv ]; then\n  rm -rf ~/.pyenv\n  curl https://pyenv.run | bash\nfi\n\nexport PATH=""$HOME/.pyenv/bin:$PATH""\neval ""$(pyenv init -)""\neval ""$(pyenv init --path)""\neval ""$(pyenv virtualenv-init -)""\n\nfor PY in ${{matrix.py-ver-mypy-protobuf}} ${{env.PY_VER_MYPY}} ${{env.PY_VER_UNIT_TESTS_3}}; do\n  if [ ! -e ~/.pyenv/versions/$PY ]; then\n    pyenv install --skip-existing $PY\n    pyenv shell $PY\n    python -m pip install virtualenv\n  fi\ndone\n', 'export PATH=""$HOME/.pyenv/bin:$PATH""\neval ""$(pyenv init -)""\neval ""$(pyenv init --path)""\neval ""$(pyenv virtualenv-init -)""\n./run_test.sh\n', ""pip3 install black isort flake8-pyi flake8-noqa flake8-bugbear\nblack --check --extend-exclude '(_pb2_grpc|_pb2).pyi?$' .\nisort --check . --diff\nflake8 .\n"", 'echo ::set-output name=PROTOBUF_VERSION::$(grep ""^protobuf>="" test_requirements.txt | cut -f2 -d=)', 'pip3 install -e .\nmkdir wintestout\nprotoc --python_out=wintestout --mypy_out=wintestout proto\\mypy_protobuf\\extensions.proto\n']"
""
""
[]
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install twine setuptools wheel --upgrade\n', 'if pip search thop | grep -o ""\\((.*)\\)"" | xargs python .github/workflows/date_extraction.py -m ; then\n  echo ""There has been more than one week since last update, start to build.""\n  python setup.py bdist_wheel\n  twine check dist/*\n  twine upload dist/*\nelse\n  echo ""Latest update within one week, skip the build""\nfi']"
""
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install git+https://github.com/amaranth-lang/amaranth.git\n', 'python -m pip install -e ./software[toolchain]', 'python -W ignore::DeprecationWarning test.py -v', 'sudo apt-get update\nsudo apt-get install sdcc\n', 'make', 'make']"
[]
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*.tar.gz\ntwine upload dist/*.whl\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\n', 'python -m pycm test\npython -m pycm\n', 'python Otherfiles/requirements-splitter.py\npip install --upgrade --upgrade-strategy=only-if-needed -r test-requirements.txt\n', 'python -m pytest --cov=pycm --cov-report=term\n', 'python Otherfiles/version_check.py\n', 'pip install notebook>=5.2.2\npython Otherfiles/notebook_check.py\n', 'python -m vulture pycm/ Otherfiles/ setup.py --min-confidence 65 --exclude=__init__.py --sort-by-size\npython -m bandit -r pycm -s B311\npython -m pydocstyle -v --match-dir=pycm\n', 'python -m cProfile -s cumtime pycm/pycm_profile.py\n']"
"['python -m pip install --upgrade pip wheel\npip install -r requirements.txt\npip install -e .\n', 'pytest tests --cov=./gita --cov-report=xml\n']"
[]
"['sudo apt-get install libsndfile1-dev\n', 'python -m pip install --upgrade pip\npython -m pip install .[docs]\n', ""find ./docs -name '*.ipynb' | xargs -P 3 -I % jupyter nbconvert --inplace --to notebook --ExecutePreprocessor.kernel_name=python --execute %\n"", 'mkdocs build\n', 'sudo apt-get install libsndfile1-dev\n', 'python -m pip install --upgrade pip\npython -m pip install .[docs]\n', ""find ./docs -name '*.ipynb' | xargs -P 3 -I % jupyter nbconvert --inplace --to notebook --ExecutePreprocessor.kernel_name=python --execute %\n"", 'mkdocs build\n', 'python -m pip install --upgrade pip\npython -m pip install .[test]\n', 'python -m pytest --no-cov\n', 'python -m pip install --upgrade pip\npython -m pip install .[test,display]\n', 'python -m pytest --cov --cov-report=xml --cov-report=term:skip-covered\n', 'python -m pip install build twine', 'python -m build --sdist', 'twine check dist/*', 'python -m pip install cibuildwheel', 'python -m cibuildwheel --output-dir wheelhouse', 'python -m pip install cibuildwheel', 'python -m cibuildwheel --output-dir wheelhouse']"
"['if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  docker build . --file Dockerfile\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine build\nnpm install\n', 'npm run build\npython -m build\ntwine upload dist/*\n']"
"['pip install -r requirements-app.txt', 'python ./scripts/index_gen.py', 'python -m pip install --upgrade pip wheel setuptools\npip install -r requirements.txt\n', 'pip install flake8\nflake8 -v --count --show-source --statistics\n', 'python setup.py clean bdist_wheel', 'pip install dist/lightnovel_crawler*.whl\nlncrawl --list-sources\n', '[ ""${GITHUB_REF##*/}"" == ""v$(head -n 1 lncrawl/VERSION)"" ] || exit 100\n', 'python -m pip install -U pip wheel setuptools\npip install -r requirements.txt\n', 'pip install flake8\nflake8 -v --count --show-source --statistics\n', 'python setup.py clean bdist_wheel', ""pip install dist/lightnovel_crawler*.whl\nlncrawl --list-sources\nlncrawl -s 'https://novelfull.com/the-kings-avatar-for-the-glory.html' --first 2 -f --format epub --suppress -lll\n"", 'choco install -y --no-progress openssl', '.\\scripts\\build.bat', '.\\dist\\lncrawl.exe --list-sources', 'sh ./scripts/build.sh', './dist/lncrawl --list-sources', 'sh ./scripts/rebrandly.sh', 'sh ./scripts/publish.sh']"
"['git submodule update --init', 'git submodule update --init', 'pip install -e .[test]', 'make style', 'git submodule update --init', 'pip install -e .[test]', 'make test-functional', 'make test-unit', 'git submodule update --init', 'git fetch --unshallow', 'git submodule update --init', 'make set-dev-version', 'pip install twine==3.4.*', 'twine upload wheelhouse/*', 'git submodule update --init', 'make set-dev-version', 'python setup.py sdist', 'pip install twine==3.4.*', 'twine upload dist/*', 'git submodule update --init', 'pip install -e .[test]', 'make style', 'git submodule update --init', 'pip install -e .[test]', 'make test-functional', 'make test-unit', 'git submodule update --init', 'git fetch --unshallow', 'git submodule update --init', 'python setup.py sdist', 'pip install twine==3.4.*', 'twine upload dist/*', 'echo ""PKG_VERSION=$(make -s get-version)"" >> $GITHUB_ENV', 'git submodule update --init', 'pip install twine==3.4.*', 'twine upload wheelhouse/*']"
"[""sed -i -e 's|FROM archlinux:base-devel|FROM actionless/pikaur|' Dockerfile"", './maintenance_scripts/docker_test.sh --coveralls']"
""
"['sudo add-apt-repository --yes ppa:inkscape.dev/stable\nsudo apt update\nsudo apt install -y texlive-latex-base texlive-fonts-recommended texlive-lang-cyrillic cm-super\nsudo apt install inkscape\n# Needed for xmllint:\nsudo apt install libxml2-utils\npython -m pip install --upgrade pip\npip install flake8 pytest\n# Needed by Inkscape extensions and image processing:\npip install lxml cssselect Pillow\n', 'python test_installation_script.py 2> /dev/null\npython setup.py\nwget https://gitlab.com/inkscape/extensions/raw/master/inkscape.extension.rng\n', 'export PYTHONPATH=""`inkscape --system-data-directory`/extensions:$HOME/.config/inkscape/extensions/""\npython -m pytest --verbose -s pytests\nxmllint --noout --relaxng inkscape.extension.rng textext/textext.inx\n']"
[]
"['python -m pip install -U ""jupyterlab>=3.0,<3.6""', 'set -eux\npython -m pip install .\n\n#jupyter server extension list\n#jupyter server extension list 2>&1 | grep -ie ""jupyros.*OK""\n\njupyter labextension list\njupyter labextension list 2>&1 | grep -ie ""@robostack/jupyter-ros.*OK""\npython -m jupyterlab.browser_check\n', 'set -eux\npip install build\npython -m build\npip uninstall -y ""jupyros"" jupyterlab\n', 'set -eux\n# Remove NodeJS, twice to take care of system and locally installed node versions.\nsudo rm -rf $(which node)\nsudo rm -rf $(which node)\npip install ""jupyterlab>=3.0,<3.6"" jupyros*.whl\n\n#jupyter server extension list\n#jupyter server extension list 2>&1 | grep -ie ""jupyros.*OK""\n\njupyter labextension list\njupyter labextension list 2>&1 | grep -ie ""@robostack/jupyter-ros.*OK""\npython -m jupyterlab.browser_check --no-chrome-test\n', 'pip install -e .\n']"
"['npm ci && npm run prod', 'python -m pip install poetry\npoetry install --only main\npoetry run python bin/website build --local ./poetry\n', 'hugo -v --minify', 'echo version=$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"") >> $GITHUB_OUTPUT', 'curl -sSL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'echo ""$APPDATA\\Python\\Scripts"" >> $GITHUB_PATH', 'git config --system core.longpaths true', 'poetry config virtualenvs.in-project true', '# `timeout` is not available on macOS, so we define a custom function.\n[ ""$(command -v timeout)"" ] || function timeout() { perl -e \'alarm shift; exec @ARGV\' ""$@""; }\n\n# Using `timeout` is a safeguard against the Poetry command hanging for some reason.\ntimeout 10s poetry run pip --version || rm -rf .venv\n', 'poetry lock --check', 'poetry install --with github-actions', 'poetry run mypy', 'poetry run pytest --integration -v', 'echo version=$(poetry show poetry-plugin-export | grep version | cut -d : -f 2 | xargs) >> $GITHUB_OUTPUT\n', 'poetry run pytest -v poetry-plugin-export/tests/', 'git diff --exit-code --stat HEAD\ngit -C poetry-plugin-export diff --exit-code --stat HEAD\n', 'curl -sSL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry build', '[[ ""$(poetry version --short)"" =~ ^[0-9]+\\.[0-9]+\\.[0-9]+$ ]] || echo prerelease=true >> $GITHUB_OUTPUT\n', 'poetry publish', 'exit 0']"
""
"['python -m pip install build twine', 'cd pyodide-build/\npython -m build .\n', 'twine check pyodide-build/dist/*\n', 'python -m pip install build twine', 'cd src/py/\npython -m build .\n', 'twine check src/py/dist/*\n', 'TAG=${{ github.event.inputs.tag }}\nif [ -z ""$TAG"" ]; then\n  DATE=$(date +\'%Y%m%d\')\n  TAG=""$DATE-chrome${{ github.event.inputs.chrome_version }}-firefox${{ github.event.inputs.firefox_version }}""\nfi\necho ""::set-output name=tag::$TAG""\n', 'echo ${{ steps.build.outputs.digest }}', 'TAG=${{ github.event.inputs.tag }}\nif [ -z ""$TAG"" ]; then\n  DATE=$(date +\'%Y%m%d\')\n  TAG=""$DATE-chrome${{ github.event.inputs.chrome_version }}-firefox${{ github.event.inputs.firefox_version }}""\nfi\necho ""::set-output name=tag::$TAG""\n', 'echo ${{ steps.build.outputs.digest }}', 'echo ""PYVERSION=$(git grep \'export PYVERSION ?=\' Makefile.envs | cut -d\' \' -f4)""  >> ""$GITHUB_OUTPUT""\n', 'pip install -r requirements.txt -r docs/requirements-doc.txt\n', 'mkdir -p test-results\npytest docs/sphinx_pyodide/tests --junitxml=test-results/junit.xml\n', 'mkdir test-results\ncd pyodide-build && python3 -m pip install -e "".[test,deploy]"" && cd ..\npython3 -m pip install pytest-cov hypothesis pytz\n', 'PYODIDE_ROOT=. pytest \\\n    --junitxml=test-results/junit.xml \\\n    --verbose \\\n    --runtime=host \\\n    --cov=pyodide_build --cov=pyodide \\\n    src pyodide-build packages/micropip/ packages/_tests tools/\n', 'python --version\nwhich python\n', 'sudo apt install -y build-essential git xxd\n', 'brew install coreutils\n', 'pip install -r requirements.txt\n', 'which ccache\n\nccache -z\nmake -C emsdk\nccache -s\n', '# This is necessary to use the ccache from emsdk\nsource pyodide_env.sh\n\nwhich ccache\n\nccache -z\nmake -C cpython\nccache -s\n', '# This is necessary to use the ccache from emsdk\nsource pyodide_env.sh\nccache -z\nPYODIDE_PACKAGES=""tag:core,numpy"" make\nccache -s\n', 'ls -lh dist/\n\npip install brotli\n./tools/check_compressed_size.py dist/pyodide.asm.* dist/python_stdlib*\n', 'pip install -r requirements.txt\npip install -e ./pyodide-build\npython -m playwright install\n', 'ls -lh\nls -lh dist/\ntools/pytest_wrapper.py src packages/micropip/ \\\n  -v \\\n  --runtime=""${BROWSER}-no-host"" \\\n  --runner ""${RUNNER}"" \\\n  --durations 50 \\\n  --junitxml=test-results/core_test.xml\n', 'ls -lh\nls -lh dist/\ntools/pytest_wrapper.py packages/*/test* \\\n  -v \\\n  -k ""numpy and not joblib"" \\\n  --runtime=""${BROWSER}-no-host"" \\\n  --runner ""${RUNNER}"" \\\n  --durations 50 \\\n  --junitxml=test-results/packages_test.xml\n', 'curl -fsSL https://deno.land/install.sh | sudo DENO_INSTALL=/usr/local sh -s v1.33.1\ndeno --version\n', 'cd src/test-deno && deno task test']"
"['sudo apt-get update\nsudo apt-get -y install openmpi-bin libopenmpi-dev libopenblas-dev\n', 'python -m pip install -U pip\npip install --progress-bar off -U .[benchmark]\npip install --progress-bar off -U .[checking]\npip install --progress-bar off -U .[integration] --extra-index-url https://download.pytorch.org/whl/cpu\npip install --progress-bar off -U .[optional]\npip install --progress-bar off -U .[test]\npip install --progress-bar off -U bayesmark\npip install --progress-bar off -U kurobako\npip install ""sqlalchemy<2.0.0""\npip install ""pytorch-lightning<2.0.0""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'black . --check --diff', 'flake8 .', 'isort . --check --diff', 'mypy . --warn-unused-ignores', 'blackdoc . --check --diff', 'python -m pip install -U pip\npip install --progress-bar off -U .[checking]\npip install ""sqlalchemy<2.0.0""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'black . --check --diff', 'flake8 .', 'isort . --check --diff', 'mypy .', 'blackdoc . --check --diff', 'sudo apt-get update\nsudo apt-get -y install openmpi-bin libopenmpi-dev libopenblas-dev\n', 'python -m pip install --upgrade pip\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install --progress-bar off .[integration] --extra-index-url https://download.pytorch.org/whl/cpu\npip install ""pytorch-lightning<2.0.0""\npip install ""fakeredis<2.11.1""\n\necho \'import coverage; coverage.process_startup()\' > sitecustomize.py\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'coverage run --source=optuna -m pytest tests -m ""not skip_coverage and not slow""\ncoverage combine\ncoverage xml\n', 'export OMPI_MCA_rmaps_base_oversubscribe=yes\nmpirun -n 2 -- coverage run -m pytest tests/integration_tests/test_pytorch_distributed.py\ncoverage combine --append\ncoverage xml\n', 'export TAG_NAME=""py${{ matrix.python_version }}""\nif [ ""${{ github.event_name }}"" = \'release\' ]; then\n  export TAG_NAME=""${{ github.event.release.tag_name }}-${TAG_NAME}""\nfi\nif [ ""${{matrix.build_type}}"" = \'dev\' ]; then\n  export TAG_NAME=""${TAG_NAME}-dev""\nfi\necho ""HUB_TAG=${DOCKER_HUB_BASE_NAME}:${TAG_NAME}"" >> $GITHUB_ENV\n', 'if [ ""${{ github.event_name }}"" = \'release\' ]; then\n  # Cache is not available because the image tag includes the Optuna version.\n  CACHE_FROM=""""\nelse\n  CACHE_FROM=""--cache-from=${HUB_TAG}""\nfi\ndocker build ${CACHE_FROM} . --build-arg PYTHON_VERSION=${{ matrix.python_version }} --build-arg BUILD_TYPE=${{ matrix.build_type }} --file Dockerfile --tag ""${HUB_TAG}""\n', 'docker run ""${HUB_TAG}"" sh -c ""pip freeze --all""\n', 'docker run ""${HUB_TAG}"" sh -c ""pip install pipdeptree && pipdeptree""\n', 'docker run ""${HUB_TAG}"" optuna --version\n', 'echo ""${DOCKER_HUB_TOKEN}"" | docker login -u optunabot --password-stdin\ndocker push ""${HUB_TAG}""\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install ""pytorch-lightning<2.0.0""\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest tests -m ""not integration""\n', 'pytest tests -m ""not integration and not slow""\n', 'brew install libomp\nbrew install open-mpi\nbrew install openblas\n', 'python -m pip install --upgrade pip\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install --progress-bar off .[integration]\npip install ""pytorch-lightning<2.0.0""\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest tests -m ""integration""\n', 'export OMPI_MCA_rmaps_base_oversubscribe=yes\nmpirun -n 2 -- pytest tests/integration_tests/test_pytorch_distributed.py\n', ""python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools wheel\n\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[benchmark]\npip install --progress-bar off bayesmark matplotlib pandas\n"", 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'python benchmarks/run_bayesmark.py \\\n  --dataset ${{ matrix.dataset }} \\\n  --model ${{ matrix.model }} \\\n  --name-prefix """" \\\n  --budget ${{ github.event.inputs.budget }} \\\n  --n-runs ${{ github.event.inputs.n-runs }} \\\n  --sampler-list \'${{ github.event.inputs.sampler-list }}\' \\\n  --sampler-kwargs-list \'${{ github.event.inputs.sampler-kwargs-list }}\' \\\n  --pruner-list \'${{ github.event.inputs.pruner-list }}\' \\\n  --pruner-kwargs-list \'${{ github.event.inputs.pruner-kwargs-list }}\' \\\n  --plot-warmup \'${{ github.event.inputs.plot-warmup }}\'\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools wheel\npip install --progress-bar off numpy scipy pandas Jinja2\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'python benchmarks/bayesmark/report_bayesmark.py\n', 'sudo apt update\nsudo apt -y install gnuplot\n', ""python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[benchmark]\n\npip install --progress-bar off kurobako\n"", 'curl -L https://github.com/optuna/kurobako/releases/download/0.2.9/kurobako-0.2.9.linux-amd64 -o kurobako\n\nchmod +x kurobako\n./kurobako -h\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'wget http://ml4aad.org/wp-content/uploads/2019/01/fcnet_tabular_benchmarks.tar.gz\ntar xf fcnet_tabular_benchmarks.tar.gz\n', 'curl -L $(./kurobako dataset nasbench url) -o nasbench_full.tfrecord\n./kurobako dataset nasbench convert nasbench_full.tfrecord nasbench_full.bin\n', 'python benchmarks/run_kurobako.py \\\n  --path-to-kurobako ""."" \\\n  --name-prefix """" \\\n  --budget ${{ github.event.inputs.budget }} \\\n  --n-runs ${{ github.event.inputs.n-runs }} \\\n  --n-jobs 10 \\\n  --n-concurrency ${{ github.event.inputs.n-concurrency }} \\\n  --sampler-list \'${{ github.event.inputs.sampler-list }}\' \\\n  --sampler-kwargs-list \'${{ github.event.inputs.sampler-kwargs-list }}\' \\\n  --pruner-list \'${{ github.event.inputs.pruner-list }}\' \\\n  --pruner-kwargs-list \'${{ github.event.inputs.pruner-kwargs-list }}\' \\\n  --seed 0 \\\n  --data-dir ""."" \\\n  --out-dir ""out""\n', 'sudo apt update\nsudo apt -y install gnuplot\n', ""python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[benchmark]\n\npip install --progress-bar off kurobako\n"", 'curl -L https://github.com/optuna/kurobako/releases/download/0.2.9/kurobako-0.2.9.linux-amd64 -o kurobako\n\nchmod +x kurobako\n./kurobako -h\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'curl -L $(./kurobako dataset nasbench url) -o nasbench_full.tfrecord\n./kurobako dataset nasbench convert nasbench_full.tfrecord nasbench_full.bin\n', 'python benchmarks/run_mo_kurobako.py \\\n  --path-to-kurobako ""."" \\\n  --name-prefix """" \\\n  --budget ${{ github.event.inputs.budget }} \\\n  --n-runs ${{ github.event.inputs.n-runs }} \\\n  --n-jobs 10 \\\n  --n-concurrency ${{ github.event.inputs.n-concurrency }} \\\n  --sampler-list \'${{ github.event.inputs.sampler-list }}\' \\\n  --sampler-kwargs-list \'${{ github.event.inputs.sampler-kwargs-list }}\' \\\n  --seed 0 \\\n  --data-dir ""."" \\\n  --out-dir ""out""\n', 'sudo apt update\nsudo apt -y install gnuplot\n', ""python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off kurobako\n"", '\ncurl -L https://github.com/optuna/kurobako/releases/download/0.2.10/kurobako-0.2.10.linux-amd64 -o kurobako\nchmod +x kurobako\n./kurobako -h\n', 'curl -L https://github.com/automl/NASLib/archive/refs/heads/Develop.zip -o NASLib-Develop.zip\nunzip NASLib-Develop.zip\nmv NASLib-Develop NASLib\ncd NASLib\npip install --upgrade pip setuptools wheel\npip install -e .\ncd ..\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', ""\ncd NASLib/naslib/data\nfunction wget_gdrive {\n\n  GDRIVE_FILE_ID=$1\n  DEST_PATH=$2\n\n  wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$GDRIVE_FILE_ID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n  wget --load-cookies cookies.txt -O $DEST_PATH 'https://docs.google.com/uc?export=download&id='$GDRIVE_FILE_ID'&confirm='$(<confirm.txt)\n  rm -fr cookies.txt confirm.txt\n}\nwget_gdrive 1sh8pEhdrgZ97-VFBVL94rI36gedExVgJ nb201_cifar10_full_training.pickle\ncd ../../../\n"", ""\ncd NASLib/naslib/data\nfunction wget_gdrive {\n\n  GDRIVE_FILE_ID=$1\n  DEST_PATH=$2\n\n  wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$GDRIVE_FILE_ID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n  wget --load-cookies cookies.txt -O $DEST_PATH 'https://docs.google.com/uc?export=download&id='$GDRIVE_FILE_ID'&confirm='$(<confirm.txt)\n  rm -fr cookies.txt confirm.txt\n}\nwget_gdrive 1hV6-mCUKInIK1iqZ0jfBkcKaFmftlBtp nb201_cifar100_full_training.pickle\ncd ../../../\n"", ""\ncd NASLib/naslib/data\nfunction wget_gdrive {\n\n  GDRIVE_FILE_ID=$1\n  DEST_PATH=$2\n\n  wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$GDRIVE_FILE_ID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n  wget --load-cookies cookies.txt -O $DEST_PATH 'https://docs.google.com/uc?export=download&id='$GDRIVE_FILE_ID'&confirm='$(<confirm.txt)\n  rm -fr cookies.txt confirm.txt\n}\nwget_gdrive 1FVCn54aQwD6X6NazaIZ_yjhj47mOGdIH nb201_ImageNet16_full_training.pickle\ncd ../../../\n"", 'python benchmarks/run_naslib.py \\\n  --path-to-kurobako ""."" \\\n  --name-prefix """" \\\n  --budget ${{ github.event.inputs.budget }} \\\n  --n-runs ${{ github.event.inputs.n-runs }} \\\n  --n-jobs 6 \\\n  --n-concurrency ${{ github.event.inputs.n-concurrency }} \\\n  --sampler-list \'${{ github.event.inputs.sampler-list }}\' \\\n  --sampler-kwargs-list \'${{ github.event.inputs.sampler-kwargs-list }}\' \\\n  --pruner-list \'${{ github.event.inputs.pruner-list }}\' \\\n  --pruner-kwargs-list \'${{ github.event.inputs.pruner-kwargs-list }}\' \\\n  --seed 0 \\\n  --out-dir ""out""\n', 'python -m pip install -U pip\npython -m pip install -U twine wheel build\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'OPTUNA_VERSION=$(cut -d \'""\' -f 2 optuna/version.py)\nDATE=`date +""%Y%m%d""`\necho ""__version__ = \\""${OPTUNA_VERSION}${DATE}\\"""" > optuna/version.py\n', 'python -m build --sdist --wheel\n', 'twine check dist/*', 'python -m pip install -U pip\npip install --progress-bar off -U .[checking]\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'black .\nblackdoc .\nisort .\n', ""python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[benchmark]\nasv machine --yes\n"", 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'asv run --strict\n', 'python -m pip install -U pip\npip install --progress-bar off -U .[document]\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'cd docs\nmake html\ncd ../\n', 'python -m pip install -U pip\npip install --progress-bar off -U .[document]\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'cd docs\nmake doctest\n', 'sudo apt-get update\nsudo apt-get -y install openmpi-bin libopenmpi-dev libopenblas-dev\n', 'python -m pip install --upgrade pip\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install --progress-bar off .[integration] --extra-index-url https://download.pytorch.org/whl/cpu\npip install ""pytorch-lightning<2.0.0""\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'if [ ${{ matrix.python-version }} == 3.11 ]; then\n  pytest tests -m ""integration"" \\\n    --ignore tests/integration_tests/test_botorch.py \\\n    --ignore tests/integration_tests/test_fastaiv2.py \\\n    --ignore tests/integration_tests/test_lightgbm.py \\\n    --ignore tests/integration_tests/test_mxnet.py \\\n    --ignore tests/integration_tests/test_pytorch_distributed.py \\\n    --ignore tests/integration_tests/test_pytorch_ignite.py \\\n    --ignore tests/integration_tests/test_pytorch_lightning.py \\\n    --ignore tests/integration_tests/test_shap.py \\\n    --ignore tests/integration_tests/test_skopt.py \\\n    --ignore tests/integration_tests/test_tensorboard.py \\\n    --ignore tests/integration_tests/lightgbm_tuner_tests \\\n    --ignore tests/importance_tests/test_init.py \\\n    --ignore tests/samplers_tests/test_samplers.py\nelse\n  pytest tests -m ""integration""\nfi\n', 'if [ ${{ matrix.python-version }} == 3.11 ]; then\n  pytest tests -m ""integration and not slow"" \\\n    --ignore tests/integration_tests/test_botorch.py \\\n    --ignore tests/integration_tests/test_fastaiv2.py \\\n    --ignore tests/integration_tests/test_lightgbm.py \\\n    --ignore tests/integration_tests/test_mxnet.py \\\n    --ignore tests/integration_tests/test_pytorch_distributed.py \\\n    --ignore tests/integration_tests/test_pytorch_ignite.py \\\n    --ignore tests/integration_tests/test_pytorch_lightning.py \\\n    --ignore tests/integration_tests/test_shap.py \\\n    --ignore tests/integration_tests/test_skopt.py \\\n    --ignore tests/integration_tests/test_tensorboard.py \\\n    --ignore tests/integration_tests/lightgbm_tuner_tests \\\n    --ignore tests/importance_tests/test_init.py \\\n    --ignore tests/samplers_tests/test_samplers.py\nelse\n  pytest tests -m ""integration and not slow""\nfi\n', 'sudo apt-get update\nsudo apt-get -y install openmpi-bin libopenmpi-dev\n', ""python -m pip install --upgrade pip\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install --progress-bar off .[integration] --extra-index-url https://download.pytorch.org/whl/cpu\n"", 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'export OMPI_MCA_rmaps_base_oversubscribe=yes\n\nif [ ${{ matrix.python-version }} != 3.11 ]; then\n  mpirun -n 2 -- pytest tests/integration_tests/test_pytorch_distributed.py\nfi\n', 'sudo apt-get update\nsudo apt-get -y install openmpi-bin libopenmpi-dev\n', ""python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\n"", 'pip install --progress-bar off PyMySQL cryptography psycopg2-binary redis\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest tests/storages_tests/test_with_server.py\n', 'pytest tests/storages_tests/test_with_server.py\n', 'pytest tests/storages_tests/test_with_server.py\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n', ""# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c 'import optuna'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\n"", '# Install dependencies with minimum versions.\npip uninstall -y alembic cmaes packaging sqlalchemy plotly scikit-learn\npip install alembic==1.5.0 cmaes==0.9.1 packaging==20.0 sqlalchemy==1.3.0 numpy==1.20.3 tqdm==4.27.0 colorlog==0.3 PyYAML==5.1\npip install plotly==5.0.0 scikit-learn==0.24.2  # optional extras\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest tests -m ""not integration""\n', 'pytest tests -m ""not integration and not slow""\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n', '# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\n\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest tests -m ""not integration""\n', 'pytest tests -m ""not integration and not slow""\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install ""pytorch-lightning<2.0.0""\npip install PyQt6 # Install PyQT for using QtAgg as matplotlib backend.\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest -m ""not integration""\n', 'pytest -m ""not integration and not slow""\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n# Install minimal dependencies and confirm that `import optuna` is successful.\npip install --progress-bar off .\npython -c \'import optuna\'\noptuna --version\npip install --progress-bar off .[test]\npip install --progress-bar off .[optional]\npip install --progress-bar off .[integration]\npip install ""pytorch-lightning<2.0.0""\npip install ""distributed<2023.3.2""\npip install ""fakeredis<2.11.1""\n', 'pip freeze --all\n', 'pip install pipdeptree\npipdeptree\n', 'pytest tests -m ""integration"" `\n  --ignore tests/integration_tests/allennlp_tests/test_allennlp.py `\n  --ignore tests/integration_tests/test_mxnet.py\n']"
"['pip install tox', 'tox']"
"['sudo apt-get update\nsudo apt-get install -y --no-install-recommends \\\n  python3-opencv python3-pil python3-tqdm python3-dev \\\n  libvulkan-dev glslang-dev glslang-tools swig\npip install wheel\n', 'pip wheel -w /tmp/wheels \\\n  rife-ncnn-vulkan-python@git+https://github.com/media2x/rife-ncnn-vulkan-python.git .\n', 'tar cJvf /tmp/video2x-nightly-wheels.txz /tmp/wheels/*\n', 'echo ::set-output name=tag::${GITHUB_REF/refs\\/tags\\//}']"
"['exit 1', 'echo ""$GITHUB_CONTEXT""\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\npr=$(head -n 1 artifact/pr.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\necho ""PR=$pr"" >> $GITHUB_ENV\n', 'pip install --upgrade --force-reinstall --no-deps .\npip install boto3\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-ModelZooMxnet-$THIS_REF"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/gpu_test.sh && ./.github/workflows/gpu_test.sh gluoncv tests/model_zoo"" \\\n                                   --wait\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-ModelZooMxnet-PR#$THIS_PR"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/gpu_test.sh && ./.github/workflows/gpu_test.sh gluoncv tests/model_zoo"" \\\n                                   --wait\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\npr=$(head -n 1 artifact/pr.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\necho ""PR=$pr"" >> $GITHUB_ENV\n', 'pip install --upgrade --force-reinstall --no-deps .\npip install boto3\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-ModelZooTorch-$THIS_REF"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/gpu_test.sh && ./.github/workflows/gpu_test.sh gluoncv/torch tests/model_zoo_torch"" \\\n                                   --wait\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-ModelZooTorch-PR#$THIS_PR"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/gpu_test.sh && ./.github/workflows/gpu_test.sh gluoncv/torch tests/model_zoo_torch"" \\\n                                   --wait\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\npr=$(head -n 1 artifact/pr.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\necho ""PR=$pr"" >> $GITHUB_ENV\n', 'pip install --upgrade --force-reinstall --no-deps .\npip install boto3\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-Auto-$THIS_REF"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/gpu_test.sh && ./.github/workflows/gpu_test.sh gluoncv tests/auto"" \\\n                                   --wait\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-Auto-PR#$THIS_PR"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/gpu_test.sh && ./.github/workflows/gpu_test.sh gluoncv tests/auto"" \\\n                                   --wait\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\n', 'ref=$(head -n 1 artifact/ref.atf)\nrepo=$(head -n 1 artifact/repo.atf)\npr=$(head -n 1 artifact/pr.atf)\nsha=$(head -n 1 artifact/sha.atf)\nshort_sha=$(git rev-parse --short ""$sha"")\necho ""REF=$ref"" >> $GITHUB_ENV\necho ""REPO=$repo"" >> $GITHUB_ENV\necho ""PR=$pr"" >> $GITHUB_ENV\necho ""$SHORT_SHA""\necho ""SHORT_SHA=$short_sha"" >> $GITHUB_ENV\n', 'pip install --upgrade --force-reinstall --no-deps .\npip install boto3\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-BuildDocs-$THIS_REF"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/build_docs.sh && ./.github/workflows/build_docs.sh $THIS_REF $THIS_REPO $SHORT_SHA $THIS_PR"" \\\n                                   --wait\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type ci \\\n                                   --name ""GluonCV-GPU-BuildDocs-PR#$THIS_PR"" \\\n                                   --source-ref ""$THIS_REF"" \\\n                                   --work-dir . \\\n                                   --remote ""https://github.com/$THIS_REPO"" \\\n                                   --command ""chmod +x ./.github/workflows/build_docs.sh && ./.github/workflows/build_docs.sh $THIS_REF $THIS_REPO $SHORT_SHA $THIS_PR "" \\\n                                   --wait\n', 'echo ""$GITHUB_CONTEXT""\n', 'conda env create -n gluon_cv_lint -f ./tests/pylint.yml\nconda env update -n gluon-cv-lint -f ./tests/pylint.yml --prune\nconda activate gluon-cv-lint\nconda list\nmake clean\nmake pylint\n', 'conda env create -n gluon_cv_py3_test -f tests/py3_mxnet_ci.yml\nconda activate gluon_cv_py3_test\npip install --upgrade pip setuptools wheel\npip install --upgrade -e .\nexport TINY_COCO=~/.mxnet/datasets/tiny_coco\nexport TINY_MOTORBIKE=~/.mxnet/datasets/tiny_motorbike\nmkdir -p $TINY_COCO/annotations\ncurl -s https://gluoncv-ci.s3-us-west-2.amazonaws.com/mini_coco/sub_val.zip --output sub_val.zip\nunzip -q sub_val.zip -d $TINY_COCO\nmv $TINY_COCO/sub_val $TINY_COCO/val2017\ncurl -s https://gluoncv-ci.s3-us-west-2.amazonaws.com/mini_coco/instances_val2017_tiny.json --output instances_val2017_tiny.json\nmv instances_val2017_tiny.json $TINY_COCO/annotations\ncurl -s https://gluoncv-ci.s3-us-west-2.amazonaws.com/tiny_motorbike.zip --output tiny_motorbike.zip\nunzip -q tiny_motorbike.zip -d $TINY_MOTORBIKE\nnosetests --with-timer --timer-ok 5 --timer-warning 20 -x --with-coverage --cover-package gluoncv -v tests/unittests\n', 'echo ""${{ github.ref }}""\necho ""${{ github.repository }}""\nmkdir git_context\necho ""${{ github.ref }}"" > git_context/ref.atf\necho ""${{ github.repository }}"" > git_context/repo.atf\n', 'echo ""${{ github.event.pull_request.head.ref }}""\necho ""${{ github.event.pull_request.head.repo.full_name }}""\necho ""${{ github.event.number }}""\necho ""${{ github.event.pull_request.head.sha }}""\nmkdir git_context\necho ""${{ github.event.pull_request.head.ref }}"" > git_context/ref.atf\necho ""${{ github.event.pull_request.head.repo.full_name }}"" > git_context/repo.atf\necho ""${{ github.event.number }}"" > git_context/pr.atf\necho ""${{ github.event.pull_request.head.sha }}"" > git_context/sha.atf\n']"
"['pip install . -r requirements/doc.txt\nmake -C docs html SPHINXOPTS=-W\n', 'pip install black mypy ruff', 'ruff examples src tests\nblack --check --diff examples src tests\n', 'brew update\nbrew install opus libvpx\nsudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate off\n', 'sudo apt-get update\nsudo apt-get install libopus-dev libvpx-dev\n', 'python -m pip install -U pip setuptools wheel\npip install .[dev]\ncoverage run -m unittest discover -v\ncoverage xml\n', 'pip install -U build\npython -m build --sdist\n', 'pip install cibuildwheel\ncibuildwheel --output-dir dist\n']"
""
"['echo ""::set-output name=today::$(date +\'%Y-%m-%d\')""', 'python -m pip install --upgrade pip\npip install -r requirements/test.txt\n', 'python manage_test.py makemigrations\npython manage_test.py migrate\n', 'coverage run  manage_test.py test\ncoverage report\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npip install pytest-dotenv\npip install python-dateutil\npytest tests/test_robinhood.py\npytest tests/test_gemini.py\npytest tests/test_tda.py\n']"
"['curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python - &&\\\n    poetry config virtualenvs.create false\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\n# if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npoetry install\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n# flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python - &&\\\n    poetry config virtualenvs.create false\n', 'poetry install\n', 'black .\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npip install tox==3.28.0 tox-gh-actions==2.12.0\n', 'tox', 'install.packages(\'remotes\')\nsaveRDS(remotes::dev_package_deps(dependencies = TRUE), "".github/depends.Rds"", version = 2)\n', 'Rscript -e ""remotes::install_github(\'r-hub/sysreqs\')""\nsysreqs=$(Rscript -e ""cat(sysreqs::sysreq_commands(\'DESCRIPTION\'))"")\nsudo -s eval ""$sysreqs""\nsudo apt-get install -y qpdf\n', 'remotes::install_deps(dependencies = TRUE)\n# remotes::install_github(""ModelOriented/ingredients"")\n# remotes::install_github(""ModelOriented/iBreakDown"")\nremotes::install_cran(c(""rcmdcheck"", ""glmnet"", ""parsnip"", ""ranger"", ""randomForest"", ""e1071"", ""caret"", ""gbm"", ""rms"", ""rpart"", ""kernlab""))\n', 'Sys.setenv(NOT_CRAN = ""true"")\nrcmdcheck::rcmdcheck(args = c(""--no-manual"", ""--as-cran"", ""--run-donttest"", ""--run-dontrun"", ""--no-multiarch""),\n                     build_args = ""--compact-vignettes=no"", error_on = ""warning"", check_dir = ""check"")\n', 'install.packages(\'remotes\')\nsaveRDS(remotes::dev_package_deps(dependencies = TRUE), "".github/depends.Rds"", version = 2)\n', 'Rscript -e ""remotes::install_github(\'r-hub/sysreqs\')""\nsysreqs=$(Rscript -e ""cat(sysreqs::sysreq_commands(\'DESCRIPTION\'))"")\nsudo -s eval ""$sysreqs""\nsudo apt-get install -y qpdf\n', 'remotes::install_deps(dependencies = TRUE)\nremotes::install_github(""ModelOriented/ingredients"")\nremotes::install_github(""ModelOriented/iBreakDown"")\ninstall.packages(""lava"")\ninstall.packages(c(""rcmdcheck"", ""parsnip"", ""ranger"", ""e1071"", ""caret"", ""gbm"", ""rms"", ""rpart"", ""kernlab""))\nremotes::install_version(""glmnet"", version = ""4.1-2"", repos = ""http://cran.us.r-project.org"")\n', 'Sys.setenv(NOT_CRAN = ""true"")\nremotes::install_local()\nsource(""tests/external_tests/external_test_yhat_model_info.R"")\n', 'install.packages(\'remotes\')\nsaveRDS(remotes::dev_package_deps(dependencies = TRUE), "".github/depends.Rds"", version = 2)\n', 'install.packages(""remotes"")\nremotes::install_deps(dependencies = TRUE)\ninstall.packages(""future.apply"")\ninstall.packages(c(""rms"", ""gbm"", ""caret"", ""e1071"", ""yardstick""))\ninstall.packages(""pkgdown"")\nremotes::install_github(""ModelOriented/DrWhyTemplate"")\n', 'R CMD INSTALL .', 'git config --local user.email ""actions@github.com""\ngit config --local user.name ""GitHub Actions""\nRscript -e \'pkgdown::deploy_to_branch(new_process = FALSE, run_dont_run = TRUE)\'\n', 'Rscript -e \'install.packages(c(""remotes"", ""roxygen2""))\' -e \'remotes::install_deps(dependencies = TRUE)\'', ""Rscript -e 'roxygen2::roxygenise()'"", ""git add man/\\* NAMESPACE\ngit commit -m 'Document'\n"", 'Rscript -e \'install.packages(""styler"")\'', ""Rscript -e 'styler::style_pkg()'"", ""git add \\*.R\ngit commit -m 'Style'\n"", True, 'install.packages(\'remotes\')\nsaveRDS(remotes::dev_package_deps(dependencies = TRUE), "".github/depends.Rds"", version = 2)\n', 'install.packages(c(""remotes"", ""randomForest""))\nremotes::install_deps(dependencies = TRUE)\nremotes::install_cran(""covr"")\n', 'covr::codecov()']"
"['pip install flake8 && flake8 --max-line-length 120 Varken.py varken/*.py', 'VERSION=edge\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/v}\nfi\nif [ ""${{ github.event_name }}"" = ""schedule"" ]; then\n  VERSION=nightly\nfi\nif [[ ${GITHUB_REF##*/} == ""develop"" ]]; then\n  VERSION=develop\nfi\nTAGS=""${VERSION}""\nif [[ $VERSION =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n  TAGS=""$TAGS,latest""\nfi\necho ::set-output name=version::${VERSION}\necho ::set-output name=tags::${TAGS}\necho ::set-output name=branch::${GITHUB_REF##*/}\necho ::set-output name=build_date::$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')\necho ::set-output name=vcs_ref::${GITHUB_SHA::8}\n', 'IFS=\',\' read -r -a images <<< ""${{ steps.gen-tags.outputs.fully-qualified-target-images }}""\nfor image in ""${images[@]}""; do\n    docker buildx imagetools inspect ${image}\ndone\n']"
""
"['unset LANG\nsudo apt update -qq > /dev/null 2>&1\nsudo LC_ALL=C.UTF-8 add-apt-repository ppa:ondrej/php -y > /dev/null 2>&1\nsudo rm -rf /etc/mysql /var/lib/mysql\nsudo apt-get purge --option=Dpkg::options::=--force-all --assume-yes graphviz* redis* php* mysql* nginx* > /dev/null 2>&1\nsudo apt-get install -qq git ccze tree > /dev/null 2>&1\nsudo apt-get -qq autoremove --purge > /dev/null 2>&1\nsudo bash -c \'echo -e ""[user]\\n\\tname = abc\\n\\temail = root@localhost.com"" > $HOME/.gitconfig\'\n', 'sudo timeout 1800 bash install --travis', 'sudo timeout 1800 bash tests/travis.sh --actions', 'python3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel twine\npython3 -m pip install --upgrade setuptools wheel twine     \n', 'python3 setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python3.8 -m pip install --upgrade pip\npip install -r dev-requirements.txt\npython3.8 setup.py install\n', 'export XMNLP_MODEL=/home/xmnlp/xmnlp-onnx-models\npytest --cov -vvvs tests\n', 'flake8 --config setup.cfg\n']"
"['python -m pip install --upgrade pip\npip install pipenv\npipenv sync --dev\n', 'pipenv run flake8\n', 'pipenv run isort\n', 'pipenv run test\n', 'git checkout HEAD^2', 'pip install --upgrade pip\npip install -U setuptools\n', 'pip install seqeval']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npytest\n']"
"['./ci/test.sh', './ci/update-nur.sh', './ci/update-nur-search.sh']"
"['sudo apt-get update\nsudo apt-get install -y libemail-outlook-message-perl\ncurl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.2.2-amd64.deb\nsudo dpkg -i --force-confnew elasticsearch-*.deb\nsudo systemctl restart elasticsearch\nsudo systemctl --no-pager status elasticsearch\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'cd docs\nmake html\n', 'flake8 *.py parsedmarc/*.py\n', 'coverage run tests.py\ncoverage json\n', 'python setup.py install\nparsedmarc --debug -c ci.ini samples/aggregate/*\nparsedmarc --debug -c ci.ini samples/forensic/*""\n', 'hatch build\n']"
""
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install ${{ matrix.config.torch-version }}\n# python setup.py install\npip install .\npip install -r requirements-dev.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
""
"['python -m pip install --upgrade pip setuptools wheel twine build\n', 'python -m build\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest pytest-cov\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 spellchecker/ --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 spellchecker/ --count --exit-zero --max-complexity=11 --max-line-length=127 --statistics\n', '# Run tests while also generating coverage statistics\npytest --cov=./ --cov-report=xml\n', 'python -m pip install --upgrade pip\npython -m pip install build twine\npython -m build\ntwine check dist/*\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\npython -m pip install --upgrade pytest\npython -m pip install --upgrade setuptools\npython -m pip install -r dev-requirements.txt\npython setup.py install\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'docker stop ${{ job.services.deluge-latest.id }}\ndocker stop ${{ job.services.deluge-1.id }}\nsudo sed -i \'s/""allow_remote"": false/""allow_remote"": true/g\' /tmp/deluge-latest/config/core.conf\nsudo sed -i \'s/""allow_remote"": false/""allow_remote"": true/g\' /tmp/deluge-1/config/core.conf\ndocker start ${{ job.services.deluge-latest.id }}\ndocker start ${{ job.services.deluge-1.id }}\n', 'echo ""DELUGE_2_PASSWORD=$(sudo cat /tmp/deluge-latest/config/auth | sed \'s/.*:\\(.*\\):.*/\\1/g\')"" >> $GITHUB_ENV\necho ""DELUGE_1_PASSWORD=$(sudo cat /tmp/deluge-1/config/auth | sed \'s/.*:\\(.*\\):.*/\\1/g\')"" >> $GITHUB_ENV\n', 'sleep $((10 + $RANDOM % 50))\n', 'py.test -s --cov-report=xml --cov --log-level=INFO\n']"
""
"['grep -Pz ""\\((\\n\\s*)?#${{ github.event.pull_request.number }}(\\n\\s*)?\\)"" CHANGES.md || \\\n(echo ""Please add \'(#${{ github.event.pull_request.number }})\' change line to CHANGES.md (or if appropriate, ask a maintainer to add the \'skip news\' label)"" && \\\nexit 1)\n', 'python -m pip install click packaging urllib3\npython -m pip install https://github.com/ichard26/diff-shades/archive/stable.zip\n', 'python scripts/diff_shades_gha_helper.py config ${{ github.event_name }} ${{ matrix.mode }}\n', 'python -m pip install https://github.com/ichard26/diff-shades/archive/stable.zip\npython -m pip install click packaging urllib3\n# After checking out old revisions, this might not exist so we\'ll use a copy.\ncat scripts/diff_shades_gha_helper.py > helper.py\ngit config user.name ""diff-shades-gha""\ngit config user.email ""diff-shades-gha@example.com""\n', '${{ matrix.baseline-setup-cmd }} && python -m pip install .\n', 'diff-shades analyze -v --work-dir projects-cache/ ${{ matrix.baseline-analysis }} ${{ matrix.force-flag }}\n', '${{ matrix.target-setup-cmd }} && python -m pip install .\n', 'diff-shades analyze -v --work-dir projects-cache/ ${{ matrix.target-analysis }} --repeat-projects-from ${{ matrix.baseline-analysis }} ${{ matrix.force-flag }}\n', 'diff-shades --dump-html diff.html compare --diff ${{ matrix.baseline-analysis }} ${{ matrix.target-analysis }}\n', 'python helper.py comment-body ${{ matrix.baseline-analysis }} ${{ matrix.target-analysis }} ${{ matrix.baseline-sha }} ${{ matrix.target-sha }} ${{ github.event.pull_request.number }}\n', 'diff-shades compare --check ${{ matrix.baseline-analysis }} ${{ matrix.target-analysis }} || (echo ""Please verify you didn\'t change the stable code style unintentionally!"" && exit 1)\n', 'diff-shades show-failed --check --show-log ${{ matrix.target-analysis }}\n', 'python -m pip install pip --upgrade\npython -m pip install click packaging urllib3\n', 'python scripts/diff_shades_gha_helper.py comment-details ${{github.event.workflow_run.id }}\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -e "".[d]""\npython -m pip install -r ""docs/requirements.txt""\n', 'sphinx-build -a -b html -W --keep-going docs/ docs/_build', 'echo ""GIT_TAG=$(git describe --candidates=0 --tags 2> /dev/null || echo latest_non_release)"" >> $GITHUB_ENV', 'echo ${{ steps.docker_build.outputs.digest }}', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'tox -e fuzz\n', 'if [ ""$GITHUB_BASE_REF"" != ""main"" ]; then\n    echo ""::error::PR targeting \'$GITHUB_BASE_REF\', please refile targeting \'main\'."" && exit 1\nfi\n', ""python -m pip install --upgrade pip\npython -m pip install -e '.[d]'\npython -m pip install tox\n"", 'tox -e run_self\n', 'python -m pip install --upgrade --disable-pip-version-check pip\npython -m pip install --upgrade build twine\n', 'python -m build', ""twine upload --verbose -u '__token__' dist/*"", ""pipx run twine upload --verbose -u '__token__' wheelhouse/*.whl"", 'git reset --hard ${{ github.event.release.tag_name }}\ngit push\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'tox -e ci-py -- -v --color=yes', 'tox -e ci-pypy3 -- -v --color=yes', 'python -m pip install pip --upgrade --disable-pip-version-check\npython -m pip install -e "".[uvloop]""\n', 'python -m black --check src/', 'python -m pip install --upgrade pip wheel\npython -m pip install .[colorama]\npython -m pip install pyinstaller\n', ""python -m PyInstaller -F --name ${{ matrix.asset_name }} --add-data 'src/blib2to3${{ matrix.pathsep }}blib2to3' src/black/__main__.py\n"", './dist/${{ matrix.asset_name }} --version\n./dist/${{ matrix.asset_name }} src --verbose\n']"
"['pip install -r requirements.txt\npip install -r requirements_extra.txt\n', 'sudo apt-get update\nsudo apt-get install unzip -y\n\nmkdir test/data-models\nmkdir test/data-json\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-a639ec97.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-idenprof-test_acc_0.82550_epoch-95.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3-idenprof-test_acc_0.81050_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3_google-1a9a5a14.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-b0353104.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-idenprof-test_acc_0.85300_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-19c8e357.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-idenprof-test_acc_0.78200_epoch-91.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny_yolov3_number-plate-dataset-imageai_mAP-0.22595_epoch-20.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3_number-plate-dataset-imageai_mAP-0.57145_epoch-11.pt -P test/data-models\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_tiny_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof_model_classes.json -P test/data-json\n\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-datasets.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-images.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-videos.zip -P test\n\nunzip test/data-datasets.zip -d test\nunzip test/data-images.zip -d test\nunzip test/data-videos.zip -d test\n', 'pytest test -vvv  \n', 'pip install -r requirements.txt\npip install -r requirements_extra.txt\n', 'sudo apt-get update\nsudo apt-get install unzip -y\n\nmkdir test/data-models\nmkdir test/data-json\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-a639ec97.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-idenprof-test_acc_0.82550_epoch-95.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3-idenprof-test_acc_0.81050_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3_google-1a9a5a14.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-b0353104.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-idenprof-test_acc_0.85300_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-19c8e357.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-idenprof-test_acc_0.78200_epoch-91.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny_yolov3_number-plate-dataset-imageai_mAP-0.22595_epoch-20.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3_number-plate-dataset-imageai_mAP-0.57145_epoch-11.pt -P test/data-models\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_tiny_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof_model_classes.json -P test/data-json\n\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-datasets.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-images.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-videos.zip -P test\n\nunzip test/data-datasets.zip -d test\nunzip test/data-images.zip -d test\nunzip test/data-videos.zip -d test\n', 'pytest test -vvv\n', 'pip install -r requirements.txt\npip install -r requirements_extra.txt\n', 'sudo apt-get update\nsudo apt-get install unzip -y\n\nmkdir test/data-models\nmkdir test/data-json\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-a639ec97.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-idenprof-test_acc_0.82550_epoch-95.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3-idenprof-test_acc_0.81050_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3_google-1a9a5a14.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-b0353104.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-idenprof-test_acc_0.85300_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-19c8e357.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-idenprof-test_acc_0.78200_epoch-91.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny_yolov3_number-plate-dataset-imageai_mAP-0.22595_epoch-20.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3_number-plate-dataset-imageai_mAP-0.57145_epoch-11.pt -P test/data-models\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_tiny_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof_model_classes.json -P test/data-json\n\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-datasets.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-images.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-videos.zip -P test\n\nunzip test/data-datasets.zip -d test\nunzip test/data-images.zip -d test\nunzip test/data-videos.zip -d test\n', 'pytest test -vvv\n', 'pip install -r requirements.txt\npip install -r requirements_extra.txt\n', 'sudo apt-get update\nsudo apt-get install unzip -y\n\nmkdir test/data-models\nmkdir test/data-json\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-a639ec97.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/densenet121-idenprof-test_acc_0.82550_epoch-95.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3-idenprof-test_acc_0.81050_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/inception_v3_google-1a9a5a14.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-b0353104.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/mobilenet_v2-idenprof-test_acc_0.85300_epoch-92.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-19c8e357.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/resnet50-idenprof-test_acc_0.78200_epoch-91.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny_yolov3_number-plate-dataset-imageai_mAP-0.22595_epoch-20.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt -P test/data-models\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3_number-plate-dataset-imageai_mAP-0.57145_epoch-11.pt -P test/data-models\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_tiny_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/number-plate-dataset-imageai_yolov3_detection_config.json -P test/data-json\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/idenprof_model_classes.json -P test/data-json\n\n\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-datasets.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-images.zip -P test\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/test-resources-v3/data-videos.zip -P test\n\nunzip test/data-datasets.zip -d test\nunzip test/data-images.zip -d test\nunzip test/data-videos.zip -d test\n', 'pytest test -vvv\n']"
"['python -m pip install --upgrade pip\npip install Cython\npip install torch\n', 'pip install -U pycorrector\npip uninstall -y pycorrector\n', 'pip install -r requirements.txt\npip install .\npip install pytest\n', 'python -m pip install --upgrade pip\npip install torch\n', 'python -c ""import sys; print(sys.version)""\npython -c ""import torch; print(torch.__version__)""\n', 'pip install -U pycorrector\npip uninstall -y pycorrector\n', 'python -m pip install --upgrade pip\npip install Cython\npip install -r requirements.txt\npip install .']"
"['python -m pip install --upgrade pip\npython -m pip install -e .[tf,docs]\npip install jupyter-book\n', 'jupyter-book build .\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -r requirements.txt\n', 'if [ ""$RUNNER_OS"" == ""Linux"" ]; then\n  sudo apt-get update\n  sudo apt-get install ffmpeg\nelif [ ""$RUNNER_OS"" == ""macOS"" ]; then\n  brew install ffmpeg\nelse\n  choco install ffmpeg\nfi\n', 'pip install pytest\npython -m pytest\n', 'pip install git+https://github.com/${{ github.repository }}.git@${{ github.sha }}\npython examples/testscript.py\npython examples/testscript_multianimal.py\n']"
"['sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install -r requirements_test.txt\npip install tensorflow==${{ matrix.tensorflow }}\npip install keras==${{ matrix.keras }}\npip install tensorflow-addons==${{ matrix.tf_addons }}\npip list\n', './run_tests.sh ${{ matrix.framework }}', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install -q -r requirements_test.txt\npip install tensorflow==${{ matrix.tensorflow }}\npip install keras==${{ matrix.keras }}\npip install scikit-learn==${{ matrix.scikit-learn }}\npip install torch==${{ matrix.torch }} -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchvision==${{ matrix.torchvision }} -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchaudio==${{ matrix.torchaudio }} -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip list\n', './run_tests.sh ${{ matrix.framework }} ${{ matrix.module }}', ""sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install -q -r <(sed '/^scipy/d;/^matplotlib/d;/^pandas/d;/^statsmodels/d;/^numba/d;/^jax/d;/^h5py/d;/^Pillow/d;/^pytest/d;/^pytest-mock/d;/^torch/d;/^torchaudio/d;/^torchvision/d;/^xgboost/d;/^requests/d;/^tensorflow/d;/^keras/d;/^kornia/d;/^librosa/d;/^tqdm/d' requirements_test.txt)\npip install scipy==1.5.4\npip install matplotlib==3.3.4\npip install pandas==1.1.5\npip install statsmodels==0.12.2\npip install numba==0.53.1\npip install tensorflow==${{ matrix.tensorflow }}\npip install keras==${{ matrix.keras }}\npip install lingvo==${{ matrix.lingvo }}\npip install tensorflow-addons==0.9.1\npip install model-pruning-google-research==0.0.3\npip install h5py==2.10.0\npip install pytest~=7.0.1\npip install pytest-flake8~=1.1.0\npip install pytest-mock\npip install pytest-cov~=3.0.0\npip install torch==1.10.2+cpu --find-links https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchaudio==0.10.2+cpu --find-links https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchvision==0.11.3+cpu --find-links https://download.pytorch.org/whl/cpu/torch_stable.html\npip install xgboost==1.5.2\npip install requests==2.27.1\npip install kornia==0.6.8\npip install librosa==0.9.2\npip install tqdm==4.64.1\npip list\n"", './run_tests.sh ${{ matrix.framework }}', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install tensorflow==2.4.1\npip install keras==2.4.3\npip3 install -q -r requirements_test.txt\npip list\n', './run_tests.sh ${{ matrix.framework }} ${{ matrix.module }}', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip3 install -q -r requirements_test.txt\npip list\n', 'pip install torch==1.12.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchvision==0.13.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchaudio==0.12.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n', 'pytest --cov-report=xml --cov=art --cov-append -q -vv tests/estimators/object_detection/test_pytorch_object_detector.py --framework=pytorch --durations=0', 'pytest --cov-report=xml --cov=art --cov-append -q -vv tests/estimators/object_detection/test_pytorch_faster_rcnn.py --framework=pytorch --durations=0', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip3 install -r requirements_test.txt\npip install tensorflow==2.10.1\npip install keras==2.10.0\npip install torch==${{ matrix.torch }} -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchvision==${{ matrix.torchvision }} -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install torchaudio==${{ matrix.torchaudio }} -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip list\n', './run_tests.sh ${{ matrix.framework }}', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install -r requirements_test.txt\npip install tensorflow==2.10.1\npip install keras==2.10.0\npip install scikit-learn==${{ matrix.scikit-learn }}\npip list\n', './run_tests.sh ${{ matrix.framework }}', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -q pylint==2.12.2 mypy==0.931 pycodestyle==2.8.0 black==21.12b0\npip install -q -r requirements_test.txt\npip install pluggy==0.13.1\npip install tensorflow==2.7.0\npip install keras==2.7.0\npip install types-six\npip install types-PyYAML\npip install types-setuptools\npip install click==8.0.2\npip install numpy==1.21.6\npip list\n', 'pycodestyle --ignore=C0330,C0415,E203,E231,W503 --max-line-length=120 art', 'pylint --disable=C0330,C0415,E203,E1136,E0401,E1102 -rn art', 'mypy art', 'pytest --flake8 -v -m flake8 --ignore=contrib', 'black --line-length 120 --check art/\nblack --line-length 120 --check tests/\nblack --line-length 120 --check examples/\n', ""sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install -q -r <(sed '/^pandas/d;/^scipy/d;/^matplotlib/d;/^xgboost/d;/^jax/d' requirements_test.txt)\npip install pandas==1.3.5\npip install scipy==1.7.2\npip install matplotlib==3.5.3\npip install xgboost==1.6.2\npip install tensorflow==${{ matrix.tensorflow }}\npip install keras==${{ matrix.keras }}\npip install jax[cpu]==0.3.25\npip list\n"", './run_tests.sh ${{ matrix.framework }}', 'sudo apt-get update\nsudo apt-get -y -q install ffmpeg libavcodec-extra\npython -m pip install --upgrade pip setuptools wheel\npip install -r requirements_test.txt\npip install tensorflow==${{ matrix.tensorflow }}\npip install keras==${{ matrix.keras }}\npip install tensorflow-addons==${{ matrix.tf_addons }}\npip list\n', './run_tests.sh ${{ matrix.framework }}']"
"['pip install -U pip poetry\npoetry config virtualenvs.create false\n', 'make deps', 'sudo curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\nsudo curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list -o /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nACCEPT_EULA=Y sudo apt-get install -y msodbcsql18\n', 'make ci', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --finish\n', 'pip install codespell', 'codespell', 'pip install -U pip poetry\npoetry config virtualenvs.create false\n', 'make docs', 'pip install -U pip poetry\npoetry config virtualenvs.create false\n', 'make docs', 'pip install -U pip poetry\npoetry config virtualenvs.create false\n', 'make deps', 'make build']"
"['npm install -g appcenter-cli', './copy_test_resources.sh', 'echo pvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" >> local.properties', 'chmod +x gradlew', './gradlew assembleEnDebug', './gradlew assembleEnDebugAndroidTest', 'appcenter test run espresso --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Porcupine-Android"" --devices ""Picovoice/android-min-max"" --app-path porcupine-activity-demo-app/build/outputs/apk/en/debug/porcupine-activity-demo-app-en-debug.apk --test-series ""porcupine-android"" --locale ""en_US"" --build-dir porcupine-activity-demo-app/build/outputs/apk/androidTest/en/debug', 'npm install -g appcenter-cli', './copy_test_resources.sh', 'echo pvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" >> local.properties', 'echo numTestIterations=""100"" >> local.properties', 'echo performanceThresholdSec=""${{ matrix.performanceThresholdSec }}"" >> local.properties', 'chmod +x gradlew', './gradlew assembleEnDebug', './gradlew assembleEnDebugAndroidTest', 'appcenter test run espresso --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Porcupine-Android"" --devices ""Picovoice/${{ matrix.device }}"" --app-path porcupine-activity-demo-app/build/outputs/apk/en/debug/porcupine-activity-demo-app-en-debug.apk --test-series ""porcupine-android"" --locale ""en_US"" --build-dir porcupine-activity-demo-app/build/outputs/apk/androidTest/en/debug', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn build en', 'yarn build ja', 'npm install yarn', 'yarn install', 'yarn build porcupine-angular', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}', 'sudo apt install clang-format', 'python3 resources/.lint/c/formatter.py -c -v', 'cmake -B ./build', 'cmake --build ./build --target porcupine_demo_mic', 'cmake -B ./build', 'cmake --build ./build --target porcupine_demo_mic', 'cmake -B ./build', 'cmake --build ./build --target porcupine_demo_file', 'pip install -r test/requirements.txt', 'python test/test_porcupine_c.py ${{secrets.PV_VALID_ACCESS_KEY}} ${{ matrix.platform }} ${{ matrix.arch }}', 'cmake -B ./build', 'cmake --build ./build --target porcupine_demo_file', 'pip install -r test/requirements.txt', 'python3 test/test_porcupine_c.py ${{secrets.PV_VALID_ACCESS_KEY}} ${{ matrix.platform }} ${{ matrix.arch }}', 'dotnet format --verify-no-changes', 'dotnet format --verify-no-changes', 'dotnet format --verify-no-changes', 'dotnet restore', 'dotnet build -c MicDemo.Release', 'dotnet restore', 'dotnet build -c FileDemo.Release', 'dotnet build Porcupine/Porcupine.csproj --framework ${{ matrix.binding-framework }}', 'dotnet test --framework ${{ matrix.test-framework }} -v d', 'dotnet build Porcupine/Porcupine.csproj --framework net6.0', 'dotnet test --framework net6.0 -v d', 'flutter analyze --no-fatal-infos --no-fatal-warnings', 'flutter analyze --no-fatal-infos --no-fatal-warnings', 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter build apk', './copy_test_resources.sh', ""sed -i 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' integration_test/app_test.dart"", 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter test integration_test', 'pod repo update', 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter build ios --release --no-codesign', './copy_test_resources.sh', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' integration_test/app_test.dart"", 'pod repo update', 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter test integration_test', 'go build', 'go build', './copy.sh', 'go get', 'go build', 'go test -v -access_key ${{secrets.PV_VALID_ACCESS_KEY}}', './copy.sh', 'go get', 'go build', 'go test -v -access_key ${{secrets.PV_VALID_ACCESS_KEY}}', 'gem install cocoapods', 'npm install -g appcenter-cli', 'mkdir ddp', 'brew update\nbrew install convmv\n', './copy_test_resources.sh', 'pod install', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' PorcupineAppTestUITests/BaseTest.swift"", 'xcrun xcodebuild build-for-testing -configuration Debug -workspace PorcupineAppTest.xcworkspace -sdk iphoneos -scheme PorcupineAppTest -derivedDataPath ddp CODE_SIGNING_ALLOWED=NO', 'appcenter test run xcuitest --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Porcupine-iOS"" --devices ""Picovoice/ios-min-max"" --test-series ""porcupine-ios"" --locale ""en_US"" --build-dir ddp/Build/Products/Debug-iphoneos', 'gem install cocoapods', 'npm install -g appcenter-cli', 'mkdir ddp', 'brew update\nbrew install convmv\n', './copy_test_resources.sh', 'pod install', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' PerformanceTest/PerformanceTest.swift"", ""sed -i '.bak' 's:{NUM_TEST_ITERATIONS}:100:' PerformanceTest/PerformanceTest.swift"", ""sed -i '.bak' 's:{PERFORMANCE_THRESHOLD_SEC}:${{ matrix.performanceThresholdSec }}:' PerformanceTest/PerformanceTest.swift"", 'xcrun xcodebuild build-for-testing -configuration Debug -workspace PorcupineAppTest.xcworkspace -sdk iphoneos -scheme PerformanceTest -derivedDataPath ddp CODE_SIGNING_ALLOWED=NO', 'appcenter test run xcuitest --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Porcupine-iOS"" --devices ""Picovoice/${{ matrix.device }}"" --test-series ""porcupine-ios"" --locale ""en_US"" --build-dir ddp/Build/Products/Debug-iphoneos', 'java -Dconfig_loc=resources/.lint/java/ -jar resources/.lint/java/checkstyle-10.5.0-all.jar -c resources/.lint/java/checkstyle.xml binding/android/ binding/java/ binding/flutter/android/ binding/react-native/android/ demo/android/ demo/java/', './gradlew build', './gradlew assemble', './gradlew test --info --tests PorcupinePerformanceTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" -DnumTestIterations=""100"" -DperformanceThresholdSec=""${{matrix.performance_threshold_sec}}""', './gradlew assemble', 'bash machine-state.sh', './gradlew test --info --tests PorcupinePerformanceTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" -DnumTestIterations=""${{matrix.num_test_iterations}}"" -DperformanceThresholdSec=""${{matrix.performance_threshold_sec}}""', 'bash machine-state.sh', './gradlew assemble', './gradlew test --info --tests PorcupineTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}""', './gradlew assemble', './gradlew test --info --tests PorcupineTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}""', 'npm install yarn', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn test test/perf.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}} --num_test_iterations=100 --performance_threshold_sec=${{matrix.performance_threshold_sec}}', 'npm install --global yarn', 'yarn install', 'bash machine-state.sh', 'yarn test test/perf.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}} --num_test_iterations=${{matrix.num_test_iterations}} --performance_threshold_sec=${{matrix.performance_threshold_sec}}', 'bash machine-state.sh', 'npm install yarn', 'yarn install', 'yarn test test/index.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}}', 'npm install --global yarn', 'yarn install', 'yarn test test/index.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}}', 'pip install flake8 pep8-naming', 'flake8 --ignore=F401,F403,F405 --max-line-length=120 binding/python demo/python', 'python test_porcupine_perf.py ${{secrets.PV_VALID_ACCESS_KEY}} 100 ${{matrix.performance_threshold_sec}}', 'bash machine-state.sh', 'python3 test_porcupine_perf.py ${{secrets.PV_VALID_ACCESS_KEY}} ${{matrix.num_test_iterations}} ${{matrix.performance_threshold_sec}}', 'bash machine-state.sh', 'pip install -r requirements.txt', 'python test_porcupine.py ${{secrets.PV_VALID_ACCESS_KEY}}', 'pip install -r requirements.txt', 'python3 test_porcupine.py ${{secrets.PV_VALID_ACCESS_KEY}}', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn build en', 'yarn build ko', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn install\n./copy_test_resources.sh\n', ""sed -i 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Tests.ts"", 'detox build --configuration android.emu.release', 'detox test --configuration android.emu.release', 'yarn install\n./copy_test_resources.sh\n', 'pod install --repo-update', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Tests.ts"", 'detox build --configuration ios.sim.release', 'detox test --configuration ios.sim.release', 'npm install yarn', 'yarn install', 'yarn', 'npm install yarn', 'yarn install', 'yarn build', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}', 'bash copy.sh', 'sudo apt install libasound2-dev -y', 'cargo clippy -- -D warnings', 'sudo apt install libasound2-dev -y', 'cargo clippy -- -D warnings', 'sudo apt install libasound2-dev -y', 'cargo clippy -- -D warnings', 'sudo apt install libasound2-dev', 'cargo build --verbose', 'sudo apt install libasound2-dev', 'cargo build --verbose', 'bash copy.sh', 'sudo apt install libasound2-dev -y', 'cargo build --verbose', 'PV_ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}} cargo test --verbose', 'bash copy.sh', 'cargo build --verbose', 'PV_ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}} cargo test --verbose', 'npm install -g cspell', 'cspell --config resources/.lint/spell-check/.cspell.json ""**/*""', 'dotnet format whitespace binding/unity/Assets/Porcupine --folder --verify-no-changes', 'dotnet format whitespace demo/unity --folder --verify-no-changes', './copy.sh\n./copy_test_resources.sh\n', ""sed -i 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Assets/Porcupine/Tests/Integration.cs"", ""xvfb-run --auto-servernum --server-args='-screen 0 640x480x24' /home/picovoice/Unity/Hub/Editor/2019.4.34f1/Editor/Unity -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform StandaloneLinux64 -logFile -"", 'sed -n 2p unityresults.xml | grep \'result=""Passed""\'', './copy.sh\n./copy_test_resources.sh\n', 'rm -rf Assets/Porcupine/Plugins/mac/arm64', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Assets/Porcupine/Tests/Integration.cs"", '/Applications/Unity/Hub/Editor/2019.4.34f1/Unity.app/Contents/MacOS/Unity -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform StandaloneOSX -logFile -', 'sed -n 2p unityresults.xml | grep \'result=""Passed""\'', 'dos2unix copy.sh\nbash copy.sh\ndos2unix copy_test_resources.sh\nbash copy_test_resources.sh\n', 'bash -c ""sed -i \'s:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:\' Assets/Porcupine/Tests/Integration.cs""', '& ""C:\\Program Files\\Unity\\Hub\\Editor\\2019.4.34f1\\Editor\\Unity.exe"" -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform StandaloneWindows64 -logFile - | Out-Host\n', 'bash -c ""sed -n 2p unityresults.xml | grep Passed""', './copy.sh\n./copy_test_resources.sh\n', 'rm -rf Assets/Porcupine/Plugins/mac/arm64', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Assets/Porcupine/Tests/Integration.cs"", '/Users/alirezakenarsari-anhari/Library/Android/sdk/emulator/emulator -avd Pixel_6_API_33 &', '/Applications/Unity/Hub/Editor/2019.4.34f1/Unity.app/Contents/MacOS/Unity -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform Android -logFile -', 'sed -n 2p unityresults.xml | grep \'result=""Passed""\'', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn build en', 'yarn build ja', 'npm install yarn', 'yarn install', 'yarn build', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}', 'npm install yarn', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'npm install yarn', 'yarn install', 'yarn copywasm && yarn copyppn', 'yarn build', 'yarn setup-test', 'yarn test-perf --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}},NUM_TEST_ITERATIONS=20,INIT_PERFORMANCE_THRESHOLD_SEC=${{matrix.initPerformanceThresholdSec}},PROC_PERFORMANCE_THRESHOLD_SEC=${{matrix.procPerformanceThresholdSec}}', 'npm install yarn', 'yarn install', 'yarn copywasm && yarn copyppn', 'yarn build', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}']"
"['npm install', 'npm run webpack', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f test-requirements.txt ]; then pip install -r test-requirements.txt; fi\n', 'flake8 app\n', 'pytest\n', 'tar -zcvf dist.tar.gz ./app ./requirements.txt ./run.py\n']"
"['curl -sSL ""https://install.python-poetry.org"" | python\n\n# Adding `poetry` to `$PATH`:\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry config virtualenvs.in-project true\npoetry run pip install -U pip\npoetry install\n', 'make test', 'npm install remark \\\n  remark-cli \\\n  remark-lint \\\n  remark-lint-code-block-style \\\n  remark-lint-no-empty-url \\\n  remark-lint-no-tabs \\\n  remark-preset-lint-consistent \\\n  remark-preset-lint-markdown-style-guide \\\n  remark-preset-lint-recommended \\\n  remark-validate-links\n', 'cat << EOF > .remarkrc\n{\n  ""plugins"": [\n    ""lint"",\n    ""lint-no-tabs"",\n    ""lint-no-empty-url"",\n    ""lint-code-block-style"",\n    [\n      ""lint-unordered-list-marker-style"",\n      ""-""\n    ],\n    ""validate-links"",\n    ""remark-preset-lint-recommended"",\n    [\n      ""lint-list-item-indent"",\n      ""space""\n    ],\n    ""preset-lint-consistent""\n  ]\n}\nEOF\n']"
""
"['python -c ""import sys; print(sys.version)""\npip install .[test]\necho Finished successful build with Python $pythonversion\n', 'pytest -v tests -m ""not postgres_db and not mssql_db""\npytest -v tests -m postgres_db\npytest -v tests -m mssql_db\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get install -y graphviz\npython -m pip install --upgrade pip\npip install tox flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --max-complexity 10 --max-line-length 127 --statistics\n', 'tox\n']"
"['python -m pip install --disable-pip-version-check --upgrade pip\npython -m pip install tox tox-gh-actions coverage\n', 'tox', 'python -m coverage xml']"
[]
"['pip install -r requirements/requirements.txt\npip install -r requirements/requirements_test.txt\n', 'pytest .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
[]
""
"['pip install -e .\n', 'pip install pdoc3\n', 'pdoc3 --html livelossplot --force --output-dir docs --skip-errors\n', 'python -m pip install --upgrade pip\npip install -e .\npip install -r requirements-dev.txt\n', 'pip install tensorflow\npytest tests/external_test_tensorboard.py\n', 'pip install keras\npytest tests/external_test_keras.py\n', 'pip install pytorch-ignite\npytest tests/external_test_pytorch_ignite.py\n', 'pip install poutyne\npytest tests/external_test_poutyne.py\n', 'pytest tests/external_test_examples.py\n', 'pip install neptune-client\npytest tests/external_api_test_neptune.py\n', 'pip install flake8\npip install yapf\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics\n', 'exit 1', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\npip install -e .\n', 'pytest\n']"
""
""
"['git checkout dev\ngit pull\n', 'python -m pip install bumpversion wheel', 'bumpversion --dry-run ${{ github.event.inputs.version_name }} --allow-dirty --verbose\n', 'echo ""version_number=`bumpversion --dry-run --list ${{ github.event.inputs.version_name }} | grep new_version | sed -r s,""^.*="",,`"" >> $GITHUB_ENV', 'echo ""version_name: ${{ github.event.inputs.version_name }}""\necho ""version_number: v${{ env.version_number }}""\n', 'git config --global user.email ""git@github.com""\ngit config --global user.name ""GitHub Bot""\ngit add .\ngit commit -m ""Update CHANGELOG for auto-release v${{ env.version_number }}""\n', 'git status', 'bumpversion ${{ github.event.inputs.version_name }} --verbose', 'git tag | grep ${{ env.version_number }}', 'rm -f dist/*\npython setup.py sdist bdist_wheel\n', 'git push && git push --tags', 'echo ""Auto-release complete!""', 'pip install -e .', 'mkdocs build', 'pytest -m ""documentation""', 'python -m pip install -e .', 'pytest -v -r a -n auto --color=yes --durations=0 --cov=janitor --cov-append --cov-report term-missing --cov-report xml --doctest-only janitor', 'pytest -v -r a -n auto --color=yes --durations=0 --cov=janitor --cov-append --cov-report term-missing --cov-report xml tests -m ""${{ matrix.test-subset }}""']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\npip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npython setup.py install\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 calamari_ocr --count --select=E9,F63,F7,F82 --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 calamari_ocr --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m unittest calamari_ocr.test.test_augmentation', 'python -m unittest calamari_ocr.test.test_command_line', 'python -m unittest calamari_ocr.test.test_cross_fold_train', 'python -m unittest calamari_ocr.test.test_data_pagexml', 'python -m unittest calamari_ocr.test.test_eval', 'python -m unittest calamari_ocr.test.test_model_zoo', 'python -m unittest calamari_ocr.test.test_model_migration', 'python -m unittest calamari_ocr.test.test_network_architectures', 'python -m unittest calamari_ocr.test.test_predict_and_eval', 'python -m unittest calamari_ocr.test.test_prediction', 'python -m unittest calamari_ocr.test.test_pretrained', 'python -m unittest calamari_ocr.test.processors.test_text_regularizer', 'python -m unittest calamari_ocr.test.test_resume_training', 'python -m unittest calamari_ocr.test.test_scripts', 'python -m unittest calamari_ocr.test.test_train_abbyyxml', 'python -m unittest calamari_ocr.test.test_train_ensemble', 'python -m unittest calamari_ocr.test.test_train_file', 'python -m unittest calamari_ocr.test.test_train_hdf5', 'python -m unittest calamari_ocr.test.test_train_mixed_data', 'python -m unittest calamari_ocr.test.test_train_pagexml']"
"['git fetch --prune --tags --unshallow -f', 'echo ""tag=${GITHUB_REF#refs/*/}"" >> $GITHUB_OUTPUT', 'conda config --set always_yes True\nconda install -c pyviz ""pyctdev>=0.5""\ndoit ecosystem_setup\n# See: https://github.com/holoviz/panel/pull/4979\nconda install -c conda-forge ""urllib3<2.0.0"" ""conda-build==3.24""\n', 'doit package_build $CHANS_DEV $PKG_TEST_PYTHON --test-group=unit', 'doit package_upload --token=$CONDA_UPLOAD_TOKEN --label=dev', 'doit package_upload --token=$CONDA_UPLOAD_TOKEN --label=dev --label=main', 'git fetch --prune --tags --unshallow -f', 'conda config --set always_yes True\nconda install -c pyviz ""pyctdev>=0.5""\ndoit ecosystem_setup\ndoit env_create $CHANS_DEV --python=$PYTHON_VERSION\n', 'conda activate test-environment\ndoit develop_install $CHANS_DEV -o tests\ndoit pip_on_conda\n', 'conda activate test-environment\ndoit ecosystem=pip package_build\n', 'conda activate test-environment\ndoit ecosystem=pip package_upload -u $PPU -p $PPP -r $PYPI\n', 'git fetch --prune --tags --unshallow -f', 'echo ""Deploying from ref ${GITHUB_REF#refs/*/}""\necho ""tag=${GITHUB_REF#refs/*/}"" >> $GITHUB_OUTPUT\n', 'conda create -n test-environment python=3.8 pyctdev\n', 'conda activate test-environment\nconda list\ndoit develop_install -o doc -o examples --conda-mode=mamba\n', 'conda activate test-environment\ndoit env_capture\n', 'conda activate test-environment\nbokeh sampledata\n', 'conda activate test-environment\nnbsite generate-rst --org holoviz --project-name hvplot\n', 'conda activate test-environment\nnbsite build --what=html --output=builtdocs --org holoviz --project-name hvplot\n', 'cat /tmp/sphinx-*.log | tail -n 100', 'echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'conda activate test-environment\nmamba install ""fiona=1.8"" ""gdal=3.3""\n', 'conda activate test-environment\ndoit test_unit\n', 'conda activate test-environment\nbokeh sampledata\ndoit test_examples\n', 'conda activate test-environment\ncodecov\n', 'conda activate test-environment\n# - Pin panel on Python 3.6 because one or more dev releases on the 0.13.* series\n# can be installed on Python 3.6 but are actually not compatible with Python 3.6\n# Panel 0.13 will support Python >= 3.7 only so the pin here can stay indefinitely.\n# - Install importlib_resources to fix tqdm that missed adding it as a dependency\n# for 3.6 (https://github.com/conda-forge/tqdm-feedstock/pull/114)\nconda install ""panel=0.12"" ""importlib_resources"" --no-update-deps\n', 'conda activate test-environment\ndoit env_capture\n', 'conda activate test-environment\ndoit test_unit\n', 'conda activate test-environment\ncodecov\n']"
"['pip install black', 'black -l 100 strawberryfields/ tests/ --check', '[ ""$OPTIONS"" = ""not tf"" ] && gawk -i inplace \'!/(\'$TF_DEPENDENCIES\')/\' requirements-ci.txt\npython3 -m pip install --upgrade pip\npip install -r requirements-ci.txt\npip install wheel codecov pytest pytest-cov pytest-randomly pytest-mock pytest-logger --upgrade\n', 'pip install -r requirements-ci.txt\npython3 setup.py bdist_wheel\npip install dist/StrawberryFields*.whl\n', 'python3 -m pytest tests --cov=strawberryfields --cov-report=term-missing --cov-report=xml -p no:warnings --randomly-seed=42 --tb=native -m ""$OPTIONS""', 'bash <(sed -i \'s/filename=\\""/filename=\\""strawberryfields\\//g\' coverage.xml)', 'python -m pip install --upgrade pip wheel\npython setup.py bdist_wheel\npip install dist/StrawberryFields*.whl\n', 'pip install -r requirements-ci.txt\npip install wheel pytest pytest-cov pytest-mock --upgrade\n', 'python -m pytest tests -p no:warnings --randomly-seed=42 --tb=native\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r test-requirements.txt\npip install pytest-cov\nunzip test-repos\n', 'pytest tests/ --cov=pydriller/ --cov-report=xml', 'pip install mypy\nmypy --ignore-missing-imports pydriller/ tests/\n', 'pip install flake8\nflake8\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r test-requirements.txt\npip install pytest-cov\nunzip test-repos\n', 'pip install mypy\nmypy --ignore-missing-imports pydriller/ tests/\n', 'pip install flake8\nflake8\n', 'pytest tests/ --cov=pydriller/ --cov-report=xml', 'pip install wheel \npython setup.py bdist_wheel\n']"
"['echo ""PR_NUMBER=${{ github.event.pull_request.number }}"" >> $GITHUB_ENV\necho ""TAGGED_MILESTONE=${{ github.event.pull_request.milestone.title }}"" >> $GITHUB_ENV\n', 'set -xe\nchanged_files=$(git diff --name-only origin/main)\n# Changelog should be updated only if tests have been modified\nif [[ ! ""$changed_files"" =~ tests ]]\nthen\n  exit 0\nfi\nall_changelogs=$(cat ./CHANGES.rst)\nif [[ ""$all_changelogs"" =~ :pr:\\`$PR_NUMBER\\` ]]\nthen\n  echo ""Changelog has been updated.""\n  # If the pull request is milestoned check the correspondent changelog\n  if exist -f ./CHANGES.rst${TAGGED_MILESTONE:0:4}.rst\n  then\n    expected_changelog=$(cat ./CHANGES.rst${TAGGED_MILESTONE:0:4}.rst)\n    if [[ ""$expected_changelog"" =~ :pr:\\`$PR_NUMBER\\` ]]\n    then\n      echo ""Changelog and milestone correspond.""\n    else\n      echo ""Changelog and milestone do not correspond.""\n      echo ""If you see this error make sure that the tagged milestone for the PR""\n      echo ""and the edited changelog filename properly match.""\n      exit 1\n    fi\n  fi\nelse\n  echo ""A Changelog entry is missing.""\n  echo """"\n  echo ""Please add an entry to the changelog at \'CHANGES.rst\'""\n  echo ""to document your change assuming that the PR will be merged""\n  echo ""in time for the next release of dirty-cat.""\n  echo """"\n  echo ""Look at other entries in that file for inspiration and please""\n  echo ""reference this pull request using the \':pr:\' directive and""\n  echo ""credit yourself (and other contributors if applicable) with""\n  echo ""the \':user:\' directive., for instance :pr:`453` by :user:`Jo Blib <JoBlib>`.""\n  echo """"\n  echo ""If you see this error and there is already a changelog entry,""\n  echo ""check that the PR number is correct.""\n  echo """"\n  echo ""If you believe that this PR does not warrant a changelog""\n  echo ""entry, say so in a comment so that a maintainer will label""\n  echo ""the PR with \'No Changelog Needed\' to bypass this check.""\n  exit 1\nfi\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run', './build_tools/github/install.sh', './build_tools/github/test.sh']"
"['sudo apt update\nsudo apt -y install python3-pip\n', 'python3 -m pip install --user setuptools_scm mypy types-requests .', 'make mypy\nbash scripts/tests/test.sh\n', 'sudo apt update\nsudo apt -y install python3-pip\n', 'python3 -m pip install --user setuptools_scm', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', 'make dist', 'python -m pip install --upgrade pip setuptools==60.1.0 wheel\npython -m pip install --user cx_Freeze setuptools_scm\n', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', 'make windist\nmv greaseweazle-${{ steps.vars.outputs.ver }}-win.zip greaseweazle-${{ steps.vars.outputs.ver }}-win32.zip\n', 'python -m pip install --upgrade pip setuptools==60.1.0 wheel\npython -m pip install --user cx_Freeze setuptools_scm\n', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', 'make windist\nmv greaseweazle-${{ steps.vars.outputs.ver }}-win.zip greaseweazle-${{ steps.vars.outputs.ver }}-win64.zip\n', 'sudo apt update\nsudo apt -y install python3-pip\n', 'python3 -m pip install --user setuptools_scm', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', ""P=greaseweazle\nV=${{ steps.vars.outputs.ver }}\nunzip $P-$V-win32.zip\nfind $P-$V -name 'api-ms-*' | xargs rm -f\nfind $P-$V/lib -name 'python*.dll' | xargs rm -f\nfind $P-$V/lib -name 'vcruntime140.dll' | xargs rm -f\nwget https://github.com/keirf/$P/releases/download/v1.12/$P-1.12-win.zip\nunzip $P-1.12-win.zip\ncp $P-1.12/api-ms*.dll $P-$V/\ncp $P-1.12/ucrtbase.dll $P-$V/\nrm $P-$V-win32.zip\nzip -r $P-$V-win32.zip $P-$V\n"", 'sudo apt update\nsudo apt -y install git python3-pip zip\n', 'python3 -m pip install --user setuptools_scm', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', 'make dist', 'python -m pip install --upgrade pip setuptools==60.1.0 wheel\npython -m pip install --user cx_Freeze setuptools_scm\n', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', 'make windist\nmv greaseweazle-${{ steps.vars.outputs.ver }}-win.zip greaseweazle-${{ steps.vars.outputs.ver }}-win32.zip\n', 'python -m pip install --upgrade pip setuptools==60.1.0 wheel\npython -m pip install --user cx_Freeze setuptools_scm\n', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', 'make windist\nmv greaseweazle-${{ steps.vars.outputs.ver }}-win.zip greaseweazle-${{ steps.vars.outputs.ver }}-win64.zip\n', 'sudo apt update\nsudo apt -y install git python3-pip zip\n', 'python3 -m pip install --user setuptools_scm', 'echo ""ver=$(make version)"" >> $GITHUB_OUTPUT\n', ""P=greaseweazle\nV=${{ steps.vars.outputs.ver }}\nunzip $P-$V-win32.zip\nfind $P-$V -name 'api-ms-*' | xargs rm -f\nfind $P-$V/lib -name 'python*.dll' | xargs rm -f\nfind $P-$V/lib -name 'vcruntime140.dll' | xargs rm -f\nwget https://github.com/keirf/$P/releases/download/v1.12/$P-1.12-win.zip\nunzip $P-1.12-win.zip\ncp $P-1.12/api-ms*.dll $P-$V/\ncp $P-1.12/ucrtbase.dll $P-$V/\nrm $P-$V-win32.zip\nzip -r $P-$V-win32.zip $P-$V\n""]"
"['python -m pip install --upgrade pip\npip install tox-gh-actions wheel twine\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'python setup.py sdist bdist_wheel\n', 'twine check dist/*\n', 'python -m pip install --upgrade pip\npip install tox-gh-actions\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'python -m unittest discover\n']"
"['sudo apt-get update && sudo apt-get install -y ffmpeg\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'hash=${{ GITHUB.SHA }}\necho ""::set-output name=tag-hash::${hash}""\necho $hash\n', 'hash=$(git log -n1 --format=format:""%H"")\necho ""::set-output name=master-hash::${hash}""\necho $hash\n', 'exit 1', 'sudo apt-get update && sudo apt-get install -y ffmpeg\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['pushd terracotta/client/app\nyarn install --frozen-lockfile\nyarn build\npopd\n', 'git fetch --prune --unshallow --tags\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist\n', 'git fetch --prune --unshallow --tags', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip setuptools wheel\npip install cython numpy\n', ""sudo /etc/init.d/mysql start\nmysql -e 'SHOW DATABASES;' -u${{ env.DB_USER }} -p${{ env.DB_PASSWORD }}\n"", 'pip install -e .[test]\npip freeze\n', 'mypy --install-types --non-interactive . || true\n', 'MYSQL_SRV=""${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@127.0.0.1:${{ env.MYSQL_PORT }}""\nPOSTGRESQL_SRV=""${{ env.DB_USER }}:${{ env.DB_PASSWORD }}@localhost:${{ env.POSTGRESQL_PORT }}""\npython -m pytest . --color=yes --cov=terracotta --mysql-server=$MYSQL_SRV --postgresql-server=$POSTGRESQL_SRV\n', 'python -m pytest --color=yes tests/benchmarks.py\n', 'git fetch --prune --unshallow --tags', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install -e .[test]\nconda list\n', 'mypy . > /dev/null || true\nmypy --install-types --non-interactive\n', 'python -m pytest . --color=yes --cov=terracotta\n', 'python -m pytest tests/benchmarks.py --color=yes\n']"
""
"['make install', 'poetry run pytest --storage ${{ matrix.monty-storage }}\n', 'poetry run pytest --storage ${{ matrix.monty-storage }} --use-bson\n', 'make lint', 'make codespell', 'make bandit', 'make install\n', 'poetry run python build_version.py ${{github.ref_name}}\n', 'poetry run pytest tests/test_packaging.py\n', 'poetry build\npoetry publish -u ${{ secrets.PYPI_USERNAME }} -p ${{ secrets.PYPI_PASSWORD }}\n']"
""
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101,B311 .', 'black --check . || true', 'codespell --ignore-words-list=""wee"" --skip=""*.css,*.js""', 'flake8 . --count --ignore=E12,E2,F40,W29,W504 --max-complexity=10 --max-line-length=156 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install psutil', 'pip install -e .', 'mkdir -p .mypy_cache', 'mypy --install-types --non-interactive . || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['git checkout -t origin/master\ngit checkout ${{ github.event.pull_request.head.sha }}\n', 'python -m pip install --upgrade pip\npip install lintrunner --user\n', 'lintrunner init', 'set +e\nif ! lintrunner --force-color -m master --tee-json=lint.json; then\n    echo """"\n    echo -e ""\\e[1m\\e[36mYou can reproduce these results locally by using \\`lintrunner\\`.\\e[0m""\n    echo -e ""\\e[1m\\e[36mSee https://github.com/pytorch/pytorch/wiki/lintrunner for setup instructions.\\e[0m""\n    exit 1\nfi\n', '# Use jq to massage the JSON lint output into GitHub Actions workflow commands.\njq --raw-output \\\n  \'""::\\(if .severity == ""advice"" or .severity == ""disabled"" then ""warning"" else .severity end) file=\\(.path),line=\\(.line),col=\\(.char),title=\\(.code) \\(.name)::"" + (.description | gsub(""\\\\n""; ""%0A""))\' \\\n  lint.json\n']"
"['echo ""PR_FETCH_DEPTH=$(( ${{ github.event.pull_request.commits }} + 1 ))"" >> ""${GITHUB_ENV}""', 'echo ""Base ref is $GITHUB_BASE_SHA""\necho ""Head ref is $GITHUB_HEAD_SHA""\ngit fetch origin $GITHUB_HEAD_SHA\nCHANGED_MDX_FILES=$(git diff --name-only ""$GITHUB_BASE_SHA"" ""$GITHUB_HEAD_SHA"" -- \'*.mdx\')\nCHANGES_ENTRY=$(echo ""$CHANGED_MDX_FILES"" | sed \'s/\\.mdx$//\' | sed \'s/^docs\\/content/- {{deploymentUrl}}/\')\nCHANGES_ENTRY=$(echo -e ""Preview available at {{deploymentUrl}}\\n\\nDirect link to changed pages:\\n$CHANGES_ENTRY"")\necho ""$CHANGES_ENTRY""\n# https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\nEOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)\necho ""CHANGES_ENTRY<<$EOF"" >> $GITHUB_ENV\necho ""$CHANGES_ENTRY"" >> $GITHUB_ENV\necho ""$EOF"" >> $GITHUB_ENV\n']"
"['python -m pip install --upgrade pip setuptools wheel\n', 'python -m pip install "".[dev]"" --upgrade\npython -m pip uninstall ray -y\n', 'wandb login e2366d661b89f2bee877c40bee15502d67b7abef\n', 'pytest test/base test/continuous --cov=tianshou --durations=0 -v --color=yes\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'python -m pip install "".[dev]"" --upgrade\n', 'wandb login e2366d661b89f2bee877c40bee15502d67b7abef\n', ""pytest test --ignore-glob='*profile.py' --cov=tianshou --cov-report=xml --durations=0 -v --color=yes\n"", 'python -m pip install --upgrade pip setuptools wheel\n', 'python -m pip install "".[dev]"" --upgrade\n', 'flake8 . --count --show-source --statistics\n', 'yapf -r -d .\nisort --check .\n', 'mypy\n', 'make check-docstyle\nmake spelling\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'python -m pip install "".[dev]"" --upgrade\n', 'wandb login e2366d661b89f2bee877c40bee15502d67b7abef\n', 'pytest test --ignore-glob=\'*profile.py\' --ignore=""test/3rd_party"" --cov=tianshou --cov-report=xml --cov-report=term-missing --durations=0 -v --color=yes\n']"
"['pip install wheel', 'python setup.py sdist bdist_wheel\n', 'pip install wheel', 'python setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox run -e format', 'python -m pip install --upgrade pip\npip install tox\n', 'tox run -e pep8', 'python -m pip install --upgrade pip\npip install tox\n', 'tox run -e py${{ matrix.python-version[1] }}']"
"['pacman --noconfirm -Syu bandit', 'bandit -r archinstall || exit 0', 'pacman --noconfirm -Syu python python-pip', 'python -m pip install --upgrade pip', 'pip install flake8', 'flake8', 'pwd', 'find .', 'cat /etc/os-release', 'pacman-key --init', 'pacman --noconfirm -Sy archlinux-keyring', './build_iso.sh', 'pacman --noconfirm -Syu python mypy python-pip', 'python -m pip install --upgrade pip', 'pip install fastapi pydantic', 'python --version', 'mypy --version', 'mypy --config-file pyproject.toml', 'pacman --noconfirm -Syu python python-pip qemu gcc', 'python -m pip install --upgrade pip', 'pip install pytest', 'python -m pytest || exit 0', 'pacman-key --init\npacman --noconfirm -Sy archlinux-keyring\npacman --noconfirm -Sy python-pyparted pkgconfig gcc\n', 'python -m pip install --upgrade pip\npip install --upgrade build twine wheel setuptools installer\npip uninstall archinstall -y\n', 'pip install --upgrade simple-term-menu pyparted\n', 'python -m build --wheel --no-isolation', 'python -m installer dist/*.whl', 'python -V\narchinstall --script guided -v\narchinstall --script swiss -v\narchinstall --script only_hd -v\narchinstall --script minimal -v\n', 'python -m pip install --upgrade pip\npip install build twine\n', 'python -m build . --wheel\n', 'twine upload dist/*\n']"
"['docker build . --file Dockerfile --tag my-image-name:$(date +%s)', 'sudo apt-get update -y\nsudo apt-get install libgnutls28-dev libcurl4-openssl-dev libssl-dev -y\npip install -r tests/requirements-test.txt\npip install -e.\npython -m playwright install\n', 'make style-check\n', 'pytest --cov=./scylla tests']"
"['python -m pip install --upgrade pip\npip install .[test]\n', 'pip freeze', '# benchmarks on shared infrastructure like the CI machines are usually unreliable (high\n# variance), so there\'s no point spending too much time, hence --benchmark-disable which\n# just runs them once (as a test)\npy.test -ra tests/ --doctest-modules \\\n    --cov=stellargraph --cov-report=xml \\\n    -p no:cacheprovider --junitxml=""${{ env.JUNIT_NAME }}.xml"" \\\n    --benchmark-disable\n', 'python scripts/ci/junit_to_github_checks.py ${{ env.JUNIT_NAME }}.xml\n', 'python -m pip install --upgrade pip\npip install .[test,demos]\n', 'pip freeze', 'scripts/ci/run_notebook.sh ${{ matrix.notebook }}', 'docker-compose -f scripts/ci/actions-docker-compose.yml --project-directory . \\\n  up --detach neo4j\n', 'python -m pip install --upgrade pip\npip install .[test,neo4j]\n', 'pip freeze', 'scripts/ci/run_notebook.sh demos/basics/loading-saving-neo4j.ipynb', 'scripts/ci/run_notebook.sh demos/connector/neo4j/load-cora-into-neo4j.ipynb', 'scripts/ci/run_notebook.sh \\\n  demos/connector/neo4j/directed-graphsage-on-cora-neo4j-example.ipynb\n', 'scripts/ci/run_notebook.sh \\\n  demos/connector/neo4j/undirected-graphsage-on-cora-neo4j-example.ipynb\n', 'scripts/ci/run_notebook.sh demos/connector/neo4j/cluster-gcn-on-cora-neo4j-example.ipynb\n', 'docker-compose -f scripts/ci/actions-docker-compose.yml --project-directory . down\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install black==19.10b0\necho ::add-path::$HOME/.local/bin\n', 'if ! black --check --diff . ; then\n    msg=""file formatting does not match $(black --version); fix using \\`black .\\`""\n    # convert the black output to syntax understood by GitHub Actions\n    black --check . 2>&1 | sed ""s/would reformat \\(.*\\)/::error file=\\1::$msg/""\n    exit 1\nfi\n', 'scripts/ci/check-copyright-headers.sh', 'scripts/whitespace.sh --github-ci', 'conda install conda-build\nconda install conda-verify\n', 'conda build . --no-anaconda-upload', 'package_path=$(conda build . --output)\npackage_name=$(basename $package_path)\necho ""::set-output name=package_path::${package_path}""\necho ""::set-output name=package_name::${package_name}""\n', 'pip install --upgrade pip\npip install nbconvert nbformat black==19.10b0\n', 'python scripts/format_notebooks.py --default --ci demos/', 'pip install --upgrade pip\npip install nbformat commonmark\n', 'python scripts/notebook_text_checker.py demos/', 'npm install --global dockerfile-utils@0.0.16', 'scripts/ci/format-dockerfiles.sh', 'npm install --global prettier@1.19.1', 'set -o pipefail\nmsg=""does not match prettier $(prettier --version); fix using \\`prettier --write ...\\`""\n\n# meta.yaml is not valid YAML because of jinja templating\nprettier --list-different ""**/*.yml"" ""**/*.yaml"" ""!meta.yaml"" \\\n  | sed ""s/\\(.*\\)/::error file=\\1::formatting of \\1 $msg/""\n', 'pip install --upgrade pip\npip install nbformat\n', 'python scripts/demo_indexing.py --action=compare', 'sudo apt-get update\nsudo apt-get install -y pandoc enchant\n', 'pip install --upgrade pip\npip install .\npip install -r docs/requirements.txt\n', 'pip freeze', 'make html SPHINXOPTS=$sphinx_opts', 'make spelling SPHINXOPTS=$sphinx_opts', 'python3 scripts/ci/validate_ci_workflow.py', True]"
""
"['python -m pip install --quiet --upgrade pip\npython -m pip install --quiet boto3\n', 'python ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type g4dn.4x \\\n                                   --name GluonNLP-Website-${{ github.ref }} \\\n                                   --remote https://github.com/${{ github.repository }} \\\n                                   --source-ref ${{ github.ref }} \\\n                                   --work-dir . \\\n                                   --saved-output docs/tutorials \\\n                                   --save-path tutorials \\\n                                   --command ""python -m pip install --quiet -e .[web] && \\\n                                              apt-get install -y pandoc zip && \\\n                                              ./tools/batch/batch_states/compile_notebooks.sh \\\n                                              \'push\' ${GITHUB_REF##*/}"" \\\n                                   --wait | tee build_website.log\n', 'python ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type g4dn.4x \\\n                                   --name GluonNLP-Website-PR#${{ github.event.number }} \\\n                                   --remote https://github.com/${{ github.event.pull_request.head.repo.full_name }} \\\n                                   --source-ref ${{ github.event.pull_request.head.sha }} \\\n                                   --work-dir . \\\n                                   --saved-output docs/tutorials \\\n                                   --save-path tutorials \\\n                                   --command ""python -m pip install --quiet -e .[web] && \\\n                                              apt-get install -y pandoc zip && \\\n                                              ./tools/batch/batch_states/compile_notebooks.sh \\\n                                              ${{ github.event.number }} ${{ github.event.pull_request.head.sha }}"" \\\n                                   --wait | tee build_website.log\n', 'head -100 build_website.log | grep -oP -m 1 \'jobId: \\K(.*)\' > jobid.log\n\necho ""Job ID is""\ncat jobid.log\ncat jobid.log | xargs -i python ./tools/batch/wait-job.py --job-id {}\necho ""Copy Log file""\ncat jobid.log | xargs -i aws s3api wait object-exists --bucket gluon-nlp-dev --key batch/{}/tutorials/index.rst\ncat jobid.log | xargs -i aws s3 cp s3://gluon-nlp-dev/batch/{}/tutorials tutorials --recursive\n', 'python -m pip install --upgrade pip\npython -m pip install boto3\n', 'python ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type c5n.4x \\\n                                   --source-ref ${{ github.ref }} \\\n                                   --work-dir tools/batch/batch_states \\\n                                   --remote https://github.com/${{ github.repository }} \\\n                                   --command ""bash test_data_pipeline.sh"" --wait\n', 'python -m pip install --upgrade pip\npython -m pip install boto3\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type g4dn.4x \\\n                                   --name GluonNLP-Nightly-Test \\\n                                   --source-ref ${{ github.ref }} \\\n                                   --work-dir . \\\n                                   --remote https://github.com/${{ github.repository }} \\\n                                   --command ""python3 -m pip install pytest-forked \\\n                                              && python3 -m pip install -U --pre \'mxnet-cu102>=2.0.0b20210418\' -f https://dist.mxnet.io/python/cu102 \\\n                                              && python3 -m pytest --forked --durations=50 --device=""cpu"" --device=""gpu"" --runslow ./tests/"" \\\n                                   --wait | tee batch_job.log\n', 'sudo apt-get install libopenblas-dev', 'python -m pip install --quiet --upgrade pip\npython -m pip install --quiet boto3\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type g4dn.4x \\\n                                   --name GluonNLP-GPU-Test-${{ github.ref }} \\\n                                   --source-ref ${{ github.ref }} \\\n                                   --work-dir . \\\n                                   --saved-output coverage.xml \\\n                                   --save-path coverage.xml \\\n                                   --remote https://github.com/${{ github.repository }} \\\n                                   --command ""python3 -m pip install pytest-forked && python3 -m pytest --forked --cov=. --cov-config=./.coveragerc --cov-report=xml --durations=50 --device=""gpu"" --runslow ./tests/"" \\\n                                   --wait | tee batch_job.log\n', 'echo ""Start submitting job""\npython ./tools/batch/submit-job.py --region us-east-1 \\\n                                   --job-type g4dn.4x \\\n                                   --name GluonNLP-GPU-Test-PR#${{ github.event.number }} \\\n                                   --source-ref ${{ github.event.pull_request.head.sha }} \\\n                                   --work-dir . \\\n                                   --saved-output coverage.xml \\\n                                   --save-path coverage.xml \\\n                                   --remote https://github.com/${{ github.event.pull_request.head.repo.full_name }} \\\n                                   --command ""python3 -m pip install pytest-forked && python3 -m pytest --forked --cov=. --cov-config=./.coveragerc --cov-report=xml --durations=50 --device=""gpu"" --runslow ./tests/"" \\\n                                   --wait | tee batch_job.log\n', 'head -100 batch_job.log | grep -oP -m 1 \'jobId: \\K(.*)\' > jobid.log\n\necho ""Job ID is""\ncat jobid.log\n\ncat jobid.log | xargs -i python ./tools/batch/wait-job.py --job-id {}\n\necho ""Copy Codecov file""\ncat jobid.log | xargs -i aws s3api wait object-exists --bucket gluon-nlp-dev --key batch/{}/coverage.xml\ncat jobid.log | xargs -i aws s3 cp s3://gluon-nlp-dev/batch/{}/coverage.xml ./coverage.xml\n', 'sudo apt-get install -y libopenblas-dev ninja-build libedit-dev libxml2-dev\npython -m pip install ""torch==1.7.1+cpu"" -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -m pip install torch==1.7.1\n', 'python -m pip install --upgrade pip\npython -m pip install setuptools pytest pytest-cov contextvars\npython -m pip install --upgrade cython\npython -m pip install --pre ""mxnet>=2.0.0b20210121"" -f https://dist.mxnet.io/python\npython -m pip install -U -e .[extras,dev]\n', 'git clone https://github.com/apache/incubator-tvm tvm --recursive\ncd tvm\nmkdir -p build\ncp cmake/config.cmake build\necho set\\(USE_LLVM ON\\) >> build/config.cmake\necho set\\(USE_GRAPH_EXECUTOR ON\\) >> build/config.cmake\necho set\\(USE_BLAS openblas\\) >> build/config.cmake\ncd build\ncmake .. -G Ninja\nninja\ncd ../python\npython -m pip install -U -e .\n', 'python -m pytest -vv --cov=./ --cov-report=xml --device=""cpu"" --durations=50 tests/\n']"
"['pip3 install wheel\npip3 install pyinstaller\npip3 install PyQt5\npip3 install -r requirements.txt --upgrade\n', 'rm -rf dist\nrm -rf build \nrm -rf tidal_dl.egg-info\nrm -rf tidal_gui.egg-info\nrm -rf MANIFEST.in\n', 'pyinstaller -F tidal_dl/__init__.py -n tidal-dl\n']"
"['latest_sam_cli=`curl -s https://api.github.com/repos/aws/aws-sam-cli/releases/latest | jq -r .tag_name | cut -c 2-`\nlatest=`curl ""https://pypi.org/pypi/aws-sam-cli/$latest_sam_cli/json"" -s | jq -r \'.info.requires_dist[] | select(contains(""aws-sam-translator""))\' | cut -c 23- | rev | cut -c 2- | rev`\nsed -i -E ""s/aws-sam-translator>=[0-9.]+/aws-sam-translator>=$latest/"" setup.py\npip install -e .\nrm -rf src/cfnlint/data/DownloadsMetadata/*\ncfn-lint --update-iam-policies\ncfn-lint --update-documentation\nscripts/update_specs_services_from_ssm.py\nscripts/update_specs_from_pricing.py\nscripts/update_serverless_aws_policies.py\ncfn-lint --update-specs\necho ""specversion=$(jq -r .ResourceSpecificationVersion src/cfnlint/data/CloudSpecs/us-east-1.json)"" >> $GITHUB_OUTPUT\necho ""date=$(date +\'%Y-%m-%d\')"" >> $GITHUB_OUTPUT\n', 'pip install --upgrade setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'pip install tox', 'tox -e style\ntox -e type\ntox -e py\n', 'pip3 install -e .\n', 'cfn-lint test/fixtures/templates/good/generic.yaml\ncat test/fixtures/templates/good/generic.yaml | cfn-lint\n', 'pip3 install -e .\npip install --upgrade setuptools\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pycodestyle pyflakes pylint dlint pyupgrade\n', 'python3 -m unittest\n']"
""
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install tox', 'tox -e ${{ matrix.tox }},coverage-report']"
"['python -m pip install --upgrade pip\nmake setup\npip install -U .\n', 'make test', 'make lint', 'codecov --token ${{ secrets.CODECOV_TOKEN }} --branch ${{ github.ref }}']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install .\n', 'pytest --cov --cov-report=xml --cov-config=setup.cfg --verbose\n', 'pip install --upgrade sphinx sphinx_bootstrap_theme numpydoc sphinx-copybutton sphinx-panels\nmake -C docs clean\nmake -C docs html\n']"
""
"['python -m pip install --upgrade pip\npip install "".[dev]""\n', 'flake8 dataclasses_json --show-source --statistics --count\n', 'pytest\n']"
"[""pipx run poetry env use '${{ steps.py.outputs.python-path }}'"", 'pipx run poetry install --only main,test', 'pipx run poetry run pytest tests --junitxml=junit/test-results-${{ matrix.os }}-${{ matrix.python-version }}.xml --cov=com --cov-report=xml --cov-report=html\n', 'pip install buildozer cython', 'buildozer android p4a -- clean-recipe-build --local-recipes $(pwd)/../../bleak/backends/p4android/recipes bleak', 'buildozer android debug', 'pipx run poetry install --only docs,lint', 'pipx run poetry run black . --check --diff', 'pipx run poetry run flake8 . --count --show-source --statistics', 'pipx run poetry run make -C docs html', 'pipx run poetry build', 'pipx run twine upload dist/*']"
"['python -m pip install --upgrade pip virtualenv\nvirtualenv venv\nsource venv/bin/activate\npip install -e .[test]\n', 'source venv/bin/activate\nflake8 --show-source --statistics\n', 'source venv/bin/activate\nisort . --check-only --diff\n', 'source venv/bin/activate\npyre check\n', 'DOCKER_IMAGE=keshavdv/unifi-cam-proxy\nVERSION=dev\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/v}\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION}""\nif [ ""${{ github.event_name }}"" = ""push"" ]; then\n  TAGS=""$TAGS,${DOCKER_IMAGE}:sha-${GITHUB_SHA::8}""\nfi\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  TAGS=""$TAGS,${DOCKER_IMAGE}:latest""\nfi\n\necho ""version=${VERSION}"" >> $GITHUB_OUTPUT\necho ""tags=${TAGS}"" >> $GITHUB_OUTPUT\necho ""created=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')"" >> $GITHUB_OUTPUT\n']"
""
"['python -m pip install --upgrade pip\npip install shub\n', 'cd data_collection/\nshub deploy\n', 'python -m pip install --upgrade pip\npip install click python-decouple scrapinghub\n', 'python scripts/scheduler.py last-month-schedule-enabled-spiders', 'python -m pip install --upgrade pip\npip install click python-decouple scrapinghub\n', 'python scripts/scheduler.py schedule-enabled-spiders', 'python -m pip install --upgrade pip\npip install click python-decouple scrapinghub\n', 'python scripts/scheduler.py schedule-spider --spider_name=${{ github.event.inputs.spider_name }}', 'python scripts/scheduler.py schedule-spider --spider_name=${{ github.event.inputs.spider_name }} --start_date=${{ github.event.inputs.start_date }}', 'python -m pip install --upgrade pip\npip install click python-decouple scrapinghub\n', 'python scripts/scheduler.py schedule-all-spiders-by-date --start_date ${{ github.event.inputs.start_date }}', 'python -m pip install --upgrade pip\npip install click python-decouple scrapinghub\npython scripts/scheduler.py schedule-spider --spider_name=${{ github.event.inputs.spider_name }} --start_date=${{ github.event.inputs.start_date }} --end_date=${{ github.event.inputs.end_date }}\n', 'pip install pre-commit', 'pre-commit run --all']"
""
"['python -m pip install --upgrade pip setuptools tox', 'python -m tox', 'python -m pip install --upgrade pip setuptools tox', 'python -m tox', 'python -m pip install --upgrade pip setuptools tox', 'python -m tox']"
""
"['pip install --upgrade build twine', 'python -m build', 'twine check --strict dist/*', 'pip install . --user', ""ue4-docker build --target=build-prerequisites --visual-studio ${{ matrix.visual-studio || '2017' }}"", 'ue4-docker diagnostics all', 'docs/build.sh']"
"['pytest -v\n', 'pytest -v\n', 'mypy fsspec\n', 'git config --global user.email ""you@example.com""\ngit config --global user.name ""Your Name""\ngit tag -a 3000 -m ""fake""\npip install -e .\n', 'git clone https://github.com/fsspec/s3fs', 'pip install -e ./s3fs --no-deps\n', 'pytest -v fsspec/tests/test_downstream.py\n', 'git clone https://github.com/dask/dask\npip install -e ./dask\n', 'pytest -v dask/dask/bytes\n', 'git clone https://github.com/fsspec/${{ matrix.FRIEND }}', 'pip install -e . --no-deps\npip install -e ./${{ matrix.FRIEND }} --no-deps\n', 'pytest -v ${{ matrix.FRIEND }}', 'python -m pip install --upgrade pip\npip install setuptools setuptools-scm wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get update\nsudo apt-get install vlc mpv libmpv-dev\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'coverage run -m pytest\ncoverage xml\n', 'curl -L https://codeclimate.com/downloads/test-reporter/test-reporter-latest-linux-amd64 > ./cc-test-reporter\nchmod +x ./cc-test-reporter\n', './cc-test-reporter after-build -t coverage.py\n']"
"['python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'poetry run task wait_for_db\npoetry run task migrate\n', 'poetry run task flake8\n', 'poetry run task isort\n', 'poetry run task black\n', 'poetry run task mypy\n', 'poetry run task test\n', 'npm install -g yarn', 'yarn install', 'yarn lint', 'yarn lint:prettier', 'hadolint ./Dockerfile*', 'git checkout HEAD^2', 'mkdir backend/client\n', 'echo -e \'[url ""https://github.com/""]\\n  insteadOf = ""git@github.com:""\' >> ~/.gitconfig', 'yarn install\nyarn build\ncp -r dist ../backend/client/\n', 'python -m pip install --upgrade pip\npip install poetry poetry-dynamic-versioning\npoetry install\n', 'poetry run task collectstatic\n', 'sed -e ""s/, from = \\""..\\""//g"" backend/pyproject.toml > pyproject.toml\npoetry build\n', 'export VERSION=`python -V | cut -f2 -d "" ""`\ncurl -LO https://www.sqlite.org/2022/sqlite-dll-win64-x64-3390300.zip\nunzip sqlite-dll-win64-x64-3390300.zip\nmv sqlite3.dll /c/hostedtoolcache/windows/Python/$VERSION/x64/DLLs/\n', 'pip install doccano', 'doccano init', 'doccano createuser --username admin --password pass']"
"['python -m pip install -r ../requirements.txt', 'pyinstaller.exe ./microk8s.spec', 'move microk8s.exe ./windows/microk8s.exe', '7z x -o""C:/Program Files (x86)/NSIS"" ""${{ github.workspace }}/envar_plugin.zip""', 'makensis.exe ${{ github.workspace }}/installer/windows/microk8s.nsi', ""sudo lxd init --auto\nsudo usermod --append --groups lxd $USER\nsg lxd -c 'lxc version'\n"", 'sudo snap install snapcraft --classic\n', 'sudo snap refresh snapd --channel=latest/beta\n', ""sg lxd -c 'snapcraft --use-lxd'\nsudo mv microk8s*.snap microk8s.snap\n"", 'set -x\nsudo apt-get install python3-setuptools\nsudo pip3 install --upgrade pip\nsudo pip3 install -U pytest sh\nsudo apt-get -y install open-iscsi\nsudo systemctl enable iscsid\n', 'sudo -E UPGRADE_MICROK8S_FROM=latest/edge UPGRADE_MICROK8S_TO=$PWD/build/microk8s.snap pytest -s ./tests/test-upgrade-path.py\n', 'set -x\nsudo apt-get install python3-setuptools\nsudo pip3 install --upgrade pip\nsudo pip3 install -U pytest sh\nsudo apt-get -y install open-iscsi\nsudo systemctl enable iscsid\n', 'set -x\nsudo snap install build/microk8s.snap --classic --dangerous\n./tests/smoke-test.sh\nexport UNDER_TIME_PRESSURE=""True""\nexport SKIP_PROMETHEUS=""False""\nsudo -E bash -c ""cd /var/snap/microk8s/common/addons/core/tests; pytest -s -ra test-addons.py""\n', 'set -x\nsudo apt-get install python3-setuptools\nsudo pip3 install --upgrade pip\nsudo pip3 install -U pytest sh\nsudo apt-get -y install open-iscsi\nsudo systemctl enable iscsid\n', 'set -x\nsudo snap install build/microk8s.snap --classic --dangerous\nsudo microk8s enable community\nsudo -E bash -c ""cd /var/snap/microk8s/common/addons/community/; pytest -s -ra ./tests/""\n', 'set -x\nsudo apt-get install python3-setuptools\nsudo pip3 install --upgrade pip\nsudo pip3 install -U pytest sh\nsudo apt-get -y install open-iscsi\nsudo systemctl enable iscsid\n', 'set -x\nexport UNDER_TIME_PRESSURE=""True""\nsudo -E bash -c ""UPGRADE_MICROK8S_FROM=latest/edge UPGRADE_MICROK8S_TO=$PWD/build/microk8s.snap pytest -s ./tests/test-upgrade.py""\n', 'set -x\nsudo apt-get install python3-setuptools\nsudo pip3 install --upgrade pip\nsudo pip3 install -U pytest sh requests\n', 'set -x\nsudo snap install build/microk8s.snap --classic --dangerous\nsudo -E bash -c ""pytest -s ./tests/test-cluster-agent.py""\n', ""sudo lxd init --auto\nsudo lxc network set lxdbr0 ipv6.address=none\nsudo usermod --append --groups lxd $USER\nsg lxd -c 'lxc version'\n"", 'sudo -E bash -x -c ""./tests/libs/airgap.sh --distro ubuntu:20.04 --channel $PWD/build/microk8s.snap""\n', ""mkdir -p sarifs\nVER=$(curl --silent -qI https://github.com/aquasecurity/trivy/releases/latest | awk -F '/' '/^location/ {print  substr($NF, 1, length($NF)-1)}');\nwget https://github.com/aquasecurity/trivy/releases/download/${VER}/trivy_${VER#v}_Linux-64bit.tar.gz\ntar -zxvf ./trivy_${VER#v}_Linux-64bit.tar.gz\n"", 'cp trivy-microk8s-repo-scan--results.sarif ./sarifs/\n', ""for i in $(cat ./build-scripts/images.txt) ; do\n  name=$(echo  $i | awk -F ':|/' '{print $(NF-1)}')\n  ./trivy image $i --format sarif > sarifs/$name.sarif\ndone\n"", 'cp build/microk8s.snap .\nunsquashfs microk8s.snap\n./trivy rootfs ./squashfs-root/ --format sarif > sarifs/snap.sarif\n', 'sudo apt-get update\nsudo apt-get install tox --fix-missing\nsudo snap install node --classic\nsudo npm install --save-dev --save-exact -g prettier\n', 'tox -e lint\n', 'set -eux\nprettier --check $(find . -name ""*.yaml"" -o -name ""*.yml"" | \\\n  grep -v ""./microk8s-resources/actions/ingress.yaml"" | \\\n  grep -v ""./microk8s-resources/actions/metallb.yaml"" | \\\n  grep -v invalid.yaml | \\\n  grep -v calico)\n', 'sudo apt-get update\nsudo apt-get install tox --fix-missing\nsudo pip3 install -U pytest==7.1.3 sh==1.14.3\n', 'tox -e scripts\ntox -e wrappers\ntox -e cluster\n', 'pytest -s ./tests/verify-branches.py\n', './build-scripts/update-images.sh ${{ matrix.channel }} build-scripts/images.txt\n']"
"['python3 -m pip install --upgrade pip', 'pip3 install poetry --user', 'poetry install', 'poetry run yolo-train -h\npoetry run yolo-test -h\npoetry run yolo-detect -h\n', 'poetry run yolo-train --data config/custom.data  --model config/yolov3.cfg --epochs 30', 'poetry run yolo-test --data config/custom.data  --model config/yolov3.cfg --weights checkpoints/yolov3_ckpt_29.pth', 'poetry run yolo-detect --batch_size 2 --weights checkpoints/yolov3_ckpt_29.pth']"
[]
"['sudo apt install graphviz libgraphviz-dev graphviz-dev pkg-config\n', 'python -m pip install --upgrade pip\necho ""installing poetry dependencies""\npoetry install -E plotting -E pydot -E pygraphviz -E econml --with docs\n', 'poetry run poe test_advanced  --splits 4 --group ${{ matrix.test-group }}', 'sudo apt install graphviz libgraphviz-dev graphviz-dev pkg-config\n', 'python -m pip install --upgrade pip\necho ""HEAD_REPO=\'$HEAD_REPO\'""\necho ""HEAD_BRANCH=\'$HEAD_BRANCH\'""\necho ""GITHUB_CONTEXT=\'$GITHUB_CONTEXT\'""\npip install git+https://github.com/${HEAD_REPO}@${HEAD_BRANCH:-main}\n', 'sudo apt install graphviz libgraphviz-dev graphviz-dev pkg-config\n', 'python -m pip install --upgrade pip\necho ""installing poetry dependencies""\npoetry install -E plotting -E pygraphviz\n', 'poetry run poe lint', 'poetry run poe format_check', 'poetry run poe test --splits 6 --group ${{ matrix.test-group }}', 'poetry install -E plotting -E pygraphviz -E econml\n', 'poetry run poe test_econml', 'poetry install -E plotting -E pydot -E pygraphviz -E econml --with docs', 'git config --global --add safe.directory /__w/dowhy/dowhy', './docs/generate_docs.sh', 'poetry install -E plotting -E pydot -E pygraphviz --with docs', 'git config --global --add safe.directory /__w/dowhy/dowhy', './docs/generate_docs.sh', 'poetry install -E plotting -E pydot -E pygraphviz -E econml --with docs', 'git config --global --add safe.directory /__w/dowhy/dowhy', './docs/generate_docs.sh', 'sudo apt install graphviz libgraphviz-dev graphviz-dev pkg-config\n', 'python -m pip install --upgrade pip\necho ""installing poetry dependencies""\npoetry install -E plotting -E pydot -E pygraphviz -E econml --with docs\n', 'poetry run poe test_advanced', 'pip install poetry-dynamic-versioning', 'poetry install', 'poetry-dynamic-versioning\npoetry build\npoetry publish --username ${{ secrets.PYPI_USERNAME }} --password ${{ secrets.PYPI_PASSWORD }}\n']"
"['pip install -e .', 'pip install -r requirements-dev.txt', 'pip install fasttext\n', 'pip uninstall -y tensorflow\npip install tensorflow-cpu\n', 'pytest --verbose --cov=cleanlab/ --cov-config .coveragerc --cov-report=xml', 'python -m  pip install --upgrade pip\npip install .\n', 'pip install pytest pytest-lazy-fixture pipdeptree\npipdeptree -j > deps.json\n', 'python ./.github/get_min_dependencies.py\npip install -r requirements-min.txt\n', 'pytest tests/test_multilabel_classification.py tests/test_multiannotator.py tests/test_filter_count.py\n', 'python -m pip install --upgrade pip\npip install .  # install dependencies\npip install -r requirements-dev.txt  # install development dependencies and type stubs\n', 'mypy --install-types --non-interactive cleanlab', 'pip install flake8', 'flake8 cleanlab tests', 'cd /tmp\nwget https://github.com/jgm/pandoc/releases/download/2.19.2/pandoc-2.19.2-linux-amd64.tar.gz\nsudo tar xzvf pandoc-2.19.2-linux-amd64.tar.gz --strip-components 1 -C /usr/local\n', 'python3 -m pip install --upgrade pip', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install -e .', 'python3 -m pip install -r docs/requirements.txt', 'npm install -g katex', 'git config --global user.email ""actions@github.com""\ngit config --global user.name ""GitHub Actions""\ngit add -A && git commit -m ""Change cleanlab ver in .ipynb files""\n', 'git tag -f $GITHUB_REF_NAME\n', 'sphinx-multiversion docs/source cleanlab-docs -D smv_branch_whitelist=${{ github.ref_name }} -D smv_tag_whitelist=None', 'sphinx-multiversion docs/source cleanlab-docs -D smv_branch_whitelist=None -D smv_tag_whitelist=${{ github.ref_name }}', 'echo ${{ steps.stable_release.outputs.release }} > cleanlab-docs/latest_release.txt', 'cp docs/source/_templates/versioning.js cleanlab-docs/versioning.js\n', 'cp docs/source/_templates/redirect-to-stable.html cleanlab-docs/index.html', 'sudo apt-get update -y', 'sudo apt-get install -y pandoc', ""find . -name '*.html' -delete\n"", ""find . -name '*.md' -exec pandoc -i {} -o {}.html \\;\n""]"
[]
"['git config user.email 41898282+github-actions[bot]@users.noreply.github.com\ngit config user.name github-actions[bot]\ngit checkout -b pages\n', 'pip install -U pip setuptools h5py\npip3 install -U -e .[all,dev]\npython -m spacy download en\npython --version\npip --version\npip list\n', 'sudo apt-get install build-essential\nmake -C docs-source html\nrm -rfv ./docs\ncp -rv ./docs-source/build/html ./docs\ntouch ./docs/.nojekyll\n', 'git add ./docs\ngit commit -m ""Documentation build via Github Actions""\n', 'pip install -U pip setuptools h5py\npip3 install -U -e .[all,dev]\npython -m spacy download en\npython --version\npip --version\npip list\n', './scripts/check_pylint.sh\nblack --check --line-length 100 --target-version py36 examples nlp_architect solutions tests\n', 'pytest ./ -rs -v -n 2 --dist=loadfile --cov=nlp_architect --junit-xml=junit/test-results-${{ runner.os }}-${{ matrix.python-version }}.xml\n']"
"['python setup.py check --metadata --strict\n', 'pip install --upgrade setuptools wheel\npython setup.py sdist\n', 'pip install twine==3.2\ntwine check dist/*\npython setup.py clean\n', 'python setup.py sdist\n', 'pip install virtualenv\nvirtualenv vEnv\nsource vEnv/bin/activate\npip install dist/*\ncd .. & python -c ""import torchsampler ; print(torchsampler.__version__)""\ndeactivate\nrm -rf vEnv\n', 'pip install flake8\npip list\n', 'flake8 .\n', 'pip install isort\npip list\n', 'isort . --check --diff\n', 'pip install --upgrade pip\npip install yapf\npip list\n', 'yapf --diff --parallel --recursive .\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
""
[]
"['python -m pip install -U pip setuptools\npip install --no-cache-dir -e .\npip install -r requirements-dev.txt -U --upgrade-strategy=only-if-needed\n', './setup.py test --pytest-args=""--cov-report= --cov=litecli""\n', './setup.py lint\n', 'coverage report\ncodecov\n']"
[]
"['export MPLBACKEND=agg\npython -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'export MPLBACKEND=agg\npip install tensorflow>=2.0\npip install coveralls\ncoverage run --source=talos ./test-ci.py\n', 'coveralls\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['sudo apt-get install graphviz pandoc\npython -m pip install --upgrade pip\npip install -e .[dev]\nmake docs\n', ""python -m pip install --upgrade pip\npython -m pip install 'torch==1.8.0' -f https://download.pytorch.org/whl/cpu/torch/\npython -m pip install 'torchvision==0.9.0' -f https://download.pytorch.org/whl/cpu/torchvision/\n"", 'python -m pip install --upgrade pip\npython -m pip install invoke .[test]\n', 'invoke integration', 'python -m pip install --upgrade pip\npython -m pip install invoke .[dev]\n', 'invoke lint', ""python -m pip install --upgrade pip\npython -m pip install 'torch==1.8.0' -f https://download.pytorch.org/whl/cpu/torch/\npython -m pip install 'torchvision==0.9.0' -f https://download.pytorch.org/whl/cpu/torchvision/\n"", ""python -m pip install --upgrade pip\npython -m pip install 'torch==1.11.0' -f https://download.pytorch.org/whl/cpu/torch/\n"", 'python -m pip install --upgrade pip\npython -m pip install invoke .[test]\n', 'invoke minimum', ""python -m pip install --upgrade pip\npython -m pip install 'torch==1.8.0' -f https://download.pytorch.org/whl/cpu/torch/\npython -m pip install 'torchvision==0.9.0' -f https://download.pytorch.org/whl/cpu/torchvision/\n"", 'python -m pip install --upgrade pip\npython -m pip install invoke .[test]\n', 'invoke unit']"
"['python -m pip install --upgrade pip wheel\npip install -r requirements/base.txt\n', 'pip install flake8\nflake8\n', 'pip install -r requirements/dev.txt\npython -m pytest\n', 'pip install coverage coveralls\ncoverage run --source=geektime_dl -m pytest tests/\ncoveralls --service=github\n', 'cd docs\nnpm install\nnpm run docs:build\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install .\npip show principalmapper\n', 'python -m unittest -v tests/test*\n']"
""
"['curl -L \\\n-X POST \\\n-H ""Accept: application/vnd.github+json"" \\\n-H ""Authorization: Bearer ${{ secrets.ECS_TYPESCRIPT_REPO_TRIGGER_KEY }}"" \\\n-H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  https://api.github.com/repos/elastic/ecs-typescript/actions/workflows/generate.yml/dispatches \\\n-d \'{""ref"":""main"",""inputs"":{""ecsRef"":""${{ env.RELEASE_VERSION }}""}}\'\n', 'git fetch --prune --unshallow --tags', 'make check']"
"['brew install create-dmg', 'chmod +x ./building/build_macos_x64.sh\n./building/build_macos_x64.sh\n', './building/build_windows_x64.ps1']"
"['gcloud auth configure-docker\n', 'docker pull gcr.io/replicated-test/rasa-demo:latest || true\n', 'echo ""BUILD_NUMBER=$GITHUB_RUN_NUMBER-$GITHUB_RUN_ID"" >> $GITHUB_ENV', 'docker build -t gcr.io/replicated-test/rasa-demo:run$BUILD_NUMBER -t gcr.io/replicated-test/rasa-demo:latest --cache-from gcr.io/replicated-test/rasa-demo:latest .\n', 'docker push gcr.io/replicated-test/rasa-demo:latest\ndocker push gcr.io/replicated-test/rasa-demo:run$BUILD_NUMBER\n', 'curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n\nsudo curl -fsSL https://github.com/roboll/helmfile/releases/download/v0.141.0/helmfile_linux_amd64 --output /usr/local/bin/helmfile\nsudo chmod +x /usr/local/bin/helmfile\n', '# Set up docker to authenticate via gcloud command-line tool.\ngcloud --quiet auth configure-docker\ngcloud container clusters get-credentials ""$GCLOUD_CLUSTER_NAME"" --project ${GCLOUD_PROJECT_ID} --zone ""$GCLOUD_ZONE_ID""\n', 'kubectl config set-context --current --namespace=""${NAMESPACE}""\n', 'tag=`helm get values --output json $RELEASE_NAME | jq .app.tag`\necho $tag\necho ""ACTION_SERVER_TAG=\'${tag}\'"" >> $GITHUB_ENV\n', 'tag=run$GITHUB_RUN_NUMBER-$GITHUB_RUN_ID\necho $tag\necho ""ACTION_SERVER_TAG=\'$tag\'"" >> $GITHUB_ENV\n', 'cd ${{ github.workspace }}/.github/deployments && helmfile repos && helmfile sync', 'kubectl wait \\\n  --for=condition=available \\\n  --timeout=600s \\\n  -l ""app.kubernetes.io/component=rasa-x"" deployment\n\n# Wait for DB migration to be done\nuntil [[ $(curl -s ""https://${{ env.RASA_X_DOMAIN }}/api/health"" | tee /tmp/output_status.txt | jq -r .database_migration.status) -eq ""completed"" ]]\ndo\n  cat /tmp/output_status.txt || true\n  sleep 5\ndone\n# Wait for deployment to be ready\nuntil [[ $(curl -s https://${{ env.RASA_X_DOMAIN }}/api/health | tee /tmp/output_health.txt | jq -r .production.status) -eq 200 ]]\ndo\n  cat /tmp/output_health.txt || true\n  sleep 5\ndone\n', 'brew tap rasahq/rasactl\nbrew install rasactl\n', '# Delete secret \'rasactl\' that added by previous runner\nkubectl -n ${NAMESPACE} delete secret rasactl || true\n# Delete label \'rasactl=true\' to prevent the error \'secrets ""rasactl"" not found\'\nkubectl label namespace sara rasactl- || true\n\nrasactl add ${NAMESPACE} --rasa-x-release-name ${RELEASE_NAME} --verbose --debug || true\nrasactl status\n', 'RASA_VERSION_SHORT=${RASA_VERSION::3}\necho ""Greping model that trained on rasa ${RASA_VERSION_SHORT}.x ...""\n# Fetch the latest model based on when it created and which MAJOR rasa version is trained on\nMODEL_URL=$(gsutil ls -l ${{ secrets.STORAGE_BUCKET_URL }}/rasa_demo_models/ | grep ""tar.gz"" | grep ""rasa$RASA_VERSION_SHORT"" | sort -k2 | tail -n1 | awk \'{print $3}\')\n# Get the model local path, get rid of bucket rul\nMODEL_PATH=$(basename $MODEL_URL)\n# Download the model from bucket\ngsutil -m cp ${MODEL_URL} ${MODEL_PATH}\n\necho ""MODEL_PATH=${MODEL_PATH}"" >> $GITHUB_ENV\necho ""MODEL_NAME=$(basename $MODEL_PATH .tar.gz)"" >> $GITHUB_ENV\n', '# Upload model\nrasactl model upload ${MODEL_PATH}\n# Tag model as production\nrasactl model tag ${MODEL_NAME} production\n', 'python -m pip install --upgrade ""pip<20""\npip install -r requirements-dev.txt\n', 'echo ""------------------------------------""\necho ""/usr/bin/git log -1 --format=\'%H\'""\n/usr/bin/git log -1 --format=\'%H\'\necho ""------------------------------------""\nmake lint\n', 'python -m pip install --upgrade ""pip<20""\npip install -r requirements-dev.txt\n', 'pip list\nmake types\n', 'make install-dev\n', 'make test-actions\n', 'python -m pip install --upgrade ""pip<20""\npip install -r requirements-dev.txt\n', 'rasa data validate --debug\n', 'python -m pip install --upgrade ""pip<20""\npip install -r requirements-dev.txt\nrasa --version\n', 'rasa --version\nrasa test nlu -f 3 --cross-validation --config config_nlu_testing.yml\npython .github/workflows/format_results.py\n', 'rasa --version\nrasa train\n', 'rasa --version\nrasa test --stories tests/test_conversations.yml --fail-on-prediction-errors\n', 'while true; do\n  # Get a list of checks information, excluding training-testing and build-images\n  CHECKS_LIST=$(gh api /repos/${{ github.repository }}/commits/${{ github.event.pull_request.head.sha }}/check-runs --jq \'.check_runs.[] | select(.name != ""Test Model"" and .name != ""Build Action Server Image"")\')\n\n  # Get the status and conclusion of echo jobs\n  STATUS_LIST=$(echo $CHECKS_LIST | jq -r \'.status\')\n  CONCLUSION_LIST=$(echo $CHECKS_LIST | jq -r \'.conclusion\')\n\n  # Make sure all other check runs are completed\n  if [[ ""$(echo $STATUS_LIST | tr \' \' \'\\n\' | sort | uniq)"" == ""completed"" ]]; then\n    # Check the conclusion of all other check runs\n    # Fail the step if there is any failture\n    if [[ ""$(echo CONCLUSION_LIST | tr \' \' \'\\n\' | sort | uniq)"" =~ ""failure"" ]]; then\n      echo ""::error:: Some check runs failed. Skip uploading model.""\n      exit 1\n    else\n      echo ""All other check runs are successed.""\n      echo ""::set-output name=upload-model::true""\n      exit 0\n    fi\n  fi\n\n  sleep $WAIT_INTERVAL_SECS\n  echo ""Wait for $WAIT_INTERVAL_SECS seconds...""\ndone\n', 'python -c ""import rasa; open(\'rasaversion.txt\',\'w+\').write(rasa.__version__)""\nrasa_version=`cat rasaversion.txt`\nmodel_path=`ls models/*.tar.gz | head -n 1`\nmodel_timestamp=$(basename ""$model_path"" .tar.gz)\nmodel_name=""$model_timestamp""_rasa""$rasa_version""\nrenamed_model_path=models/""$model_name"".tar.gz\nmv $model_path $renamed_model_path\necho ""MODEL_NAME=${model_name}"" >> $GITHUB_ENV\necho ""MODEL_PATH=${renamed_model_path}"" >> $GITHUB_ENV\n', 'gsutil cp ""${MODEL_PATH}"" ${{ secrets.STORAGE_BUCKET_URL }}/rasa_demo_models', 'gcloud auth configure-docker\n', 'docker pull gcr.io/replicated-test/rasa-demo:latest || true\n', 'docker build --cache-from gcr.io/replicated-test/rasa-demo:latest .\n']"
"['ci/run_integration_test.sh', 'set -e\nci/generate_version_number.sh > VERSION\necho ""Build ID: $(cat ./VERSION)""\necho ""BUILD_ID=$(cat ./VERSION)"" >> ${GITHUB_ENV}\n', 'set -e\nimage_name=""opencuebuild/${{ matrix.component }}:${BUILD_ID}""\ncontainer_id=$(docker create ${image_name})\nartifacts=""${{ matrix.ARTIFACTS }}""\nmkdir -p ""${GITHUB_WORKSPACE}/artifacts/""\nfor artifact in $artifacts; do\n  docker cp ${container_id}:/opt/opencue/${artifact} ""${GITHUB_WORKSPACE}/artifacts/""\ndone\ndocker rm $container_id\n', 'artifacts=""${{ matrix.ARTIFACTS }}""\nfor artifact in $artifacts; do\n  aws s3 cp ${GITHUB_WORKSPACE}/artifacts/${artifact} s3://${S3_BUCKET}/opencue/${BUILD_ID}/\ndone\n', 'set -e\nci/generate_version_number.sh > VERSION\necho ""Build ID: $(cat ./VERSION)""\necho ""BUILD_ID=$(cat ./VERSION)"" >> ${GITHUB_ENV}\n', 'mkdir -p ""${GITHUB_WORKSPACE}/artifacts/""\nci/extract_schema.sh ${BUILD_ID} ""${GITHUB_WORKSPACE}/artifacts/""\n', 'mkdir -p ""${GITHUB_WORKSPACE}/artifacts/""\necho ""{\\""git_commit\\"": \\""$(BUILD_SOURCEVERSION)\\""}"" | tee ""${GITHUB_WORKSPACE}/artifacts/build_metadata.json""\n', 'aws s3 cp LICENSE s3://${S3_BUCKET}/opencue/${BUILD_ID}/\naws s3 cp VERSION s3://${S3_BUCKET}/opencue/${BUILD_ID}/\naws s3 cp ""${GITHUB_WORKSPACE}/artifacts/schema-${BUILD_ID}.sql"" s3://${S3_BUCKET}/opencue/${BUILD_ID}/\naws s3 cp ""${GITHUB_WORKSPACE}/artifacts/seed_data-${BUILD_ID}.sql"" s3://${S3_BUCKET}/opencue/${BUILD_ID}/\naws s3 cp ""${GITHUB_WORKSPACE}/artifacts/build_metadata.json"" s3://${S3_BUCKET}/opencue/${BUILD_ID}/\n', 'aws s3 ls s3://${S3_BUCKET}/opencue/${BUILD_ID}/\n', '# Trigger the workflow in the opencue.io repository.\n\ncurl -X POST \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  -H ""Content-Type: application/json"" \\\n  https://${GITHUB_PAT}@api.github.com/repos/${DOCS_REPO}/actions/workflows/${WORKFLOW_ID}/dispatches \\\n  --data \'{""ref"": ""master"", ""inputs"": {""release_version"": ""${{ github.event.release.tag_name }}""}}\'\n', 'set -e\nci/generate_version_number.sh > VERSION\necho ""Build ID: $(cat ./VERSION)""\necho ""BUILD_ID=$(cat ./VERSION)"" >> ${GITHUB_ENV}\n', 'echo ""TAG_NAME=${GITHUB_REF/refs\\/tags\\//}"" >> ${GITHUB_ENV}', 'set -e\nif [ ""v$(cat VERSION)"" != ""${TAG_NAME}"" ]; then\n  echo ""Version check failed: code version v$(cat VERSION) does not match tag name ${TAG_NAME}""\n  echo ""Original GITHUB_REF: ${GITHUB_REF}""\n  exit 1\nfi\n', 'set -e\nci/generate_version_number.sh > VERSION\necho ""Build ID: $(cat ./VERSION)""\necho ""BUILD_ID=$(cat ./VERSION)"" >> ${GITHUB_ENV}\n', 'set -e\ndocker pull opencuebuild/${{ matrix.component }}:${BUILD_ID}\n', 'set -e\nci/generate_version_number.sh > VERSION\necho ""Build ID: $(cat ./VERSION)""\necho ""BUILD_ID=$(cat ./VERSION)"" >> ${GITHUB_ENV}\n', 'mkdir -p ""${GITHUB_WORKSPACE}/artifacts/""\naws s3 sync ""s3://${S3_BUCKET}/opencue/${BUILD_ID}/"" ""${GITHUB_WORKSPACE}/artifacts/""\necho ""filenames=$(ls ""${GITHUB_WORKSPACE}/artifacts/"" | xargs)"" >> ${GITHUB_OUTPUT}\n', 'echo ${{ steps.fetch_artifacts.outputs.filenames }}\n', 'last_tagged_version=$(git describe --tags --abbrev=0 $(git rev-list --tags --skip=1 --max-count=1))\ncommits_since_last_release=$(git log --reverse --pretty=""* %H %s"" ${last_tagged_version}..HEAD)\n# Use a delimiter to preserve the multiline string.\n# See https://github.community/t/set-output-truncates-multiline-strings/16852\ndelimiter=""$(openssl rand -hex 8)""\necho ""commits<<${delimiter}"" >> ${GITHUB_OUTPUT}\necho ""${commits_since_last_release}"" >> ${GITHUB_OUTPUT}\necho ""${delimiter}"" >> ${GITHUB_OUTPUT}\n', 'ci/python_coverage_report.sh', 'chown -R aswfuser:aswfgroup .\nsu -c ""cd cuebot && ./gradlew jacocoTestReport sonarqube -Dsonar.login=$(SONAR_TOKEN)"" aswfuser\n', 'ci/run_python_tests.sh', 'chown -R aswfuser:aswfgroup .\nsu -c ""cd cuebot && ./gradlew build --stacktrace --info"" aswfuser\n', 'ci/run_python_tests.sh', 'chown -R aswfuser:aswfgroup .\nsu -c ""cd cuebot && ./gradlew build --stacktrace --info"" aswfuser\n', 'ci/run_python_tests.sh', 'chown -R aswfuser:aswfgroup .\nsu -c ""cd cuebot && ./gradlew build --stacktrace --info"" aswfuser\n', 'ci/run_python_tests.sh', 'chown -R aswfuser:aswfgroup .\nsu -c ""cd cuebot && ./gradlew build --stacktrace --info"" aswfuser\n', 'ci/test_pyside6.sh', 'ci/run_python_lint.sh', 'ci/build_sphinx_docs.sh', 'ci/check_changed_files.py ${{ steps.get_changed_files.outputs.modified_files }} ${{ steps.get_changed_files.outputs.deleted_files }}', 'ci/check_database_migrations.py', 'ci/check_version_bump.py ${{ steps.get_changed_files.outputs.all_changed_and_modified_files }}']"
"['docker build --file infrastructure/dockerfiles/pepy/Dockerfile --tag ghcr.io/psincraian/pepy:latest .', 'docker push ghcr.io/psincraian/pepy:latest', 'docker build --file infrastructure/dockerfiles/pepy-test/Dockerfile --tag ghcr.io/psincraian/pepy-test:latest .', 'docker push ghcr.io/psincraian/pepy-test:latest', 'DOCKER_TAG=latest make start-containers', 'DOCKER_TAG=latest make params=-T tests', 'doctl kubernetes cluster kubeconfig save --expiry-seconds 600 ${{ secrets.CLUSTER_NAME }}', 'kubectl apply --recursive -f $GITHUB_WORKSPACE/infrastructure/k8s', 'kubectl rollout restart deployment pepy', 'kubectl rollout status deployment/pepy', 'docker build --file infrastructure/dockerfiles/pepy/Dockerfile --tag ghcr.io/psincraian/pepy:sha-$GITHUB_SHA .', 'docker push ghcr.io/psincraian/pepy:sha-$GITHUB_SHA', 'docker build --file infrastructure/dockerfiles/pepy-test/Dockerfile --tag ghcr.io/psincraian/pepy-test:sha-$GITHUB_SHA .', 'docker push ghcr.io/psincraian/pepy-test:sha-$GITHUB_SHA', 'DOCKER_TAG=sha-$GITHUB_SHA make start-containers', 'DOCKER_TAG=sha-$GITHUB_SHA make params=-T tests', 'gh pr review --approve ""$PR_URL""', 'gh pr merge --auto --squash ""$PR_URL""']"
"['python -m pip install --upgrade pip setuptools wheel', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python3 -m unittest\n']"
"['python -m py_compile src/wsdd.py', 'pip install flake8\nflake8 --count --show-source --statistics src\n', 'pip install mypy==0.910\nmypy --python-version=3.7 src/wsdd.py\nmypy --python-version=3.8 src/wsdd.py\nmypy --python-version=3.9 src/wsdd.py\nmypy --python-version=3.10 src/wsdd.py\nmypy --python-version=3.11 src/wsdd.py\n']"
"['pip install black[jupyter]', 'black --check .', 'pip install isort==5.6.4', 'isort --check-only --diff .', 'pip install flake8==3.9.2', 'flake8', 'pip install pre-commit', 'pre-commit run --all-files', 'curl -sSL https://install.python-poetry.org | python3 -\npython -m pip install poetry-dynamic-versioning[plugin]\n', 'echo ""/Users/runner/.local/bin"" >> $GITHUB_PATH', 'poetry build\n', 'poetry publish --username ""${{ secrets.PYPI_USERNAME }}"" --password ""${{ secrets.PYPI_PASSWORD }}""\n', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")', 'curl -sSL https://install.python-poetry.org | python3 -\n', 'echo ""/Users/runner/.local/bin"" >> $GITHUB_PATH', 'poetry config virtualenvs.in-project true', 'poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'poetry run python -m pip install pip -U', 'poetry install --no-interaction --no-root\npoetry run python -m pip install tensorflow==${{ matrix.tf-version }}\n', ""poetry run coverage run -m unittest discover -s ./tests  -p 'test_*.py'"", 'poetry run coverage report -i\npoetry run coverage xml -i\n', 'sudo apt-get update && sudo apt-get install -y pandoc\npython -m pip install --upgrade pip\npip install -r docs/requirements_docs.txt\n', 'cd docs\nmake clean\nmake html --debug --jobs 2 SPHINXOPTS=""-W""\n']"
"['python -m pip install --upgrade pip build', 'python -m build .', 'python .github/workflows/parse_changelog.py CHANGELOG.md ${{ github.ref_name }} body.md', 'python -m pip install --upgrade pip\npython -m pip install .[examples]\n# The pyqt5 package may not be needed for non-interactive runs (such as\n# in this CI), but may be needed as a backend for Matplotlib to run\n# interactively.\n# pip install pyqt5\n', 'cd examples/water_boiler/\nNO_DISPLAY=1 python water_boiler.py\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 mypy pytest black colorama\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python -m pytest -v\n', 'python -m flake8 . --count --statistics\n', 'python -m mypy --strict simple_pid\n', 'python -m black -l 100 -S --check --diff --color --exclude ""docs/"" .\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8\npython -m pip install -r requirements_dev.txt\npython setup.py develop\n', 'py.test --doctest-modules --cov-report=xml --cov=sknetwork\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements_dev.txt\npython setup.py sdist\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements_dev.txt\npython setup.py sdist\n']"
""
"['changed_files=""$(gh pr view --repo ${{ github.repository }} ${{ needs.check-comment.outputs.pull_number }} --json files --jq \'.files.[].path\')""\nprotos=$([[ -z $(echo ""$changed_files"" | grep \'^mlflow/protos\') ]] && echo ""false"" || echo ""true"")\npython=$([[ -z $(echo ""$changed_files"" | grep \'\\.py$\') ]] && echo ""false"" || echo ""true"")\njs=$([[ -z $(echo ""$changed_files"" | grep \'^mlflow/server/js\') ]] && echo ""false"" || echo ""true"")\nr=$([[ -z $(echo ""$changed_files"" | grep \'^mlflow/R/mlflow\') ]] && echo ""false"" || echo ""true"")\necho ""protos=$protos"" >> $GITHUB_OUTPUT\necho ""python=$python"" >> $GITHUB_OUTPUT\necho ""js=$js"" >> $GITHUB_OUTPUT\necho ""r=$r"" >> $GITHUB_OUTPUT\n', ""git config user.name 'mlflow-automation'\ngit config user.email 'mlflow-automation@users.noreply.github.com'\ngit remote add mlflow https://github.com/mlflow/mlflow.git\ngit remote -v\ngit fetch mlflow ${{ needs.check-comment.outputs.base_ref }}\ngit merge mlflow/${{ needs.check-comment.outputs.base_ref }}\n"", 'pre-commit run prettier --all-files --color=always || true\n', 'docker build -t gen-protos -f dev/Dockerfile.protos .\n', 'docker run --rm -w /mlflow -v $(pwd):/mlflow gen-protos ./dev/generate-protos.sh\n', 'pip install -r requirements/lint-requirements.txt\n', 'black .\n', 'yarn install\n', 'yarn lint:fix\n', 'yarn i18n\n', './build-rdoc.sh\n', 'if [ -n ""$(git status --porcelain)"" ]; then\n  git add -A\n  git commit -s -m ""Autoformat: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}""\n  git push\nfi\n', 'pip install -r dev/requirements.txt\npip install pytest pytest-cov\n', 'python -m pytest --noconftest tests/dev/test_set_matrix.py\n', 'EVENT_NAME=""${{ github.event_name }}""\nif [ ""$EVENT_NAME"" = ""pull_request"" ]; then\n  REPO=""${{ github.repository }}""\n  PR_NUMBER=""${{ github.event.pull_request.number }}""\n  BASE_REF=""${{ github.base_ref }}""\n  REF_VERSIONS_YAML=""https://raw.githubusercontent.com/$REPO/$BASE_REF/mlflow/ml-package-versions.yml""\n  CHANGED_FILES=""$(python dev/list_changed_files.py --repository $REPO --pr-num $PR_NUMBER)""\n  ENABLE_DEV_TESTS=""${{ fromJson(steps.check-labels.outputs.result).enable_dev_tests }}""\n  NO_DEV_FLAG=$([ ""$ENABLE_DEV_TESTS"" == ""true"" ] && echo """" || echo ""--no-dev"")\n  ONLY_LATEST=""${{ fromJson(steps.check-labels.outputs.result).only_latest }}""\n  ONLY_LATEST_FLAG=$([ ""$ONLY_LATEST"" == ""true"" ] && echo ""--only-latest"" || echo """")\n  python dev/set_matrix.py --ref-versions-yaml $REF_VERSIONS_YAML --changed-files ""$CHANGED_FILES"" $NO_DEV_FLAG $ONLY_LATEST_FLAG\nelif [ ""$EVENT_NAME"" = ""workflow_dispatch"" ]; then\n  python dev/set_matrix.py --flavors ""${{ github.event.inputs.flavors }}"" --versions ""${{ github.event.inputs.versions }}""\nelse\n  python dev/set_matrix.py\nfi\n', 'python_version=$(python dev/get_minimum_required_python.py -p ${{ matrix.package }} -v ${{ matrix.version }} --python-versions ""3.8"")\necho ""version=$python_version"" >> $GITHUB_OUTPUT\n', 'sudo rm -rf ""$AGENT_TOOLSDIRECTORY""\nsudo rm -rf /usr/share/dotnet\nsudo rm -rf /usr/local/lib/android\n', 'if [ ""${{ matrix.package }}"" = ""mleap"" ]\nthen\n  pip install packaging\n  supports_java11=$(python -c ""from packaging.version import parse; print(parse(\'${{ matrix.version }}\') >= parse(\'0.21.0\'))"")\n  if [ ""$supports_java11"" = ""True"" ]\n  then\n    java_version=11\n  else\n    java_version=8\n  fi\nelse\n  java_version=11\nfi\necho ""version=$java_version"" >> $GITHUB_OUTPUT\n', 'date=$(date -u ""+%Y%m%d"")\nhash=$(echo -n ""$INSTALL_COMMAND"" | sha256sum)\necho ""key=$ImageOS-$ImageVersion-wheels-$date-$hash"" >> $GITHUB_OUTPUT\n', 'python --version\n# prophet==1.0.1 can only be installed `setup.py install` fallback but this was removed\n# in pip 23.1. See https://github.com/pypa/pip/issues/8368 more details.\nif [ ""${{ matrix.package }}==${{ matrix.version }}"" = ""prophet==1.0.1"" ]\nthen\n  PIP_SPEC=""<23.1""\nelse\n  PIP_SPEC=""""\nfi\npip install -U ""pip$PIP_SPEC"" wheel\npip install -e .[extras]\npip install -r requirements/test-requirements.txt\n', '${{ matrix.install }}\n', 'python dev/show_package_release_dates.py\n', 'export MLFLOW_HOME=$(pwd)\n${{ matrix.run }}\n', 'git config --global user.name ""test""\ngit config --global user.email ""test@mlflow.org""\n', 'TERM=xterm bash ./dev/test-dev-env-setup.sh\n', 'IMAGE_NAME=$(yq \'.services.mlflow.image\' .devcontainer/docker-compose.yml)\necho ""IMAGE_NAME=$IMAGE_NAME"" >> $GITHUB_ENV\n', 'docker build -f .devcontainer/Dockerfile.devcontainer -t $IMAGE_NAME .\n', 'docker history $IMAGE_NAME\n', 'docker image inspect $IMAGE_NAME\n', 'docker run --rm -v $(pwd):/workspaces/mlflow $IMAGE_NAME bash -c ""pip install --no-deps -e . && pytest tests/test_version.py""\n', 'echo ""${{ secrets.MLFLOW_AUTOMATION_TOKEN }}"" | docker login ghcr.io -u mlflow-automation --password-stdin\ndocker push $IMAGE_NAME\n', 'sudo rm -rf ""$AGENT_TOOLSDIRECTORY""\nsudo rm -rf /usr/share/dotnet\nsudo rm -rf /usr/local/lib/android\n', 'if [ ! ""$(git branch --show-current)"" == ""master"" ]; then\n  git fetch origin master:master\nfi\n\nREGEXP=""tests/examples\\|examples""\nCHANGED_FILES=$(git diff --name-only master..HEAD | grep ""$REGEXP"") || true;\nEXAMPLES_CHANGED=$([[ ! -z ""$CHANGED_FILES"" ]] && echo ""true"" || echo ""false"")\n\necho -e ""CHANGED_FILES:\\nCHANGED_FILES""\necho ""EXAMPLES_CHANGED: $EXAMPLES_CHANGED""\necho ""examples_changed=$EXAMPLES_CHANGED"" >> $GITHUB_OUTPUT\n', 'source ./dev/install-common-deps.sh --ml\n', 'export MLFLOW_HOME=$(pwd)\npytest tests/examples --durations=30\n', 'conda remove --all --yes --name test-environment\n./dev/remove-conda-envs.sh\n', 'df -h\n', 'if [ ! ""$(git branch --show-current)"" == ""master"" ]; then\n  git fetch origin master:master\nfi\n\nREGEXP=""Dockerfile\\|\\.dockerignore""\nCHANGED_FILES=$(git diff --name-only master..HEAD | grep ""$REGEXP"") || true;\nDOCKER_CHANGED=$([[ ! -z ""$CHANGED_FILES"" ]] && echo ""true"" || echo ""false"")\n\necho -e ""CHANGED_FILES:\\nCHANGED_FILES""\necho ""DOCKER_CHANGED: $DOCKER_CHANGED""\necho ""docker_changed=$DOCKER_CHANGED"" >> $GITHUB_OUTPUT\n', 'docker build -t mlflow_test_build . && docker images | grep mlflow_test_build\n', 'df -h\n', 'pip install -e .[gateway]\npip install pytest pytest-timeout\n', 'pytest tests/gateway\n', 'echo ""::remove-matcher owner=eslint-compact::""\necho ""::remove-matcher owner=eslint-stylish::""\n', '# On Windows, `yarn install` changes hash of @databricks/design-system in yarn.lock.\n# Use `--no-immutable` to allow the change.\nyarn install --no-immutable\ngit diff\n', 'yarn install --immutable\n', 'yarn lint\n', 'yarn i18n:check\n', 'yarn test\n', 'yarn build\n', 'echo ""::add-matcher::.github/workflows/matchers/pylint.json""\necho ""::add-matcher::.github/workflows/matchers/black.json""\n', 'source ./dev/install-common-deps.sh --ml\npip install -r requirements/lint-requirements.txt\n', 'pytest pylint_plugins/tests\n', 'pre-commit install -t pre-commit -t prepare-commit-msg\n', '# pylint is ridiculously slow. It takes ~15 minutes to run on all files.\n# To fail fast for faster iteration, first run pylint only on changed files,\n# then run it on all files in the next step.\nchanged_files=$(python dev/list_changed_files.py --repository mlflow/mlflow --pr-num ${{ github.event.number }})\npre-commit run --color=always --files $changed_files\n', 'changed_files=$(python dev/list_changed_files.py --repository mlflow/mlflow --pr-num ${{ github.event.number }})\nif [ ! -z ""$(echo $changed_files | grep \'\\.py$\')""]; then\n  pre-commit run --color=always --all-files pylint\nfi\n', 'if [ -f "".black.log"" ]; then\n  grep -o \'reformatted .*\\.py\' .black.log | sed \'s/reformatted \\(.*\\.py\\)/\\1: This file is unformatted. Run `black .` or comment `@mlflow-automation autoformat` on the PR if you\'\\\'\'re an MLflow maintainer./\'\nfi\n', 'source ./dev/install-common-deps.sh --skinny\n', './dev/run-python-skinny-tests.sh\n', 'source ./dev/install-common-deps.sh --ml\n', 'python tests/check_mlflow_lazily_imports_ml_packages.py\n', 'source dev/setup-ssh.sh\n./dev/run-python-tests.sh\n', './tests/db/compose.sh pull -q postgresql mysql mssql\n./tests/db/compose.sh build --build-arg DEPENDENCIES=""$(python setup.py -q dependencies)""\n', ""set +e\nerr=0\ntrap 'err=1' ERR\n\nfor service in $(./tests/db/compose.sh config --services | grep '^mlflow-')\ndo\n  # Set `--no-TTY` to show container logs on GitHub Actions:\n  # https://github.com/actions/virtual-environments/issues/5022\n  ./tests/db/compose.sh run --rm --no-TTY $service pytest \\\n    tests/store/tracking/test_sqlalchemy_store.py \\\n    tests/store/model_registry/test_sqlalchemy_store.py \\\n    tests/db\ndone\n\ntest $err = 0\n"", ""set +e\nerr=0\ntrap 'err=1' ERR\n\n./tests/db/compose.sh down --volumes --remove-orphans\nfor service in $(./tests/db/compose.sh config --services | grep '^migration-')\ndo\n  ./tests/db/compose.sh run --rm --no-TTY $service\ndone\n\ntest $err = 0\n"", 'sed -i \'s/sqlalchemy.*/sqlalchemy<2.0/g\' requirements/core-requirements.txt\ngit diff\n./tests/db/compose.sh build --build-arg DEPENDENCIES=""$(python setup.py -q dependencies)""\n', ""set +e\nerr=0\ntrap 'err=1' ERR\n\nfor service in $(./tests/db/compose.sh config --services | grep '^mlflow-')\ndo\n  # Set `--no-TTY` to show container logs on GitHub Actions:\n  ./tests/db/compose.sh run --rm --no-TTY $service pytest \\\n    tests/store/tracking/test_sqlalchemy_store.py \\\n    tests/store/model_registry/test_sqlalchemy_store.py \\\n    tests/db\ndone\n\ntest $err = 0\n"", './tests/db/compose.sh down --volumes --remove-orphans --rmi all\n', 'source ./dev/install-common-deps.sh\n', 'cd mlflow/java\nmvn clean package -q\n', 'source ./dev/install-common-deps.sh --ml\n', './dev/run-python-flavor-tests.sh;\n', 'source ./dev/install-common-deps.sh\npip install pyspark\n', 'export MLFLOW_HOME=$(pwd)\npytest tests/models\n', 'source ./dev/install-common-deps.sh\npip install pyspark ""numpy<1.24.0""\n', 'export MLFLOW_HOME=$(pwd)\npytest tests/evaluate\n', 'source ./dev/install-common-deps.sh\npip install tensorflow pyspark\n', 'export MLFLOW_HOME=$(pwd)\npytest --durations=30 tests/pyfunc\n', 'source ./dev/install-common-deps.sh --ml\n', './dev/run-python-sagemaker-tests.sh;\n', 'pip install -r requirements/test-requirements.txt\npip install --no-dependencies tests/resources/mlflow-test-plugin\npip install -e .[extras]\npip install pyspark\npip install mleap\n# Install Hugging Face datasets to test Hugging Face usage with MLflow dataset tracking\npip install datasets\n# Install TensorFlow to test TensorFlow dataset usage with MLflow dataset tracking\npip install tensorflow\n', 'git clone https://github.com/cdarlint/winutils\n', '# Set Hadoop environment variables required for testing Spark integrations on Windows\nexport HADOOP_HOME=`realpath winutils/hadoop-3.2.2`\nexport PATH=$PATH:$HADOOP_HOME/bin\n# Run Windows tests\npytest --ignore-flavors --ignore=tests/projects --ignore=tests/examples tests --ignore=tests/recipes --ignore=tests/evaluate\n# MLeap is incompatible on Windows with PySpark3.4 release. \n# Reinstate tests when MLeap has released a fix. [ML-30491]\n# pytest tests/mleap\n', 'pip install requests\n', 'python dev/preview_docs.py \\\n  --commit-sha ${{ github.event.pull_request.head.sha }} \\\n  --pull-number ${{ github.event.pull_request.number }} \\\n  --workflow-run-id ${{ github.run_id }}\n', 'docker build -t gen-protos -f Dockerfile.protos .\ndocker run --rm gen-protos protoc --version\n', 'wget https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-linux-x86_64.zip -O $HOME/protoc.zip\nsudo unzip $HOME/protoc.zip -d /tmp/protoc\nsudo chmod -R 777 /tmp/protoc\necho ""/tmp/protoc/bin"" >> $GITHUB_PATH\n', './dev/test-generate-protos.sh\n', 'pip install -e .\nmlflow models build-docker -n model-server:latest --mlflow-home `pwd`\n', 'set -x\n\ntags=$(echo -n ""${{ steps.modelmeta.outputs.tags }}"" | tr \'\\n\' \' \')\nfor tag in $tags; do\n  docker tag model-server:latest $tag\n  docker push $tag\ndone\n', 'Rscript -e \'source("".dump-r-dependencies.R"", echo = TRUE)\'\n', '# `os_name` will be like ""Ubuntu-20.04.1-LTS""\nos_name=$(lsb_release -ds | sed \'s/\\s/-/g\')\necho ""os-name=$os_name"" >> $GITHUB_OUTPUT\n', 'Rscript -e \'source("".install-deps.R"", echo=TRUE)\'\n', 'if [ ""$GITHUB_EVENT_NAME"" = ""schedule"" ]; then\n  USE_R_DEVEL=true\nelif [ ""$GITHUB_EVENT_NAME"" = ""pull_request"" ]; then\n  # Use r-devel on a pull request targeted to a release branch\n  USE_R_DEVEL=$([[ $GITHUB_BASE_REF =~ branch-[0-9]+\\.[0-9]+$ ]] && echo true || echo false)\nelse\n  # Use r-devel on a push to a release branch\n  USE_R_DEVEL=$([[ $GITHUB_REF_NAME =~ branch-[0-9]+\\.[0-9]+$ ]] && echo true || echo false)\nfi\necho ""USE_R_DEVEL=$USE_R_DEVEL"" >> $GITHUB_ENV\n', './build-package.sh\n', 'pip install -e $(git rev-parse --show-toplevel)\nH2O_VERSION=$(Rscript -e ""print(packageVersion(\'h2o\'))"" | grep -Eo \'[0-9][0-9.]+\')\npip install xgboost tensorflow ""h2o==$H2O_VERSION""\nRscript -e \'source("".install-mlflow-r.R"", echo=TRUE)\'\n', 'cd tests\nexport MLFLOW_HOME=$(git rev-parse --show-toplevel)\nRscript -e \'source(""../.run-tests.R"", echo=TRUE)\'\n', 'echo ""::add-matcher::.github/workflows/matchers/pylint.json""\necho ""::add-matcher::.github/workflows/matchers/black.json""\n', 'pip install -r ${{ github.event.inputs.repository }}/requirements/lint-requirements.txt\n', 'cd ${{ github.event.inputs.repository }} && ../../dev/lint.sh\n', 'pip install -e .\npip install -r ${{ github.event.inputs.repository }}/requirements.txt\npip install -r ${{ github.event.inputs.repository }}/requirements/test-requirements.txt\n# This dependency is needed pytest.ini uses pytest-cov\npip install pytest-cov\n', 'cd ${{ github.event.inputs.repository }} && pytest tests --rootdir .\n', 'pip install -e .\npip install -r ${{ github.event.inputs.repository }}/requirements.txt\npip install -r ${{ github.event.inputs.repository }}/requirements/test-requirements.txt\n# This dependency is needed pytest.ini uses pytest-cov\npip install pytest-cov\n', 'cd ${{ github.event.inputs.repository }} &&  pytest tests --rootdir .\n', 'source ./dev/install-common-deps.sh\npip install -e .\npip install pyspark\n', 'export MLFLOW_HOME=$(pwd)\npytest tests/recipes\n', 'pip install -r requirements/test-requirements.txt\npip install --no-dependencies tests/resources/mlflow-test-plugin\npip install -e .\npip install pyspark\n# TODO: Importing datasets in a pandas UDF (created by mlflow.pyfunc.spark_udf) crashes\n# the Python worker. To avoid this, uninstall `datasets`. This is a temporary workaround.\npip uninstall -y datasets\n', 'git clone https://github.com/cdarlint/winutils\n', '# Set Hadoop environment variables required for testing Spark integrations on Windows\nexport HADOOP_HOME=`realpath winutils/hadoop-3.2.2`\nexport PATH=$PATH:$HADOOP_HOME/bin\n# Run recipes tests\nexport MLFLOW_HOME=$(pwd)\npytest tests/recipes\n', 'pip install -r dev/requirements.txt\n', 'source ./dev/install-common-deps.sh --skinny\n', './dev/run-python-skinny-tests.sh\n', 'pip install -r dev/requirements.txt\n', 'source ./dev/install-common-deps.sh --ml\n', 'source dev/setup-ssh.sh\n./dev/run-python-tests.sh\n', 'echo ""MLFLOW_SKINNY=true"" >> $GITHUB_ENV\n', 'mkdir -p mlflow/server/js/build\necho \'\n<!DOCTYPE html>\n<html lang=""en"">\n  <head>\n    <meta charset=""utf-8"">\n    <title>MLflow</title>\n  </head>\n<body>\n  MLflow\n</body>\n</html>\n\' > mlflow/server/js/build/index.html\n', 'pip install pyyaml\n./dev/validate-requirements-consistency.sh\n', 'pip install wheel twine\n', '# Build distribution files\npython setup.py sdist bdist_wheel\n\n# List distribution files and check their file sizes\nls -lh dist\n\n# Set step outputs\nsdist_path=$(find dist -type f -name ""*.tar.gz"")\nwheel_path=$(find dist -type f -name ""*.whl"")\nwheel_name=$(basename $wheel_path)\nwheel_size=$(stat -c %s $wheel_path)\necho ""sdist-path=${sdist_path}"" >> $GITHUB_OUTPUT\necho ""wheel-path=${wheel_path}"" >> $GITHUB_OUTPUT\necho ""wheel-name=${wheel_name}"" >> $GITHUB_OUTPUT\necho ""wheel-size=${wheel_size}"" >> $GITHUB_OUTPUT\n', 'twine check --strict ${{ steps.build-dist.outputs.wheel-path }}\n', 'pip install ${{ steps.build-dist.outputs.sdist-path }}\npython -c ""import mlflow; print(mlflow.__version__)""\n', 'pip install --force-reinstall ${{ steps.build-dist.outputs.wheel-path }}\npython -c ""import mlflow; print(mlflow.__version__)""\n']"
"['echo ""::set-output name=branch::${GITHUB_REF#refs/heads/}""']"
"['pip install -e .', 'pip install -r requirements-dev.txt', 'pip freeze', ""python -c 'import flair'\npytest --runintegration --durations=0 -vv\n""]"
"['python -m pip install --upgrade pip', 'python -m pip install --upgrade wheel', 'pip install torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cpu\n', 'pip install torch==2.0.1 torchvision==0.15.2', 'pip install .[tests]', 'pip install flake8==5.0.3 flake8-docstrings==1.6.0 isort==5.11.5 mypy==0.991 types-PyYAML types-setuptools types-pkg-resources\n', 'pytest', 'flake8', 'mypy .', 'isort --profile black albumentations', 'python -m pip install --upgrade pip', 'pip install black==22.6.0', 'black --config=black.toml --check .', 'python -m pip install --upgrade pip', 'pip install .[develop]', 'python tools/make_transforms_docs.py check README.md', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['python -m pip install --upgrade pip\npip install --upgrade --upgrade-strategy eager -e "".[dev]""\n', 'prefect server start&\nPREFECT_API_URL=""http://127.0.0.1:4200/api"" ./scripts/wait-for-server.py\n\n# TODO: Replace `wait-for-server` with dedicated command\n#       https://github.com/PrefectHQ/prefect/issues/6990\n', 'uniquename=${{ github.head_ref || \'main\' }}-${{ github.sha }}\nPREFECT_API_URL=""http://127.0.0.1:4200/api"" python benches --benchmark-save=""${uniquename//\\//_}"" ${{ steps.bench-cache.outputs.cache-hit && \'--benchmark-compare\' || \'\'}}\n', 'version=""unreleased""\nif [[ $GITHUB_EVENT_NAME == ""release"" ]]; then\n  version=""${{ github.event.release.tag_name }}""\nfi\n\ncurl -XPOST -H ""Authorization: Bearer $REPO_ACCESS_TOKEN"" \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  ""https://api.github.com/repos/PrefectHQ/docs/dispatches"" \\\n  -d \'{""event_type"": ""Automated Documentation Build"", ""client_payload"": {""version"": ""\'""$version""\'""}}\'\n', 'echo ""LATEST_TAG=$(curl -qsSL \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" \\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  ""${{ github.api_url }}/repos/${{ github.repository }}/releases/latest"" \\\n| jq -r .tag_name)"" >> $GITHUB_ENV\n', 'echo ""value=$(date +\'%Y.%m.%d\')"" >> $GITHUB_OUTPUT', 'gh release create ${{ steps.get_date.outputs.value }} \\\n  --repo PrefectHQ/prefect-helm \\\n  --generate-notes \\\n  --notes ""Packaged with Prefect version \\\n  [${{ github.ref_name }}](https://github.com/PrefectHQ/prefect/releases/tag/${{ github.ref_name }})""\n', 'python -m pip install --upgrade pip\npip install --upgrade --upgrade-strategy eager -e .[dev]\n', 'prefect dev build-ui\n', 'prefect server start&\n', 'echo ""version=$(npx playwright -V)"" >> $GITHUB_OUTPUT\n', 'npx playwright install --with-deps', 'npx playwright test', 'python -m pip install --upgrade pip\npip install --upgrade --upgrade-strategy eager -e .[dev]\n', 'docker run --name ""prefect-server-${{ matrix.prefect-version }}"" --detach --publish 4200:4200 ${{ matrix.extra_docker_run_options }} prefecthq/prefect:${{ matrix.prefect-version }}-python3.10 ${{ matrix.server_command || \'prefect server start\' }} --host 0.0.0.0\nPREFECT_API_URL=""http://127.0.0.1:4200/api"" ./scripts/wait-for-server.py\n# TODO: Replace `wait-for-server` with dedicated command #       https://github.com/PrefectHQ/prefect/issues/6990\n', 'TEST_SERVER_VERSION=${{ matrix.prefect-version }} PREFECT_API_URL=""http://127.0.0.1:4200/api"" TEST_CLIENT_VERSION=$(python -c \'import prefect; print(prefect.__version__)\') ./scripts/run-integration-flows.py\n', '# First, we must stop the server container if it exists\n# TODO: Once we have `prefect server stop` we can run these tests first and the\n#       optional tests above second\n#       https://github.com/PrefectHQ/prefect/issues/6989\ndocker stop ""prefect-server-${{ matrix.prefect-version }}"" || echo ""That\'s okay!""\n\nprefect server start&\nPREFECT_API_URL=""http://127.0.0.1:4200/api"" ./scripts/wait-for-server.py\n', 'docker run --env PREFECT_API_URL=""http://127.0.0.1:4200/api"" --env TEST_SERVER_VERSION=$(python -c \'import prefect; print(prefect.__version__)\') --env TEST_CLIENT_VERSION=${{ matrix.client_version }} --volume $(pwd)/flows:/opt/prefect/integration/flows --volume $(pwd)/scripts:/opt/prefect/integration/scripts --network host ${{ matrix.extra_docker_run_options }} prefecthq/prefect:${{ matrix.prefect-version }}-python3.10 /opt/prefect/integration/scripts/run-integration-flows.py /opt/prefect/integration/flows\n', 'gh issue edit --repo prefecthq/prefect ${{ github.event.issue.number }} --remove-label ""status:triage"" --remove-label ""status:in-progress"" --remove-label ""status:accepted""', 'gh issue edit --repo prefecthq/prefect ${{ github.event.issue.number }} --remove-label ""status:triage""', 'python -m pip install --upgrade pip\npip install --upgrade --upgrade-strategy eager -e .[dev]\n', 'prefect dev build-ui\n', 'python setup.py sdist bdist_wheel\n', 'echo ""actor: ${{ github.actor }}""\ncurl -iL \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.ORG_MEMBER_RO }}""\\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  https://api.github.com/orgs/prefecthq/members/${{ github.actor }}\n\nif [[ $? -eq 0 ]]; then\n  echo ""status=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""status=false"" >> $GITHUB_OUTPUT\nfi\n', ""echo '${{ toJSON(matrix) }}'"", './scripts/generate-lower-bounds.py requirements.txt > requirements-lower.txt\n./scripts/generate-lower-bounds.py requirements-dev.txt > requirements-dev-lower.txt\nmv requirements-lower.txt requirements.txt\nmv requirements-dev-lower.txt requirements-dev.txt\n', 'SHORT_SHA=$(git rev-parse --short=7 HEAD)\ntmp=""sha-$SHORT_SHA-python${{ matrix.python-version }}""\necho ""image_tag=${tmp}"" >> $GITHUB_OUTPUT\n', 'docker load --input /tmp/image.tar\ndocker run --rm prefecthq/prefect-dev:${{ steps.get_image_tag.outputs.image_tag }} prefect version\n', 'docker load --input /tmp/image-conda.tar\ndocker run --rm prefecthq/prefect-dev:${{ steps.get_image_tag.outputs.image_tag }}-conda prefect version\ndocker run --rm prefecthq/prefect-dev:${{ steps.get_image_tag.outputs.image_tag }}-conda conda --version\n', ""python -m pip install -U pip\n# If using not using lower bounds, upgrade eagerly to get the latest versions despite caching\npip install ${{ ! matrix.lower-bound-requirements && '--upgrade --upgrade-strategy eager' || ''}} -e .[dev]\n"", 'docker run --name ""postgres"" --detach --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 --publish 5432:5432 --tmpfs /var/lib/postgresql/data --env POSTGRES_USER=""prefect"" --env POSTGRES_PASSWORD=""prefect"" --env POSTGRES_DB=""prefect"" --env LANG=""C.UTF-8"" --env LANGUAGE=""C.UTF-8"" --env LC_ALL=""C.UTF-8"" --env LC_COLLATE=""C.UTF-8"" --env LC_CTYPE=""C.UTF-8"" ${{ matrix.database }}\n./scripts/wait-for-healthy-container.sh postgres 30\necho ""PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://prefect:prefect@localhost/prefect"" >> $GITHUB_ENV\n', '# Parallelize tests by scope to reduce expensive service fixture duplication\n# Do not allow the test suite to build images, as we want the prebuilt images to be tested\n# Do not run Kubernetes service tests, we do not have a cluster available\npytest tests -vvv --numprocesses auto --dist loadscope --disable-docker-image-builds --exclude-service kubernetes --durations=25 --cov=src/ --cov=tests/ --no-cov-on-fail --cov-report=term --cov-config=setup.cfg ${{ matrix.pytest-options }}\n', 'docker container inspect postgres && docker container logs postgres || echo ""Ignoring bad exit code""\n', 'pip install pre-commit\n', 'pre-commit run --show-diff-on-failure --color=always --all-files\n', 'npm ci install', 'npm run build', ""echo '${{ toJSON(matrix) }}'"", 'python -m pip install --upgrade pip\npip install--upgrade --upgrade-strategy eager -e .[dev]\n', '# Parallelize tests by scope to reduce expensive service fixture duplication\npytest tests -vv --numprocesses auto --dist loadscope --exclude-services --durations=25\n']"
""
"['scripts/install', 'scripts/build', 'scripts/publish', 'scripts/install', 'scripts/check', 'scripts/build', 'scripts/test', 'scripts/coverage']"
"['python -m pip install --upgrade pip\npython -m pip install flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# all Python files should follow PEP8 (except some notebooks, see setup.cfg)\nflake8 jupytext tests\n# exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --statistics\n', 'git checkout HEAD^2', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\n# install sphinx_gallery and matplotlib if available (may not work on pypy)\npython -m pip install sphinx_gallery~=0.7.0 || true\npython -m pip install jupyter-fs || true\n', 'python -m pip install .', 'python -m ipykernel install --name python_kernel --user', ""# download the latest release\ngh release download --repo quarto-dev/quarto-cli --pattern 'quarto-*-linux-amd64.deb'\n# install it\nsudo apt install ./*.deb\n"", 'pytest --cov=./ --cov-report=xml', 'python -m pip install .', 'python -m ipykernel install --name jupytext-ci --user\n', 'conda list', 'if(""${{ matrix.os }}"" -eq ""windows-latest""){\n   pytest\n} else {\n   pytest --cov=./ --cov-report=xml\n}\n', 'python -m pip install --upgrade pip\n# All dependencies but markdown-it-py\npython -m pip install nbformat pyyaml toml\npython -m pip install -r requirements-dev.txt\n', 'python -m ipykernel install --name python_kernel --user', 'pytest --cov=./ --cov-report=xml', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\n', 'pytest --cov=./ --cov-report=xml', 'python -m pip install --upgrade pip\n# All dependencies but markdown-it-py\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\n# Notebook pre-release version #933\npython -m pip install --pre -U notebook\n', 'python -m ipykernel install --name python_kernel --user', 'pytest --cov=./ --cov-report=xml', 'python -m pip install wheel jupyter-packaging \'jupyterlab>=3,<4\'\nBUILD_JUPYTERLAB_EXTENSION=1 python setup.py sdist bdist_wheel\n# Don\'t publish a tar.gz file over 1MB  (Issue #730)\nif (($(wc -c < dist/*.tar.gz) > 1000000)); then exit 1; fi\n# node_modules should not be in the package\nif (($(tar -tf dist/*.tar.gz | grep node_modules | wc -l)>0)); then echo ""node_modules should not be included"" && exit 1; fi\n# Check that the lab and the notebook extensions are there\nif (($(tar -tf dist/*.tar.gz | grep packages/labextension/package.json$ | wc -l)==0)); then echo ""Missing lab extension"" && exit 1; fi\nif (($(tar -tf dist/*.tar.gz | grep jupytext/nbextension/index.js$ | wc -l)==0)); then echo ""Missing notebook extension"" && exit 1; fi\n# Install\npython -m pip install dist/*.tar.gz\necho ""Install went OK""\n', ""python -m pip install wheel jupyter-packaging 'jupyterlab>=3,<4'\nBUILD_JUPYTERLAB_EXTENSION=1 python setup.py sdist bdist_wheel\n""]"
"['python3 -m pip install --upgrade pip\npython3 -m pip install molecule[ansible,docker] jmespath\nansible --version\ndocker --version\nmolecule --version\npython --version\n', 'cd ansible\nmolecule create\nmolecule verify\nmolecule converge\nmolecule idempotence\nmolecule verify\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install --upgrade pipenv\npipenv install --system --dev --ignore-pipfile\n', 'cd docs/\nmake html\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install --upgrade pipenv\npipenv install --system --dev --ignore-pipfile\n', 'cd src/\npycodestyle\n', '.github/workflow-scripts/check-trailing-whitespace\n', '.github/workflow-scripts/check-trailing-whitespace\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'sudo apt-get update -qq\nsudo apt-get install -qq --no-install-recommends unpaper tesseract-ocr imagemagick ghostscript optipng\npip install --upgrade pipenv\npipenv install --system --dev --ignore-pipfile\n', 'cd src/\npytest\n', 'cd src/\ncoveralls --service=github\n', 'git_hash=$(git rev-parse --short ""$GITHUB_SHA"")\ngit_branch=${GITHUB_REF#refs/heads/}\nsed -i -E ""s/version: \\""(.*)\\""/version: \\""${git_branch} ${git_hash}\\""/g"" src-ui/src/environments/environment.prod.ts\n', './compile-frontend.sh', 'sudo apt-get update -qq\nsudo apt-get install -qq --no-install-recommends gettext liblept5\npip3 install -r requirements.txt\n', 'mkdir dist\nmkdir dist/paperless-ng\nmkdir dist/paperless-ng/scripts\ncp .dockerignore .env Dockerfile Pipfile Pipfile.lock LICENSE README.md requirements.txt dist/paperless-ng/\ncp paperless.conf.example dist/paperless-ng/paperless.conf\ncp gunicorn.conf.py dist/paperless-ng/gunicorn.conf.py\ncp docker/ dist/paperless-ng/docker -r\ncp scripts/*.service scripts/*.sh dist/paperless-ng/scripts/\ncp src/ dist/paperless-ng/src -r\ncp docs/_build/html/ dist/paperless-ng/docs -r\n', 'cd dist/paperless-ng/src\npython3 manage.py compilemessages\n', 'cd dist/paperless-ng/src\npython3 manage.py collectstatic --no-input\n', 'cd dist\nfind . -name __pycache__ | xargs rm -r\ntar -cJf paperless-ng.tar.xz paperless-ng/\n', 'echo ::set-output name=version::${GITHUB_REF#refs/tags/ng-}\n', 'IMAGE_NAME=jonaswinkler/paperless-ng\nif [[ $GITHUB_REF == refs/tags/ng-* ]]; then\n  TAGS=${IMAGE_NAME}:${GITHUB_REF#refs/tags/ng-},${IMAGE_NAME}:latest\n  INSPECT_TAG=${IMAGE_NAME}:latest\nelif [[ $GITHUB_REF == refs/heads/* ]]; then\n  TAGS=${IMAGE_NAME}:${GITHUB_REF#refs/heads/}\n  INSPECT_TAG=${TAGS}\nelse\n  exit 1\nfi\necho ::set-output name=tags::${TAGS}\necho ::set-output name=inspect_tag::${INSPECT_TAG}\n', 'docker buildx imagetools inspect ${{ steps.prepare.outputs.inspect_tag }}\n']"
"['conda activate mmf\npython -m pip install --upgrade pip\npip install --upgrade setuptools certifi\npip install --progress-bar off torch torchvision torchaudio\npip install --progress-bar off scipy==1.4.1 pybind11==2.5.0 pywin32==225\npython -c \'import torch; print(""Torch version:"", torch.__version__)\'\npython -m torch.utils.collect_env\n', 'conda activate mmf\npython -m pip install --upgrade pip\npip install setuptools==65.6.3\npip install --progress-bar off pytest\npip install -r requirements.txt\npython -c \'import torch; print(""Torch version:"", torch.__version__)\'\npython -m torch.utils.collect_env\n', 'conda activate mmf\npython setup.py clean --all\npython setup.py install\n', 'conda activate mmf\ncd tests\npytest --junitxml=artifacts/junit-${{ matrix.platform }}-python${{ matrix.python-version }}.xml -v .\n', 'echo ""::set-output name=dir::$(yarn cache dir)""', 'conda activate mmf\ncd ${GITHUB_WORKSPACE}/mmf_main\npython setup.py install\npython -c \'import torch; print(""Torch version:"", torch.__version__)\'\npython -m torch.utils.collect_env\ncd website\nnode -v\nyarn -v\nyarn install --frozen-lockfile\ncd ..\ncd docs\npip install -r requirements.txt\ncd ..\n./website/build_docs.sh -b\npwd\n', 'conda activate mmf\npython -m pip install --upgrade pip\npip install --progress-bar off flake8==3.7.9\npip install --progress-bar off ufmt==2.0.1\npip install --progress-bar off usort==1.0.5\npip install --progress-bar off black==22.12.0\n', 'conda activate mmf\nflake8 --version\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F72,F82 --show-source --statistics\n# exit-zero treats all errors as warnings\nflake8 . --count --exit-zero --max-complexity=18 --max-line-length=88 --statistics\n', 'conda activate mmf\nufmt --version\nufmt check .\n']"
"['docker login -u $DOCKER_USER -p $DOCKER_PASSWORD \n', 'docker build . --file Dockerfile --tag rhinosecuritylabs/pacu:latest', 'docker push rhinosecuritylabs/pacu:latest', 'python -m pip install --upgrade pip\npip install poetry\n', 'poetry version ${{ github.event.inputs.SemVer_level }}\n', 'ver=""$(poetry version -s)""\ngit config --global user.email ""<>""\ngit config --global user.name ""Github-Actions""\ngit config --global credential.helper store\necho ""https://${{secrets.GIT_USERNAME}}:${{secrets.GIT_TOKEN}}@github.com"" > ~/.git-credentials\ngit add .\ngit commit -m ""Release v${ver}""\ngit push\ngit tag ""v${ver}""\ngit push origin ""refs/tags/v${ver}""\n', 'poetry publish --build -u ""__token__"" -p ""${{ secrets.PYPI_TOKEN }}""\n', 'python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'poetry run make lint\n', 'poetry run make test\n']"
""
"['echo ""$GITHUB_CONTEXT""', 'echo ""TAG_NAME=$(echo ${{ github.event.head_commit.message }} | cut -c15-)"" >> $GITHUB_ENV\n', 'curl --silent -L https://github.com/${{env.GHR_FORK}}/releases/download/v${{env.GHR_VERSION}}/ghr_v${{env.GHR_VERSION}}_linux_amd64.tar.gz > ghr_v${{env.GHR_VERSION}}_linux_amd64.tar.gz\ntar xvzf ghr_v${{env.GHR_VERSION}}_linux_amd64.tar.gz\nmv ghr_v${{env.GHR_VERSION}}_linux_amd64/ghr /usr/local/bin/\nrm -rf ghr_v${{env.GHR_VERSION}}_linux_amd64 ghr_v${{env.GHR_VERSION}}_linux_amd64.tar.gz\n', 'ghr -u shinny-yangyang -t ${{ secrets.GITHUB_USER_TOKEN }} ${{ env.TAG_NAME }}\n']"
"['pip install black codespell flake8 isort pytest', 'black --check . || true', 'codespell --quiet-level=2 || true', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'isort --profile black . || true', 'pip install -r requirements.txt || true', 'pytest . || true']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py install', 'python utils/completion.py', 'ls -al', 'python setup.py sdist bdist_wheel', 'twine upload dist/*']"
['python sync-eaf-resources.py --really-run']
"['set -xe\npython -VV\npython -m site\npython -m pip install -r requirements/tox.pip\n', 'set -xe\npython -m tox\n', 'python -m coverage combine\nmv .metacov .metacov.${{ matrix.python-version }}.${{ matrix.os }}\n', 'set -xe\npython -VV\npython -m site\npython -m pip install -e .\npython igor.py zip_mods\n', 'set -xe\npython igor.py combine_html\n', 'echo ""total=$(python -m coverage report --format=total)"" >> $GITHUB_OUTPUT\n', 'set -xe\nexport SHA10=$(echo ${{ github.sha }} | cut -c 1-10)\nexport SLUG=$(date +\'%Y%m%d\')_$SHA10\nexport REPORT_DIR=reports/$SLUG/htmlcov\nexport REF=""${{ github.ref }}""\necho ""total=${{ needs.combine.outputs.total }}"" >> $GITHUB_ENV\necho ""sha10=$SHA10"" >> $GITHUB_ENV\necho ""slug=$SLUG"" >> $GITHUB_ENV\necho ""report_dir=$REPORT_DIR"" >> $GITHUB_ENV\necho ""url=https://htmlpreview.github.io/?https://github.com/nedbat/coverage-reports/blob/main/reports/$SLUG/htmlcov/index.html"" >> $GITHUB_ENV\necho ""branch=${REF#refs/heads/}"" >> $GITHUB_ENV\n', ""echo '### Total coverage: ${{ env.total }}%' >> $GITHUB_STEP_SUMMARY\n"", ""set -xe\ngit clone --depth=1 --no-checkout https://${{ secrets.COVERAGE_REPORTS_TOKEN }}@github.com/nedbat/coverage-reports reports_repo\ncd reports_repo\ngit sparse-checkout init --cone\ngit sparse-checkout set --skip-checks '/*' '!/reports'\ngit config user.name nedbat\ngit config user.email ned@nedbatchelder.com\ngit checkout main\n"", 'set -xe\n# Make the redirect to the latest report.\necho ""<html><head>"" > reports_repo/latest.html\necho ""<meta http-equiv=\'refresh\' content=\'0;url=${{ env.url }}\' />"" >> reports_repo/latest.html\necho ""<body>Coverage report redirect..."" >> reports_repo/latest.html\n# Make the commit message.\necho ""${{ env.total }}% - $COMMIT_MESSAGE"" > commit.txt\necho """" >> commit.txt\necho ""[View the report](${{ env.url }})"" >> commit.txt\necho """" >> commit.txt\necho ""${{ env.url }}"" >> commit.txt\necho ""${{ env.sha10 }}: ${{ env.branch }}"" >> commit.txt\n# Commit.\ncd ./reports_repo\ngit sparse-checkout set --skip-checks \'/*\' \'${{ env.report_dir }}\'\nrm ${{ env.report_dir }}/.gitignore\ngit add ${{ env.report_dir }} latest.html\ngit commit --file=../commit.txt\ngit push\necho \'[${{ env.url }}](${{ env.url }})\' >> $GITHUB_STEP_SUMMARY\n', 'python -m pip install -r requirements/kit.pip\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'ls -al wheelhouse/\n', 'python -m pip install -r requirements/kit.pip\n', 'python -m build\n', 'ls -al dist/\n', 'pypy3 -m pip install -r requirements/kit.pip\n', '# One wheel works for all PyPy versions. PYVERSIONS\n# yes, this is weird syntax: https://github.com/pypa/build/issues/202\npypy3 -m build -w -C=""--global-option=--python-tag"" -C=""--global-option=pp37.pp38.pp39""\n', 'ls -al dist/\n', 'ls -alR\n', 'set -xe\npython -VV\npython -m site\npython -m coverage debug sys\npython -m coverage debug pybehave\n', 'python -m pip install -r requirements/tox.pip\n', 'python -m tox -- -rfsEX\n', 'python -m pip install -r requirements/tox.pip\n', 'python -m tox -e lint\n', ""# We run on 3.8, but the pins were made on 3.7, so don't insist on\n# hashes, which won't match.\npython -m pip install -r requirements/tox.pip\n"", 'python -m tox -e mypy\n', 'set -xe\npython -VV\npython -m site\npython -m pip install -r requirements/tox.pip\n', 'python -m tox -e doc\n', 'set -xe\npython -VV\npython -m site\npython -m pip install -r requirements/tox.pip\n# For extreme debugging:\n# python -c ""import urllib.request as r; exec(r.urlopen(\'https://bit.ly/pydoctor\').read())""\n', 'python -m tox -- -rfsEX\n', '# `exit 1` makes sure that the job remains red with flaky runs\npython -m tox -- -rfsEX --lf -vvvvv && exit 1\n']"
""
"['sudo gem install mdl', 'if [[ ""$GITHUB_HEAD_REF"" == \'development\' && ""$GITHUB_REPOSITORY"" == \'aliparlakci/bulk-downloader-for-reddit\'  ]]; then exit 0; else exit 1; fi;\n', 'python -m pip install --upgrade pip\npip install build setuptools wheel twine\n', 'python -m build\ntwine upload dist/*\n', 'python -m pip install --upgrade pip Flake8-pyproject pytest pytest-cov\npip install .\n', './devscripts/configure${{ matrix.ext }}\n', 'flake8 . --select=E9,F63,F7,F82\n', ""pytest -m 'not slow' --verbose --cov=./bdfr/ --cov-report term:skip-covered --cov-report html\n""]"
"['docker version', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'DOCKER_BUILDKIT=1 docker build . \\\n  -f docker/Dockerfile \\\n  --target garage-dev \\\n  -t ""${DOCKER_TAG}"" \\\n  --build-arg GARAGE_GH_TOKEN \\\n  --cache-from=""rlworkgroup/garage""\n', 'docker tag ""${DOCKER_TAG}"" ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""\ndocker push ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n  -e MJKEY \\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'pushd docs && make doctest clean && popd\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e GITHUB_ACTIONS `# used by codecov` \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'[ ! -f ${MJKEY_PATH} ] || mv ${MJKEY_PATH} ${MJKEY_PATH}.bak &&\n  pytest --cov=garage --reruns 1 --cov-report=xml -m \\\n      ""not nightly and not huge and not flaky and not large and not mujoco and not mujoco_long"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 1\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e GITHUB_ACTIONS \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'[ ! -f ${MJKEY_PATH} ] || mv ${MJKEY_PATH} ${MJKEY_PATH}.bak &&\n  pytest --cov=garage --cov-report=xml --reruns 1 -m ""large and not flaky"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 1\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e MJKEY \\\n  -e GITHUB_ACTIONS \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'pytest --cov=garage --cov-report=xml --reruns 1 -m ""mujoco and not flaky"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 1\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e MJKEY \\\n  -e GITHUB_ACTIONS \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'pytest --cov=garage --cov-report=xml --reruns 1 -m ""mujoco_long and not flaky"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 1\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n  -e MJKEY \\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" pytest -v -m nightly\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n-e MJKEY \\\n--memory 6500m \\\n--memory-swap 6500m \\\n""${DOCKER_TAG}"" \\\n/bin/bash -c \\\n\'CONDA_ROOT=$HOME/miniconda \\\nCONDA=${CONDA_ROOT}/bin/conda \\\nGARAGE_BIN=${CONDA_ROOT}/envs/garage-ci/bin; \\\ntouch ${MJKEY_PATH} && \\\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && \\\nbash miniconda.sh -b -p ${CONDA_ROOT} && \\\nhash -r && \\\n${CONDA} config --set always_yes yes --set changeps1 no && \\\n${CONDA} install -c anaconda setuptools && \\\n${CONDA} update -q conda && \\\n${CONDA} init && \\\n${CONDA} info -a && \\\n${CONDA} create -n garage-ci python=3.6 pip -y && \\\n${GARAGE_BIN}/pip install --upgrade pip setuptools && \\\n${GARAGE_BIN}/pip install dist/garage.tar.gz[all,dev] && \\\n${GARAGE_BIN}/pylint --disable=all --enable=import-error garage\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n-e MJKEY \\\n--memory 6500m \\\n--memory-swap 6500m \\\n""${DOCKER_TAG}"" \\\n/bin/bash -c \\\n""export PATH=\\$PATH_NO_VENV && \\\nexport VIRTUAL_ENV= && \\\nexport PIPENV_MAX_RETRIES=2 && \\\ntouch \\${MJKEY_PATH} && \\\npip3 install --upgrade pip setuptools && \\\npip3 install pipenv && \\\npipenv --python=3.6 && \\\npipenv install dist/garage.tar.gz[all,dev] && \\\npipenv graph && \\\n# pylint will verify all imports work\npipenv run pylint --disable=all --enable=import-error garage""\n', 'echo ""$PAYLOAD_CONTEXT""', 'echo ::set-output name=run-url::https://github.com/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID', 'docker version', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'DOCKER_BUILDKIT=1 docker build . \\\n  -f docker/Dockerfile \\\n  --target garage-dev \\\n  -t ""${DOCKER_TAG}"" \\\n  --build-arg GARAGE_GH_TOKEN \\\n  --cache-from=""rlworkgroup/garage""\n', 'docker tag ""${DOCKER_TAG}"" ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""\ndocker push ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'echo ""PR_COMMIT_RANGE=${{ github.event.client_payload.pull_request.base.sha }}...${{ github.event.client_payload.pull_request.head.sha }}"" >> $GITHUB_ENV\n', 'docker run \\\n  -e PR_COMMIT_RANGE \\\n  -e MJKEY \\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" scripts/ci/check_precommit.sh\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n  -e MJKEY \\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'pip list && pushd docs && make doctest clean && popd; cat /tmp/*.log\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e GITHUB_ACTIONS `# used by codecov` \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'[ ! -f ${MJKEY_PATH} ] || mv ${MJKEY_PATH} ${MJKEY_PATH}.bak &&\n  pytest --cov=garage --cov-report=xml --reruns 1 -m \\\n      ""not gpu and not nightly and not huge and not flaky and not large and not mujoco and not mujoco_long"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 0\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e GITHUB_ACTIONS \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'[ ! -f ${MJKEY_PATH} ] || mv ${MJKEY_PATH} ${MJKEY_PATH}.bak &&\n  pytest --cov=garage --cov-report=xml --reruns 1 -m ""large and not flaky"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 0\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e MJKEY \\\n  -e GITHUB_ACTIONS \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'pytest --cov=garage --cov-report=xml --reruns 1 -m ""mujoco and not flaky"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 0\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'ci_env=""$(bash <(curl -s https://codecov.io/env))"" &&\ndocker run \\\n  -e MJKEY \\\n  -e GITHUB_ACTIONS \\\n  -e CODECOV_TOKEN \\\n  $ci_env\\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" \\\n  /bin/bash -c \\\n  \'pytest --cov=garage --cov-report=xml --reruns 1 -m ""mujoco_long and not flaky"" --durations=20 &&\n  for i in {1..5}; do\n      bash <(curl -s https://codecov.io/bash --retry 5) -Z && break\n      if [ $i == 5 ]; then\n          exit 0\n      else\n          echo ""Retry ${i}...""\n          sleep 30\n      fi\n  done\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n  -e MJKEY \\\n  --memory 6500m \\\n  --memory-swap 6500m \\\n  ""${DOCKER_TAG}"" pytest -v --reruns 1 -m nightly\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n-e MJKEY \\\n--memory 6500m \\\n--memory-swap 6500m \\\n""${DOCKER_TAG}"" \\\n/bin/bash -c \\\n\'CONDA_ROOT=$HOME/miniconda \\\nCONDA=${CONDA_ROOT}/bin/conda \\\nGARAGE_BIN=${CONDA_ROOT}/envs/garage-ci/bin; \\\ntouch ${MJKEY_PATH} && \\\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && \\\nbash miniconda.sh -b -p ${CONDA_ROOT} && \\\nhash -r && \\\n${CONDA} config --set always_yes yes --set changeps1 no && \\\n${CONDA} install -c anaconda setuptools && \\\n${CONDA} update -q conda && \\\n${CONDA} init && \\\n${CONDA} info -a && \\\n${CONDA} create -n garage-ci python=3.6 pip -y && \\\n${GARAGE_BIN}/pip install --upgrade pip setuptools && \\\n${GARAGE_BIN}/pip install dist/garage.tar.gz[all,dev] && \\\n${GARAGE_BIN}/pylint --disable=all --enable=import-error garage\'\n', 'echo ${{ secrets.CI_REGISTRY_TOKEN }} | docker login docker.pkg.github.com -u ${CI_USER} --password-stdin', 'docker pull ""docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG}""', 'docker tag docker.pkg.github.com/${OWNER}/${DOCKER_CACHE_REPO}/${DOCKER_TAG} ${DOCKER_TAG}', 'docker run \\\n-e MJKEY \\\n--memory 6500m \\\n--memory-swap 6500m \\\n""${DOCKER_TAG}"" \\\n/bin/bash -c \\\n""export PATH=\\$PATH_NO_VENV && \\\nexport VIRTUAL_ENV= && \\\nexport PIPENV_MAX_RETRIES=2 && \\\ntouch \\${MJKEY_PATH} && \\\npip3 install --upgrade pip setuptools && \\\npip3 install pipenv && \\\npipenv --python=3.6 && \\\npipenv install dist/garage.tar.gz[all,dev] && \\\npipenv graph && \\\n# pylint will verify all imports work\npipenv run pylint --disable=all --enable=import-error garage""\n', 'echo ${GITHUB_REF##*/} > VERSION', 'python setup.py sdist', 'pr_details=$(curl -v -H ""Accept: application/vnd.github.sailor-v-preview+json"" -u ${{ secrets.CI_REGISTRY_TOKEN }} ${{ github.event.issue.pull_request.url }})\necho ""::set-output name=is_from_fork::$(echo $pr_details | jq \'.head.repo.fork\')""\necho ""::set-output name=base_ref::$(echo $pr_details | jq \'.base.ref\' | sed \'s/\\""//g\')""\n']"
"['export CI_IMAGE=selitvin/petastorm_ci_auto:ci-image-06-29-2021\ndocker pull $CI_IMAGE\ndocker images\npip install -U codecov\ndocker run -v `pwd`:/petastorm --name petastorm_ci $CI_IMAGE /bin/sh -c ""sleep 3600"" &\n', 'sleep 30\nexport PYARROW_VERSION=${{matrix.PYARROW_VERSION}}\nexport NUMPY_VERSION=${{matrix.NUMPY_VERSION}}\nexport TF_VERSION=${{matrix.TF_VERSION}}\nexport PY=${{matrix.PY}}\nexport PYSPARK_VERSION=${{matrix.PYSPARK_VERSION}}\nexport ARROW_PRE_0_15_IPC_FORMAT=${{matrix.ARROW_PRE_0_15_IPC_FORMAT}}\nexport RUN=""docker exec -e ARROW_PRE_0_15_IPC_FORMAT=$ARROW_PRE_0_15_IPC_FORMAT petastorm_ci bash /run_in_venv.sh ${PY}""\nexport PYTEST=""pytest --timeout=360 -v --color=yes --cov=./ --cov-report xml:coverage.xml""\n$RUN pip install -U pip setuptools\n$RUN pip install -e /petastorm/[test,tf,torch,docs,opencv]\n$RUN pip install --upgrade numpy==$NUMPY_VERSION\n$RUN pip install -U pyarrow==${PYARROW_VERSION} tensorflow==${TF_VERSION} pyspark==${PYSPARK_VERSION}\n$RUN pip list\n$RUN mypy petastorm\n$RUN flake8 . --count --show-source --statistics\n$RUN flake8 . --count --exit-zero --max-complexity=20 --statistics\n$RUN pylint --rcfile=.pylintrc petastorm examples -f parseable -r n\n$RUN ulimit -c unlimited -S\n$RUN bash -c ""cd /petastorm/docs/autodoc && pwd && make html""\n$RUN $PYTEST -m ""forked"" --forked -Y \\\n--ignore=examples/mnist/tests/test_pytorch_mnist.py \\\n--ignore=petastorm/tests/test_pytorch_utils.py \\\n--ignore=petastorm/tests/test_pytorch_dataloader.py \\\n--ignore=petastorm/tests/test_tf_autograph.py \\\npetastorm examples\n$RUN $PYTEST -m ""not forked"" -Y  --cov-append \\\n--ignore=examples/mnist/tests/test_pytorch_mnist.py \\\n--ignore=petastorm/tests/test_pytorch_utils.py \\\n--ignore=petastorm/tests/test_pytorch_dataloader.py \\\n--ignore=petastorm/tests/test_tf_autograph.py \\\npetastorm examples\n$RUN $PYTEST  --cov-append \\\nexamples/mnist/tests/test_pytorch_mnist.py \\\npetastorm/tests/test_pytorch_dataloader.py \\\npetastorm/tests/test_pytorch_utils.py\n$RUN $PYTEST -Y --cov-append petastorm/tests/test_tf_autograph.py\n', 'codecov --required', 'export VERSION_NUMBER=$(echo ""${{ github.ref }}"" | sed -nre \'s/^[^0-9]*(([0-9]+\\.)*[0-9]+).*/\\1/p\' | sed -e \'s/\\.//g\')\necho ""Version link appendix: ${VERSION_NUMBER}""\necho ""https://github.com/uber/petastorm/blob/master/docs/release-notes.rst#release-${VERSION_NUMBER}"" > /tmp/release.md\n', 'python -c ""import sys; print(sys.version)""', 'pip install wheel\npython setup.py bdist_wheel\n']"
"['python3 -m  pip install  --user  --upgrade setuptools wheel', 'python3 setup.py  sdist  bdist_wheel']"
"['python -m pip install --upgrade pip\nmake setup\npip install -U .\n', 'make test', 'make lint', 'codecov --token ${{ secrets.CODECOV_TOKEN }} --branch ${{ github.ref }}']"
"['pip install tox\ntox -e linters\n', 'python scripts/split-tox-gh-actions/split-tox-gh-actions.py --fail-on-changes\n', 'echo ""Creating directory containing Python SDK Lambda Layer""\npip install virtualenv\n# This will also trigger ""make dist"" that creates the Python packages\nmake aws-lambda-layer\n', 'pip install virtualenv\nmake apidocs\ncd docs/_build && zip -r gh-pages ./\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'pip install coverage ""tox>=3,<4""\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n']"
"['make install', 'make lint', 'make test']"
"['python -m pip install --upgrade pip', 'pip install tox', 'tox -e isort', 'python -m pip install --upgrade pip', 'pip install tox', 'tox -e black', 'python -m pip install --upgrade pip', 'pip install tox', 'tox -e flake8', 'printenv', 'pip install tox', 'tox -e py', 'pip install tox', 'tox -e e2e', 'python -m pip install --upgrade pip', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
['pip install tox\ntox\n']
"['.github/integration-test.py build-image \\\n    --build-arg ""BASE_IMAGE=${{ matrix.distro_image }}""\n', '.github/integration-test.py run-test integration-tests \\\n    --installer-args ""--admin test-admin-username:test-admin-password"" \\\n    --installer-args ""--plugin /srv/src/integration-tests/plugins/simplest"" \\\n    ${{ matrix.extra_flags }} \\\n    test_hub.py \\\n    test_proxy.py \\\n    test_install.py \\\n    test_extensions.py \\\n    test_admin_installer.py \\\n    test_simplest_plugin.py\n', '.github/integration-test.py show-logs integration-tests\n', 'pip install -r integration-tests/requirements.txt', 'pytest integration-tests/test_bootstrap.py\n', ""export DEBIAN_FRONTEND=noninteractive\napt-get update\napt-get install --yes \\\n    python3-venv \\\n    bzip2 \\\n    git\n\npython3 -m venv /srv/venv\necho '/srv/venv/bin' >> $GITHUB_PATH\n"", 'pip install -r dev-requirements.txt\npip install -e .\n', 'pip freeze\n', 'pytest tests']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --ignore=F401 --statistics\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
[]
"['python -m pip install --upgrade pip\npip install coverage\n', 'coverage run --include=kconfig_hardened_check/engine.py,kconfig_hardened_check/test_engine.py -m unittest -v -b\ncoverage xml -i -o coverage_unittest.xml\n', 'python -m pip install --upgrade pip\necho ""Install the package via pip...""\npip --verbose install git+https://github.com/a13xp0p0v/kconfig-hardened-check\necho ""Run the installed tool...""\nkconfig-hardened-check\n', 'echo ""Check all configs with the installed tool...""\nCONFIG_DIR=`find /opt/hostedtoolcache/Python/ -name config_files`\nKCONFIGS=`find $CONFIG_DIR -type f | grep -e ""\\.config"" -e ""\\.gz""`\nCOUNT=0\nfor C in $KCONFIGS\ndo\n        COUNT=$(expr $COUNT + 1)\n        echo -e ""\\n>>>>> checking kconfig number $COUNT <<<<<""\n        kconfig-hardened-check -c $C -l /proc/cmdline\ndone\necho -e ""\\nHave checked $COUNT kconfigs""\n', 'pip install coverage\nsh .github/workflows/functional_test.sh\n', 'coverage xml -i -o coverage.xml\n']"
"['wget https://github.com/freqtrade/freqtrade/raw/develop/build_helpers/ta-lib-0.4.0-src.tar.gz\ntar zxvf ta-lib-0.4.0-src.tar.gz\ncd ta-lib\n./configure --prefix ${HOME}/dependencies/\nmake\nwhich sudo && sudo make install || make bigip_software_install\ncd ..\nrm -rf ta-lib/\n', 'python -m pip install --upgrade pip\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npip install -r requirements-dev.txt\npip install -e .\n', 'pytest --random-order --cov=technical --cov-config=.coveragerc\n', 'ruff check --format=github .\n', 'isort --check .\n', 'cp -r ./freqtrade_tmp/build_helpers .\n# mkdir build_helpers/\n# TODO: Update this to not download 3 files while only needing 1.\n# Invoke-WebRequest -Outfile build_helpers/install_windows.ps1 https://github.com/freqtrade/freqtrade/raw/develop/build_helpers/install_windows.ps1\n\n# if ( ${{ matrix.python-version }} -eq ""3.8"" ) {\n#   Invoke-WebRequest -Outfile build_helpers/TA_Lib-0.4.25-cp38-cp38-win_amd64.whl https://github.com/freqtrade/freqtrade/raw/develop/build_helpers/TA_Lib-0.4.25-cp38-cp38-win_amd64.whl\n# }\n# if ( ${{ matrix.python-version }} -eq ""3.8"" ) {\n#   Invoke-WebRequest -Outfile build_helpers/TA_Lib-0.4.25-cp39-cp39-win_amd64.whl https://github.com/freqtrade/freqtrade/raw/develop/build_helpers/TA_Lib-0.4.25-cp39-cp39-win_amd64.whl\n# }\n# if ( ${{ matrix.python-version }} -eq ""3.10"" ) {\n#   Invoke-WebRequest -Outfile build_helpers/TA_Lib-0.4.25-cp310-cp310-win_amd64.whl https://github.com/freqtrade/freqtrade/raw/develop/build_helpers/TA_Lib-0.4.25-cp310-cp310-win_amd64.whl\n# }\n./build_helpers/install_windows.ps1\n', 'pytest --random-order --cov=technical --cov-config=.coveragerc tests\n', 'ruff check --format=github technical tests\n', 'echo ""##[set-output name=branch;]$(echo ${GITHUB_REF#refs/heads/})""', 'pip install -U setuptools wheel\npython setup.py sdist bdist_wheel\n']"
"['python -m pip install cibuildwheel', 'brew config\nbrew install libomp\neval `brew shellenv`\ntee -a $GITHUB_ENV << END\nCC=/usr/bin/clang\nCXX=/usr/bin/clang++\nCFLAGS=${CFLAGS} -I${HOMEBREW_PREFIX}/include\nCXXFLAGS=${CXXFLAGS} -I${HOMEBREW_PREFIX}/include\nLDFLAGS=${LDFLAGS} -Wl,-rpath,${HOMEBREW_PREFIX}/lib -L${HOMEBREW_PREFIX}/lib -lomp\nEND\n', 'python -m cibuildwheel --output-dir dist\n', 'python -m cibuildwheel --output-dir dist\n', 'pipx run build --sdist', 'ls -l dist\ntest -f ""dist/stardist-${{ github.ref_name }}.tar.gz""\n', 'python -m pip install --upgrade pip wheel setuptools', 'pip install ""tensorflow==${{ matrix.tensorflow }}.*""', 'echo ""package_extras=test,tf1"" >> $GITHUB_ENV', 'echo ""package_extras=test"" >> $GITHUB_ENV', 'pip install edt ""numpy>=1.20""', 'pip install "".[${{env.package_extras}}]""', 'pip install "".[bioimageio]""', 'pytest -v --durations=50 -m ""not gpu""', 'python -m pip install --upgrade pip wheel setuptools', 'pip install ""tensorflow==${{ matrix.tensorflow }}.*""', 'echo ""package_extras=test,tf1"" >> $GITHUB_ENV', 'echo ""package_extras=test"" >> $GITHUB_ENV', 'pip install edt ""numpy>=1.20""', 'pip install --no-binary stardist ""stardist[${{env.package_extras}}]""', 'pip install --no-binary stardist ""stardist[${{env.package_extras}}]""', 'pip uninstall -y stardist\npip install --no-cache-dir --only-binary stardist ""stardist[${{env.package_extras}}]>0""\n', 'pip install ""stardist[bioimageio]""', 'pytest -v --durations=50 -m ""not gpu""']"
"['pip install -e .[dev] -c etc/requirements_locked.txt\n', 'pytest etc/bench.py --benchmark-json output.json\n', 'echo ""PIP_CONSTRAINT=etc/${{matrix.requirements_file}}"" >> $GITHUB_ENV\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install -e .[dev]\n', 'flake8\n', 'pytest tests -n auto --dist loadscope\n', 'python setup.py sdist', ""python -m pip install --upgrade pip\npip install --extra-index-url https://test.pypi.org/simple trading-calendars\npython -c 'import trading_calendars;print(trading_calendars.__version__)'\npip uninstall -y trading-calendars\n"", ""pip install trading-calendars\npython -c 'import trading_calendars;print(trading_calendars.__version__)'\n""]"
""
"['echo ${{ steps.docker_build.outputs.digest }}', 'pip install -U pip\nmake deps\n', 'python -m pip install wheel\nmake build\n', 'pip install -U pip\npip install pyinstaller\nmake deps\n', 'make pyinstaller\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install -U pip\nmake dev-deps\nmake install\n', 'make test\n']"
""
""
"['python -m pip install --upgrade pip\npip install wheel\npip install -e .[asgi-file-uploads,tracing,telemetry,test,dev]\n', 'pytest benchmark --benchmark-storage=file://benchmark/results --benchmark-autosave\ncd benchmark && python rotate_results.py ""${{ matrix.python-version }}""\n', 'git config user.name ""pytest-benchmark""\ngit config user.email ""<>""\ngit add benchmark/results --all\ngit commit -m ""Benchmark results for ${{ matrix.python-version }}""\ngit push\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\npip install wheel\npip install -e .[asgi-file-uploads,tracing,telemetry,test,dev]\n', 'pytest --cov=ariadne --cov=tests\n', 'pylint ariadne tests\nmypy ariadne tests_mypy --ignore-missing-imports --check-untyped-defs\nblack --check .\n', 'pytest benchmark --benchmark-storage=file://benchmark/results --benchmark-compare\n', 'python -m pip install --upgrade pip\npip install wheel\npip install -r tests_integrations/${{ matrix.library }}/requirements.txt\npip install -e .[asgi-file-uploads,tracing,telemetry,test,dev]\n', 'pytest tests_integrations/${{ matrix.library }}\n']"
"['docker login -u $DOCKER_USER -p $DOCKER_PASSWORD \n', 'docker build . --file Dockerfile --tag rhinosecuritylabs/cloudgoat:latest', 'docker push rhinosecuritylabs/cloudgoat:latest', 'pip install -r requirements.txt', 'pip install pytest', 'pytest -v']"
"['sudo apt-get update\nsudo apt-get install -y libusb-1.0-0-dev libudev-dev\n', 'brew install libusb\n', 'python -m pip install --upgrade pip\n', 'python -m venv vtest\nexport PATH=$PWD/vtest/bin/:$PWD/vtest/Scripts/:$PATH\npython -m pip install --upgrade pip setuptools setuptools_scm wheel\npython -m pip install \\\n  colorlog \\\n  crcmod==1.7 \\\n  docopt \\\n  hidapi \\\n  pillow \\\n  pytest \\\n  pyusb \\\n  ""libusb-package; sys_platform == \'win32\' or sys_platform == \'cygwin\'"" \\\n  ""smbus; sys_platform == \'linux\'"" \\\n  ""winusbcdc>=1.5; sys_platform == \'win32\'""\n', 'export PATH=$PWD/vtest/bin/:$PWD/vtest/Scripts/:$PATH\nXDG_RUNTIME_DIR=.tests_rundir python -m pytest\n', 'python -m venv vinstall\nexport PATH=$PWD/vinstall/bin/:$PWD/vinstall/Scripts/:$PATH\npython -m pip install --upgrade pip setuptools setuptools_scm wheel\npython -m pip install .\n', 'export PATH=$PWD/vinstall/bin/:$PWD/vinstall/Scripts/:$PATH\nliquidctl list --verbose --debug\n', 'python -m pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --extend-exclude vtest,vinstall --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --extend-exclude vtest,vinstall --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics\n']"
""
"['echo ""Looking for Blender $BLENDER_MAJOR.$BLENDER_MINOR""\nBLENDER_URL=""$(curl -s https://builder.blender.org/download/daily/ | \\\n  grep -oe \'http[^\\""]*blender-\'$BLENDER_MAJOR\'\\.\'$BLENDER_MINOR\'[^\\""]*linux[^\\""]*\\.tar\\.xz\' | \\\n  tail -n1)""\nif [ -z ""$BLENDER_URL"" ]; then\n  echo ""Not found! Download URL may have changed; CI may need update.""\n  exit 1\nfi\necho ""Found: $BLENDER_URL""\necho ""blender-url=$BLENDER_URL"" >> $GITHUB_OUTPUT\n', 'mkdir /opt/blender\necho ""Downloading: ${{ steps.blender_version.outputs.blender-url }}""\ncurl -SL ""${{ steps.blender_version.outputs.blender-url }}"" | \\\n  tar -Jx -C /opt/blender --strip-components=1\n', 'sudo ln -s /opt/blender/blender /usr/local/bin/blender\nblender --version\nADDON_DIR=/opt/blender/$BLENDER_MAJOR.$BLENDER_MINOR/scripts/addons\nrm -rf $ADDON_DIR/io_scene_gltf2\ncp -r addons/io_scene_gltf2 $ADDON_DIR\ncd tests\nyarn install\nmkdir -p out\n', 'cd tests\nOUT_PREFIX=$GITHUB_WORKSPACE/tests/out yarn test-bail --reporter-options reportDir=out/mochawesome\n']"
"['pip install -U pip setuptools wheel\npip install -r ./requirements.txt\n', 'mypy --strict mypy_django_plugin', 'mypy --strict django_stubs_ext', 'mypy --strict scripts', 'mypy --cache-dir=/dev/null --no-incremental django-stubs', 'sudo apt-get update\nsudo apt-get install binutils libproj-dev gdal-bin\n', 'pip install -U pip setuptools wheel\npip install -r ./requirements.txt\n', ""PYTHONPATH='.' pytest"", 'sudo apt-get update\nsudo apt-get install binutils libproj-dev gdal-bin\n', 'pip install -U pip setuptools wheel\npip install -r ./requirements.txt\n', 'python ./scripts/typecheck_tests.py --django_version=""${{ matrix.django-version }}""']"
"['python -m pip install --upgrade pip\npip install setuptools\npip install tox tox-gh-actions\n', 'python setup.py install\n', 'tox\n', 'cd examples\nfor file in $(ls example*.py); do\n    echo ""::group::Run $file""\n    python3 ""$file""\n    echo ""::endgroup::""\ndone\n', 'python -m pip install --upgrade pip\npip install setuptools\n', 'python setup.py sdist\n']"
"['git checkout HEAD^2', ""pip install -e '.[test,tui]'\n"", 'pytest\n', 'pip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', ""pip install -e '.[docs]'\n"", 'codespell docs/*.rst --ignore-words docs/codespell-ignore-words.txt\ncodespell sqlite_utils --ignore-words docs/codespell-ignore-words.txt\n', 'sudo apt-get install libsqlite3-mod-spatialite', 'python -m pip install --upgrade pip\npython -m pip install -e .[test]\npython -m pip install pytest-cov\n', 'ls -lah\npytest --cov=sqlite_utils --cov-report xml:coverage.xml --cov-report term\nls -lah', ""pip install -e '.[test,mypy,flake8,tui]'\n"", 'pip install numpy', 'sudo apt-get install libsqlite3-mod-spatialite', '(cd tests && gcc ext.c -fPIC -shared -o ext.so && ls -lah)', 'pytest -v\n', 'mypy sqlite_utils tests', 'flake8', 'black . --check', 'cog --check README.md docs/*.rst\n']"
"['docker run -v /tmp/lokalise:/opt/dest lokalise/lokalise-cli-2 lokalise2 \\\n--token ""${{ secrets.lokalise_token }}"" \\\n--project-id 465185555eee18dd537ca6.39714580 file download \\\n--format json \\\n--unzip-to /opt/dest \\\n--original-filenames=false \\\n--export-sort a_z \\\n--placeholder-format icu \\\n--json-unescaped-slashes=true \\\n--indentation 2sp \\\n--export-empty-as base \\\n--add-newline-eof \\\n--replace-breaks=false\n', 'cp /tmp/lokalise/locale/* /home/runner/work/alexa_media_player/alexa_media_player/custom_components/alexa_media/translations/\n', 'git config --global user.name ""semantic-release""\ngit config --global user.email ""semantic-release@GitHub""\n', 'git add custom_components/alexa_media/translations/*.json\ngit commit -m ""docs: update localization"" -a\n', 'cd /home/runner/work/alexa_media_player/alexa_media_player/custom_components/alexa_media\nzip alexa_media.zip -r ./\n', 'echo ""release_version=`git describe --abbrev=0`"" >> $GITHUB_ENV\n', 'sleep 5\n']"
"['cd docs\nnpm install\nnpm run build\ncd -\n', 'cd frontend\nnpm install\nnpm run build\ncd ..\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist\ntwine upload dist/*\n', 'pip install pip -U\npip install -r requirements.txt.lock -U\npip install -r requirements.dev.txt -U\n', 'python -m pytest tests --cov=lyrebird\n', 'pip install pip -U\npip install -r requirements.txt.lock -U\npip install -r requirements.dev.txt -U\n', 'python -m pytest e2e_tests --cov=lyrebird\n']"
"['pip3 install --upgrade pip setuptools wheel\npip3 install virtualenv poetry pipenv\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'sudo apt-get install libcurl4-gnutls-dev libgnutls28-dev\n', 'pip3 install tests/pycurl-7.43.0.5-cp39-cp39-win_amd64.whl\n', 'brew upgrade\nbrew uninstall --ignore-dependencies curl\nbrew install openssl\nbrew install curl\npip3 install wheel\npip3 install pycurl\n', 'pip3 install --upgrade wheel\npip3 install -r requirements.txt\npip3 install -r requirements_dev.txt\n', 'invoke coverage --gui\n', 'invoke lint || pylint-exit $?\n', 'coveralls --service=github\n']"
[]
""
""
"['python -m pip install --upgrade pip\npip install -e .[dev]\nmake docs\n', 'pip install --upgrade pip\npip install invoke .[test]\n', 'make checkdeps OUTPUT_PATH=tests/requirement_files/latest_requirements.txt', 'pip install invoke .[dev]', 'invoke lint', 'pip install .[dev]', 'make docs', 'pip install invoke rundoc .', 'invoke readme', 'pip install invoke .[test]', 'invoke pytest', 'pip install invoke .[test]', 'invoke minimum', 'python -m pip install pywinpty==2.0.1', 'pip install invoke jupyter matplotlib .', 'invoke tutorials']"
"['python -c ""import sys; print(sys.version)""', 'python -m pip install wheel setuptools pip --upgrade', 'python -m pip install numpy==1.20.3', 'python -m pip install numpy==1.21.6', 'python -m pip install numpy==1.22.4', 'python -m pip install numpy==1.24.3', 'python -c ""import numpy; print(numpy.__version__)""', 'python -m pip install scipy Cython pytest pytest-cov flake8\npython -m pip install -e .[tests]\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pytest --cov=cornac\n', 'python -c ""import sys; print(sys.version)""', 'python -m pip install numpy==1.20.3', 'python -m pip install numpy==1.21.6', 'python -m pip install numpy==1.22.4', 'python -m pip install numpy==1.24.3', 'python -c ""import numpy; print(numpy.__version__)""', 'python -m pip install scipy Cython wheel\n', 'python setup.py bdist_wheel', 'for f in dist/*.whl; do mv ""$f"" ""$(echo ""$f"" | sed s/linux/manylinux1/)""; done', 'python -c ""import sys; print(sys.version)""', 'python -m pip install numpy==1.22.4\npython -c ""import numpy; print(numpy.__version__)""\n', 'python -m pip install scipy Cython wheel\n', 'python setup.py sdist']"
"['python -c ""\nimport os, re\ngithub_ref = os.environ.get(\'GITHUB_REF\')\nif github_ref == \'refs/heads/master\':\n  target = \'master\'\nelif github_ref.startswith(\'refs/tags/\'):\n  m = re.match(r\'^refs/tags/(.*)$\', github_ref);\n  target = m.group(1)\nelse:\n  target = \'\'\nif \'GITHUB_OUTPUT\' in os.environ:\n  with open(os.environ[\'GITHUB_OUTPUT\'], \'a\') as f:\n    print(\'{0}={1}\'.format(\'target\', target), file=f)""\n', ""make docs git_tags='${{steps.get_all_tags.outputs.data}}'\n""]"
""
""
"['python -m pip install --upgrade pip setuptools wheel twine\npython -m pip install -e .[dev]\n', 'make lint\n', 'make tests\n', 'make dist\n']"
"['pip install torch', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
"['for SERVICE in ""shippingservice"" ""productcatalogservice""; do\n  echo ""testing $SERVICE...""\n  pushd src/$SERVICE\n  go test\n  popd\ndone\n', 'dotnet test src/cartservice/\n', 'PR_NUMBER=$(echo $GITHUB_REF | awk \'BEGIN { FS = ""/"" } ; { print $3 }\')\nNAMESPACE=""pr${PR_NUMBER}""\necho ""::set-env name=NAMESPACE::$NAMESPACE""\necho ""::set-env name=PR_NUMBER::$PR_NUMBER""\n\ngcloud container clusters get-credentials $PR_CLUSTER --zone $ZONE --project $PROJECT_ID\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: $NAMESPACE\nEOF\necho Deploying application\nskaffold config set --global local-cluster false\nskaffold run --default-repo=gcr.io/$PROJECT_ID/$GITHUB_REF --tag=$GITHUB_SHA --namespace=$NAMESPACE -p network-policies\n', 'set -x\nkubectl config set-context --current --namespace=$NAMESPACE\nkubectl wait --for=condition=available --timeout=1000s deployment/redis-cart\nkubectl wait --for=condition=available --timeout=1000s deployment/adservice\nkubectl wait --for=condition=available --timeout=1000s deployment/cartservice\nkubectl wait --for=condition=available --timeout=1000s deployment/checkoutservice\nkubectl wait --for=condition=available --timeout=1000s deployment/currencyservice\nkubectl wait --for=condition=available --timeout=1000s deployment/emailservice\nkubectl wait --for=condition=available --timeout=1000s deployment/frontend\nkubectl wait --for=condition=available --timeout=1000s deployment/loadgenerator\nkubectl wait --for=condition=available --timeout=1000s deployment/paymentservice\nkubectl wait --for=condition=available --timeout=1000s deployment/productcatalogservice\nkubectl wait --for=condition=available --timeout=1000s deployment/recommendationservice\nkubectl wait --for=condition=available --timeout=1000s deployment/shippingservice\n', 'set -x\n# start fresh loadgenerator pod\nkubectl delete pod -l app=loadgenerator\n# wait for requests to come in\nREQUEST_COUNT=""0""\nwhile [[ ""$REQUEST_COUNT""  -lt ""50""  ]]; do\n    sleep 5\n    REQUEST_COUNT=$(kubectl logs -l app=loadgenerator | grep Aggregated | awk \'{print $2}\')\ndone\n# ensure there are no errors hitting endpoints\nERROR_COUNT=$(kubectl logs -l app=loadgenerator | grep Aggregated | awk \'{print $3}\' | sed ""s/[(][^)]*[)]//g"")\nif [[ ""$ERROR_COUNT"" -gt ""0"" ]]; then\n  exit 1\nfi\n', 'for SERVICE in ""shippingservice"" ""productcatalogservice""; do\n  echo ""testing $SERVICE...""\n  pushd src/$SERVICE\n  go test\n  popd\ndone\n', 'dotnet test src/cartservice/\n', 'NAMESPACE=""pr${PR_NUMBER}""\necho ""::set-env name=NAMESPACE::$NAMESPACE""\n\ngcloud container clusters get-credentials $PR_CLUSTER --zone $ZONE --project $PROJECT_ID\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: $NAMESPACE\nEOF\necho Deploying application\nskaffold config set --global local-cluster false\nskaffold run --default-repo=gcr.io/$PROJECT_ID/refs/pull/$PR_NUMBER --tag=$PR_NUMBER --namespace=$NAMESPACE -p network-policies\n', 'set -x\nkubectl config set-context --current --namespace=$NAMESPACE\nkubectl wait --for=condition=available --timeout=1000s deployment/redis-cart\nkubectl wait --for=condition=available --timeout=1000s deployment/adservice\nkubectl wait --for=condition=available --timeout=1000s deployment/cartservice\nkubectl wait --for=condition=available --timeout=1000s deployment/checkoutservice\nkubectl wait --for=condition=available --timeout=1000s deployment/currencyservice\nkubectl wait --for=condition=available --timeout=1000s deployment/emailservice\nkubectl wait --for=condition=available --timeout=1000s deployment/frontend\nkubectl wait --for=condition=available --timeout=1000s deployment/loadgenerator\nkubectl wait --for=condition=available --timeout=1000s deployment/paymentservice\nkubectl wait --for=condition=available --timeout=1000s deployment/productcatalogservice\nkubectl wait --for=condition=available --timeout=1000s deployment/recommendationservice\nkubectl wait --for=condition=available --timeout=1000s deployment/shippingservice\n', 'set -x\nNAMESPACE=""pr${PR_NUMBER}""\nget_externalIP() {\n  kubectl get service frontend-external --namespace $NAMESPACE -o jsonpath=\'{.status.loadBalancer.ingress[0].ip}\'\n}\nuntil [[ -n ""$(get_externalIP)"" ]]; do\n  echo ""Querying for external IP for frontend-external on namespace: $NAMESPACE{}""\nsleep 3\ndone\nEXTERNAL_IP=$(get_externalIP)\necho ""::set-env name=EXTERNAL_IP::$EXTERNAL_IP""\n', 'set -x\n# start fresh loadgenerator pod\nkubectl delete pod -l app=loadgenerator\n# wait for requests to come in\nREQUEST_COUNT=""0""\nwhile [[ ""$REQUEST_COUNT""  -lt ""50""  ]]; do\n    sleep 5\n    REQUEST_COUNT=$(kubectl logs -l app=loadgenerator | grep Aggregated | awk \'{print $2}\')\ndone\n# ensure there are no errors hitting endpoints\nERROR_COUNT=$(kubectl logs -l app=loadgenerator | grep Aggregated | awk \'{print $3}\' | sed ""s/[(][^)]*[)]//g"")\nif [[ ""$ERROR_COUNT"" -gt ""0"" ]]; then\n  exit 1\nfi\n', 'curl \\\n  -X POST \\\n  $COMMENTS_URL \\\n  -H ""Content-Type: application/json"" \\\n  -H ""Authorization: token $GITHUB_TOKEN"" \\\n  --data \'{ ""body"": ""ðŸš² PR staged at \'""http://${EXTERNAL_IP}""\'""}\'\nsleep 60\n', 'gcloud container clusters get-credentials $PR_CLUSTER \\\n    --zone $ZONE --project $PROJECT_ID\nNAMESPACE=""pr${PR_NUMBER}""\nkubectl delete namespace $NAMESPACE\n', 'cd helm-chart/\nhelm lint --strict\n', 'cd helm-chart/\nhelm template . > helm-template.yaml\ncat helm-template.yaml \nkustomize create --resources helm-template.yaml\nkustomize build .\n', '# Test related to https://medium.com/google-cloud/b5bd26253a4c\ncd helm-chart/\nSPANNER_CONNECTION_STRING=projects/PROJECT_ID/instances/SPANNER_INSTANCE_NAME/databases/SPANNER_DATABASE_NAME\nhelm template . \\\n  --set nativeGrpcHealthCheck=true \\\n  -n onlineboutique \\\n  > helm-template.yaml\ncat helm-template.yaml\nkustomize build .\n', '# Test related to https://medium.com/google-cloud/f7248e077339\ncd helm-chart/\nSPANNER_CONNECTION_STRING=projects/PROJECT_ID/instances/SPANNER_INSTANCE_NAME/databases/SPANNER_DATABASE_NAME\nSPANNER_DB_USER_GSA_ID=spanner-db-user@my-project.iam.gserviceaccount.com\nhelm template . \\\n  --set cartDatabase.inClusterRedis.create=false \\\n  --set cartDatabase.type=spanner \\\n  --set cartDatabase.connectionString=${SPANNER_CONNECTION_STRING} \\\n  --set serviceAccounts.create=true \\\n  --set serviceAccounts.annotationsOnlyForCartservice=true \\\n  --set ""serviceAccounts.annotations.iam\\.gke\\.io/gcp-service-account=${SPANNER_DB_USER_GSA_ID}"" \\\n  -n onlineboutique \\\n  > helm-template.yaml\ncat helm-template.yaml\nkustomize build .\n', '# Test related to https://medium.com/google-cloud/246119e46d53\ncd helm-chart/\nhelm template . \\\n  --set networkPolicies.create=true \\\n  --set sidecars.create=true \\\n  --set serviceAccounts.create=true \\\n  --set authorizationPolicies.create=true \\\n  --set frontend.externalService=false \\\n  --set frontend.virtualService.create=true \\\n  --set frontend.virtualService.gateway.name=asm-ingressgateway \\\n  --set frontend.virtualService.gateway.namespace=asm-ingress \\\n  --set frontend.virtualService.gateway.labelKey=asm \\\n  --set frontend.virtualService.gateway.labelValue=ingressgateway \\\n  -n onlineboutique \\\n  > helm-template.yaml\ncat helm-template.yaml\nkustomize build .\n', '# Test related to https://medium.com/google-cloud/64b71969318d\ncd helm-chart/\nREDIS_IP=0.0.0.0\nREDIS_PORT=7378\nREDIS_CERT=dsjfgkldsjflkdsjflksdajfkldsjkfljsdaklfjaskjfakdsjfaklsdjflskadjfklasjfkls\nhelm template . \\\n  --set cartDatabase.inClusterRedis.create=false \\\n  --set cartDatabase.connectionString=${REDIS_IP}:${REDIS_PORT} \\\n  --set cartDatabase.externalRedisTlsOrigination.enable=true \\\n  --set cartDatabase.externalRedisTlsOrigination.certificate=""${REDIS_CERT}"" \\\n  --set cartDatabase.externalRedisTlsOrigination.endpointAddress=${REDIS_IP} \\\n  --set cartDatabase.externalRedisTlsOrigination.endpointPort=${REDIS_PORT} \\\n  -n onlineboutique \\\n  > helm-template.yaml\ncat helm-template.yaml\nkustomize build .\n', 'cd kustomize/\nkubectl kustomize .\n', 'cd kustomize/tests\nKUSTOMIZE_TESTS_SUBFOLDERS=$(ls -d */)\nfor test in $KUSTOMIZE_TESTS_SUBFOLDERS;\ndo\n    echo ""## kustomize build for "" + $test\n    kustomize build $test\ndone\n', 'skaffold config set --global local-cluster false\n# tag with git hash\nskaffold build --default-repo=gcr.io/$PROJECT_ID \\\n               --tag=$GITHUB_SHA\n', 'cd terraform/\nterraform init -backend=false\nterraform validate\n']"
""
"['python -m pip install --upgrade pip wheel\nif [ ""${{ matrix.torch }}"" == ""1.7.0"" ]; then\n    pip install -r requirements.txt torch==1.7.0 torchvision==0.8.1 --extra-index-url https://download.pytorch.org/whl/cpu\nelse\n    pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu\nfi\n', 'python -c ""import utils; utils.notebook_init()""\necho ""RUNNER_OS is ${{ runner.os }}""\necho ""GITHUB_EVENT_NAME is ${{ github.event_name }}""\necho ""GITHUB_WORKFLOW is ${{ github.workflow }}""\necho ""GITHUB_ACTOR is ${{ github.actor }}""\necho ""GITHUB_REPOSITORY is ${{ github.repository }}""\necho ""GITHUB_REPOSITORY_OWNER is ${{ github.repository_owner }}""\npython --version\npip --version\npip list\n', '# export PYTHONPATH=""$PWD""  # to run \'$ python *.py\' files in subdirectories\nm=${{ matrix.model }}  # official weights\nb=runs/train/exp/weights/best  # best.pt checkpoint\npython train.py --imgsz 64 --batch 32 --weights $m.pt --cfg $m.yaml --epochs 1 --device cpu  # train\nfor d in cpu; do  # devices\n  for w in $m $b; do  # weights\n    python val.py --imgsz 64 --batch 32 --weights $w.pt --device $d  # val\n    python detect.py --imgsz 64 --weights $w.pt --device $d  # detect\n  done\ndone\npython hubconf.py --model $m  # hub\n# python models/tf.py --weights $m.pt  # build TF model\npython models/yolo.py --cfg $m.yaml  # build PyTorch model\npython export.py --weights $m.pt --img 64 --include torchscript  # export\npython - <<EOF\nimport torch\nim = torch.zeros([1, 3, 64, 64])\nfor path in \'$m\', \'$b\':\n    model = torch.hub.load(\'.\', \'custom\', path=path, source=\'local\')\n    print(model(\'data/images/bus.jpg\'))\n    model(im)  # warmup, build grids for trace\n    torch.jit.trace(model, [im])\nEOF\n', ""m=${{ matrix.model }}-seg  # official weights\nb=runs/train-seg/exp/weights/best  # best.pt checkpoint\npython segment/train.py --imgsz 64 --batch 32 --weights $m.pt --cfg $m.yaml --epochs 1 --device cpu  # train\npython segment/train.py --imgsz 64 --batch 32 --weights '' --cfg $m.yaml --epochs 1 --device cpu  # train\nfor d in cpu; do  # devices\n  for w in $m $b; do  # weights\n    python segment/val.py --imgsz 64 --batch 32 --weights $w.pt --device $d  # val\n    python segment/predict.py --imgsz 64 --weights $w.pt --device $d  # predict\n    python export.py --weights $w.pt --img 64 --include torchscript --device $d  # export\n  done\ndone\n"", ""m=${{ matrix.model }}-cls.pt  # official weights\nb=runs/train-cls/exp/weights/best.pt  # best.pt checkpoint\npython classify/train.py --imgsz 32 --model $m --data mnist160 --epochs 1  # train\npython classify/val.py --imgsz 32 --weights $b --data ../datasets/mnist160  # val\npython classify/predict.py --imgsz 32 --weights $b --source ../datasets/mnist160/test/7/60.png  # predict\npython classify/predict.py --imgsz 32 --weights $m --source data/images/bus.jpg  # predict\npython export.py --weights $b --img 64 --include torchscript  # export\npython - <<EOF\nimport torch\nfor path in '$m', '$b':\n    model = torch.hub.load('.', 'custom', path=path, source='local')\nEOF\n"", 'LYCHEE_URL=$(curl -s https://api.github.com/repos/lycheeverse/lychee/releases/latest | grep ""browser_download_url"" | grep ""x86_64-unknown-linux-gnu.tar.gz"" | cut -d \'""\' -f 4)\ncurl -L $LYCHEE_URL -o lychee.tar.gz\ntar xzf lychee.tar.gz\nsudo mv lychee /usr/local/bin\n']"
""
"['# setup-python puts `python` into the `PATH`, not `python3`, yet\n# `git-filter-repo` expects `python3` in the `PATH`. Let\'s add\n# a shim.\nprintf \'#!/bin/sh\\n\\nexec python ""$@""\\n\' >python3 &&\n\nexport PATH=$PWD:$PATH &&\n\nif ! t/run_tests -q -v -x\nthen\n  mkdir failed &&\n  tar czf failed/failed.tar.gz t\n  exit 1\nfi\n']"
"['pip install psutil', 'pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} --no-cache-dir', 'pip install wheel\npython setup.py bdist_wheel\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'sudo apt-get update && sudo apt-get install -y ffmpeg libturbojpeg', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests/test_image tests/test_transforms tests/test_video tests/test_arraymisc.py tests/test_visualization.py tests/test_utils/test_env.py --ignore=tests/test_image/test_io.py', 'sudo apt-get update && sudo apt-get install -y ffmpeg libturbojpeg', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests --ignore=tests/test_ops', 'sudo apt-get update && sudo apt-get install -y ffmpeg libturbojpeg', 'pip install pip wheel --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'sudo apt-get update && sudo apt-get install -y ffmpeg libturbojpeg', 'pip install pip wheel --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip wheel --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y git ffmpeg libturbojpeg', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip wheel --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y git ffmpeg libturbojpeg', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip wheel --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y git ffmpeg libturbojpeg', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip wheel --upgrade', 'pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests --ignore=tests/test_ops --ignore tests/test_image/test_io.py', 'python -m pip install pip wheel --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests/ --ignore tests/test_image/test_io.py', 'brew install ffmpeg jpeg-turbo', 'pip install pip wheel --upgrade', 'pip install torch==${{ matrix.torch }} torchvision==${{ matrix.torchvision }}', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests/', 'pip install pip wheel --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y git ffmpeg libturbojpeg', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip wheel --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y git ffmpeg libturbojpeg', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'coverage run --branch --source mmcv -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip wheel --upgrade', 'pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests --ignore=tests/test_ops --ignore tests/test_image/test_io.py', 'python -m pip install pip wheel --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests/ --ignore tests/test_image/test_io.py', 'brew install ffmpeg jpeg-turbo', 'pip install pip wheel --upgrade', 'pip install torch==${{ matrix.torch }} torchvision==${{ matrix.torchvision }}', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', 'pip install ninja psutil', 'pip install -e . -v', 'pip install -r requirements/test.txt', 'pytest tests/', 'pip install setuptools --upgrade', 'sed -i ""s/os.getenv(\'MMCV_WITH_OPS\', \'1\')/os.getenv(\'MMCV_WITH_OPS\', \'0\')/g"" setup.py\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install setuptools --upgrade', 'python setup.py sdist', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
"['unzip benchmark-assets.zip', 'python -m pip install -U jupyterlab~=3.0 jupyter_packaging~=0.10\n', 'python -m pip install .\n', 'jupyter nbextension list 2>&1 | grep -ie ""voila/extension.*enabled"" -\njupyter labextension list 2>&1 | grep -ie ""@voila-dashboards/jupyterlab-preview.*enabled.*ok"" -\njupyter server extension list 2>&1 | grep -ie ""voila\\.server_extension.*enabled"" -\n', 'python -m jupyterlab.browser_check\n', 'jlpm\njlpm run eslint:check\njlpm run prettier:check\n', 'mamba install -q python=${{ matrix.python_version }} pip jupyterlab_pygments==0.1.0 pytest-cov pytest-rerunfailures nodejs=18 yarn=1 ipywidgets matplotlib xeus-cling  ""traitlets>=5.0.3,<6"" ipykernel', 'whereis python\npython --version\nyarn install --network-timeout 100000\npython -m pip install "".[test,dev]""\n(cd tests/test_template; pip install .)\n(cd tests/skip_template; pip install .)\n', 'pre-commit run --all-files --show-diff-on-failure\n', ""VOILA_TEST_XEUS_CLING=1 py.test tests/ --async-test-timeout=240 --reruns 2 --reruns-delay 1\nvoila --help  # Making sure we can run `voila --help`\n# tests if voila sends a 'heartbeat' to avoid proxies from closing an apparently stale connection\n# Note that wget is the only easily available software that has a read-timeout\nvoila tests/notebooks/sleep10seconds.ipynb --port=8878 --VoilaConfiguration.http_keep_alive_timeout=2 &\nsleep 2\nwget --read-timeout=5 --tries=1 http://localhost:8878\n"", 'mamba install -q python=${{ matrix.python_version }} pip jupyterlab_pygments==0.1.0 pytest-cov pytest-rerunfailures nodejs=18 yarn=1 ipywidgets matplotlib xeus-cling ""traitlets>=5.0.3,<6"" ipykernel', 'whereis python\npython --version\nyarn install --network-timeout 100000\npython -m pip install "".[test]""\n(cd tests/test_template; pip install .)\n(cd tests/skip_template; pip install .)\n', ""py.test tests/ --async-test-timeout=240 --reruns 2 --reruns-delay 1\nvoila --help  # Making sure we can run `voila --help`\n# tests if voila sends a 'heartbeat' to avoid proxies from closing an apparently stale connection\n# Note that wget is the only easily available software that has a read-timeout\nvoila tests/notebooks/sleep10seconds.ipynb --port=8878 --VoilaConfiguration.http_keep_alive_timeout=2 &\nsleep 2\nwget --read-timeout=5 --tries=1 http://localhost:8878\n"", 'python -m pip install jupyterlab_pygments==0.1.0 pytest-cov pytest-rerunfailures ipywidgets matplotlib traitlets ipykernel\nyarn install --network-timeout 100000\npython -m pip install "".[test]""\ncd tests/test_template\npip install .\ncd ../skip_template\npip install .\n', 'set VOILA_TEST_DEBUG=1\npy.test tests/ --async-test-timeout=240 --reruns 2 --reruns-delay 1\n', 'python -m pip install setuptools jupyter_packaging~=0.10 ""jupyterlab>=3,<4"" build\n', 'python -m build\n', 'pushd packages/jupyterlab-preview\nnpm pack\npopd\ncp packages/*/*.tgz dist\n', 'cd dist\nsha256sum * | tee SHA256SUMS\n', '${{ matrix.py_cmd }} -m pip install pip wheel jupyterlab~=3.0 notebook~=6.4\n', 'cd dist\n${{ matrix.py_cmd }} -m pip install -vv ${{ matrix.dist }}\n', '${{ matrix.py_cmd }} -m pip freeze\n${{ matrix.py_cmd }} -m pip check\n', 'jupyter labextension list\njupyter labextension list 2>&1 | grep -ie ""@voila-dashboards/jupyterlab-preview.*enabled.*ok"" -\njupyter server extension list\njupyter server extension list 2>&1 | grep -ie ""voila.server_extension.*enabled"" -\njupyter nbextension list\njupyter nbextension list 2>&1 | grep -ie ""voila/extension.*enabled"" -\nvoila --version\nvoila --help\n', 'echo ""Optional): Review Draft Release: ${{ steps.prep-release.outputs.release_url }}""\n', 'echo ""Verify the final release""\necho ${{ steps.finalize-release.outputs.release_url }}\n', 'echo ""Failed to Publish the Draft Release Url:""\necho ${{ steps.populate-release.outputs.release_url }}\n', 'python -m pip install -r requirements-visual-test.txt\npython -m pip install "".[test]""\njlpm\njlpm build\njupyter labextension develop . --overwrite\ncd ui-tests\njlpm install --frozen-lockfile\n', '# Mount a volume to overwrite the server configuration\njlpm start 2>&1 > /tmp/voila_server.log &\n', 'jlpm playwright install chromium', 'jlpm run test', 'cd ui-tests\n\n# Publish image to cml.dev\necho """" >> ${REPORT}\ncml-publish ./benchmark-results/voila-benchmark.png --md >> ${REPORT}\necho """" >> ${REPORT}\n\n# Test if metadata have changed\nexport METADATA_DIFF=""/tmp/metadata.diff""\ndiff -u <(jq --sort-keys .metadata benchmark-results/voila-benchmark.json) <(jq --sort-keys .metadata voila-benchmark-expected.json) > ${METADATA_DIFF} || true\nif [[ -s ${METADATA_DIFF} ]]; then\n  echo ""<details><summary>:exclamation: Test metadata have changed</summary>"" >> ${REPORT}\n  echo """" >> ${REPORT}\n  echo ""\\`\\`\\`diff"" >> ${REPORT}\n  cat ${METADATA_DIFF} >> ${REPORT}\n  echo ""\\`\\`\\`"" >> ${REPORT}\n  echo """" >> ${REPORT}\n  echo ""</details>"" >> ${REPORT}\nfi\n\n# Save PR number for comment publication\necho ""${{ github.event.number }}"" > ./benchmark-results/NR\n', 'cat /tmp/voila_server.log\n', 'git config --global hub.protocol https', 'hub pr checkout ${{ github.event.issue.number }}', 'python -m pip install -r requirements-visual-test.txt\npython -m pip install "".[test]""\njlpm\njlpm build\njupyter labextension develop . --overwrite\ncd ui-tests\njlpm install --frozen-lockfile\n']"
""
"['python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'mkdir -p $HOME/bin && curl -sfL https://raw.githubusercontent.com/reviewdog/reviewdog/master/install.sh| sh -s -- -b $HOME/bin\necho ::add-path::$HOME/bin\n', 'LINE_LENGTH=89 catalyst-codestyle-flake8 . | reviewdog -f=pep8 -reporter=github-pr-review\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'catalyst-check-codestyle --line-length 89\nmake check-docs\n', 'pip install --upgrade setuptools wheel\npython setup.py sdist bdist_wheel --universal\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'REMOVE_BUILDS=0 make check-docs\n', 'git clone https://github.com/catalyst-team/catalyst.git --branch gh-pages --single-branch gh-pages\ncd gh-pages\ncp -a ../builds ""${{ github.event.release.tag_name }}""\n# add link to new version into `versions.html`\nmaster_version_link=\'<li class=""toctree-l1""><a class=""reference internal"" href=""index.html"">master<\\/a><\\/li>\'\nnew_version_link=""\\        <li class=\\""toctree-l1\\""><a class=\\""reference internal\\"" href=\\""${TAG}/index.html\\"">${TAG}<\\/a><\\/li>""\nsed -i ""/${master_version_link}/a ${new_version_link}"" versions.html\n# commit changes\ngit config --local user.email ""action@ithub.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""${TAG:=update docs}"" || true\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'REMOVE_BUILDS=0 make check-docs\n', 'git clone https://github.com/catalyst-team/catalyst.git --branch gh-pages --single-branch gh-pages\ncd gh-pages\n# Remove master docs, do not touch the past versions\nfind . -maxdepth 1  ! -name \'versions.html\' ! -name "".buildinfo"" ! -name "".nojekyll"" -type f -exec rm -f {} + || true\nrm -rf .doctrees _modules _sources _static api info\ncp -a ../builds/* .\n# commit changes\ngit config --local user.email ""action@ithub.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""$(git log -1 --pretty=%B)"" || true\n', 'brew install libomp\nbrew install gnu-sed\n', 'python -c ""req = open(\'./requirements/requirements.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-cv.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-cv.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-ml.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-ml.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-optuna.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-optuna.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-comet.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-comet.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-mlflow.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-mlflow.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-neptune.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-neptune.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-wandb.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-wandb.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-profiler.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-profiler.txt\', \'w\').write(req)""\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'pip install -e . --no-deps\nPYTHONPATH=""${PYTHONPATH}:."" CPU_REQUIRED=""1"" CONFIGS_REQUIRED=""1"" CATALYST_COMPUTE_PER_CLASS_METRICS=""1"" OMP_NUM_THREADS=""1"" MKL_NUM_THREADS=""1"" pytest .\n', 'brew install libomp\nbrew install gnu-sed\n', 'python -c ""req = open(\'./requirements/requirements.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-cv.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-cv.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-ml.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-ml.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-optuna.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-optuna.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-comet.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-comet.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-mlflow.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-mlflow.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-neptune.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-neptune.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-wandb.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-wandb.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-profiler.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-profiler.txt\', \'w\').write(req)""\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'pip install -e . --no-deps\nPYTHONPATH=""${PYTHONPATH}:."" CPU_REQUIRED=""1"" CONFIGS_REQUIRED=""1"" CATALYST_COMPUTE_PER_CLASS_METRICS=""1"" OMP_NUM_THREADS=""1"" MKL_NUM_THREADS=""1"" pytest .\n', 'brew install libomp\nbrew install gnu-sed\n', 'python -c ""req = open(\'./requirements/requirements.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-cv.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-cv.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-ml.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-ml.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-optuna.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-optuna.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-comet.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-comet.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-mlflow.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-mlflow.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-neptune.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-neptune.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-wandb.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-wandb.txt\', \'w\').write(req)""\npython -c ""req = open(\'./requirements/requirements-profiler.txt\').read().replace(\'>\', \'=\') ; open(\'./requirements/requirements-profiler.txt\', \'w\').write(req)""\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', '# python -m pip install --upgrade --user pip\npip install -r ./requirements/requirements.txt -r ./requirements/requirements-dev.txt -r ./requirements/requirements-cv.txt -r ./requirements/requirements-ml.txt -r ./requirements/requirements-optuna.txt -r ./requirements/requirements-comet.txt -r ./requirements/requirements-mlflow.txt -r ./requirements/requirements-neptune.txt -r ./requirements/requirements-wandb.txt -r ./requirements/requirements-profiler.txt\npython --version\npip --version\npip list\n', 'pip install -e .\nbash bin/workflows/check_projector.sh\nbash bin/workflows/check_settings.sh\n']"
""
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f test_requirements.txt ]; then pip install -r test_requirements.txt; fi\n', 'python setup.py develop\n', 'python -m pytest . -s\n', 'python -m flake8 .\npython -m isort -rc --check-only --diff .']"
"[""python -m pip install --upgrade pip setuptools wheel\npip install -e '.[all]'\npip install flake8\npip list\npython -m rpy2.situation\n"", 'wget ${UCI_DB}/adult/adult.data -P aif360/data/raw/adult/\nwget ${UCI_DB}/adult/adult.test -P aif360/data/raw/adult/\nwget ${UCI_DB}/adult/adult.names -P aif360/data/raw/adult/\nwget ${UCI_DB}/statlog/german/german.data -P aif360/data/raw/german/\nwget ${UCI_DB}/statlog/german/german.doc -P aif360/data/raw/german/\nwget ${PROPUBLICA_GH}/compas-scores-two-years.csv -P aif360/data/raw/compas/\n(cd aif360/data/raw/meps;Rscript generate_data.R <<< y)\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest tests --cov=aif360 --cov-report=term-missing', 'install.packages(c(""reticulate"", ""rstudioapi"", ""testthat""))', ""python -m pip install --upgrade pip setuptools wheel\npip install '.[all]'\n"", 'R CMD INSTALL aif360/aif360-r']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel\npython setup.py ${{ matrix.dist }}\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel\npython -m pip install --upgrade pre-commit\n', 'pre-commit run --all-files', 'git config user.name ""KivyMD Bot""\ngit config user.email 69076719+KivyMD-Bot@users.noreply.github.com\npip install -e .\npip install pre-commit\n', '# Use personal token to push (when using GITHUB_TOKEN, it will not run workflows)\ngit remote set-url origin https://KivyMD-Bot:${{ secrets.GH_PAT }}@github.com/${{ github.repository }}\npython kivymd/tools/release/make_release.py release ""${{ github.event.inputs.version }}"" ""${{ github.event.inputs.next_version }}"" --yes --push\n', '# Use personal token to push\ngit remote set-url origin https://KivyMD-Bot:${{ secrets.GH_PAT }}@github.com/${{ github.repository }}\npython kivymd/tools/release/make_release.py prepare --yes --push\ngit format-patch origin/master... --stdout\n', 'python kivymd/tools/release/make_release.py test --yes\ngit format-patch origin/master... --stdout\ngit format-patch origin/master... --stdout > release_test.patch\n', '.ci/ubuntu_dependencies.sh', '.ci/macos_dependencies.sh', '.ci\\windows_dependencies.ps1', 'python -m pip install -e .', 'python -m pytest kivymd/tests --timeout=300 --cov=kivymd --cov-report=term', 'python -m PyInstaller.utils.run_tests --include_only kivymd.', 'python -m coveralls']"
""
""
"['pip install cython setuptools\npip install -r requirements.txt\npip install pytest\n', 'python setup.py install', 'python -m geatpy --version\npytest\n']"
"['python3 -m pip install --upgrade pip', 'sudo apt install libgflags-dev libopencv-dev', 'sh scripts/download-test-data.sh\nsh scripts/download-tinyvgg-model.sh\nsh scripts/download-openpose-thin-model.sh\nsh scripts/download-openpose-res50-model.sh\nsh scripts/download-openpose-coco-model.sh\n', 'cmake . -DBUILD_TESTS=1 -DBUILD_FAKE=1 -DBUILD_EXAMPLES=1 -DBUILD_LIB=1 -DBUILD_USER_CODES=0 -DEXECUTABLE_OUTPUT_PATH=./bin', 'cmake --build . --config Release']"
"['pip install build', 'python -m build', 'pip install -e . pyright pytest', 'pyright --verifytypes anyio', 'pip install -e .[test] coveralls', 'coverage run -m pytest -v', 'coveralls --service=github']"
"['python -m pip install --upgrade pip\npython -m pip install pytest-cov setuptools-scm coveralls\npip install .\n', 'pytest -vv --cov=protontricks --cov-report term --cov-report xml tests\n', 'coveralls --service=github\n', 'python -m pip install --upgrade coveralls\ncoveralls --finish\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'make test', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nmake install\n', 'make dist\necho ""To be add: pack testing""\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nmake install\n', 'if [[ ${{ github.ref }} =~ ^refs/heads/(master|develop-ui|v[0-9]+\\.[0-9]+.*)$ ]]; then\n    echo ::set-output name=match::true\nfi  # See: https://stackoverflow.com/a/58869470/1123955\n', 'make ui\n', 'make deploy-version\npython setup.py sdist bdist_wheel\ntwine upload --skip-existing dist/*\n', ""echo 'Not A Publish Branch'""]"
[]
"['echo ""A ${{ github.event_name }} event trigged build of branch ${{ github.ref }}, repository ${{ github.repository }}""', 'wget --no-verbose ""${{ env.mpy-cross-path }}${{ env.mpy-cross-name }}${{ env.mpy-cross-8-release }}"" -O ${{ github.workspace }}/mpy-cross; \\\nchmod +x ${{ github.workspace }}/mpy-cross\n', 'echo ""SHA=`git rev-parse --short HEAD`"" >> $GITHUB_OUTPUT', 'PATH=.:$PATH make compile', 'ls -la .compiled/kmk/\n', 'python -m pip install --upgrade pipenv wheel', 'make test']"
"['python3 -m pip install --upgrade pip', 'pip3 install -r requirements_dev.txt', 'pip3 install -e .', 'pytest -s', ""echo 'All good man!'""]"
"['python -m pip install --upgrade pip\npython -m pip install pytest mypy ruff\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f dev_requirements.txt ]; then pip install -r dev_requirements.txt; fi\n', 'ruff check .\n', 'python -m pip install types-aiofiles types-python-dateutil types-pytz\npython -m mypy asyncua/\n', 'pytest -v -s\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['echo ""Building ...""\npython -m pip install --upgrade pip\npython -m pip install build\npython -m build\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m pytest --cov-report=xml --cov=symspellpy', 'python -m pip install --upgrade pip\npip install -r requirements.txt -v\n', 'python -m pytest']"
""
""
['python -m pip install build\npython -m build\n']
"['python -m venv ./venv\n', './venv/scripts/activate\npython -m pip install -r requirements.txt\n# For some reason the shiboken2.abi3.dll is not found properly, so I copy it instead\nCopy-Item .\\venv\\Lib\\site-packages\\shiboken2\\shiboken2.abi3.dll .\\venv\\Lib\\site-packages\\PySide2\\ -Force\n', './venv/scripts/activate\nmypy game\n', './venv/scripts/activate\nmypy gen\n', '[IO.File]::WriteAllLines($pwd.path + ""\\resources\\buildnumber"", $env:GITHUB_RUN_NUMBER)\n', './venv/scripts/activate\n$env:PYTHONPATH="".;./pydcs""\npyinstaller pyinstaller.spec\n', 'python -m venv ./venv\n', './venv/scripts/activate\npython -m pip install -r requirements.txt\n# For some reason the shiboken2.abi3.dll is not found properly, so I copy it instead\nCopy-Item .\\venv\\Lib\\site-packages\\shiboken2\\shiboken2.abi3.dll .\\venv\\Lib\\site-packages\\PySide2\\ -Force\n', 'New-Item -ItemType file resources\\final\n', './venv/scripts/activate\nmypy game\n', './venv/scripts/activate\nmypy gen\n', './venv/scripts/activate\n$env:PYTHONPATH="".;./pydcs""\npyinstaller pyinstaller.spec\n', 'Copy-Item .\\changelog.md .\\dist\n', 'Get-ChildItem -Recurse -Depth 1\n$version = ($env:TAG_NAME -split ""/"") | Select-Object -Last 1\n$prerelease = (""2.1.1-alpha3"" -match \'[^\\.\\d]\').ToString().ToLower()\nWrite-Host $version\nWrite-Host $prerelease\nWrite-Output ""::set-output name=number::$version""\nWrite-Output ""::set-output name=prerelease::$prerelease""\n$changelog = Get-Content .\\changelog.md\n$last_change = ($changelog | Select-String -Pattern ""^#\\s"" | Select-Object -Skip 1 -First 1).LineNumber - 2\n($changelog | Select-Object -First $last_change) -join ""`n"" | Out-File .\\releasenotes.md\nCompress-Archive -Path .\\dcs_liberation -DestinationPath ""dcs_liberation.$version.zip"" -Compression Optimal\n']"
"['python -m pip install --upgrade pip setuptools\npip install pylint\n', ""pylint $(git ls-files '*.py')\n"", 'python --version', 'python -m pip install --upgrade pip setuptools\npip install -r requirements_tests.txt\n# For Codecov\npip install pytest-cov\n# Download models and data files\npython utils/wl_downloader_ci.py\n', 'pytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_dependency_parsing.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_lemmatization.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_matching.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_nlp_utils.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_pos_tagging.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_sentence_tokenization.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_stop_word_lists.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_syl_tokenization.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_word_detokenization.py\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_nlp/test_word_tokenization.py\n\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_file_area\npytest --cov=./ --cov-report=xml --cov-append tests/wl_tests_work_area\n\npytest --cov=./ --cov-report=xml --cov-append --ignore=tests/wl_tests_nlp --ignore=tests/wl_tests_file_area --ignore=tests/wl_tests_work_area\n', 'python --version', 'python -m pip install --upgrade pip setuptools\npip install -r requirements_tests.txt\n# Download models and data files\npython utils/wl_downloader_ci.py\n', 'pytest tests/wl_tests_nlp/test_dependency_parsing.py\npytest tests/wl_tests_nlp/test_lemmatization.py\npytest tests/wl_tests_nlp/test_matching.py\npytest tests/wl_tests_nlp/test_nlp_utils.py\npytest tests/wl_tests_nlp/test_pos_tagging.py\npytest tests/wl_tests_nlp/test_sentence_tokenization.py\npytest tests/wl_tests_nlp/test_stop_word_lists.py\npytest tests/wl_tests_nlp/test_syl_tokenization.py\npytest tests/wl_tests_nlp/test_word_detokenization.py\npytest tests/wl_tests_nlp/test_word_tokenization.py\n\npytest tests/wl_tests_file_area\n# Ignore tests of Profiler due to unknown errors\npytest tests/wl_tests_work_area --ignore=tests/wl_tests_work_area/test_profiler.py\n\npytest --ignore=tests/wl_tests_nlp --ignore=tests/wl_tests_file_area --ignore=tests/wl_tests_work_area\n', 'python --version', 'python -m pip install --upgrade pip setuptools\npip install -r requirements_tests.txt\n# Download models and data files\npython utils/wl_downloader_ci.py\n', '# Fix PyQt\nexport QT_QPA_PLATFORM=offscreen\n\npytest tests/wl_tests_nlp/test_dependency_parsing.py\npytest tests/wl_tests_nlp/test_lemmatization.py\npytest tests/wl_tests_nlp/test_matching.py\npytest tests/wl_tests_nlp/test_nlp_utils.py\npytest tests/wl_tests_nlp/test_pos_tagging.py\npytest tests/wl_tests_nlp/test_sentence_tokenization.py\npytest tests/wl_tests_nlp/test_stop_word_lists.py\npytest tests/wl_tests_nlp/test_syl_tokenization.py\npytest tests/wl_tests_nlp/test_word_detokenization.py\npytest tests/wl_tests_nlp/test_word_tokenization.py\n\npytest tests/wl_tests_file_area\npytest tests/wl_tests_work_area\n\npytest --ignore=tests/wl_tests_nlp --ignore=tests/wl_tests_file_area --ignore=tests/wl_tests_work_area\n']"
"['sudo apt-get install -y freeglut3-dev xvfb', 'pip install -r docs/requirements.txt', 'pip install .', 'xvfb-run -a -s ""-screen 0 1024x768x24 -ac +extension GLX +render -noreset"" python docs/_scripts/gen_docs_page.py', 'xvfb-run -a -s ""-screen 0 1024x768x24 -ac +extension GLX +render -noreset"" python docs/_scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/_scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'sudo apt-get install -y freeglut3-dev xvfb', 'pip install -r docs/requirements.txt', 'pip install .', 'xvfb-run -a -s ""-screen 0 1024x768x24 -ac +extension GLX +render -noreset"" python docs/_scripts/gen_docs_page.py', 'xvfb-run -a -s ""-screen 0 1024x768x24 -ac +extension GLX +render -noreset"" python docs/_scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/_scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'python -m pip install -U build', 'python -m build --sdist --wheel --outdir dist/ .', 'docker build -f py.Dockerfile \\\n  --build-arg PYTHON_VERSION=${{ matrix.python-version }} \\\n  --tag gym-miniworld-docker .\n', 'docker run gym-miniworld-docker pytest', 'sudo apt-get install -y freeglut3-dev xvfb', 'pip install -r docs/requirements.txt', 'pip install .', 'xvfb-run -a -s ""-screen 0 1024x768x24 -ac +extension GLX +render -noreset"" python docs/_scripts/gen_docs_page.py', 'xvfb-run -a -s ""-screen 0 1024x768x24 -ac +extension GLX +render -noreset"" python docs/_scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/_scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'pip install pre-commit', 'pre-commit --version', 'pre-commit install', 'pre-commit run --all-files']"
"['echo ""Updating template...""', 'echo ""Checking out $GITHUB_REF...""\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'pip install -r requirements-dev.txt\npython setup.py develop\n', 'python -m tox -e py$(echo ""${{ matrix.python-version }}"" | tr -d \'.\')\n', 'coveralls --service github\n', 'echo ""Checking out $GITHUB_REF...""\n', 'echo $GITHUB_TOKEN | docker login $DOCKER_REGISTRY -u $DOCKER_USERNAME --password-stdin\n', 'make docker-build\n', 'make docker-push', 'echo ""Checking out $GITHUB_REF...""\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'pip install -r requirements-jobs.txt\npython setup.py develop\n', 'bpyutils --run-jobs pipupgrade.jobs']"
['bash integration/runner.sh']
""
"['pip install . && pip install hatch\n', 'hatch run pypi:deploy\n', 'python test.py ${{ matrix.frameworks }}\n']"
"['./gradlew lint\n', './gradlew installPipLatest\n', './.github/scripts/libomp-${{ runner.os }}.sh\n', './gradlew ""test_${{matrix.flavour}}""\n', 'sudo apt-get install -y pandoc\n', './gradlew setupPip\n', './gradlew buildDocs\n', './gradlew checkExample -PexampleName=${{matrix.example-name}}\n', 'sudo apt-get install -y pandoc\n', './gradlew setupPip\n', './gradlew buildDocs\n', './gradlew lint\n', './gradlew setupPip\n', './.github/scripts/libomp-${{ runner.os }}.sh\n', './gradlew ""test_${{matrix.flavour}}""\n', './gradlew checkExample -PexampleName=${{matrix.example-name}}\n', 'sudo apt-get install -y pandoc\n', './gradlew setupPip\n', './gradlew buildDocs\n', 'python -m pip install --upgrade pip\n', 'pip install -q -r requirements/release.txt\n', 'bump2version --new-version ${{ steps.bump_dry.outputs.new_tag }} patch\n', 'docker login -u $DOCKER_HUB_USER -p $DOCKER_HUB_TOKEN', './gradlew dockerPushVersion -P version=${{ steps.bump_dry.outputs.tag }}\n', './gradlew dockerPushLatest\n', 'sudo apt-get install -y pandoc\n', './gradlew setupPip\n', './gradlew buildDocs\n']"
"[""cat << EOF > my.cnf\n[mysqld]\nserver-id=100\nlog_bin=ON\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_general_ci\nlower_case_table_names=1\ndefault-time_zone = '+8:00'\n[client]\ndefault-character-set=utf8mb4\nEOF\ndocker cp my.cnf mysql:/etc/mysql/conf.d/\ndocker restart mysql\n"", 'sudo apt-get update && sudo apt-get install libsasl2-dev libldap2-dev libssl-dev unixodbc unixodbc-dev\npython -m pip install --upgrade pip\npip install codecov coverage flake8 -r requirements.txt\n', 'mysql -h127.0.0.1 -uroot -e ""CREATE DATABASE archery CHARSET UTF8MB4;""\nmysql -h127.0.0.1 -uroot -e ""DROP DATABASE IF EXISTS test_archery;CREATE DATABASE test_archery CHARSET UTF8MB4;""\nmysql -h127.0.0.1 -uroot test_archery<src/init_sql/mysql_slow_query_review.sql\n', ""python manage.py makemigrations\npython manage.py makemigrations sql\ncoverage run --source='.' manage.py test -v 3 --keepdb\ncoverage xml\n""]"
"['mkdir -p .github/linters\ncp pyproject.toml .github/linters\n', 'pip install "".[test]""\nsolc-select use 0.4.25 --always-install\nsolc-select use 0.8.0 --always-install\nsolc-select use 0.5.1 --always-install\n', 'bash ""scripts/ci_test_${TEST_TYPE}.sh""\n', 'pip install -e "".[doc]""', ""pdoc -o html/ slither '!slither.tools'"", 'echo ""::group::Install slither""\npip3 install .\necho ""::endgroup::""\n\n# escape cwd so python doesn\'t pick up local module\ncd /\n\necho ""::group::Via module""\npython3 -m slither.tools.doctor .\necho ""::endgroup::""\n\necho ""::group::Via binary""\nslither-doctor .\necho ""::endgroup::""\n', 'echo ""::group::Install slither""\npip3 install --user .\necho ""::endgroup::""\n\n# escape cwd so python doesn\'t pick up local module\ncd /\n\necho ""::group::Via module""\npython3 -m slither.tools.doctor .\necho ""::endgroup::""\n\necho ""::group::Via binary""\nslither-doctor .\necho ""::endgroup::""\n', 'echo ""::group::Install slither""\npython3 -m venv venv\nsource venv/bin/activate || source venv/Scripts/activate\nhash -r\npip3 install .\necho ""::endgroup::""\n\n# escape cwd so python doesn\'t pick up local module\ncd /\n\necho ""::group::Via module""\npython3 -m slither.tools.doctor .\necho ""::endgroup::""\n\necho ""::group::Via binary""\nslither-doctor .\necho ""::endgroup::""\n', 'mkdir -p .github/linters\ncp pyproject.toml .github/linters\n', 'python -m venv /tmp/pip-audit-env\nsource /tmp/pip-audit-env/bin/activate\n\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install .\n', 'mkdir -p .github/linters\ncp pyproject.toml .github/linters\n', 'pip install "".[test]""\n', 'if [ ${{ matrix.type }} = ""tool"" ]; then\n  # Setup Ganache for slither-read-storage tests.\n  npm install --global ganache\nelif [ ${{ matrix.type }} = ""integration"" ]; then\n  # Setup Hardhat for compilation tests.\n  pushd tests/e2e/compilation/test_data/test_node_modules/ || exit\n  npm install hardhat\n  popd || exit\nfi\n', 'if [ ${{ matrix.os }} = ""ubuntu-latest"" ]; then\n  TEST_ARGS=""--cov=slither --cov-append""\nelif [ ${{ matrix.os }} = ""windows-2022"" ]; then\n  TEST_ARGS=""""\nfi\nbash ""./.github/scripts/${TEST_TYPE}_test_runner.sh"" $TEST_ARGS\n', 'pip install coverage[toml]', 'set +e\npython -m coverage combine\necho ""## python coverage"" >> $GITHUB_STEP_SUMMARY\npython -m coverage report -m --format=markdown >> $GITHUB_STEP_SUMMARY']"
"['echo ""num-shards=$(jq -n -c \'[${{ env.PYTEST_NUM_SHARDS }}]\')"" >> $GITHUB_OUTPUT\necho ""shard-ids=$(jq -n -c \'[range(1;${{ env.PYTEST_NUM_SHARDS }}+1)]\')"" >> $GITHUB_OUTPUT\n', 'pytest --durations=100 -vv -n auto --shard-id=$((${{ matrix.shard-id }} - 1)) --num-shards=${{ env.PYTEST_NUM_SHARDS }} \\\n  --ignore=""tensorflow_datasets/datasets/nsynth/nsynth_dataset_builder_test.py"" \\\n  --ignore=""tensorflow_datasets/image/lsun_test.py"" \\\n  --ignore=""tensorflow_datasets/datasets/imagenet2012_corrupted/imagenet2012_corrupted_dataset_builder_test.py"" \\\n  --ignore=""tensorflow_datasets/scripts/documentation/build_api_docs_test.py"" \\\n  --ignore=""tensorflow_datasets/import_without_tf_test.py"" \\\n  --ignore=""tensorflow_datasets/core/github_api/github_path_test.py"" \\\n  --ignore=""tensorflow_datasets/translate/wmt19_test.py"" \\\n  --ignore=""tensorflow_datasets/core/dataset_utils_test.py"" \\\n  --ignore=""tensorflow_datasets/core/features/sequence_feature_test.py""\n', 'pytest -vv -n auto tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder_test.py', 'pytest --durations=100 -vv -n auto tensorflow_datasets/core/github_api/github_path_test.py', 'pip install ipython', 'ipython kernel install --user --name tfds-notebook\nfor notebook in docs/*ipynb\n  do\n    # These notebooks time out because they rely on loading huge datasets.\n    if [[ ""$notebook"" != ""docs/determinism.ipynb"" ]] && \\\n       [[ ""$notebook"" != ""docs/dataset_collections.ipynb"" ]]\n    then\n      jupyter nbconvert \\\n        --ExecutePreprocessor.timeout=600 \\\n        --ExecutePreprocessor.kernel_name=tfds-notebook \\\n        --to notebook \\\n        --execute $notebook\n    fi\n  done\n', 'pip install setuptools wheel twine', 'python setup.py sdist --nightly\npython setup.py bdist_wheel --nightly\n', 'twine check dist/*\ntwine upload dist/*\n', 'mv tensorflow_datasets/version_stable.py tensorflow_datasets/version.py']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['make copy_docs', 'pip install mkdocs==1.4.2 mkdocs-material==9.1.5 mdx-truly-sane-lists==1.3 mkdocs-awesome-pages-plugin==2.8.0', 'mkdocs gh-deploy --force', 'ls -la ./\nrm -rf ./* || true\nrm -rf ./.??* || true\nls -la ./\n', 'sudo npm install -g doctoc@2 && make check_toc', ""pip install 'codespell<3.0.0,>=2.0.0' --user &&  make codespell"", 'make install_test', 'make lint', 'make lint_generators', 'echo ""spec_test_preset_type=${{ github.event.inputs.test_preset_type || env.TEST_PRESET_TYPE }}"" >> $GITHUB_ENV\n', 'echo ""spec_test_preset_type=${{ env.TEST_PRESET_TYPE}}"" >> $GITHUB_ENV    \n', 'echo ""spec_test_preset_type=mainnet"" >> $GITHUB_ENV    \n', 'echo ""spec_test_preset_type=mainnet"" >> $GITHUB_ENV\n', 'make install_test', 'make citest fork=${{ matrix.version }} TEST_PRESET_TYPE=${{env.spec_test_preset_type}}', 'ls -la ./\nrm -rf ./* || true\nrm -rf ./.??* || true\nls -la ./']"
['python -m pip install --upgrade pip\npip install setuptools wheel\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n']
"['python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/* --verbose\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/* --verbose\n', 'python -m pip install --upgrade pip\npip install numpy -I\npip install pytest torch\n', 'pip install -e .\n', 'cd docs/\npip install -r requirements.txt\n', 'cd docs/\nmake html\ntouch build/html/.nojekyll\n', 'python -m pip install --upgrade pip\npip install numpy -I\npip install pytest torch\n', 'pip install -e .\n', 'cd docs/\npip install -r requirements.txt\n', 'cd docs/\nmake html\ntouch build/html/.nojekyll\naws s3 sync build/html/ s3://hangzh/encoding/docs/${{ env.PULL_NUMBER }}/ --acl public-read --follow-symlinks --delete\n', 'python -m pip install --upgrade pip\npip install -e .\n  \n', 'pip install nose\nnosetests -v tests/unit_test/\n']"
""
"['poetry install\n', 'poetry run nosetests --with-coverage --cover-package=nubia\n']"
"['sudo apt update\nsudo apt install qttools5-dev-tools latexmk texlive texlive-latex-extra\n', 'pip install -r docs/source/requirements.txt', 'python setup.py qtlrelease sample manual', 'pip install python-appimage', 'python setup.py build-appimage --linux-tag ${{ matrix.linux-tag }} --python-version ${{ matrix.python-version }}', './setup/macos/build.sh', 'pip install flake8', 'flake8 --version\nflake8 novelwriter --count --show-source --statistics\nflake8 tests --count --show-source --statistics\n', 'sudo apt update\nsudo apt install libenchant-2-dev qttools5-dev-tools aspell-en\n', 'pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'python setup.py qtlrelease sample', 'export QT_QPA_PLATFORM=offscreen\npytest -v --cov=novelwriter --timeout=60\n', 'brew install enchant\n', 'pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install pyobjc\n', 'export QT_QPA_PLATFORM=offscreen\npytest -v --cov=novelwriter --timeout=60\n', 'pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'pytest -v --cov=novelwriter --timeout=60\n']"
"['poetry config virtualenvs.in-project true\n', 'if [ -d "".venv"" ]; then rm -rf .venv; fi\npoetry install --without docs\n', 'poetry install --without docs\n', 'poetry run pytest --cov=pypeln --cov-report=xml\n', 'poetry config virtualenvs.in-project true\n', 'poetry install --only main\n', 'poetry install --only main\n', 'poetry run python -c ""import pypeln""\n']"
[]
"['pip install tf-nightly mock pytest\npip install -e .[test,auto_mtf,transformer]\n', 'pytest', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
"['pip install requests\nif [ ""$RUNNER_OS"" == ""macOS"" ]; then\n  pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0\nelse\n  pip install torch==1.8.0+cpu torchvision==0.9.0+cpu torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\nfi\n', 'pip install requests\nif [ ""$RUNNER_OS"" == ""macOS"" ]; then\n  pip install torch torchvision torchaudio\nelse\n  pip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\nfi\n', 'pip install requests\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install scipy\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=79 --statistics\n', 'pip install pytest\npytest\n', 'pip install requests\npip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt\n', 'pip install pytest pytest-cov coverage scipy\npython -m pytest --cov=torchgan ./tests --cov-report term-missing\nbash <(curl -s https://codecov.io/bash)\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'echo ""This job is used to prevent the workflow to fail when all other jobs are skipped.""']"
"['docker build --tag scispacy .\ndocker run --rm scispacy pytest tests/\ndocker run --rm scispacy flake8 scispacy\ndocker run --rm scispacy black scispacy --check --line-length 88\ndocker run --rm scispacy bash scripts/mypy.sh\ndocker run --rm scispacy pytest tests/ --cov scispacy --cov-fail-under=20\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload -u scispacy -p ${{ secrets.PYPI_PASSWORD }} dist/*\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\npython -m pip install numpy\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install -e .\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['pip install -r requirements.txt\npip install -r requirements_dev.txt\npip install coveralls\npip install pytest\npip install pytest-benchmark\n', 'coverage run -m unittest discover -s tests -t tests', 'python3 -m pytest --benchmark-verbose --benchmark-columns=mean,stddev,iqr,ops,rounds tests/benchmarks/benchmark_model.py tests/benchmarks/benchmark_management_api.py tests/benchmarks/benchmark_role_manager.py', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --finish\n', 'npm install -g semantic-release @semantic-release/github @semantic-release/changelog @semantic-release/commit-analyzer @semantic-release/git @semantic-release/release-notes-generator semantic-release-pypi', 'python -m pip install --upgrade setuptools wheel twine', 'npx semantic-release']"
"['docker build -t pyrdp .', 'docker run pyrdp pyrdp-convert.py -h', 'docker run pyrdp pyrdp-player.py -h', 'docker run pyrdp pyrdp-mitm.py -h', 'docker build -f Dockerfile.slim -t pyrdp .', 'docker run pyrdp pyrdp-convert.py -h', 'docker run pyrdp pyrdp-player.py -h', 'docker run pyrdp pyrdp-mitm.py -h', 'python --version', 'pip --version', 'sudo apt update -y', 'sudo apt install python3-setuptools', 'sudo apt install libgl1-mesa-glx git python3-dev', 'pip install wheel', 'pip install -U -e .[full]', 'pip install -r requirements-ci.txt', 'coverage run test/test_prerecorded.py', 'coverage run --append test/test_mitm_initialization.py dummy_value', './test/integration.sh', 'coverage run --append -m unittest discover -v', 'coverage report --fail-under=40', 'python --version', 'pip --version', 'pip install wheel', 'pip install -U -e .[full]', 'pip install coverage', 'coverage run test/test_prerecorded.py', 'coverage run --append test/test_mitm_initialization.py dummy_value', 'coverage run --append bin/pyrdp-player.py --headless test/files/test_session.replay', 'coverage run --append bin/pyrdp-convert.py test/files/test_convert.pyrdp -f mp4', 'file test_convert.mp4 | grep ""MP4 Base Media""', 'coverage run --append -m unittest discover -v', 'coverage report --fail-under=40']"
"['pip install tox', 'tox -e py']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install cibuildwheel==2.11.4 twine==4.0.2', 'git submodule init third_party/pybind\ngit submodule update third_party/pybind\n', 'python -m cibuildwheel loadgen/ --output-dir wheels', 'python -m twine upload wheels/* -u __token__ -p ${{ secrets.PYPI_API_TOKEN }}', 'python3 -m pip install cmind\ncm pull repo mlcommons@ck\n', 'cm run script --tags=get,mlperf,inference,loadgen --quiet --version=custom --adr.inference-src.env.CM_GIT_CHECKOUT=${{ github.event.pull_request.head.ref }} --adr.inference-src.env.CM_GIT_URL=${{ github.event.pull_request.head.repo.html_url }}\n', 'python3 -m pip install cmind\ncm pull repo mlcommons@ck\ncm run script --quiet --tags=get,sys-utils-cm\n', 'cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short --quiet --submitter=""MLCommons"" --hw_name=default --model=resnet50 --lang=python --backend=${{ matrix.backend }} --device=cpu --scenario=Offline --test_query_count=500 --adr.compiler.tags=gcc --adr.inference-src.env.CM_GIT_CHECKOUT=${{ github.event.pull_request.head.ref }} --adr.inference-src.env.CM_GIT_URL=${{ github.event.pull_request.head.repo.html_url }}\n', 'python3 -m pip install cmind\ncm pull repo mlcommons@ck\ncm run script --quiet --tags=get,sys-utils-cm\n', 'cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short --quiet --submitter=""MLCommons"" --hw_name=default --model=retinanet --lang=python --backend=${{ matrix.backend }} --device=cpu --scenario=Offline --test_query_count=10 --adr.compiler.tags=gcc --adr.inference-src.env.CM_GIT_CHECKOUT=${{ github.event.pull_request.head.ref }} --adr.inference-src.env.CM_GIT_URL=${{ github.event.pull_request.head.repo.html_url }}\n']"
[]
"['docker build -f build/go/fileshot/Dockerfile .', 'docker build -f build/go/filestream/Dockerfile .', 'docker build -f build/go/oneshot/Dockerfile .', 'docker-compose -f build/docker-compose.yaml build', 'docker-compose -f build/docker-compose.yaml build', '# Stop the script on errors\nset -e\n\n# Enable Docker BuildKit\nexport DOCKER_BUILDKIT=1\n\n# Set tag variables\nCOMMIT_HASH=$(git rev-parse --short HEAD)\nTAG_NAME=$(git describe --tags --abbrev=0)\n\n# Define a dictionary with paths to Dockerfiles as keys and image names as values\ndeclare -A images\nimages=(\n  [""build/python/backend""]=""target/strelka-backend""\n  [""build/go/frontend""]=""target/strelka-frontend""\n  [""build/go/fileshot""]=""target/strelka-fileshot""\n  [""build/go/filestream""]=""target/strelka-filestream""\n  [""build/go/oneshot""]=""target/strelka-oneshot""\n  [""build/go/manager""]=""target/strelka-manager""\n)\n\n# Build, tag, and push each image\nfor path in ""${!images[@]}""; do\n  IMAGE_NAME=${images[$path]}\n  docker build -f ""${path}/Dockerfile"" -t ""${IMAGE_NAME}:${TAG_NAME}"" -t ""${IMAGE_NAME}:latest"" .\n  docker push ""${IMAGE_NAME}:${TAG_NAME}""\n  docker push ""${IMAGE_NAME}:latest""\ndone\n', 'docker logout', 'python -m pip install --upgrade pip\npip install black mypy flake8 isort\n', 'python -m black --extend-exclude .*_pb2.*\\.py --check --diff src/python\npython -m flake8 --exclude src/python/build/,src/python/strelka/proto/ src/python\npython -m isort --extend-skip-glob *_pb2*.py --profile black --check-only src/python\n']"
"['git --no-pager log -2\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 \nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python -m pip install --upgrade pip\npython -m pip install -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'git --no-pager log -2\npython -m pip install --upgrade pip\npython -m pip install torch==1.12.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\npython -m pip install -e .\npython -m unittest tests/test_*py\n', 'git --no-pager log -2\n', 'python -m pip install --upgrade pip\npython -m pip install sphinx pydata-sphinx-theme\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -m pip install torch==1.7.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\npython -m pip install -e .\n', 'bash .github/scripts/publish-docs.sh\n']"
"['pip install --upgrade pip', 'pip install pylint==2.17.2', 'pylint --fail-under=9.5 qdata', 'pip install --upgrade pip', 'pip install setuptools==47.1.0', 'pip install twine==4.0.0', 'pip install wheel==0.36.2', 'python setup.py sdist bdist_wheel && twine upload --repository-url ${{ vars.PYPI_REPOSITORY }} -u ${{ secrets.PYPI_USERNAME }} -p ${{ secrets.PYPI_PASSWORD }} dist/*']"
"['python -m pip install --upgrade pip\npip install tox==3.15.1 tox-gh-actions==1.2.0 pipenv==2020.6.2\n', 'tox']"
"['sudo apt-get update\npip install --upgrade pip\npython -m pip install --upgrade build twine\npython -m build\ntwine check --strict dist/*\n', 'export PKG=$(ls dist/ | grep tar)\nset -- $PKG\necho ""name=$1"" >> $GITHUB_ENV\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['pip install pyinstaller httpx[http2]\npip install -r requirements.txt\n', 'pyinstaller accesser.spec\n', 'pip install --upgrade build\n', 'python3 -m build\n']"
"['make all_connectors -e pw_acceptlicenses=y\n', 'echo ${{ steps.docker_build_main.outputs.digest }}\necho ${{ steps.docker_build_barebone.outputs.digest }}\necho ${{ steps.docker_build_default.outputs.digest }}\n', './scripts/ci_check_no_file_changes.sh python', 'cp dev-project/.env.template dev-project/.env\ndocker-compose -f dev-project/docker-compose.yml up -d\n', 'until docker logs pipelinewise_dev | grep ""PipelineWise Dev environment is ready""\ndo\n  echo \'Sleeping for 10s\';\n  sleep 10;\ndone\n', 'docker exec -t \\\n-e TAP_S3_CSV_AWS_KEY=$TAP_S3_CSV_AWS_KEY \\\n-e TAP_S3_CSV_AWS_SECRET_ACCESS_KEY=$TAP_S3_CSV_AWS_SECRET_ACCESS_KEY \\\n-e TAP_S3_CSV_BUCKET=$TAP_S3_CSV_BUCKET \\\npipelinewise_dev pytest tests/end_to_end/test_target_postgres.py -vx --timer-top-n 10\n', './scripts/ci_check_no_file_changes.sh python', 'cp dev-project/.env.template dev-project/.env\ndocker-compose -f dev-project/docker-compose.yml up -d\n', 'until docker logs pipelinewise_dev | grep ""PipelineWise Dev environment is ready""\ndo\n  echo \'Sleeping for 10s\';\n  sleep 10;\ndone\n', 'docker exec -t \\\n-e TARGET_SNOWFLAKE_ACCOUNT=$TARGET_SNOWFLAKE_ACCOUNT \\\n-e TARGET_SNOWFLAKE_AWS_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_DBNAME=$TARGET_SNOWFLAKE_DBNAME \\\n-e TARGET_SNOWFLAKE_FILE_FORMAT=$TARGET_SNOWFLAKE_FILE_FORMAT \\\n-e TARGET_SNOWFLAKE_PASSWORD=$TARGET_SNOWFLAKE_PASSWORD \\\n-e TARGET_SNOWFLAKE_S3_BUCKET=$TARGET_SNOWFLAKE_S3_BUCKET \\\n-e TARGET_SNOWFLAKE_S3_KEY_PREFIX=$TARGET_SNOWFLAKE_S3_KEY_PREFIX \\\n-e TARGET_SNOWFLAKE_SCHEMA=$TARGET_SNOWFLAKE_SCHEMA \\\n-e TARGET_SNOWFLAKE_STAGE=$TARGET_SNOWFLAKE_STAGE \\\n-e TARGET_SNOWFLAKE_USER=$TARGET_SNOWFLAKE_USER \\\n-e TARGET_SNOWFLAKE_WAREHOUSE=$TARGET_SNOWFLAKE_WAREHOUSE \\\npipelinewise_dev pytest tests/end_to_end/target_snowflake/tap_mariadb -vx --timer-top-n 10\n', './scripts/ci_check_no_file_changes.sh python', 'cp dev-project/.env.template dev-project/.env\ndocker-compose -f dev-project/docker-compose.yml up -d\n', 'until docker logs pipelinewise_dev | grep ""PipelineWise Dev environment is ready""\ndo\n  echo \'Sleeping for 10s\';\n  sleep 10;\ndone\n', 'docker exec -t \\\n-e TARGET_SNOWFLAKE_ACCOUNT=$TARGET_SNOWFLAKE_ACCOUNT \\\n-e TARGET_SNOWFLAKE_AWS_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_DBNAME=$TARGET_SNOWFLAKE_DBNAME \\\n-e TARGET_SNOWFLAKE_FILE_FORMAT=$TARGET_SNOWFLAKE_FILE_FORMAT \\\n-e TARGET_SNOWFLAKE_PASSWORD=$TARGET_SNOWFLAKE_PASSWORD \\\n-e TARGET_SNOWFLAKE_S3_BUCKET=$TARGET_SNOWFLAKE_S3_BUCKET \\\n-e TARGET_SNOWFLAKE_S3_KEY_PREFIX=$TARGET_SNOWFLAKE_S3_KEY_PREFIX \\\n-e TARGET_SNOWFLAKE_SCHEMA=$TARGET_SNOWFLAKE_SCHEMA \\\n-e TARGET_SNOWFLAKE_STAGE=$TARGET_SNOWFLAKE_STAGE \\\n-e TARGET_SNOWFLAKE_USER=$TARGET_SNOWFLAKE_USER \\\n-e TARGET_SNOWFLAKE_WAREHOUSE=$TARGET_SNOWFLAKE_WAREHOUSE \\\npipelinewise_dev pytest tests/end_to_end/target_snowflake/tap_postgres -vx --timer-top-n 10\n', './scripts/ci_check_no_file_changes.sh python', 'cp dev-project/.env.template dev-project/.env\ndocker-compose -f dev-project/docker-compose.yml up -d\n', 'until docker logs pipelinewise_dev | grep ""PipelineWise Dev environment is ready""\ndo\n  echo \'Sleeping for 10s\';\n  sleep 10;\ndone\n', 'docker exec -t \\\n-e TARGET_SNOWFLAKE_ACCOUNT=$TARGET_SNOWFLAKE_ACCOUNT \\\n-e TARGET_SNOWFLAKE_AWS_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_DBNAME=$TARGET_SNOWFLAKE_DBNAME \\\n-e TARGET_SNOWFLAKE_FILE_FORMAT=$TARGET_SNOWFLAKE_FILE_FORMAT \\\n-e TARGET_SNOWFLAKE_PASSWORD=$TARGET_SNOWFLAKE_PASSWORD \\\n-e TARGET_SNOWFLAKE_S3_BUCKET=$TARGET_SNOWFLAKE_S3_BUCKET \\\n-e TARGET_SNOWFLAKE_S3_KEY_PREFIX=$TARGET_SNOWFLAKE_S3_KEY_PREFIX \\\n-e TARGET_SNOWFLAKE_SCHEMA=$TARGET_SNOWFLAKE_SCHEMA \\\n-e TARGET_SNOWFLAKE_STAGE=$TARGET_SNOWFLAKE_STAGE \\\n-e TARGET_SNOWFLAKE_USER=$TARGET_SNOWFLAKE_USER \\\n-e TARGET_SNOWFLAKE_WAREHOUSE=$TARGET_SNOWFLAKE_WAREHOUSE \\\npipelinewise_dev pytest tests/end_to_end/target_snowflake/tap_mongodb -vx --timer-top-n 10\n', './scripts/ci_check_no_file_changes.sh python', 'cp dev-project/.env.template dev-project/.env\ndocker-compose -f dev-project/docker-compose.yml up -d\n', 'until docker logs pipelinewise_dev | grep ""PipelineWise Dev environment is ready""\ndo\n  echo \'Sleeping for 10s\';\n  sleep 10;\ndone\n', 'docker exec -t \\\n-e TARGET_SNOWFLAKE_ACCOUNT=$TARGET_SNOWFLAKE_ACCOUNT \\\n-e TARGET_SNOWFLAKE_AWS_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY=$TARGET_SNOWFLAKE_AWS_SECRET_ACCESS_KEY \\\n-e TARGET_SNOWFLAKE_DBNAME=$TARGET_SNOWFLAKE_DBNAME \\\n-e TARGET_SNOWFLAKE_FILE_FORMAT=$TARGET_SNOWFLAKE_FILE_FORMAT \\\n-e TARGET_SNOWFLAKE_PASSWORD=$TARGET_SNOWFLAKE_PASSWORD \\\n-e TARGET_SNOWFLAKE_S3_BUCKET=$TARGET_SNOWFLAKE_S3_BUCKET \\\n-e TARGET_SNOWFLAKE_S3_KEY_PREFIX=$TARGET_SNOWFLAKE_S3_KEY_PREFIX \\\n-e TARGET_SNOWFLAKE_SCHEMA=$TARGET_SNOWFLAKE_SCHEMA \\\n-e TARGET_SNOWFLAKE_STAGE=$TARGET_SNOWFLAKE_STAGE \\\n-e TARGET_SNOWFLAKE_USER=$TARGET_SNOWFLAKE_USER \\\n-e TARGET_SNOWFLAKE_WAREHOUSE=$TARGET_SNOWFLAKE_WAREHOUSE \\\n-e TAP_S3_CSV_AWS_KEY=$TAP_S3_CSV_AWS_KEY \\\n-e TAP_S3_CSV_AWS_SECRET_ACCESS_KEY=$TAP_S3_CSV_AWS_SECRET_ACCESS_KEY \\\n-e TAP_S3_CSV_BUCKET=$TAP_S3_CSV_BUCKET \\\npipelinewise_dev pytest tests/end_to_end/target_snowflake/tap_mongodb -vx --timer-top-n 10\n', './scripts/ci_check_no_file_changes.sh python', 'cp dev-project/.env.template dev-project/.env\ndocker-compose -f dev-project/docker-compose.yml up -d\n', 'until docker logs pipelinewise_dev | grep ""PipelineWise Dev environment is ready""\ndo\n  echo \'Sleeping for 10s\';\n  sleep 10;\ndone\n', 'docker exec -t \\\n-e TAP_S3_CSV_AWS_KEY=$TAP_S3_CSV_AWS_KEY \\\n-e TAP_S3_CSV_AWS_SECRET_ACCESS_KEY=$TAP_S3_CSV_AWS_SECRET_ACCESS_KEY \\\n-e TAP_S3_CSV_BUCKET=$TAP_S3_CSV_BUCKET \\\npipelinewise_dev pytest tests/end_to_end -vx --ignore=tests/end_to_end/test_target_postgres.py --ignore=tests/end_to_end/target_snowflake --timer-top-n 10\n', './scripts/ci_check_no_file_changes.sh python', 'make pipelinewise', "". .virtualenvs/pipelinewise/bin/activate\nfind pipelinewise tests -type f -name '*.py' | xargs unify --check-only\n"", '. .virtualenvs/pipelinewise/bin/activate\npylint pipelinewise tests\n', '. .virtualenvs/pipelinewise/bin/activate\nflake8 pipelinewise --count --select=E9,F63,F7,F82 --show-source --statistics\n', '. .virtualenvs/pipelinewise/bin/activate\nflake8 pipelinewise --count --max-complexity=15 --max-line-length=120 --statistics\n', '. .virtualenvs/pipelinewise/bin/activate\npytest --cov=pipelinewise --cov-fail-under=77 -v tests/units\n', 'ls -l gh_doc_automation', './scripts/ci_check_no_file_changes.sh doc', './scripts/publish_docs.sh']"
""
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest pytest-asyncio cython\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 thriftpy2 --count --select=E9,F63,F7,F82 --show-source --statistics --ignore C901\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 thriftpy2 --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --ignore C901\n', 'pip install -e "".[dev]""\ncd tests\npytest\n']"
""
"['python -m pip install --upgrade pip wheel\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -r build/test-requirements.txt\nif [ ""${{ matrix.package-overrides }}"" != ""none"" ]; then\n  pip install ${{ matrix.package-overrides }}\nfi\nif [ ""${{ matrix.use-latest-jaxlib }}"" == ""true"" ]; then\n  pip install .[cpu]\nelse\n  pip install .[minimum-jaxlib]\nfi\n', 'pip install -e .\necho ""JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES""\necho ""JAX_ENABLE_X64=$JAX_ENABLE_X64""\necho ""JAX_ENABLE_CUSTOM_PRNG=$JAX_ENABLE_CUSTOM_PRNG""\necho ""JAX_THREEFRY_PARTITIONABLE=$JAX_THREEFRY_PARTITIONABLE""\necho ""JAX_ENABLE_CHECKS=$JAX_ENABLE_CHECKS""\npytest -n auto --tb=short --maxfail=20 tests examples\n', 'python -m pip install --upgrade pip wheel\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -r docs/requirements.txt\n', 'pytest -n auto --tb=short docs\npytest -n auto --tb=short --doctest-modules jax --ignore=jax/experimental/jax2tf --ignore=jax/_src/lib/mlir --ignore=jax/interpreters/mlir.py --ignore=jax/_src/iree.py --ignore=jax/experimental/array_serialization --ignore=jax/collect_profile.py\n', 'pip install -r build/test-requirements.txt\n', 'pip uninstall -y jax jaxlib libtpu-nightly\nif [ ""${{ matrix.jaxlib-version }}"" == ""latest"" ]; then\n  pip install .[tpu] \\\n    -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\nelif [ ""${{ matrix.jaxlib-version }}"" == ""nightly"" ]; then\n  pip install .\n  pip install --pre jaxlib \\\n    -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\n  pip install --pre libtpu-nightly \\\n    -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n  pip install requests\n\nelse\n  echo ""Unknown jaxlib-version: ${{ matrix.jaxlib-version }}""\n  exit 1\nfi\n\npython3 -c \'import sys; print(""python version:"", sys.version)\'\npython3 -c \'import jax; print(""jax version:"", jax.__version__)\'\npython3 -c \'import jaxlib; print(""jaxlib version:"", jaxlib.__version__)\'\npython3 -c \'import jax; print(""libtpu version:"",\n  jax.lib.xla_bridge.get_backend().platform_version)\'\n', '# Run single-accelerator tests in parallel\nJAX_ENABLE_TPU_XDIST=true python3 -m pytest -n=4 --tb=short \\\n  --maxfail=20 -m ""not multiaccelerator"" tests examples\n# Run multi-accelerator across all chips\npython3 -m pytest --tb=short --maxfail=20 -m ""multiaccelerator"" tests\n', 'curl --location --request POST \'${{ secrets.BUILD_CHAT_WEBHOOK }}\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw ""{\n\'text\': \'\\""$GITHUB_WORKFLOW\\"", jaxlib/libtpu version \\""${{ matrix.jaxlib-version }}\\"", TPU type ${{ matrix.tpu-type }} job failed, timed out, or was cancelled: $GITHUB_SERVER_URL/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID\'\n}""\n', ""\n# Create SSH keys\nmkdir -p ./.ssh && chmod 700 ./.ssh\nssh-keygen -N '' -f ./.ssh/id_rsa\n"", '\n# Setup cluster, get username and IP address\npip install -r ""./.github/workflows/slurm_job_scripts/requirements.txt""\npython3 ./.github/workflows/slurm_job_scripts/oci_cluster_manager.py create_cluster --pubkey ""$(cat ./.ssh/id_rsa.pub)"" &> oci_automation_create.log\nUSER=$(tail -n 2 oci_automation_create.log | head -n 1)\nIP=$(tail -n 1 oci_automation_create.log)\n\n# Hide IP address from logs\necho ""::add-mask::${IP}""\n\n# Create SSH config\ngrep ""^${IP} "" oci_automation_create.log >> ./.ssh/known_hosts\necho ""Host headnode\n  User ${USER}\n  HostName ${IP}\n  IdentityFile ${GITHUB_WORKSPACE}/.ssh/id_rsa\n  ServerAliveInterval 30"" > ./.ssh/config\n', '\n# SSH into the cluser & check SLURM\nssh ${CONFIG} headnode sinfo\n\n# Run dummy job\nSRUN=""srun --container-name=nvidia --container-image=docker://nvcr.io#nvidia/tensorflow:22.11-tf2-py3 -N 2 -t 15:00 --gpus-per-node=8 --cpus-per-task=8 --ntasks-per-node=8""\nCMD=""bash -c \'hostname && nvidia-smi --query-gpu=gpu_name,driver_version --format=csv\'""\nssh ${CONFIG} headnode ""${SRUN} ${CMD}""\n', '\nssh ${CONFIG} headnode ""rm -rf ${GITHUB_WORKSPACE_REMOTE} && mkdir -p ${GITHUB_WORKSPACE_REMOTE}""\nscp ${CONFIG} -r ./.github     headnode:${GITHUB_WORKSPACE_REMOTE}/.github\nscp ${CONFIG} -r ./tests       headnode:${GITHUB_WORKSPACE_REMOTE}/tests\n', '\nENV=""GITHUB_WORKSPACE_REMOTE=${GITHUB_WORKSPACE_REMOTE}""\nSALLOC=""salloc -N 2 --gpus-per-node=8 --exclusive -t 0:20:00 -p compute""\nCMD=""bash ${GITHUB_WORKSPACE_REMOTE}/.github/workflows/slurm_job_scripts/run_e2e_t5x_tests.sub""\nssh ${CONFIG} headnode ""${ENV} ${SALLOC} ${CMD}""\n', '\nscp ${CONFIG} -r headnode:${GITHUB_WORKSPACE_REMOTE}/outputs ./\n', '\npip install -r ""./.github/workflows/slurm_job_scripts/requirements.txt""\npython3 ./.github/workflows/slurm_job_scripts/oci_cluster_manager.py destroy_clusters &> ./oci_automation_destroy.log\n', 'ls /tmp/workspace/logs/output-from-nodes/ && mv /tmp/workspace/logs/output-from-nodes/output*t5x*1-0-0.txt ${GITHUB_WORKSPACE}/output.log\npip install -r ""${GITHUB_WORKSPACE}/.github/workflows/slurm_job_scripts/requirements.txt""\npython ${GITHUB_WORKSPACE}/.github/workflows/slurm_job_scripts/extract_e2e_tests_metrics.py --logfile ${GITHUB_WORKSPACE}/output.log --outmd ${GITHUB_WORKSPACE}/report.md --outjson ${GITHUB_WORKSPACE}/report.json --name end-to-end-t5x\ncat report.md >> $GITHUB_STEP_SUMMARY\n', 'ls /tmp/workspace/logs/output-from-nodes/\npython .github/workflows/cat_slurm_logs.py /tmp/workspace/logs/output-from-nodes/*.txt --outfile=parsed-logs.txt\n', 'curl --location --request POST \'${{ secrets.RELEASES_WEBHOOK }}\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    ""text"": ""Release ${{github.event.release.name}} at ${{github.event.release.published_at}} by ${{github.event.release.author.login}}. <${{github.event.release.url}}|[github]>""\n}\'\n', 'pip install -r build/test-requirements.txt\npip install pytest-reportlog\n', 'pip install \\\n  -i https://pypi.anaconda.org/scipy-wheels-nightly/simple \\\n  --no-deps \\\n  --pre \\\n  --upgrade \\\n  numpy \\\n  scipy\n', 'pip install .[minimum-jaxlib]\n', 'echo ""JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES""\necho ""JAX_ENABLE_X64=$JAX_ENABLE_X64""\necho ""JAX_ENABLE_CHECKS=$JAX_ENABLE_CHECKS""\necho ""JAX_SKIP_SLOW_TESTS=$JAX_SKIP_SLOW_TESTS""\npytest -n auto --tb=short -rf --maxfail=20 \\\n    --report-log output-${{ matrix.python-version }}-log.jsonl \\\n    tests \\\n    || (\n      echo \'ARTIFACTS_AVAILABLE=true\' >> $GITHUB_OUTPUT && false\n    )\n', 'python -m pip install pytest\n', 'rsync -a /tmp/workspace/logs/output-*/ ./logs\nls -R ./logs\ncat logs/*.jsonl > pytest-logs.txt\npython .github/workflows/parse_logs.py pytest-logs.txt --outfile=parsed-logs.txt\n']"
"['chmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.d2l-torch -t d2l-containers:d2l-torch-latest .\ndocker tag d2l-containers:d2l-torch-latest 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-torch-latest\ndocker push 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-torch-latest\n# Clean up to reclaim space\necho ""y"" | docker system prune -a\n', 'chmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.d2l-tf -t d2l-containers:d2l-tensorflow-latest .\ndocker tag d2l-containers:d2l-tensorflow-latest 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-tensorflow-latest\ndocker push 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-tensorflow-latest\n# Clean up to reclaim space\necho ""y"" | docker system prune -a\n', 'chmod +x ./login_ecr.sh; ./login_ecr.sh\necho ${{ secrets.NVCR_JAX_DOCKER_PASSWORD }} | docker login -u ""\\$oauthtoken"" --password-stdin nvcr.io\ndocker build -f Dockerfile.d2l-jax -t d2l-containers:d2l-jax-latest .\ndocker tag d2l-containers:d2l-jax-latest 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-jax-latest\ndocker push 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-jax-latest\n# Clean up to reclaim space\necho ""y"" | docker system prune -a\n', 'chmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.d2l-mxnet -t d2l-containers:d2l-mxnet-latest .\ndocker tag d2l-containers:d2l-mxnet-latest 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-mxnet-latest\ndocker push 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-mxnet-latest\n# Clean up to reclaim space\necho ""y"" | docker system prune -a\n', 'chmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.d2l-builder -t d2l-containers:d2l-builder-latest .\ndocker tag d2l-containers:d2l-builder-latest 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-builder-latest\ndocker push 650140442593.dkr.ecr.us-west-2.amazonaws.com/d2l-containers:d2l-builder-latest\n', 'echo ""Terminating Submitted AWS Batch Job: ""${{ env.Batch_JobID }}""""\naws batch terminate-job --job-id ""${{ env.Batch_JobID }}"" --reason ""Job terminated by cancelled workflow""\n', 'echo ""Terminating Submitted AWS Batch Job: ""${{ env.Batch_JobID }}""""\naws batch terminate-job --job-id ""${{ env.Batch_JobID }}"" --reason ""Job terminated by cancelled workflow""\n', 'echo ""Terminating Submitted AWS Batch Job: ""${{ env.Batch_JobID }}""""\naws batch terminate-job --job-id ""${{ env.Batch_JobID }}"" --reason ""Job terminated by cancelled workflow""\n', 'echo ""Terminating Submitted AWS Batch Job: ""${{ env.Batch_JobID }}""""\naws batch terminate-job --job-id ""${{ env.Batch_JobID }}"" --reason ""Job terminated by cancelled workflow""\n', 'echo ""Terminating Submitted AWS Batch Job: ""${{ env.Batch_JobID }}""""\naws batch terminate-job --job-id ""${{ env.Batch_JobID }}"" --reason ""Job terminated by cancelled workflow""\n', 'CACHE_S3_URL=""s3://preview.d2l.ai/${{ github.event.inputs.cache_dir }}/d2l-en-${{ github.event.inputs.target_branch }}/""\necho ""Removing cache in $CACHE_S3_URL""\naws s3 rm --recursive $CACHE_S3_URL\n']"
"['python -m pip install --upgrade pip\npython -m pip install nox\n', 'nox --non-interactive --session create_test_package_list-${{ matrix.python-version }} -- ./new_tests_packages\n', 'python -m pip install --upgrade pip\npython -m pip install nox\n', 'nox --non-interactive --session test_all_packages-${{ matrix.python-version }}\n', ""ls # DEBUG\nmkdir reports\ncat all_packages_report_legend.txt > all_nodeps_reports_lf.txt\ncat all_packages_nodeps_report_* >> all_nodeps_reports_lf.txt\ntr -d '\\r' < all_nodeps_reports_lf.txt > reports/all_nodeps_reports.txt\ncat all_packages_nodeps_errors_* > all_nodeps_errors_lf.txt\ntr -d '\\r' < all_nodeps_errors_lf.txt > reports/all_nodeps_errors.txt\ncat all_packages_report_legend.txt > all_deps_reports_lf.txt\ncat all_packages_deps_report_* >> all_deps_reports_lf.txt\ntr -d '\\r' < all_deps_reports_lf.txt > reports/all_deps_reports.txt\ncat all_packages_deps_errors_* > all_deps_errors_lf.txt\ntr -d '\\r' < all_deps_errors_lf.txt > reports/all_deps_errors.txt\n"", 'python -m pip install --upgrade pip\npip install nox\n', 'nox --error-on-missing-interpreters --non-interactive --session build\n', 'python -m pip install --upgrade pip\npip install nox\n', 'nox --error-on-missing-interpreters --non-interactive --session build_docs\n', 'python -m pip install --upgrade pip\npython -m pip install nox\n', 'nox --error-on-missing-interpreters --non-interactive --session lint\n', 'python -m pip install --upgrade pip\npython -m pip install nox\n', 'nox --error-on-missing-interpreters --non-interactive --session build_docs\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install nox\n', 'nox --non-interactive --session tests-${{ matrix.python-version }}\n', 'python -m pip install --upgrade pip\npip install nox\n', 'nox --error-on-missing-interpreters --non-interactive --session build\n', 'python -m pip install --upgrade pip\npip install shiv\n', 'shiv -c pipx -o ./pipx.pyz git+https://github.com/pypa/pipx@${{ github.ref_name }}', 'python ./pipx.pyz --version\npython ./pipx.pyz install black\n']"
"['env | grep ^GITHUB', 'echo ""===============> Version from $GITHUB_REF""', 'python -m pip install --upgrade pip\npip install -U setuptools\npip install -r dev-requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 .\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'autopep8 --diff --recursive --max-line-length=127 .\n', 'python -m pytest --cov=./ --cov-report=xml\n']"
[]
"['python -m pip install pipenv\npipenv install --dev --system\n', 'pre-commit run --all-files --show-diff-on-failure', 'python -m pip install pipenv\npipenv install --dev --system\n', 'python --version\npytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install tools/accuracy_checker openvino -r tools/accuracy_checker/requirements-test.in\n', 'python -m pytest tools/accuracy_checker\n', 'python -m pip install pylint==2.10.2\nPYTHONPATH=. python -m pylint --rcfile=.pylintrc `find -wholename \'?*/**/*.py\' -not -path ""./tests/*"" -not -path ""./build/*""`\n', 'python -m pip install --upgrade pip\npython -m pip install numpy\n', 'wget https://storage.openvinotoolkit.org/repositories/openvino/packages/2022.3/linux/l_openvino_toolkit_ubuntu20_2022.3.0.9052.9752fafe8eb_x86_64.tgz\ntar xf l_openvino_toolkit_ubuntu20_2022.3.0.9052.9752fafe8eb_x86_64.tgz\nsudo l_openvino_toolkit_ubuntu20_2022.3.0.9052.9752fafe8eb_x86_64/install_dependencies/install_openvino_dependencies.sh\n', 'mkdir opencv/build\ncd opencv/build\ncmake -DCMAKE_BUILD_TYPE=Release -DWITH_INF_ENGINE=ON -DOpenVINO_DIR=$GITHUB_WORKSPACE/l_openvino_toolkit_ubuntu20_2022.3.0.9052.9752fafe8eb_x86_64/runtime/cmake/ ..\ncmake --build . -j $((`nproc`*2+2))\n', 'source l_openvino_toolkit_ubuntu20_2022.3.0.9052.9752fafe8eb_x86_64/setupvars.sh\nOpenCV_DIR=$GITHUB_WORKSPACE/opencv/build ./demos/build_demos.sh -DENABLE_PYTHON=y\n']"
"['python -m pip install --upgrade pip\npython -m pip install black\npython --version\npython -m black --version\necho $PWD ; ls\npython -m black ./awesome_autodl/bins/* -l 88 --check --diff --verbose\npython -m black ./awesome_autodl/utils/* -l 88 --check --diff --verbose\n', 'python -m pip install . --force\n', 'python -m pip install pytest\npython -m pytest . --durations=0\n']"
"['if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-base""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-elasticsearch""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-jupyter""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-kibana""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-logstash""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-base""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-elasticsearch""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-jupyter""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-kibana""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'if [[ ""x$DOCKER_REPO"" == ""x"" ]]; then\n  DOCKER_REPO=outflanknl\nfi\nDOCKER_IMAGE=""${DOCKER_REPO}/redelk-logstash""\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_repo::${DOCKER_REPO}\necho ::set-output name=build_args::BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\') \\\n  VCS_REF=${GITHUB_SHA::8}\n', 'python -m pip install --upgrade pip\npip install black\npip install -r elkserver/docker/redelk-base/redelkinstalldata/scripts/requirements.txt\n', 'black --check --diff --color elkserver/docker/redelk-base/redelkinstalldata/scripts\n']"
"['brew install openssl readline xz\n', 'brew install --cask sparkle\nbrew install create-dmg\npip3 install --upgrade pip\npip3 install -r dev.txt\n', 'pip3 install .\n', 'pyinstaller --clean --noconfirm package/vorta.spec\ncp -R /usr/local/Caskroom/sparkle/*/Sparkle.framework dist/Vorta.app/Contents/Frameworks/\ncurl -LJO https://github.com/borgbackup/borg/releases/download/${{ github.event.inputs.borg_version }}/borg-macos64.tgz\ntar xvf borg-macos64.tgz -C dist/Vorta.app/Contents/Resources/\ncd dist && zip -rq --symlinks Vorta.zip Vorta.app\n', 'echo $MACOS_CERTIFICATE | base64 --decode > certificate.p12\nsecurity create-keychain -p 123 build.keychain\nsecurity default-keychain -s build.keychain\nsecurity unlock-keychain -p 123 build.keychain\nsecurity import certificate.p12 -k build.keychain -A -P $MACOS_CERTIFICATE_PWD -T /usr/bin/codesign\nsecurity set-key-partition-list -S apple-tool:,apple:,codesign: -s -k 123 build.keychain\npython3 ../package/fix_app_qt_folder_names_for_codesign.py Vorta.app\nsh ../package/macos-package-app.sh\n', 'make lint', 'sudo apt update && sudo apt install -y \\\n  xvfb libssl-dev openssl libacl1-dev libacl1 build-essential borgbackup \\\n  libxkbcommon-x11-0 dbus-x11 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 \\\n  libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xfixes0 libxcb-shape0 \\\n  libegl1 libxcb-cursor0\n', 'brew install openssl readline xz borgbackup\n', 'xvfb-run --server-args=""-screen 0 1024x768x24+32"" \\\n         -a dbus-run-session -- make test\n', 'make test']"
"['git submodule update --init --recursive\nwget https://github.com/kendryte/kendryte-gnu-toolchain/releases/download/v8.2.0-20190409/kendryte-toolchain-ubuntu-amd64-8.2.0-20190409.tar.xz\nsudo tar -Jxf kendryte-toolchain-ubuntu-amd64-8.2.0-20190409.tar.xz -C /opt\nrm -f kendryte-toolchain-ubuntu-amd64-8.2.0-20190409.tar.xz\ncd tools/release\nchmod +x release.sh && ./release.sh\n', 'curr_branch=master\ngit fetch --unshallow\ngit submodule update --init --recursive\nwget https://github.com/kendryte/kendryte-gnu-toolchain/releases/download/v8.2.0-20190409/kendryte-toolchain-ubuntu-amd64-8.2.0-20190409.tar.xz\nsudo tar -Jxf kendryte-toolchain-ubuntu-amd64-8.2.0-20190409.tar.xz -C /opt\nrm -f kendryte-toolchain-ubuntu-amd64-8.2.0-20190409.tar.xz\ncd tools/release\nchmod +x release.sh && ./release.sh\ntime_now=$(date ""+%Y_%m_%d_%H_%M_%S"")\ncd bin\nfirmware_dir=`ls`\nSSHPATH=""$HOME/.ssh""\nrm -rf ""$SSHPATH""\nmkdir -p ""$SSHPATH""\necho ""${{ secrets.ACCESSS_KEY }}"" > ""$SSHPATH/id_rsa""\nchmod 600 ""$SSHPATH/id_rsa""\nsudo sh -c ""echo StrictHostKeyChecking no >>/etc/ssh/ssh_config""\nchmod -R 777 $firmware_dir\nrsync -av --progress $firmware_dir ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_ADDR }}:${{ secrets.SERVER_FOLDER_PATH }}/${curr_branch}/\n']"
[]
[]
""
[]
"['python -m pip install --upgrade pip setuptools wheel\npip install flit\nflit install --extras=all\n', 'make ci']"
"['python -m pip install --upgrade pip\npip install --upgrade wheel build setuptools\npython -m build .\npip install dist/*.whl\n', ""python -c 'import fastmri'\n"", 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npip install --upgrade wheel\npip install --editable "".[tests]""\n', 'python --version\nblack --version\nblack fastmri_examples fastmri tests --check\nmypy --version\nmypy fastmri\nflake8 --version\nflake8 fastmri_examples fastmri tests\nisort --version\nisort --check-only fastmri tests fastmri_examples\n', 'echo -e ""PyTorch \\c"" && pip show torch | grep Version \npytest tests\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['pip install -r requirements/dev-requirements.txt', './scripts/release.sh', 'python -m pip install --upgrade pip\npip install -r requirements/requirements.txt\npip install -r requirements/dev-requirements.txt\npip install -r requirements/test-requirements.txt\n', './scripts/lint.sh', 'python -m pip install --upgrade pip\npip install -r requirements/requirements.txt\npip install -r requirements/test-requirements.txt\npip install -r requirements/extras/postgres.txt\n', 'export PGPASSWORD=postgres\npsql -h localhost -c \'CREATE DATABASE piccolo;\' -U postgres\npsql -h localhost -c ""CREATE USER piccolo PASSWORD \'piccolo\';"" -U postgres\npsql -h localhost -c ""GRANT ALL PRIVILEGES ON DATABASE piccolo TO piccolo;"" -U postgres\npsql -h localhost -c ""CREATE EXTENSION IF NOT EXISTS \\""uuid-ossp\\"";"" -d piccolo -U postgres\n', './scripts/test-integration.sh', 'python -m pip install --upgrade pip\npip install -r requirements/requirements.txt\npip install -r requirements/test-requirements.txt\npip install -r requirements/extras/postgres.txt\n', 'export PGPASSWORD=postgres\npsql -h localhost -c \'CREATE DATABASE piccolo;\' -U postgres\npsql -h localhost -c ""CREATE USER piccolo PASSWORD \'piccolo\';"" -U postgres\npsql -h localhost -c ""GRANT ALL PRIVILEGES ON DATABASE piccolo TO piccolo;"" -U postgres\npsql -h localhost -c ""CREATE EXTENSION IF NOT EXISTS \\""uuid-ossp\\"";"" -d piccolo -U postgres\n', './scripts/test-postgres.sh', 'python -m pip install --upgrade pip\npip install -r requirements/requirements.txt\npip install -r requirements/test-requirements.txt\npip install -r requirements/extras/postgres.txt\n', ""wget -qO- https://binaries.cockroachdb.com/cockroach-${{ matrix.cockroachdb-version }}.linux-amd64.tgz | tar xz\n./cockroach-${{ matrix.cockroachdb-version }}.linux-amd64/cockroach start-single-node --insecure --background\n./cockroach-${{ matrix.cockroachdb-version }}.linux-amd64/cockroach sql --insecure -e 'create database piccolo;'\n"", './scripts/test-cockroach.sh', 'python -m pip install --upgrade pip\npip install -r requirements/requirements.txt\npip install -r requirements/test-requirements.txt\npip install -r requirements/extras/sqlite.txt\n', './scripts/test-sqlite.sh']"
""
"['sudo apt-get update -qq\nsudo apt-get install -y python3 python3-pip\n', 'pip3 install -r requirements.txt', 'bash tests/lint/pycodestyle/run.sh .', 'sudo apt-get update -qq\nsudo apt-get install -y python3 python3-pip\n', 'pip3 install -r requirements.txt', 'bash tests/lint/pylint/run.sh .', 'npm install -g markdownlint-cli', 'bash tests/lint/markdownlint/run.sh .', 'sudo apt-get update -qq\nsudo apt-get install -y python3 python3-pip\n', 'pip3 install -r requirements.txt', 'bash tests/lint/yamllint/run.sh .', 'sudo apt-get update -qq\nsudo apt-get install -y shellcheck\n', 'bash tests/lint/shellcheck/run.sh .', 'mkdir release\ncd src\nzip -r screencast_keys.zip screencast_keys\ncd ..\nmv src/screencast_keys.zip release\n']"
"['pip install -U pip setuptools flake8 pytest python-dateutil\npip install torch --extra-index-url https://download.pytorch.org/whl/cpu\npython setup.py install\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest -s\n', 'pip install -U pip setuptools\npip install torch --extra-index-url https://download.pytorch.org/whl/cpu\npython setup.py install\npip install -U -r docs/requirements.txt\ncd docs\nsphinx-build -T -E -b html -d build/doctrees source build/html\nchmod -R 0777 build/html\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['conda config --set always_yes yes --set changeps1 no\n\nconda update -q conda\n\nconda info -a\n\nconda create -q -n ${CONDA_ENV} python=${{ matrix.python-version }}\n', 'conda install -n ${CONDA_ENV} numpy scipy pytest pytest-cov\nconda install -n ${CONDA_ENV} pytorch=${{ matrix.pytorch-version }} torchvision cpuonly -c pytorch\n\nconda run -n ${CONDA_ENV} python3 -m pip install --upgrade pip\n\nconda run -n ${CONDA_ENV} python3 -m pip install ""tensorflow>=2.0.0a""\nconda run -n ${CONDA_ENV} python3 -m pip install scikit-learn\n\nconda run -n ${CONDA_ENV} python3 -m pip install jaxlib jax\n\nconda run -n ${CONDA_ENV} python3 -m pip install -r requirements.txt\nconda run -n ${CONDA_ENV} python3 -m pip install -r requirements_optional.txt\n', 'conda run -n ${CONDA_ENV} python3 setup.py develop', 'conda run -n ${CONDA_ENV} pytest --cov=kymatio', 'sudo apt update\nsudo apt install texlive texlive-latex-extra cm-super-minimal dvipng\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\npython3 -m pip install -r requirements_optional.txt\npip install torch==1.11.0+cpu \\\n            torchvision==0.12.0+cpu \\\n            -f https://download.pytorch.org/whl/torch_stable.html\n', 'python3 setup.py develop', 'pushd doc\nmake clean\nmake html\npopd\n', 'tools/push_doc.sh', 'python3 -m pip install --upgrade pip\npython3 -m pip install pytest pytest-cov\n\nif [ ${{ matrix.pytorch-version }} == \'1.10\' ]; then\n    pip install torch==1.10.1+cpu \\\n                torchvision==0.11.2+cpu \\\n                -f https://download.pytorch.org/whl/torch_stable.html\nelif [ ${{ matrix.pytorch-version }} == \'1.11\' ]; then\n    pip install torch==1.11.0+cpu \\\n                torchvision==0.12.0+cpu \\\n                -f https://download.pytorch.org/whl/torch_stable.html\nfi\npython3 -m pip install ""tensorflow>=2.0.0a""\npython3 -m pip install scikit-learn\n\npython3 -m pip install jaxlib jax\n\npython3 -m pip install -r requirements.txt\npython3 -m pip install -r requirements_optional.txt\n', 'python3 setup.py develop', 'pytest --cov=kymatio']"
"['pip install flake8\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --max-complexity 22 --max-line-length 127 --statistics\n', 'pip install tox\ntox\n']"
[]
"['python -m pip install --upgrade pip\npython -m pip install --upgrade torch --extra-index-url https://download.pytorch.org/whl/cpu\npython -m pip install -e .\n', 'yapf -dr joeynmt test/unit scripts/*.py\npylint --rcfile=.pylintrc joeynmt test/unit scripts/*.py\nflake8 --max-line-length 88 joeynmt test/unit scripts/*.py\n', 'python -m unittest\n']"
""
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', ""sudo apt-get -q update\nsudo apt-get -qy install dpkg-sig fakeroot moreutils pandoc\ncurl -L 'https://www.zotero.org/download/client/manifests/release/updates-linux-x86_64.json' -o zotero-releases.json\ncurl -L 'https://github.com/Juris-M/assets/releases/download/client%2Freleases%2Fincrementals-linux/incrementals-release-linux' -o jurism-releases.json\n"", 'curl https://rclone.org/install.sh | sudo bash', 'mkdir -p ~/.config/rclone\ncat <<EOF > ~/.config/rclone/rclone.conf\n\n[b2-apt-package-archive]\ntype = b2\naccount = ${{ secrets.B2_APPLICATION_KEY_ID }}\nkey = ${{ secrets.B2_APPLICATION_KEY }}\nhard_delete = true\ndownload_url = https://zotero.retorque.re\nEOF\n', './update.py --url https://zotero.retorque.re/file/apt-package-archive --update ""${{ github.event.inputs.build }}""', 'rm -rf $REPO', 'mkdir -p $REPO\nrclone sync b2-apt-package-archive:apt-package-archive $REPO -v\n', './rebuild.py --mode apt && find $REPO -type f', './update.py --url https://zotero.retorque.re/file/apt-package-archive --update ""${{ steps.rebuild.outputs.update }}""', 'echo publish=${{ steps.rebuilt.outputs.update }}', 'rclone sync $REPO b2-apt-package-archive:apt-package-archive -v', 'rclone cleanup b2-apt-package-archive:apt-package-archive -v', 'sleep 60', 'export INSTALLSH=https://raw.githubusercontent.com/retorquere/zotero-deb/master//install.sh\necho INSTALLSH=$INSTALLSH\ncurl -sL $INSTALLSH | tee install.sh | sudo bash\necho ==== install.sh ===\ncat install.sh\necho ==== zotero.list ===\ncat /etc/apt/sources.list.d/zotero.list\n', 'nslookup zotero.retorque.re\ncurl -s https://zotero.retorque.re/file/apt-package-archive/Release | grep -m 1 Packages.bz2\ncurl -s https://zotero.retorque.re/file/apt-package-archive/Packages.bz2 | md5sum\nsudo apt-get clean\nsudo apt-get -q update\nsudo apt-get -qy install zotero zotero-beta zotero-dev || true\ndpkg -l | grep zotero\n\nfor c in zotero zotero-beta zotero-dev; do\n  ls -lh /usr/lib/$c/${c/-*/} /usr/lib/$c/${c/-*/}-bin /usr/bin/$c\ndone\n', ""curl -L 'https://www.zotero.org/download/client/manifests/release/updates-linux-x86_64.json' -o zotero-releases.json\ncurl -L 'https://github.com/Juris-M/assets/releases/download/client%2Freleases%2Fincrementals-linux/incrementals-release-linux' -o jurism-releases.json\n"", 'rm -rf $REPO']"
"['sudo apt-get update\nsudo apt install gfortran gcc libopenblas-dev\n', 'python -m pip install --upgrade pip wheel setuptools virtualenv asv wget cmake casadi numpy\n', 'python scripts/install_KLU_Sundials.py', '# This workflow also runs for merge commits\n# on develop. In this case, we don\'t want to be\n# fetching the develop branch.\ncurrent_branch=$(git rev-parse --abbrev-ref HEAD)\nif [ $current_branch != ""develop"" ]; then\n    git fetch origin develop:develop\nfi\n', 'asv machine --machine ""GitHubRunner""\n# Get IDs of branch and PR commits\nBASE_COMMIT=$(git rev-parse develop)\nHEAD_COMMIT=$(git rev-parse HEAD)\necho $BASE_COMMIT | tee commits_to_compare.txt\necho $HEAD_COMMIT | tee -a commits_to_compare.txt\nasv run HASHFILE:commits_to_compare.txt --m ""GitHubRunner"" --show-stderr --strict -v\n', 'BASE_COMMIT=$(head -1 commits_to_compare.txt)\nHEAD_COMMIT=$(tail -1 commits_to_compare.txt)\necho ""SUMMARY OF CHANGES""\necho ""==================""\nasv compare $BASE_COMMIT $HEAD_COMMIT | tee compare_result.txt\n# Make sure grep returns error code 0 even if code 1 is\n# returned because no match is found\nREGRESSIONS=$({ grep ""+"" compare_result.txt || test $? = 1; })\nif [ ! -z ""$REGRESSIONS"" ]; \\\nthen \\\necho ""REGRESSIONS FOUND""; \\\necho ""=================""; \\\necho ""$REGRESSIONS""; \\\necho ""=================""; \\\nprintf ""Found %d regression(s)\\n"" $(echo ""$REGRESSIONS"" | wc -l); \\\nexit 1; \\\nfi\n', 'echo ""VERSION=$(date +\'v%y.%-m\')"" >> $GITHUB_ENV\necho ""TODAY=$(date +\'%d\')"" >> $GITHUB_ENV\n', 'pip install wheel\npip install --editable .\n', 'python -c ""from scripts.update_version import get_changelog; get_changelog()""', 'sudo apt-get update\nsudo apt install gfortran gcc libopenblas-dev\n', 'python -m pip install --upgrade pip wheel setuptools virtualenv asv wget cmake casadi numpy\n', 'python scripts/install_KLU_Sundials.py', 'asv machine --machine ""GitHubRunner""\nasv run --machine ""GitHubRunner"" NEW --show-stderr --strict -v\n', 'pip install asv', 'cp -vr new_results/* results\ngit config --global user.email ""$PUSH_BENCH_EMAIL""\ngit config --global user.name ""$PUSH_BENCH_NAME""\ngit add results\ngit commit -am ""Add new results""\ngit push\n', 'asv publish\ngit fetch origin gh-pages:gh-pages\nasv gh-pages\n', 'python -m pip install cibuildwheel==1.9.0', 'git clone --depth 1 --branch v2.6.2 https://github.com/pybind/pybind11.git', ""cd C:\\\nrm -r -fo 'C:\\vcpkg'\ngit clone https://github.com/microsoft/vcpkg\ncd vcpkg\n.\\bootstrap-vcpkg.bat\n"", 'python -m cibuildwheel --output-dir wheelhouse\n', 'python -m pip install cibuildwheel==1.9.0', 'git clone --depth 1 --branch v2.6.2 https://github.com/pybind/pybind11.git', '# https://github.com/actions/virtual-environments/issues/1280\nrm -f /usr/local/bin/2to3*\nrm -f /usr/local/bin/idle3*\nrm -f /usr/local/bin/pydoc3*\nrm -f /usr/local/bin/python3*\nbrew update\nbrew reinstall gcc\npython -m pip install cmake wget\npython scripts/install_KLU_Sundials.py\n', 'python -m cibuildwheel --output-dir wheelhouse', 'pip install wheel', 'python setup.py sdist --formats=gztar', 'mkdir files\nmv windows_wheels/* wheels/* sdist/* files/\n', 'pip install -U pip ""tox<4"" asv', 'git fetch origin develop:develop\n', 'asv machine --machine ""GitHubRunner""\nasv run -m ""GitHubRunner"" -s ${{ github.event.inputs.ncommits }} \\\n${{ github.event.inputs.commit_start }}..${{ github.event.inputs.commit_end }}\n', 'pip install asv', 'cp -vr new_results/* results\ngit config --global user.email ""$PUSH_BENCH_EMAIL""\ngit config --global user.name ""$PUSH_BENCH_NAME""\ngit add results\ngit commit -am ""Add new results""\ngit push\n', 'asv publish\ngit fetch origin gh-pages:gh-pages\nasv gh-pages\n', 'python -m pip install pre-commit\npre-commit run ruff\n', 'sudo apt-get update\nsudo apt install gfortran gcc libopenblas-dev graphviz\nsudo apt install texlive-full\n', 'rm -f /usr/local/bin/2to3*\nrm -f /usr/local/bin/idle3*\nrm -f /usr/local/bin/pydoc3*\nrm -f /usr/local/bin/python3*\nbrew update\nbrew install graphviz\nbrew install openblas\n', 'choco install graphviz --version=2.38.0.20190211', 'python -m pip install --upgrade pip wheel setuptools\npython -m pip install ""tox<4""\n', 'tox -e pybamm-requires', 'python -m tox -e unit', 'tox -e coverage', 'python -m tox -e integration', 'python -m tox -e mac-windows-unit', 'python -m tox -e mac-windows-integration', 'tox -e doctests', 'tox -e examples', 'pip install wheel\npip install --editable .\n', 'echo ""VERSION=$(date +\'v%y.%-m\')"" >> $GITHUB_ENV', 'python scripts/update_version.py', 'echo ""VERSION=$(date +\'%y.%-m\')"" >> $GITHUB_ENV', 'python -m pip install pybamm==${{ env.VERSION }}', 'python benchmarks/work_precision_sets/time_vs_dt_max.py\npython benchmarks/work_precision_sets/time_vs_mesh_size.py\npython benchmarks/work_precision_sets/time_vs_no_of_states.py\npython benchmarks/work_precision_sets/time_vs_reltols.py\npython benchmarks/work_precision_sets/time_vs_abstols.py\n']"
"['npm install -g appcenter-cli', './copy_test_resources.sh', 'echo pvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" >> local.properties', 'chmod +x gradlew', './gradlew assembleEnDebug', './gradlew assembleEnDebugAndroidTest', 'appcenter test run espresso --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Rhino-Android-Activity"" --devices ""Picovoice/android-min-max"" --app-path rhino-activity-demo-app/build/outputs/apk/en/debug/rhino-activity-demo-app-en-debug.apk --test-series ""rhino-android"" --locale ""en_US"" --build-dir rhino-activity-demo-app/build/outputs/apk/androidTest/en/debug', 'npm install -g appcenter-cli', './copy_test_resources.sh', 'echo pvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" >> local.properties', 'echo numTestIterations=""100"" >> local.properties', 'echo performanceThresholdSec=""${{ matrix.performanceThresholdSec }}"" >> local.properties', 'chmod +x gradlew', './gradlew assembleEnDebug', './gradlew assembleEnDebugAndroidTest', 'appcenter test run espresso --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Rhino-Android-Activity"" --devices ""Picovoice/${{ matrix.device }}"" --app-path rhino-activity-demo-app/build/outputs/apk/en/debug/rhino-activity-demo-app-en-debug.apk --test-series ""rhino-android"" --locale ""en_US"" --build-dir rhino-activity-demo-app/build/outputs/apk/androidTest/en/debug', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn --force', 'yarn install', 'yarn build en', 'yarn build ja', 'npm install yarn', 'yarn install', 'yarn build rhino-angular', 'yarn lint', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}', 'cmake -B ./build', 'cmake --build ./build --target rhino_demo_mic', 'cmake -B ./build', 'cmake --build ./build --target rhino_demo_mic', 'cmake -B ./build', 'cmake --build ./build --target rhino_demo_file', 'pip install -r test/requirements.txt', 'python test/test_rhino_c.py ${{secrets.PV_VALID_ACCESS_KEY}} ${{ matrix.platform }} ${{ matrix.arch }}', 'cmake -B ./build', 'cmake --build ./build --target rhino_demo_file', 'pip3 install -r test/requirements.txt', 'python3 test/test_rhino_c.py ${{secrets.PV_VALID_ACCESS_KEY}} ${{ matrix.platform }} ${{ matrix.arch }}', 'dotnet format --verify-no-changes', 'dotnet format --verify-no-changes', 'dotnet restore', 'dotnet build -c MicDemo.Release', 'dotnet restore', 'dotnet build -c FileDemo.Release', 'dotnet build Rhino/Rhino.csproj --framework ${{ matrix.binding-framework }}', 'dotnet test --framework ${{ matrix.test-framework }} -v d', 'dotnet build Rhino/Rhino.csproj --framework net6.0', 'dotnet test --framework net6.0 -v d', 'flutter analyze --no-fatal-infos --no-fatal-warnings', 'flutter analyze --no-fatal-infos --no-fatal-warnings', 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter build apk', './copy_test_resources.sh', ""sed -i 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' integration_test/app_test.dart"", 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter test integration_test', 'pod repo update', 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter build ios --release --no-codesign', './copy_test_resources.sh', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' integration_test/app_test.dart"", 'pod repo update', 'flutter pub get', 'dart scripts/prepare_demo.dart en', 'flutter test integration_test', 'go build', 'go build', './copy.sh', 'go get', 'go build', 'go test -v -access_key ${{secrets.PV_VALID_ACCESS_KEY}}', './copy.sh', 'go get', 'go build', 'go test -v -access_key ${{secrets.PV_VALID_ACCESS_KEY}}', 'gem install cocoapods', 'npm install -g appcenter-cli', 'mkdir ddp', 'brew update\nbrew install convmv\n', './copy_test_resources.sh', 'pod install', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' RhinoAppTestUITests/BaseTest.swift"", 'xcrun xcodebuild build-for-testing -configuration Debug -workspace RhinoAppTest.xcworkspace -sdk iphoneos -scheme RhinoAppTest -derivedDataPath ddp CODE_SIGNING_ALLOWED=NO', 'appcenter test run xcuitest --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Rhino-iOS"" --devices ""Picovoice/ios-min-max"" --test-series ""rhino-ios"" --locale ""en_US"" --build-dir ddp/Build/Products/Debug-iphoneos', 'gem install cocoapods', 'npm install -g appcenter-cli', 'mkdir ddp', 'brew update\nbrew install convmv\n', './copy_test_resources.sh', 'pod install', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' PerformanceTest/PerformanceTest.swift"", ""sed -i '.bak' 's:{NUM_TEST_ITERATIONS}:100:' PerformanceTest/PerformanceTest.swift"", ""sed -i '.bak' 's:{PERFORMANCE_THRESHOLD_SEC}:${{ matrix.performanceThresholdSec }}:' PerformanceTest/PerformanceTest.swift"", 'xcrun xcodebuild build-for-testing -configuration Debug -workspace RhinoAppTest.xcworkspace -sdk iphoneos -scheme PerformanceTest -derivedDataPath ddp CODE_SIGNING_ALLOWED=NO', 'appcenter test run xcuitest --token ${{secrets.APPCENTERAPITOKEN}} --app ""Picovoice/Rhino-iOS"" --devices ""Picovoice/${{ matrix.device }}"" --test-series ""rhino-ios"" --locale ""en_US"" --build-dir ddp/Build/Products/Debug-iphoneos', 'java -Dconfig_loc=resources/.lint/java/ -jar resources/.lint/java/checkstyle-10.5.0-all.jar -c resources/.lint/java/checkstyle.xml binding/android/ binding/java/ binding/flutter/android/ binding/react-native/android/ demo/android/ demo/java/', './gradlew build', './gradlew assemble', './gradlew test --info --tests RhinoPerformanceTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" -DnumTestIterations=""100"" -DperformanceThresholdSec=""${{matrix.performance_threshold_sec}}""', './gradlew assemble', 'bash machine-state.sh', './gradlew test --info --tests RhinoPerformanceTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}"" -DnumTestIterations=""${{matrix.num_test_iterations}}"" -DperformanceThresholdSec=""${{matrix.performance_threshold_sec}}""', 'bash machine-state.sh', './gradlew assemble', './gradlew test --info --tests RhinoTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}""', './gradlew assemble', './gradlew test --info --tests RhinoTest -DpvTestingAccessKey=""${{secrets.PV_VALID_ACCESS_KEY}}""', 'npm install yarn', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn test test/perf.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}} --num_test_iterations=100 --performance_threshold_sec=${{matrix.performance_threshold_sec}}', 'npm install --global yarn', 'yarn install', 'bash machine-state.sh', 'yarn test test/perf.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}} --num_test_iterations=${{matrix.num_test_iterations}} --performance_threshold_sec=${{matrix.performance_threshold_sec}}', 'bash machine-state.sh', 'npm install yarn', 'yarn install', 'yarn test test/index.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}}', 'npm install --global yarn', 'yarn install', 'yarn test test/index.test.ts --access_key=${{secrets.PV_VALID_ACCESS_KEY}}', 'pip install flake8 pep8-naming', 'flake8 --ignore=F401,F403,F405 --max-line-length=120 binding/python demo/python', 'python test_rhino_perf.py ${{secrets.PV_VALID_ACCESS_KEY}} 100 ${{matrix.performance_threshold_sec}}', 'bash machine-state.sh', 'python3 test_rhino_perf.py ${{secrets.PV_VALID_ACCESS_KEY}} ${{matrix.num_test_iterations}} ${{matrix.performance_threshold_sec}}', 'bash machine-state.sh', 'pip install -r requirements.txt', 'python test_rhino.py ${{secrets.PV_VALID_ACCESS_KEY}}', 'pip install -r requirements.txt', 'python3 test_rhino.py ${{secrets.PV_VALID_ACCESS_KEY}}', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn build en', 'yarn build ko', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn install\n./copy_test_resources.sh\n', ""sed -i 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Tests.ts"", 'detox build --configuration android.emu.release', 'detox test --configuration android.emu.release', 'yarn install\n./copy_test_resources.sh\n', 'pod install --repo-update', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Tests.ts"", 'detox build --configuration ios.sim.release', 'detox test --configuration ios.sim.release', 'npm install yarn', 'yarn install', 'yarn', 'npm install yarn', 'yarn install', 'yarn build', 'yarn lint', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}', 'bash copy.sh', 'sudo apt install libasound2-dev -y', 'cargo clippy -- -D warnings', 'sudo apt install libasound2-dev -y', 'cargo clippy -- -D warnings', 'sudo apt install libasound2-dev -y', 'cargo clippy -- -D warnings', 'sudo apt install libasound2-dev', 'cargo build --verbose', 'sudo apt install libasound2-dev', 'cargo build --verbose', 'bash copy.sh', 'sudo apt install libasound2-dev -y', 'cargo build --verbose', 'PV_ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}} cargo test --verbose', 'bash copy.sh', 'cargo build --verbose', 'PV_ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}} cargo test --verbose', 'npm install -g cspell', 'cspell --config resources/.lint/spell-check/.cspell.json ""**/*""', 'dotnet format whitespace binding/unity/Assets/Rhino --folder --verify-no-changes', 'dotnet format whitespace demo/unity --folder --verify-no-changes', './copy.sh\n./copy_test_resources.sh\n', ""sed -i 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Assets/Rhino/Tests/Integration.cs"", ""xvfb-run --auto-servernum --server-args='-screen 0 640x480x24' /home/picovoice/Unity/Hub/Editor/2019.4.34f1/Editor/Unity -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform StandaloneLinux64 -logFile -"", 'sed -n 2p unityresults.xml | grep \'result=""Passed""\'', './copy.sh\n./copy_test_resources.sh\n', 'rm -rf Assets/Rhino/Plugins/mac/arm64', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Assets/Rhino/Tests/Integration.cs"", '/Applications/Unity/Hub/Editor/2019.4.34f1/Unity.app/Contents/MacOS/Unity -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform StandaloneOSX -logFile -', 'sed -n 2p unityresults.xml | grep \'result=""Passed""\'', 'dos2unix copy.sh\nbash copy.sh\ndos2unix copy_test_resources.sh\nbash copy_test_resources.sh\n', 'bash -c ""sed -i \'s:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:\' Assets/Rhino/Tests/Integration.cs""', '& ""C:\\Program Files\\Unity\\Hub\\Editor\\2019.4.34f1\\Editor\\Unity.exe"" -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform StandaloneWindows64 -logFile - | Out-Host\n', 'bash -c ""sed -n 2p unityresults.xml | grep Passed""', './copy.sh\n./copy_test_resources.sh\n', 'rm -rf Assets/Rhino/Plugins/mac/arm64', ""sed -i '.bak' 's:{TESTING_ACCESS_KEY_HERE}:${{secrets.PV_VALID_ACCESS_KEY}}:' Assets/Rhino/Tests/Integration.cs"", '/Users/alirezakenarsari-anhari/Library/Android/sdk/emulator/emulator -avd Pixel_6_API_33 &', '/Applications/Unity/Hub/Editor/2019.4.34f1/Unity.app/Contents/MacOS/Unity -runTests -batchmode -projectPath . -testResults unityresults.xml -testPlatform Android -logFile -', 'sed -n 2p unityresults.xml | grep \'result=""Passed""\'', 'npm install yarn', 'yarn && yarn lint', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn build en', 'yarn build ja', 'npm install yarn', 'yarn install', 'yarn build', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}', 'npm install yarn', 'yarn && yarn lint', 'npm install yarn', 'yarn install', 'yarn copywasm', 'yarn build', 'yarn setup-test', 'yarn test-perf --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}},NUM_TEST_ITERATIONS=20,INIT_PERFORMANCE_THRESHOLD_SEC=${{matrix.initPerformanceThresholdSec}},PROC_PERFORMANCE_THRESHOLD_SEC=${{matrix.procPerformanceThresholdSec}}', 'npm install yarn', 'yarn install', 'yarn copywasm', 'yarn build', 'yarn setup-test', 'yarn test --env ACCESS_KEY=${{secrets.PV_VALID_ACCESS_KEY}}']"
"['sudo apt-get install jq\necho ""::set-output name=matrix::$(bash scripts/get-all-test-paths.sh)""\n', 'python -m pip install --upgrade pip\npython -m pip install wheel\npip install --no-cache-dir ""client/[test]""\npip install --no-cache-dir ""server/[onnx]""\npip install --no-cache-dir ""server/[transformers]""\npip install --no-cache-dir ""server/[search]""\npip install --no-cache-dir ""server/[cn_clip]""\n', 'pytest --suppress-no-test-exit-code --cov=clip_client --cov=clip_server --cov-report=xml \\\n  -v -s -m ""not gpu"" ${{ matrix.test-path }}\necho ""::set-output name=codecov_flag::cas""\n', 'python -m pip install --upgrade pip\npython -m pip install wheel pytest pytest-cov nvidia-pyindex\npip install -e ""client/[test]""\npip install -e ""server/[tensorrt]""\n', 'pytest --suppress-no-test-exit-code --cov=clip_client --cov=clip_server --cov-report=xml \\\n  -v -s -m ""gpu"" ./tests/test_tensorrt.py\necho ""::set-output name=codecov_flag::cas""\n', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*\npip install twine wheel\n./scripts/release.sh\n', 'echo ""module.exports = {extends: [\'@commitlint/config-conventional\']}"" > commitlint.config.js', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude .git,__pycache__,docs/source/conf.py,old,build,dist,tests/,jina/resources/\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --exclude .git,__pycache__,docs/source/conf.py,old,build,dist,tests/,jina/resources/\n', './scripts/black.sh', 'sudo apt-get install jq\necho ""::set-output name=matrix::$(bash scripts/get-all-test-paths.sh)""\n', 'python -m pip install --upgrade pip\npython -m pip install wheel pytest pytest-cov\npip install --no-cache-dir ""client/[test]""\npip install --no-cache-dir ""server/[onnx]""\npip install --no-cache-dir ""server/[transformers]""\npip install --no-cache-dir ""server/[search]""\npip install --no-cache-dir ""server/[cn_clip]""\n', 'pytest --suppress-no-test-exit-code --cov=clip_client --cov=clip_server --cov-report=xml \\\n  -v -s ${{ matrix.test-path }}\necho ""::set-output name=codecov_flag::cas""\n', 'python -m pip install pip==23.0.1\npython -m pip install wheel pytest pytest-cov nvidia-pyindex\npip install -e ""client/[test]""\npip install -e ""server/[tensorrt]""\npip install -e ""server/[onnx]""\npip install -e ""server/[transformers]""\n{\n  pip install -e ""server/[flash-attn]""\n} || {\n  echo ""flash attention was not installed.""\n}\npip install --no-cache-dir ""server/[cn_clip]""\n', 'pytest --suppress-no-test-exit-code --cov=clip_client --cov=clip_server --cov-report=xml \\\n  -v -s -m ""gpu"" ./tests/test_tensorrt.py\npytest --suppress-no-test-exit-code --cov=clip_client --cov=clip_server --cov-report=xml \\\n  -v -s -m ""gpu"" ./tests/test_simple.py\necho ""::set-output name=codecov_flag::cas""\n', 'python -m pip install pip==23.0.1\npython -m pip install wheel pytest pytest-cov nvidia-pyindex\npip install -e ""client/[test]""\npip install -e ""server/[onnx]""\npip install -e ""server/[transformers]""\n{\n  pip install -e ""server/[flash-attn]""\n} || {\n  echo ""flash attention was not installed.""\n}\npip install --no-cache-dir ""server/[cn_clip]""\n', 'pytest --suppress-no-test-exit-code --cov=clip_client --cov=clip_server --cov-report=xml \\\n  -v -s -m ""gpu"" ./tests/test_model.py\necho ""::set-output name=codecov_flag::cas""\n', 'exit 1', 'echo ""All Done""', 'echo ""success!""', 'VCS_REF=${{ github.ref }}\necho ""VCS_REF=$VCS_REF"" >> $GITHUB_ENV\necho ""Will build $VCS_REF""\necho ""BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')"" >> $GITHUB_ENV\n\nif [[ ""${{ matrix.pip_tag }}"" == ""perf"" ]]; then\n  echo ""JINA_PIP_INSTALL_PERF=1"" >> $GITHUB_ENV\nfi\n\nif [[ ""${{ matrix.pip_tag }}"" == """" ]]; then\n  echo ""JINA_PIP_INSTALL_CORE=1"" >> $GITHUB_ENV\nfi\n\nJINA_VERSION=$(sed -n \'/^__version__/p\' ./server/clip_server/__init__.py | cut -d \\\' -f2)\nV_JINA_VERSION=v${JINA_VERSION}\nJINA_MINOR_VERSION=${JINA_VERSION%.*}\nJINA_MAJOR_VERSION=${JINA_MINOR_VERSION%.*}\n\nPY_TAG=${{matrix.py_version}}\nif [ -n ""${PY_TAG}"" ]; then\n  PY_TAG=-py${PY_TAG//./}\nfi\n\nPIP_TAG=${{ matrix.pip_tag }}\nif [ -n ""${PIP_TAG}"" ]; then\n    PIP_TAG=-${PIP_TAG}\nfi\n\nif [[ ""${{ github.event.inputs.triggered_by }}"" == ""CD"" ]]; then\n\n  if [[ ""${{ matrix.py_version }}"" == ""$DEFAULT_PY_VERSION"" ]]; then\n    echo ""TAG_ALIAS=\\\n                    jinaai/clip-server:master${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:master${PIP_TAG}"" \\\n                    >> $GITHUB_ENV\n  else\n    # on every CD\n    echo ""TAG_ALIAS=\\\n                    jinaai/clip-server:master${PY_TAG}${PIP_TAG}"" \\\n                    >> $GITHUB_ENV\n  fi\n\nelif [[ ""${{ github.event.inputs.triggered_by }}"" == ""TAG"" ]]; then\n  # on every tag release\n\n  if [[ ""${{ matrix.py_version }}"" == ""$DEFAULT_PY_VERSION"" ]]; then\n    echo ""TAG_ALIAS=\\\n                    jinaai/clip-server:latest${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_VERSION}${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_MINOR_VERSION}${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_MAJOR_VERSION}${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:latest${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_VERSION}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_MINOR_VERSION}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_MAJOR_VERSION}${PIP_TAG} \\\n                    "" >> $GITHUB_ENV\n  else\n    echo ""TAG_ALIAS=\\\n                    jinaai/clip-server:latest${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_VERSION}${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_MINOR_VERSION}${PY_TAG}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_MAJOR_VERSION}${PY_TAG}${PIP_TAG} \\\n                    "" >> $GITHUB_ENV\n  fi\nelif [[ ""${{ github.event.inputs.triggered_by }}"" == ""MANUAL"" ]]; then\n  # on every manual release\n  if [[ ""${{ matrix.py_version }}"" == ""$DEFAULT_PY_VERSION"" ]]; then\n    echo ""TAG_ALIAS=\\\n                    jinaai/clip-server:${JINA_VERSION}${PIP_TAG}, \\\n                    jinaai/clip-server:${JINA_VERSION}${PY_TAG}${PIP_TAG} \\\n                    "" >> $GITHUB_ENV\n  else\n    echo ""TAG_ALIAS=\\\n                    jinaai/clip-server:${JINA_VERSION}${PY_TAG}${PIP_TAG} \\\n                    "" >> $GITHUB_ENV\n  fi\nelse\n  echo ""Bad triggered_by: ${{ github.event.inputs.triggered_by }}!""\n  exit 1\nfi\n\necho ""JINA_VERSION=${JINA_VERSION}"" >> $GITHUB_ENV\n', '# https://github.com/docker/buildx/issues/464#issuecomment-741507760\n# https://github.com/kubernetes-sigs/azuredisk-csi-driver/pull/808/files\ndocker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\ndocker run --rm --privileged tonistiigi/binfmt --install all\n', 'echo ""success!""', 'VCS_REF=${{ github.ref }}\necho ""VCS_REF=$VCS_REF"" >> $GITHUB_ENV\necho ""Will build $VCS_REF""\necho ""BUILD_DATE=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')"" >> $GITHUB_ENV\necho ""BUILD_TARGET=clip_executor"" >> $GITHUB_ENV\n\nCAS_VERSION=$(sed -n \'/^__version__/p\' ./server/clip_server/__init__.py | cut -d \\\' -f2)\nV_CAS_VERSION=v${CAS_VERSION}\nCAS_MINOR_VERSION=${CAS_VERSION%.*}\nCAS_MAJOR_VERSION=${CAS_MINOR_VERSION%.*}\n\nENGINE_TAG=${{matrix.engine_tag}}\nif [ -n ""${ENGINE_TAG}"" ]; then\n  ENGINE_TAG=-${ENGINE_TAG//./}\nfi\n\nPIP_TAG=${{ matrix.pip_tag }}\nBACKEND_TAG=torch\nif [ -n ""${PIP_TAG}"" ]; then\n    BACKEND_TAG=${PIP_TAG}\n    PIP_TAG=-${PIP_TAG}\nfi\n\nif [[ ""${{ github.event.inputs.triggered_by }}"" == ""CD"" ]]; then\n  # on every CD release\n  echo ""TAG_ALIAS=\\\n                  jinaai/clip_executor:master${PIP_TAG}${ENGINE_TAG}"" \\\n                  >> $GITHUB_ENV\n\nelif [[ ""${{ github.event.inputs.triggered_by }}"" == ""TAG"" ]]; then\n  # on every tag release\n  echo ""TAG_ALIAS=\\\n                  jinaai/clip_executor:latest${PIP_TAG}${ENGINE_TAG}, \\\n                  jinaai/clip_executor:${CAS_VERSION}${PIP_TAG}${ENGINE_TAG}, \\\n                  jinaai/clip_executor:${CAS_MINOR_VERSION}${PIP_TAG}${ENGINE_TAG} \\\n                  "" >> $GITHUB_ENV\n  \nelif [[ ""${{ github.event.inputs.triggered_by }}"" == ""MANUAL"" ]]; then\n  # on every manual release\n  echo ""TAG_ALIAS=\\\n                  jinaai/clip_executor:${CAS_VERSION}${PIP_TAG}${ENGINE_TAG} \\\n                  "" >> $GITHUB_ENV\nelse\n  echo ""Bad triggered_by: ${{ github.event.inputs.triggered_by }}!""\n  exit 1\nfi\n\necho ""CAS_VERSION=${CAS_VERSION}"" >> $GITHUB_ENV\necho ""BACKEND_TAG=${BACKEND_TAG}"" >> $GITHUB_ENV\n', '# https://github.com/docker/buildx/issues/464#issuecomment-741507760\n# https://github.com/kubernetes-sigs/azuredisk-csi-driver/pull/808/files\ndocker run --privileged --rm tonistiigi/binfmt --uninstall qemu-aarch64\ndocker run --rm --privileged tonistiigi/binfmt --install all\n', 'echo ""success!""', 'git config --local user.email ""dev-bot@jina.ai""\ngit config --local user.name ""Jina Dev Bot""\npip install --no-cache-dir client/\npip install --no-cache-dir server/\nmkdir gen-html\ncd docs\npip install -r requirements.txt\npip install --pre -U furo\nbash makedoc.sh\ncd ./_build/dirhtml/\ncp -r ./ ../../../gen-html\ncd -  # back to ./docs\ncd ..\ngit checkout -f gh-pages\ngit rm -rf ./docs\nmkdir -p docs\ncd gen-html\ncp -r ./ ../docs\ncd ../docs\nls -la\ntouch .nojekyll\ncp 404/index.html 404.html\nsed -i \'s/href=""\\.\\./href=""/\' 404.html # fix asset urls that needs to be updated in 404.html\necho clip-as-service.jina.ai > CNAME\ncd ..\ngit add docs\ngit status\ngit commit -m ""chore(docs): update docs due to ${{github.event_name}} on ${{github.repository}}""\ngit push --force origin gh-pages', 'echo ""success!""', 'VCS_REF=${{ github.ref }}\necho ""VCS_REF=$VCS_REF"" >> $GITHUB_ENV\necho ""Will push $VCS_REF""\n\nCAS_VERSION=$(sed -n \'/^__version__/p\' ./server/clip_server/__init__.py | cut -d \\\' -f2)\nV_CAS_VERSION=v${CAS_VERSION}\nCAS_MINOR_VERSION=${CAS_VERSION%.*}\nCAS_MAJOR_VERSION=${CAS_MINOR_VERSION%.*}\n\nif [[ ""${{ github.event.inputs.triggered_by }}"" == ""CD"" ]]; then\n  # on every CD release\n  echo ""TAG_ALIAS=\\\n                  -t latest \\\n                  "" >> $GITHUB_ENV\n  echo ""GPU_TAG_ALIAS=\\\n                  -t latest-gpu \\\n                  "" >> $GITHUB_ENV\n\nelif [[ ""${{ github.event.inputs.triggered_by }}"" == ""TAG"" ]]; then\n  # on every tag release\n  echo ""TAG_ALIAS=\\\n                  -t latest \\\n                  -t ${CAS_VERSION} \\\n                  -t ${CAS_MINOR_VERSION} \\\n                  "" >> $GITHUB_ENV\n  echo ""GPU_TAG_ALIAS=\\\n                  -t latest-gpu \\\n                  -t ${CAS_VERSION}-gpu \\\n                  -t ${CAS_MINOR_VERSION}-gpu \\\n                  "" >> $GITHUB_ENV\n\nelif [[ ""${{ github.event.inputs.triggered_by }}"" == ""MANUAL"" ]]; then\n  # on every manual release\n  echo ""TAG_ALIAS=\\\n                  -t ${CAS_VERSION} \\\n                  "" >> $GITHUB_ENV\n  echo ""GPU_TAG_ALIAS=\\\n                  -t ${CAS_VERSION}-gpu \\\n                  "" >> $GITHUB_ENV\nelse\n  echo ""TAG_ALIAS=\\\n                  -t latest \\\n                  "" >> $GITHUB_ENV\n  echo ""GPU_TAG_ALIAS=\\\n                  -t latest-gpu \\\n                  "" >> $GITHUB_ENV\nfi\n\necho ""CAS_VERSION=${CAS_VERSION}"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade jina yq\n', '# FIX the import issue\necho -e ""\\\n__version__ = \'$CAS_VERSION\'\nfrom .executors.clip_torch import CLIPEncoder\\n\\\n"" > server/clip_server/__init__.py\n          \necho -e ""\\\njtype: CLIPEncoder\\n\\\nmetas:\\n\\\n  py_modules:\\n\\\n    - clip_server/__init__.py\\n\\\n"" > server/config.yml\n\necho -e ""\\\nmanifest_version: 1\\n\\\nname: CLIPTorchEncoder\\n\\\ndescription: Embed images and sentences into fixed-length vectors with CLIP\\n\\\nurl: https://github.com/jina-ai/clip-as-service\\n\\\nkeywords: [clip, clip-model, clip-as-service, pytorch]\\n\\\n"" > server/manifest.yml\n\npython scripts/get-requirements.py """" server/requirements.txt\n\ncp .github/README-exec/torch.readme.md server/README.md\n\nexec_name=`yq -r .name server/manifest.yml`\necho executor name is $exec_name\n\ncp Dockerfiles/base.Dockerfile server/Dockerfile\nJINA_AUTH_TOKEN=${{secrets.JINAHUB_TOKEN}} jina hub push --force $exec_name --secret ${{secrets.TORCH_EXEC_SECRET}} server ${{env.TAG_ALIAS}}\n\ncp Dockerfiles/cuda.Dockerfile server/Dockerfile\nJINA_AUTH_TOKEN=${{secrets.JINAHUB_TOKEN}} jina hub push --force $exec_name --secret ${{secrets.TORCH_EXEC_SECRET}} server ${{env.GPU_TAG_ALIAS}}\n', '# FIX the import issue\necho -e ""\\\n__version__ = \'$CAS_VERSION\'\nfrom .executors.clip_onnx import CLIPEncoder\\n\\\n"" > server/clip_server/__init__.py\n\necho -e ""\\\njtype: CLIPEncoder\\n\\\nmetas:\\n\\\n  py_modules:\\n\\\n    - clip_server/__init__.py\\n\\\n"" > server/config.yml\n\necho -e ""\\\nmanifest_version: 1\\n\\\nname: CLIPOnnxEncoder\\n\\\ndescription: Embed images and sentences into fixed-length vectors with CLIP\\n\\\nurl: https://github.com/jina-ai/clip-as-service\\n\\\nkeywords: [clip, clip-model, clip-as-service, onnx, onnx-runtime]\\n\\\n"" > server/manifest.yml\n\npython scripts/get-requirements.py onnx server/requirements.txt\n\ncp .github/README-exec/onnx.readme.md server/README.md\n\nexec_name=`yq -r .name server/manifest.yml`\necho executor name is $exec_name\n\ncp Dockerfiles/base.Dockerfile server/Dockerfile\nsed -i \'s/ARG BACKEND_TAG=torch/ARG BACKEND_TAG=onnx/g\' server/Dockerfile          \nJINA_AUTH_TOKEN=${{secrets.JINAHUB_TOKEN}} jina hub push --force $exec_name --secret ${{secrets.ONNX_EXEC_SECRET}} server ${{env.TAG_ALIAS}}\n\ncp Dockerfiles/cuda.Dockerfile server/Dockerfile\nsed -i \'s/ARG BACKEND_TAG=torch/ARG BACKEND_TAG=onnx/g\' server/Dockerfile\nJINA_AUTH_TOKEN=${{secrets.JINAHUB_TOKEN}} jina hub push --force $exec_name --secret ${{secrets.ONNX_EXEC_SECRET}} server ${{env.GPU_TAG_ALIAS}}\n', '# FIX the import issue\necho -e ""\\\n__version__ = \'$CAS_VERSION\'\nfrom .executors.clip_tensorrt import CLIPEncoder\\n\\\n"" > server/clip_server/__init__.py\n\necho -e ""\\\njtype: CLIPEncoder\\n\\\nmetas:\\n\\\n  py_modules:\\n\\\n    - clip_server/__init__.py\\n\\\n"" > server/config.yml\n\necho -e ""\\\nmanifest_version: 1\\n\\\nname: CLIPTensorRTEncoder\\n\\\ndescription: Embed images and sentences into fixed-length vectors with CLIP\\n\\\nurl: https://github.com/jina-ai/clip-as-service\\n\\\nkeywords: [clip, clip-model, clip-as-service, onnx, tensorrt]\\n\\\n"" > server/manifest.yml\n\npython scripts/get-requirements.py tensorrt server/requirements.txt\n\ncp Dockerfiles/tensorrt.Dockerfile server/Dockerfile\n\nexec_name=`yq -r .name server/manifest.yml`\necho executor name is $exec_name\n\n# FIXME: disable uploading at debugging\n# JINA_AUTH_TOKEN=${{secrets.JINAHUB_TOKEN}} jina hub push --force $exec_name --secret ${{secrets.TENSORRT_EXEC_SECRET}} server ${{env.TAG_ALIAS}}\n', 'echo ""success!""', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*\nnpm install git-release-notes\npip install twine wheel\n./scripts/release.sh final ""${{ github.event.inputs.release_reason }}"" ""${{github.actor}}""\n', 'echo ""nothing to release""', ""echo '::set-output name=docs::true'"", 'echo ""BRANCH_NAME=${{ github.head_ref }}"" >> $GITHUB_ENV\n', 'npm i -g netlify-cli\npython -m pip install --upgrade pip\npip install -r requirements.txt\ngit fetch origin\nexport NUM_RELEASES=2 # only 2 last tags to save build time\nbash makedoc.sh development\nnetlify deploy --dir=_build/dirhtml --alias=""ft-${{ env.BRANCH_NAME }}"" --message=""Deploying docs to ${{ env.BRANCH_NAME }} branch""\n', 'python scripts/get-last-release-note.py\n']"
"['(cat setup.cfg) | %{$_ -replace ""tag_build.?=.?dev"",""""} | set-content setup.cfg\n', 'python -m pip install --upgrade pip\npip --version\npip install build\npip list\nDISABLE_SQLALCHEMY_CEXT=y python -m build --wheel --outdir ./wheelhouse\n', 'pip install -U twine\ntwine upload --skip-existing ./wheelhouse/*\n', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e github-${{ matrix.build-type }} -- -q --nomemory --notimingintensive ${{ matrix.pytest-args }}', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e ${{ matrix.tox-env }} ${{ matrix.pytest-args }}', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e github-${{ matrix.build-type }} -- -q --nomemory --notimingintensive ${{ matrix.pytest-args }}', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e ${{ matrix.tox-env }} ${{ matrix.pytest-args }}']"
"[""echo '${{ steps.file_changes.outputs.files}}'"", 'build_tools/fail_on_missing_init_files.sh', 'python -m pip install --upgrade pip\npython -m pip install .[all_extras,binder,dev,mlflow]\n', 'build_tools/run_examples.sh', 'python -c ""import sys; print(sys.version)""', 'python -m pip install .\n', 'python sktime/_nopytest_tests.py\n', 'python -c ""import sys; print(sys.version)""', 'python -m pip install .[dev]\n', 'python -m pip list', 'make PYTESTOPTIONS=""--cov --cov-report=xml --timeout=600"" test_softdeps', 'python -c ""import sys; print(sys.version)""', 'python -m pip install .[dev]\n', 'python -m pip list', 'make PYTESTOPTIONS=""--cov --cov-report=xml --timeout=600"" test_softdeps_full', 'python -c ""import sys; print(sys.version)""', 'python -m pip install .[all_extras,dev,mlflow_tests] --no-cache-dir\n', 'python -m pip list', 'make PYTESTOPTIONS=""--cov --cov-report=xml --timeout=600"" test_mlflow', 'python -c ""import sys; print(sys.version)""', 'brew install libomp', 'python -m pip install .[dev,cython_extras] --no-cache-dir\n', 'python -m pip list', 'make PYTESTOPTIONS=""--cov --cov-report=xml --only_cython_estimators=True --matrixdesign=False --timeout=600"" test_check_suite', 'conda --version', 'which python', 'echo ""C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append', 'conda install -c anaconda libpython', 'python -m pip install .[all_extras,dev]', 'python -m pip list', 'mkdir -p testdir/\ncp .coveragerc testdir/\ncp setup.cfg testdir/\npython -m pytest\n', 'python -c ""import sys; print(sys.version)""', 'python -m pip install .[all_extras,dev] --no-cache-dir\n', 'python -m pip list', 'make test', 'python -c ""import sys; print(sys.version)""', 'python -m pip install .[all_extras,dev,pandas1] --no-cache-dir\n', 'python -m pip list', 'make test', 'npm install -g all-contributors-cli@6.24.0', 'npx all-contributors generate', 'echo ""changes detected and committed.""', 'python -m pip install build\npython -m build --wheel --sdist --outdir wheelhouse\n', 'echo ""WHEELNAME=$(ls ./wheelhouse/sktime-*none-any.whl)"" >> $GITHUB_ENV', 'python -m pip install ""${{ env.WHEELNAME }}[all_extras,dev]""', 'make test', 'conda --version', 'which python', 'conda install -c anaconda -n test -y libpython', 'ls -l wheelhouse', 'echo ""WHEELNAME=$(ls ./wheelhouse/sktime-*none-any.whl)"" >> $env:GITHUB_ENV', 'conda activate test', 'python -m pip install ""${env:WHEELNAME}[all_extras,dev]""', 'conda list -n test', 'mkdir -p testdir/\ncp .coveragerc testdir/\ncp setup.cfg testdir/\npython -m pytest\n']"
""
"['if ([ ""$RUNNER_OS"" = ""macOS"" ]); then\n  brew install ta-lib\nfi\nif ([ ""$RUNNER_OS"" = ""Linux"" ]); then\n  if [ ! -f ""$GITHUB_WORKSPACE/ta-lib/src"" ]; then wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -q && tar -xzf ta-lib-0.4.0-src.tar.gz; fi\n  cd ta-lib/\n  ./configure --prefix=/usr\n  if [ ! -f ""$HOME/ta-lib/src"" ]; then make; fi\n  sudo make install\n  cd\nfi\nif ([ ""$RUNNER_OS"" = ""Windows"" ]); then\n  curl -sL http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-msvc.zip -o $GITHUB_WORKSPACE/ta-lib.zip --create-dirs && 7z x $GITHUB_WORKSPACE/ta-lib.zip -o/c/ta-lib && mv /c/ta-lib/ta-lib/* /c/ta-lib/ && rm -rf /c/ta-lib/ta-lib && cd /c/ta-lib/c/make/cdr/win32/msvc && nmake\nfi\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install numba\n', 'mkdir -p $HOME/.local/ta-lib\necho ""LD_LIBRARY_PATH=$HOME/.local/ta-lib/lib:$LD_LIBRARY_PATH"" >> $GITHUB_ENV\necho ""TA_INCLUDE_PATH=$HOME/.local/ta-lib/include"" >> $GITHUB_ENV\necho ""TA_LIBRARY_PATH=$HOME/.local/ta-lib/lib"" >> $GITHUB_ENV\n', 'if ([ ""$RUNNER_OS"" = ""macOS"" ]); then\n  brew install ta-lib\nfi\nif ([ ""$RUNNER_OS"" = ""Windows"" ]); then\n  curl -sL http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-msvc.zip -o $GITHUB_WORKSPACE/ta-lib.zip --create-dirs && 7z x $GITHUB_WORKSPACE/ta-lib.zip -o/c/ta-lib && mv /c/ta-lib/ta-lib/* /c/ta-lib/ && rm -rf /c/ta-lib/ta-lib && cd /c/ta-lib/c/make/cdr/win32/msvc && nmake\nfi\n', 'wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -q && tar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib/\n./configure --prefix=$HOME/.local/ta-lib\nmake\nsudo make install\ncd\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ ! ${{ matrix.python-version }} = ""3.10""]; then pip install numba; fi\npip install -e . -U\n', 'pip install pytest\npytest\n']"
"['curl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain=stable --profile=default -y', 'pip install -U autoflake isort black ruff mypy types-python-dateutil types-pytz types-simplejson types-ujson', 'cargo fmt', './script/lint', 'git diff --exit-code', 'curl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain $RUST_TOOLCHAIN -y', 'rustup default $RUST_TOOLCHAIN', 'python3 -m pip install --user --upgrade pip ""maturin>=1,<2"" wheel', 'maturin build', 'cargo fetch', 'mkdir .cargo', 'cp ci/sdist.toml .cargo/config.toml', 'cargo vendor include/cargo --versioned-dirs', 'maturin sdist --out=dist', 'python3 -m pip install --user dist/orjson*.tar.gz', 'python3 -m pip install --user -r test/requirements.txt -r integration/requirements.txt mypy', 'pytest -s -rxX -v test', './integration/run thread', './integration/run http', './integration/run init', './integration/run typestubs', 'yum install -y clang lld', 'curl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain nightly-2023-05-31 --profile minimal -y', 'rustup component add rust-src --toolchain nightly-2023-05-31-x86_64-unknown-linux-gnu', 'mkdir .cargo\ncp ci/config.toml .cargo/config.toml\n', 'python3 -m pip install --user --upgrade pip ""maturin>=1,<2"" wheel', 'maturin build --release --strip \\\n  --out=dist \\\n  --features=unstable-simd,yyjson \\\n  --compatibility manylinux_2_17 \\\n  --interpreter python${{ matrix.python.version }} \\\n  --target=x86_64-unknown-linux-gnu\n', 'python3 -m pip install --user dist/orjson*.whl', 'python3 -m pip install --user -r test/requirements.txt -r integration/requirements.txt', 'pytest -s -rxX -v test', './integration/run thread', './integration/run http', './integration/run init', 'mkdir .cargo\ncp ci/config.toml .cargo/config.toml\n', 'mkdir .cargo\ncp ci/config.toml .cargo/config.toml\n', 'pip install pip ""maturin>=1,<2""', 'ls -1 .', 'maturin upload --skip-existing --username ""$MATURIN_USERNAME"" *.whl', 'maturin upload --skip-existing --username ""$MATURIN_USERNAME"" *.tar.gz', 'curl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain nightly-2023-05-31 --profile minimal -y', 'python3 -m pip install --user --upgrade pip ""maturin>=1,<2"" wheel', 'maturin build --release \\\n  --out=dist \\\n  --features=unstable-simd,yyjson \\\n  --compatibility off \\\n  --interpreter python${{ matrix.python.version }} \\\n  --target=x86_64-unknown-linux-gnu\n', 'python3 -m pip install --user dist/orjson*.whl', 'python3 -m pip install --user -r test/requirements.txt -r integration/requirements.txt', 'pytest -s -rxX -v test', './integration/run thread', './integration/run http', './integration/run init']"
""
"['brew install pandoc\npython3 -m pip install requests\n', 'python3 _scripts/generate_appcast.py\n', 'python -m pip install --upgrade pip\npython -m pip install -U .[lint]\n', 'black --check --diff src tests\n', 'flake8 src tests\n', 'mypy src\n', 'python -m pip install --upgrade pip\npip install build twine\n', 'python -m build\ntwine upload --skip-existing --sign dist/*\n', 'git fetch --prune --unshallow --force --tags', 'DOCKER_IMAGE=maestraldbx/maestral\nLAST_GIT_TAG=$(git describe --tags --abbrev=0)\nMAESTRAL_VERSION=${LAST_GIT_TAG:1}\nVERSION=noop\nGIT_BRANCH=${GITHUB_REF##*/}\nif [ ""${{ github.event_name }}"" = ""schedule"" ]; then\n  VERSION=nightly\nelif [ ""${{ github.event_name }}"" = ""pull_request_target"" ]; then\n  VERSION=pr-${{ github.event.number }}\nelif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/}\nelif [[ $GITHUB_REF == refs/heads/* ]]; then\n  VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r \'s#/+#-#g\')\n  if [ ""${{ github.event.repository.default_branch }}"" = ""$VERSION"" ]; then\n    VERSION=edge\n  fi\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION}""\nif [[ $VERSION =~ ^v[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n  MAESTRAL_VERSION=${VERSION:1}\n  MINOR=${MAESTRAL_VERSION%.*}\n  MAJOR=${MINOR%.*}\n  TAGS=""${DOCKER_IMAGE}:${MAESTRAL_VERSION},${DOCKER_IMAGE}:${MINOR},${DOCKER_IMAGE}:${MAJOR},${DOCKER_IMAGE}:latest""\nfi\n\necho ""version=${VERSION}"" >> $GITHUB_OUTPUT\necho ""maestral_version=${MAESTRAL_VERSION}"" >> $GITHUB_OUTPUT\necho ""docker_image=${DOCKER_IMAGE}"" >> $GITHUB_OUTPUT\necho ""git_branch=${GIT_BRANCH}"" >> $GITHUB_OUTPUT\necho ""tags=${TAGS}"" >> $GITHUB_OUTPUT\necho ""created=$(date -u +\'%Y-%m-%dT%H:%M:%SZ\')"" >> $GITHUB_OUTPUT\n', 'echo ""Waiting for PyPI release ${{ steps.prep.outputs.maestral_version }}""\nwhile [[ $(curl -s ""https://pypi.org/pypi/maestral/${{ steps.prep.outputs.maestral_version }}/json"") == *""Not Found""* ]]\ndo\n  sleep 2\ndone\n\n# There is delay until packages actually can be downloaded for all platforms.\nsleep 10\n', 'rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', 'python -m pip install --upgrade pip\npython -m pip install -U .[test]\n', 'python -m pytest --cov=maestral --cov-report=xml tests/offline\n', 'python -m pip install --upgrade pip\npython -m pip install -U .[test]\n', 'auth_result=$(curl https://api.dropbox.com/oauth2/token \\\n    -d grant_type=refresh_token \\\n    -d refresh_token=${{ secrets[matrix.token] }} \\\n    -d client_id=2jmbq42w7vof78h)\ntoken=$(echo $auth_result | python -c ""import sys, json; print(json.load(sys.stdin)[\'access_token\'])"")\necho ""::add-mask::$token""\necho ""DROPBOX_ACCESS_TOKEN=$token"" >> $GITHUB_ENV\n', 'pytest -v --cov=maestral --cov-report=xml tests/linked/unit\n', 'python -m pip install --upgrade pip\npython -m pip install -U .[test]\n', 'auth_result=$(curl https://api.dropbox.com/oauth2/token \\\n    -d grant_type=refresh_token \\\n    -d refresh_token=${{ secrets[matrix.token] }} \\\n    -d client_id=2jmbq42w7vof78h)\ntoken=$(echo $auth_result | python -c ""import sys, json; print(json.load(sys.stdin)[\'access_token\'])"")\necho ""::add-mask::$token""\necho ""DROPBOX_ACCESS_TOKEN=$token"" >> $GITHUB_ENV\n', 'pytest --verbose --cov=maestral --cov-report=xml tests/linked/integration --fs-observer ${{ matrix.observer }}\n']"
"['sudo apt-get update\nsudo apt-get -y install graphviz \npython -m pip install --upgrade pip\npip install --upgrade setuptools\npip install -U importlib-metadata>=1.7.0\npip install -U -r requirements.txt\npip install -U -r requirements_dev.txt\npip install ipython\npython setup.py install\n', 'pytest tests --cov=supervised/\n', 'conda activate test\nconda --version\npython --version\n', 'conda install -c conda-forge mljar-supervised', 'python -c ""import supervised;print(supervised.__version__)""']"
"['npm install -g ganache@7.0.2', 'pip install tox', 'tox -e py37', 'npm install -g ganache@7.0.2', 'pip install tox', 'tox -e py38', 'npm install -g ganache@7.0.2', 'pip install tox', 'tox -e py39', 'npm install -g ganache@7.0.2', 'pip install tox', 'tox -e ${{ matrix.job }}', 'npm install -g ganache@7.0.2', 'pip install tox', 'tox -e pmtest', 'pip install tox', 'tox -e ${{ matrix.job }}']"
"['python -m pip install nox pre_commit \\\n  mypy==0.982 \\\n  types-click \\\n  types-pyyaml \\\n  types-pkg_resources \\\n  types-requests \\\n  types-pytz\n', 'python -m pip list', 'nox -db virtualenv -r --non-interactive --python ${{ matrix.python-version }} --session requirements-${{ matrix.python-version }}\n', 'pre-commit run isort --all-files', 'pre-commit run black --all-files', 'pre-commit run pylint --all-files', 'pre-commit run mypy --all-files', ""sed -i '/ray/d' environment.yml"", ""sed -i .bak '/ray/d' environment.yml"", 'mamba install -c conda-forge asv pandas geopandas bokeh\nmamba env update -n pandera-dev -f environment.yml\npip install pandas==${{ matrix.pandas-version }}\npip install --user dask>=2023.3.2\n', 'mamba install -c conda-forge asv pandas==${{ matrix.pandas-version }} geopandas bokeh\nmamba env update -n pandera-dev -f environment.yml\n', 'conda info\nconda list\nconda config --show-sources\nconda config --show\nprintenv | sort\n', 'pytest tests/core ${{ env.PYTEST_FLAGS }}', 'pytest tests/hypotheses ${{ env.PYTEST_FLAGS }}', 'pytest tests/io ${{ env.PYTEST_FLAGS }}', 'pytest -v tests/mypy ${{ env.PYTEST_FLAGS }}', 'pytest tests/strategies ${{ env.PYTEST_FLAGS }} ${{ env.HYPOTHESIS_FLAGS }}', 'pytest tests/fastapi ${{ env.PYTEST_FLAGS }}', 'pytest tests/geopandas ${{ env.PYTEST_FLAGS }}', 'pytest tests/dask ${{ env.PYTEST_FLAGS }}', 'pytest tests/pyspark ${{ env.PYTEST_FLAGS }}', 'pytest tests/modin ${{ env.PYTEST_FLAGS }}', 'pytest tests/modin ${{ env.PYTEST_FLAGS }}', 'nox ${{ env.NOX_FLAGS }} --session doctests', 'nox ${{ env.NOX_FLAGS }} --session docs', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', '# from refs/tags/v1.2.3 get 1.2.3\nVERSION=$(echo $GITHUB_REF | sed \'s#.*/v##\')\nPLACEHOLDER=""__version__\\ =\\ \\""0.0.0+dev0\\""""\ngrep ""$PLACEHOLDER"" ""pandera/version.py""\nsed -i ""s/$PLACEHOLDER/__version__ = \\""$VERSION\\""/g"" ""pandera/version.py""\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python setup.py sdist\n', 'pip install twine\nlast_dist=$(ls -t dist/autoPyTorch-*.tar.gz | head -n 1)\ntwine check ""$last_dist"" --strict\n', 'last_dist=$(ls -t dist/autoPyTorch-*.tar.gz | head -n 1)\npip install $last_dist\n', 'pip install mypy\n\ncd ..  # required to use the installed version of autoPyTorch\n\n# Note this doesn\'t perform mypy checks, those are handled in pre-commit.yaml\n# This only checks if autoPyTorch exports type information\nif ! mypy -c ""import autoPyTorch""; then exit 1; fi\n', 'echo ""##[set-output name=branch;]$(echo ${GITHUB_REF#refs/heads/})""', 'docker login ghcr.io -u $GITHUB_ACTOR -p $GITHUB_TOKEN', 'docker pull ghcr.io/$GITHUB_REPOSITORY/autoPyTorch:$BRANCH', 'docker run -i -d --name unittester -v $GITHUB_WORKSPACE:/workspace -w /workspace ghcr.io/$GITHUB_REPOSITORY/autoPyTorch:$BRANCH', 'docker exec  -i unittester python3 -c \'import autoPyTorch; print(f""Auto-PyTorch imported from {autoPyTorch.__file__}"")\'', 'docker exec  -i unittester python3 -m pytest -v test', 'pip install -e .[docs,examples,forecasting]\n', 'cd docs\nmake html\n', 'cd ..\ngit clone https://github.com/automl/Auto-PyTorch.git --branch gh-pages --single-branch gh-pages\n', 'branch_name=${GITHUB_REF##*/}\ncd ../gh-pages\nrm -rf $branch_name\ncp -r ../Auto-PyTorch/docs/build/html $branch_name\n', 'last_commit=$(git log --pretty=format:""%an: %s"")\ncd ../gh-pages\nbranch_name=${GITHUB_REF##*/}\ngit add $branch_name/\ngit config --global user.name \'Github Actions\'\ngit config --global user.email \'not@mail.com\'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}\ngit commit -am ""$last_commit""\ngit push\n', 'python -m pip install --upgrade pip\npip install -e .[forecasting,test]\n', 'python -m pytest --durations=200 cicd/test_preselected_configs.py -vs\n', 'git submodule update --init --recursive\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files\n', 'git submodule update --init --recursive\npython -m pip install --upgrade pip\npip install -e .[forecasting,test]\n', 'git submodule update --init --recursive\n\npython setup.py sdist\nlast_dist=$(ls -t dist/autoPyTorch-*.tar.gz | head -n 1)\npip install $last_dist[forecasting,test]\n', 'echo ""::set-output name=BEFORE::$(git status --porcelain -b)""\n', 'if [ ${{ matrix.code-cov }} ]; then\n  python -m pytest ${{ env.pytest-args }} ${{ env.code-cov-args }} test\nelse\n  python -m pytest ${{ env.pytest-args }} test\nfi\n', 'before=""${{ steps.status-before.outputs.BEFORE }}""\nafter=""$(git status --porcelain -b)""\nif [[ ""$before"" != ""$after"" ]]; then\n    echo ""git status from before: $before""\n    echo ""git status from after: $after""\n    echo ""Not all generated files have been deleted!""\n    exit 1\nfi\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine flake8 pytest\npip install -r requirements.txt\npip install .\n', 'pytest\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e py-${{ matrix.sqlalchemy }}', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e pep484', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e py-${{ matrix.sqlalchemy }}', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e pep484']"
"['echo ""Branch $GITHUB_REF""', '# Make group jtop\nsudo groupadd jtop\n# Upgrade pip\n# https://github.com/actions/setup-python/issues/225\nsudo -H env ""PATH=$PATH"" python -m pip install --upgrade pip\n# Install tox\n# https://github.com/actions/setup-python/issues/225\nsudo -H env ""PATH=$PATH"" pip install tox\n', 'sudo env ""PATH=$PATH"" python -c ""import sys; print(sys.version)""', '# https://github.com/actions/setup-python/issues/225\nsudo env ""PATH=$PATH"" tox -e py${{ matrix.python-version }}\n', '# Upgrade pip\n# https://github.com/actions/setup-python/issues/225\nsudo -H env ""PATH=$PATH"" python -m pip install --upgrade pip\n# Install jtop\nsudo -H env ""PATH=$PATH"" pip install -v -e .\n# Install sphinx requirements\nsudo -H env ""PATH=$PATH"" pip install -r docs/requirements.txt\n# Run sphinx\ncd docs\nsphinx-build -b html -W . _build/html\n', 'sudo -H python -m pip install --upgrade pip\nsudo -H pip install setuptools wheel twine\n', 'sudo python setup.py sdist\ntwine upload dist/*\n', 'if ${{ startsWith(github.ref, \'refs/tags/\') }} ; then\n  TAG_RELEASE=${GITHUB_REF/refs\\/tags\\//}\nelse\n  TAG_RELEASE=${GITHUB_REF/refs\\/heads\\//}\nfi\necho ::set-output name=tag::${TAG_RELEASE}\necho ""tag=${TAG_RELEASE}""\n', 'gh pr review --approve ""$PR_URL""', 'gh pr merge --auto --squash ""$PR_URL""']"
"['bash tools/run_build.sh flake8-test', 'bash tools/run_build.sh black-test', 'bash tools/run_build.sh source_code_test', 'bash tools/run_build.sh valid_build_files', 'bash tools/run_build.sh clang-format', 'bash tools/run_build.sh check-bazel-format', 'bash tools/pre-commit.sh', 'bash tools/run_build.sh docs_tests', 'pip install --default-timeout=1000 -r tools/install_deps/tensorflow-cpu.txt -r tools/install_deps/pytest.txt', 'pip install -e ./', 'pytest -v -n auto --skip-custom-ops ./tensorflow_addons', 'bash tools/run_cpu_tests.sh', 'python3 -m pip install -U git+https://github.com/tensorflow/docs', '# Run on all notebooks to prevent upstream change.\necho ""Check formatting with nbfmt:""\npython3 -m tensorflow_docs.tools.nbfmt --test \\\n    $(find docs/tutorials/ -type f -name *.ipynb)\n', 'python3 -m pip install -U git+https://github.com/tensorflow/docs', '# Run on all notebooks to prevent upstream change.\necho ""Lint check with nblint:""\npython3 -m tensorflow_docs.tools.nblint \\\n    --arg=repo:tensorflow/addons \\\n    $(find docs/tutorials/ -type f -name *.ipynb ! -path ""docs/tutorials/_template.ipynb"")\n', 'pip install pygithub click', 'python .github/workflows/notify_codeowners.py \\\n    --pull-request-id=auto \\\n    --no-dry-run\n', 'pip install --default-timeout=1000 -r tools/install_deps/pytest.txt -r tools/install_deps/tensorflow-cpu.txt -r requirements.txt\nbash tools/install_deps/install_bazelisk.sh ./\npython configure.py\nbazel test -k --test_timeout 300,450,1200,3600 --test_output=errors //tensorflow_addons/...\n', 'echo ""SKIP_CUSTOM_OP_TESTS=--skip-custom-ops"" >> $GITHUB_ENV', 'echo ""NIGHTLY_FLAG=--nightly"" >> $GITHUB_ENV', 'bash tools/install_deps/install_bazelisk.sh ./', 'bash .github/workflows/make_wheel_${OS}_${CPU}.sh', 'set -e -x\nls -la dist/\nsha256sum dist/*.whl\n', 'set -e -x\necho ${{ secrets.DOCKER_PW }} | docker login --username ${{ secrets.DOCKER_USER }} --password-stdin\nbash .github/workflows/github_build_dev_container.sh\ndocker push tfaddons/dev_container:latest-gpu\n', 'pip install pygithub click', 'python .github/workflows/notify_codeowners.py']"
"['pip install bandit black codespell flake8 isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101 . || true', 'black --check . || true', 'codespell || true', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt || true', 'mypy --ignore-missing-imports . || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
"['python -m pip install --upgrade pip\npip install .\npip install .[tests]\n', 'pytest']"
""
""
"['pip install cython==0.29.34\npip install -r requirements.txt\npip install flake8\n', 'cython blacksheep/url.pyx\ncython blacksheep/exceptions.pyx\ncython blacksheep/headers.pyx\ncython blacksheep/cookies.pyx\ncython blacksheep/contents.pyx\ncython blacksheep/messages.pyx\ncython blacksheep/scribe.pyx\ncython blacksheep/baseapp.pyx\npython setup.py build_ext --inplace\n', 'pytest --doctest-modules --junitxml=junit/pytest-results-${{ matrix.os }}-${{ matrix.python-version }}.xml --cov=$PROJECT_NAME --cov-report=xml\n', 'flake8 blacksheep\nflake8 tests\n', 'pip install -r requirements.txt\npip install black isort flake8\n', 'make compile\n', 'pytest --doctest-modules --junitxml=junit/pytest-results-${{ matrix.os }}-${{ matrix.python-version }}.xml --cov=$PROJECT_NAME --cov-report=xml\n', 'echo ""Running linters - if build fails here, please be patient! Feel free to ask for assistance.""\n\nflake8 blacksheep\nflake8 tests\nflake8 itests\nisort --check-only blacksheep 2>&1\nisort --check-only tests 2>&1\nisort --check-only itests 2>&1\nblack --check blacksheep 2>&1\nblack --check tests 2>&1\nblack --check itests 2>&1\n', 'PYVER=`python -V 2>&1`\n\nif [ ""${PYVER:0:-2}"" == ""Python 3.10"" ]; then\n  echo ""Skipping Hypercorn tests, since it fails with Python 3.10.0 on Ubuntu (2021/10/24)""\nelse\n  APP_DEFAULT_ROUTER=false ASGI_SERVER=hypercorn pytest itests/test_server.py\nfi\n', 'bash <(curl -s https://codecov.io/bash)\n', 'pip install --upgrade twine setuptools wheel', 'python setup.py sdist', 'pip install --upgrade cython build\n', 'make clean\n\ncython blacksheep/url.pyx\ncython blacksheep/exceptions.pyx\ncython blacksheep/headers.pyx\ncython blacksheep/cookies.pyx\ncython blacksheep/contents.pyx\ncython blacksheep/messages.pyx\ncython blacksheep/scribe.pyx\ncython blacksheep/baseapp.pyx\n', 'python -m build\n', 'pip install blacksheep -f ""file:///${GITHUB_WORKSPACE}/dist""\npip freeze | grep blacksheep\n', 'pip install twine\n', 'twine upload -r testpypi dist/*\n', 'twine upload -r pypi dist/*\n']"
"['make setup-dev\nmake venv\nvenv/bin/python3 -m pip install tox-gh-actions\n', 'make fmt-check', 'make lint', 'make test']"
[]
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['sudo apt-get update\nsudo apt-get install python3-sphinx\npip install pandas bokeh\npip install .\n', 'git config --global user.email ""patrik.hlobil@gmail.com""\ngit config --global user.name ""Patrik Hlobil""\ngit fetch\nbash docs/sphinx/build_docs.sh\ngit checkout gh-pages\nmv docs/sphinx/build/html/ html/\nrm -rf docs\nmv html docs\ngit add . \ngit commit -m ""Update documentation for \'${GITHUB_SHA}\'""\ngit push\n', 'python -m pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'sudo apt update\nsudo apt install proj-bin\nsudo apt install openjdk-8-jre\npython -m pip install --upgrade pip\npython -m pip install poetry\npoetry install\n# Only Test GeoPandas and Pyspark API for supported Python versions:\nif [ ""${{ matrix.python-version }}"" != ""3.11"" ]; then echo ""Install Pyspark"" && poetry add pyspark; fi\nif [ ""${{ matrix.python-version }}"" == ""3.8"" ]; then echo ""Install GeoPandas"" && poetry add geopandas; fi\n', 'poetry run pytest --color=yes -s -x -vv --cov-report term-missing --cov=pandas_bokeh Tests/\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine bokeh pandas\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install -r ./scripts/requirements.txt', 'yamllint analytics/ data_model/ sensors/', 'pip install -r ./scripts/requirements.txt', 'yamale -s scripts/analytic_schema.yaml --no-strict analytics/', 'pip install -r ./scripts/requirements.txt', 'yamale -s scripts/datamodel_schema.yaml --no-strict data_model/', 'pip install -r ./scripts/requirements.txt', 'yamale -s scripts/sensor_schema.yaml --no-strict sensors/', 'find analytics data_model sensors -mindepth 1 -maxdepth 1 \\( ! -name ""*.yaml"" \\) -o \\( ! -type f \\)', 'ret=0; for file in analytics/*.yaml; do\n  echo ""Checking $file"";\n  if ! [ ""$(basename $file | sed -e ""s/\\.yaml$//"")"" = ""$(yq \'.id\' < $file)"" ]; then\n    echo ""Failed"";\n    ret=1;\n  fi;\ndone; exit ""$ret""\n', 'ret=0; for file in data_model/*.yaml; do\n  echo ""Checking $file"";\n  if ! [ ""$(basename $file | sed -e ""s/\\.yaml$//"")"" = ""$(yq \'.name | downcase | sub("" "", ""_"")\' < $file)"" ]; then\n    echo ""Failed"";\n    ret=1;\n  fi;\ndone; exit ""$ret""\n', 'ret=0; for file in sensors/*.yaml; do\n  echo ""Checking $file"";\n  if ! [ ""$(basename $file | sed -e ""s/\\.yaml$//"")"" = ""$(yq \'(.sensor_name | downcase) + ""_"" + .sensor_version\' < $file)"" ]; then\n    echo ""Failed"";\n    ret=1;\n  fi;\ndone; exit ""$ret""\n', 'rm -rfv ./docs/data_model', 'rm -rfv ./docs/analytics', 'rm -rfv ./docs/sensors', 'pip install -r ./scripts/requirements.txt', 'python generate_datamodels.py', 'python generate_analytics.py', 'python generate_sensors.py', 'python generate_attack_nav_layer.py']"
"['# get and sanitize the branch name\nbranch=${GITHUB_REF#refs/heads/}\nbranch=${branch//\\//-}\n# derive the docker image tag name from the git branch name\nif [[ $branch == \'master\' ]]; then\n  tag=\'latest\'\nelse\n  tag=""$branch""\nfi\necho ""##[set-output name=tag;]$(echo ${tag})""\n', 'set -e\n\n# get tag name\ntag=${{ steps.tag_name.outputs.tag }}\n\ndocker build -t lardbit/nefarious:$tag .\n\n# create docker network to link containers\ndocker network create tests\n\n# run redis\ndocker run --network tests --name redis --rm -d redis\n\n# run unit tests\ndocker run --network tests -e REDIS_HOST=redis --entrypoint /env/bin/python lardbit/nefarious:$tag manage.py test\n', 'set -e\n\n# get tag name\ntag=${{ steps.tag_name.outputs.tag }}\n\n# store git commit in image for version identification\necho ""$GITHUB_SHA"" > src/.commit\n\n# build image (cache result)\ndocker buildx build \\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 \\\n  --output ""type=image,push=false"" \\\n  --cache-to ""type=local,dest=/tmp/.buildx-cache"" \\\n  --tag lardbit/nefarious:${tag} \\\n  --file Dockerfile .\n\n# push image (from cached result)\ndocker buildx build \\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 \\\n  --output ""type=image,push=true"" \\\n  --cache-from ""type=local,src=/tmp/.buildx-cache"" \\\n  --tag lardbit/nefarious:${tag} \\\n  --file Dockerfile .\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python -m build\ntwine upload dist/*\n', 'conda install --yes numpy mkl-service\nconda install --yes pytorch==${{ matrix.pytorch-version }} cpuonly -c pytorch\npip install -e .""[dev]""\n', 'conda --version && which python && python --version && python -c \'import torch; print(""torch"", torch.__version__)\'\n', 'make lint', 'pytest -v geoopt tests --durations=0 --doctest-modules --cov geoopt --cov-report=xml --cov-report term \n']"
"['python -m pip install --upgrade wheel setuptools build unasync tokenize-rt\npython -m build\n', 'pip install --upgrade twine\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\npython -m pip install .\npython -m pip install tox\npython -m pip install ahk-binary\n', 'tox -e py\n', 'pip install --upgrade coveralls\ncoveralls --service=github\n']"
"['sudo apt-get install swig\nsudo apt-get install unrar\npip install torch~=1.11 --extra-index-url https://download.pytorch.org/whl/cpu\nmake install\n', 'make lint\n', 'make test\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pip install -e .[test]\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['pip install build', 'cd auxiliary/malojalib', 'python -m build', 'rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', 'pip install build', 'python -m build']"
"['echo ""$GITHUB_CONTEXT""', 'pip install .[doc]', 'pip install git+https://${{ secrets.ACTIONS_TOKEN }}@github.com/squidfunk/mkdocs-material-insiders.git', 'python ./scripts/docs.py build-all', 'bash ./scripts/zip-docs.sh', 'mkdir -p /home/runner/work/_temp/_github_home && printf ""[safe]\\n\\tdirectory = /github/workspace"" > /home/runner/work/_temp/_github_home/.gitconfig', 'rm -rf ./site\nmkdir ./site\n', 'cd ./site\nunzip docs.zip\nrm -f docs.zip\n', 'echo ""$GITHUB_CONTEXT""', 'pip install build', 'python -m build', 'echo ""$GITHUB_CONTEXT""', 'pip install smokeshow', 'smokeshow upload coverage-html', 'pip install -e .[all,dev,doc,test]', 'bash scripts/lint.sh', 'mkdir coverage', 'bash scripts/test.sh', 'pip install coverage[toml]', 'ls -la coverage', 'coverage combine coverage', 'coverage report', 'coverage html --show-contexts --title ""Coverage for ${{ github.sha }}""']"
"['python -m pip install --upgrade pip\npip install ruff flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nruff . --format=github --select=E9,F63,F7,F82\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'cd sherlock && python -m unittest tests.all.SherlockDetectTests --verbose\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'cd sherlock && python -m unittest tests.all.SherlockSiteCoverageTests --verbose\n', 'URL=""https://api.github.com/repos/sherlock-project/sherlock/pulls/${{ github.event.pull_request.number }}/files""\nFILES=$(curl -s -X GET -G $URL | jq -r \'.[] | .filename\')\nif echo $FILES | grep -q "".json""; then\n  echo ""::set-output name=matrix::{\\""include\\"":[{\\""python\\"":\\""3.x\\""}]}""\nelse\n  echo ""::set-output name=matrix::{\\""include\\"":[{\\""python\\"":\\""3.7\\""},{\\""python\\"":\\""3.8\\""}]},{\\""python\\"":\\""3.9\\""},{\\""python\\"":\\""3.10\\""}]},{\\""python\\"":\\""3.11\\""}]}""\nfi\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'cd sherlock && python -m unittest tests.all.SherlockDetectTests --verbose\n', 'python site_list.py', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\nif ! git diff --exit-code; then\n  git commit -a -m ""Updated Site List""\nfi\n']"
"['make install-python-requirements', 'cd scripts && bash Generate_Site_mkDocs.sh', 'ls -al generated/site/', 'cd generated && zip -r ../bundle.zip site', 'zip -T bundle.zip', 'sudo apt-get install -y unzip zip', 'git checkout gh-pages', 'shopt -s extglob\nrm -rdfv !(""CNAME""|""robots.txt""|""_config.yml"")\n', 'mv Bundle/bundle.zip bundle.zip\nrm -rf Bundle 1>/dev/null 2>&1\n', 'zip -T bundle.zip', 'unzip bundle.zip\nmv site/* .\nupd=`date +""%Y-%m-%d at %T""`; echo ""Website last update: $upd."" > README.md\nrm -rf site\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""GitHub Action""\ngit add --all .\ngit commit -a -m ""Deploy the generated website via GitHub Actions""\n', 'pip install requests', 'chmod +x scripts/Identify_Old_Issue_And_PR.py', 'python scripts/Identify_Old_Issue_And_PR.py ${{ secrets.SLACK_WEBHOOK }}', 'npm install', 'npm run link-check', 'cat log | awk -v RS=""FILE:"" \'match($0, /(\\S*\\.md).*\\[âœ–\\].*(\\d*\\slinks\\schecked\\.)(.*)/, arr ) { print ""FILE:""arr[1] arr[3] > ""brokenlinks""}\'\nrm -f err log\ncat brokenlinks\nlinks=`cat brokenlinks`\nlinks=""${links//\'%\'/\'%25\'}""\nlinks=""${links//$\'\\n\'/\'%0A\'}""\nlinks=""${links//$\'\\r\'/\'%0D\'}""\necho ::set-output name=links::**Following links are broken:** %0A$links\n', 'npm install', 'npm test', 'make install-python-requirements', '(cd scripts && bash Generate_Site_mkDocs.sh)', 'ls -lah generated/site/']"
"['curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -\nsudo apt-get install -y nodejs\nsudo mkdir -p /opt/hostedtoolcache/\nsudo chmod 777 -R /opt/hostedtoolcache/\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', 'python --version\npip --version\npython -m pip install -U pip\ncmake --version\n\n# remove torch and ray from the dependencies so we can add them depending on the matrix args for the job.\ncat requirements.txt | sed \'/^torch[>=<\\b]/d\' | sed \'/^torchtext/d\' | sed \'/^torchvision/d\' | sed \'/^torchaudio/d\' > requirements-temp && mv requirements-temp requirements.txt\ncat requirements_distributed.txt | sed \'/^ray[\\[]/d\'\n\nif [ ""$MARKERS"" != ""distributed"" ]; then\n  # Skip distributed and hyperopt requirements to test optional imports\n  echo > requirements-temp && mv requirements-temp requirements_distributed.txt\n  echo > requirements-temp && mv requirements-temp requirements_hyperopt.txt\n\n  # Skip distributed tree requirement (lightgbm-ray)\n  cat requirements_tree.txt | sed \'/^lightgbm-ray/d\' > requirements-temp && mv requirements-temp requirements_tree.txt\nelse\n  if [ ""$RAY_VERSION"" == ""nightly"" ]; then\n    # NOTE: hardcoded for python 3.10 on Linux\n    echo ""ray[default,data,serve,tune] @ https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl"" >> requirements_distributed.txt\n  else\n    echo ""ray[default,data,serve,tune]==$RAY_VERSION"" >> requirements_distributed.txt\n  fi\nfi\n\nif [ ""$PYTORCH"" == ""nightly"" ]; then\n  extra_index_url=https://download.pytorch.org/whl/nightly/cpu\n  # TODO: https://github.com/ludwig-ai/ludwig/issues/3395\n  pip install --pre torch==2.1.0.dev20230508+cpu torchtext torchvision torchaudio --index-url $extra_index_url\nelse\n  extra_index_url=https://download.pytorch.org/whl/cpu\n  pip install torch==$PYTORCH torchtext torchvision torchaudio --extra-index-url $extra_index_url\nfi\n\npip install \'.[test]\' --extra-index-url $extra_index_url\npip list\n\nif [ ""$PYTORCH"" == ""nightly"" ]; then\n  python -c ""from packaging import version; import torch; assert version.parse(torch.__version__).release >= version.parse(\\""2.0.0\\"").release, f\\""torch {version.parse(torch.__version__).release} < version.parse(\\\'2.0.0\\\').release\\""""\nelse\n  python -c ""from packaging import version; import torch; assert version.parse(torch.__version__).release == version.parse(\\""$PYTORCH\\"").release, f\\""torch {version.parse(torch.__version__).release} != version.parse(\\\'$PYTORCH\\\').release\\""""\nfi\n\nif [ ""$MARKERS"" == ""distributed"" ]; then\n  python -c ""from packaging import version; import ray; assert version.parse(ray.__version__).release == version.parse(\\""$RAY_VERSION\\"").release, f\\""ray {version.parse(ray.__version__).release} != version.parse(\\\'$RAY_VERSION\\\').release\\""""\nelse\n  python -c ""import importlib.util; assert importlib.util.find_spec(\'ray\') is None, \\""found ray but expected it to not be installed\\""""\nfi\n', 'sudo mkdir -p ""$NEUROPOD_BASE_DIR""\ncurl -L https://github.com/uber/neuropod/releases/download/v${{ env.NEUROPOD_VERISON }}/libneuropod-cpu-linux-v${{ env.NEUROPOD_VERISON }}-torchscript-${{ env.TORCHSCRIPT_VERISON }}-backend.tar.gz | sudo tar -xz -C ""$NEUROPOD_BASE_DIR""\n', 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=5400 pytest -v --timeout 300 --durations 100 -m ""$MARKERS and not slow and not combinatorial and not horovod and not llm"" --junitxml pytest.xml tests/ludwig\n', 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=5400 pytest -v --timeout 300 --durations 100 -m ""$MARKERS and not slow and not combinatorial and not horovod or benchmark and not llm"" --junitxml pytest.xml tests/regression_tests\n', 'pip install -r requirements_extra.txt\nHOROVOD_BUILT=$(python -c ""import horovod.torch; horovod.torch.nccl_built(); print(\'SUCCESS\')"" || true)\nif [[ $HOROVOD_BUILT != ""SUCCESS"" ]]; then\n  pip uninstall -y horovod\n  pip install --no-cache-dir git+https://github.com/horovod/horovod.git@master\nfi\nhorovodrun --check-build\n', 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=5400 pytest -v --timeout 300 --durations 100 -m ""$MARKERS and horovod and not slow and not combinatorial and not llm"" --junitxml pytest.xml tests/\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\n\n# remove torch and ray from the dependencies so we can add them depending on the matrix args for the job.\ncat requirements.txt | sed '/^torch[>=<\\b]/d' | sed '/^torchtext/d' | sed '/^torchvision/d' | sed '/^torchaudio/d' > requirements-temp && mv requirements-temp requirements.txt\ncat requirements_distributed.txt | sed '/^ray[\\[]/d'\npip install torch==2.0.0 torchtext torchvision torchaudio\npip install ray==2.3.0\npip install '.[test]'\npip list\n"", 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=7200 pytest -v --timeout 300 --durations 100 -m ""not slow and not combinatorial and not horovod and not llm and integration_tests_a"" --junitxml pytest.xml tests/integration_tests\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\n\n# remove torch and ray from the dependencies so we can add them depending on the matrix args for the job.\ncat requirements.txt | sed '/^torch[>=<\\b]/d' | sed '/^torchtext/d' | sed '/^torchvision/d' | sed '/^torchaudio/d' > requirements-temp && mv requirements-temp requirements.txt\ncat requirements_distributed.txt | sed '/^ray[\\[]/d'\npip install torch==2.0.0 torchtext torchvision torchaudio\npip install ray==2.3.0\npip install '.[test]'\npip list\n"", 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=7200 pytest -v --timeout 300 --durations 100 -m ""not slow and not combinatorial and not horovod and not llm and integration_tests_b"" --junitxml pytest.xml tests/integration_tests\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\n\n# remove torch and ray from the dependencies so we can add them depending on the matrix args for the job.\ncat requirements.txt | sed '/^torch[>=<\\b]/d' | sed '/^torchtext/d' | sed '/^torchvision/d' | sed '/^torchaudio/d' > requirements-temp && mv requirements-temp requirements.txt\ncat requirements_distributed.txt | sed '/^ray[\\[]/d'\npip install torch==2.0.0 torchtext torchvision torchaudio\npip install ray==2.3.0\npip install '.[test]'\npip list\n"", 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=7200 pytest -v --timeout 300 --durations 100 -m ""not slow and not combinatorial and not horovod and not llm and integration_tests_c"" --junitxml pytest.xml tests/integration_tests\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\n\n# remove torch and ray from the dependencies so we can add them depending on the matrix args for the job.\ncat requirements.txt | sed '/^torch[>=<\\b]/d' | sed '/^torchtext/d' | sed '/^torchvision/d' | sed '/^torchaudio/d' > requirements-temp && mv requirements-temp requirements.txt\ncat requirements_distributed.txt | sed '/^ray[\\[]/d'\npip install torch==2.0.0 torchtext torchvision torchaudio\npip install ray==2.3.0\npip install '.[test]'\npip list\n"", 'RUN_PRIVATE=$IS_NOT_FORK LUDWIG_TEST_SUITE_TIMEOUT_S=7200 pytest -v --timeout 300 --durations 100 -m ""not slow and not combinatorial and not horovod and not llm and not integration_tests_a and not integration_tests_b and not integration_tests_c"" --junitxml pytest.xml tests/integration_tests\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\n\n# remove torch and ray from the dependencies so we can add them depending on the matrix args for the job.\ncat requirements.txt | sed '/^torch[>=<\\b]/d' | sed '/^torchtext/d' | sed '/^torchvision/d' | sed '/^torchaudio/d' > requirements-temp && mv requirements-temp requirements.txt\ncat requirements_distributed.txt | sed '/^ray[\\[]/d'\npip install torch==2.0.0 torchtext torchvision torchaudio\npip install ray==2.3.0\npip install '.[test]'\npip list\n"", 'pytest -vs --durations 100 -m ""llm"" --junitxml pytest.xml tests\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\npip install '.[test]'\npip list\n"", 'pytest -vs --durations 100 -m ""combinatorial"" --junitxml pytest.xml tests/ludwig/config_sampling\n', 'pytest -rx --durations 100 -m ""combinatorial"" --junitxml pytest.xml tests/training_success\n', 'sudo apt-get install -y cmake libsndfile1\n', 'brew install libuv\n', ""python --version\npip --version\npython -m pip install -U pip\npip install torch==2.0.0 torchtext\npip install ray==2.3.0\npip install '.'\npip list\n"", 'ludwig check_install\n', 'cd examples/getting_started && sh ./run.sh\n', 'sudo apt-get update && sudo apt-get install -y libsndfile1 cmake ccache build-essential g++-8 gcc-8\ncmake --version\n', 'brew install libuv\n', 'python --version\npip --version\npython -m pip install -U pip\ncmake --version\n\necho ""MARKERS:"" $MARKERS\n\nif [ ""$PYTORCH"" == ""nightly"" ]; then\n  cat requirements.txt | sed \'/^torch[>=<]/d\' | sed \'/^torchtext[>=<]/d\' | sed \'/^torchvision[>=<]/d\' | sed \'/^torchaudio[>=<]/d\' > requirements-temp && mv requirements-temp requirements.txt\n  extra_index_url=https://download.pytorch.org/whl/nightly/cpu\n  pip install --pre torch torchtext torchvision torchaudio --extra-index-url $extra_index_url\nelse\n  extra_index_url=https://download.pytorch.org/whl/cpu\n  pip install torch==$PYTORCH torchtext torchvision torchaudio --extra-index-url $extra_index_url\nfi\n\nif [ ""$RAY_VERSION"" == ""nightly"" ]; then\n  # NOTE: hardcoded for python 3.9 on Linux\n  pip install https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp39-cp39-manylinux2014_x86_64.whl\nelse\n  # installing `six` early resolves ModuleNotFound error in ray==2.1.0\n  pip install six\n  pip install ray==$RAY_VERSION\nfi\nray_expected=$(python -c ""import ray; print(ray.__version__)"")\n\ntorch_expected=$(python -c ""import torch; print(torch.__version__)"")\n\npip install \'.[test]\' --extra-index-url $extra_index_url\npip list\n\npython -c ""import torch; assert torch.__version__ == \\""$torch_expected\\"", f\\""torch {torch.__version__} != $torch_expected\\""""\npython -c ""import ray; assert ray.__version__ == \\""$ray_expected\\"", f\\""ray {ray.__version__} != $ray_expected\\""""\n', 'sudo mkdir -p ""$NEUROPOD_BASE_DIR""\ncurl -L https://github.com/uber/neuropod/releases/download/v${{ env.NEUROPOD_VERSION }}/libneuropod-cpu-linux-v${{ env.NEUROPOD_VERSION }}-torchscript-${{ env.TORCHSCRIPT_VERSION }}-backend.tar.gz | sudo tar -xz -C ""$NEUROPOD_BASE_DIR""\n', 'RUN_PRIVATE=1 LUDWIG_TEST_SUITE_TIMEOUT_S=6000 pytest -vs --timeout 450 --durations 100 -m ""($MARKERS) and (not $EXCLUDED_MARKERS)"" --junitxml pytest_slow.xml tests\n', 'mkdir -p artifacts && cd artifacts\n\nartifacts_url=${{ github.event.workflow_run.artifacts_url }}\n\ngh api ""$artifacts_url"" -q \'.artifacts[] | [.name, .archive_download_url] | @tsv\' | while read artifact\ndo\n  IFS=$\'\\t\' read name url <<< ""$artifact""\n  gh api $url > ""$name.zip""\n  unzip -d ""$name"" ""$name.zip""\ndone\n', 'python -m pip install --upgrade pip\npython -m pip install setuptools wheel twine\n', 'python setup.py sdist\npython -m twine upload dist/*\n']"
"['docker buildx build --push --platform linux/amd64,linux/arm64 -t shmilylty/oneforall:latest .', 'python -m pip install --upgrade pip\npip install uvloop\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install coverage\ncoverage run test.py\n']"
""
""
""
"['pipx install poetry', 'poetry env use python3.10', 'poetry install', 'poetry run strawberry export-schema schema:schema > schema.graphql', 'pip install poetry', 'poetry install', 'mkdir -p .mypy_cache', 'poetry run mypy --config-file mypy.ini', 'import os, json, textwrap\n\nall_files_path = os.path.join(os.environ[""HOME""], ""files.json"")\nevent_json_path = os.environ[""GITHUB_EVENT_PATH""]\n\nwith open(all_files_path) as f:\n    all_files = json.load(f)\n\nwith open(event_json_path) as f:\n    event_data = json.load(f)\n\nlinks = [\n  ""https://strawberry.rocks/docs/pr/{pr_number}/{path}"".format(\n    pr_number=event_data[""number""],\n    path=file.replace("".md"", """").replace(""docs/"", """")\n  )\n  for file in all_files\n  if file.startswith(""docs/"") and file.endswith("".md"")\n]\n\nmessage = textwrap.dedent(\n  """"""\n  Hi ðŸ‘‹ You can find a preview of the docs here:\n\n  {links}\n  """"""\n).format(links=""\\n"".join(links))\n\nmessage = message.replace(""%"", ""%25"").replace(""\\n"", ""%0A"").replace(""\\r"", ""%0D"")\noutput = ""::set-output name=message::{}"".format(message)\n\nprint(output)\n', 'python -m pip install pip --upgrade\npip install poetry\npip install githubrelease\npip install autopub\npip install httpx\n', 'echo ""::set-output name=release::$(autopub check)""', 'echo ""::set-output name=commit::$(git rev-parse HEAD)""', 'autopub prepare\npoetry version $(poetry version -s).dev.$(date \'+%s\')\npoetry build\npoetry publish --username __token__\necho ""::set-output name=version::$(poetry version -s)""\n', 'echo ""${{ steps.get-info.outputs.contributor-twitter-username }}""', 'bots = [\n   ""dependabot-preview[bot]"",\n   ""dependabot-preview"",\n   ""dependabot"",\n   ""dependabot[bot]"",\n ]\n\nusername = ""${{ needs.get-contributor-info.outputs.contributor-username }}""\n\nif username in bots:\n  print(f""Skipping {username} because it is a bot"")\n  print(""::set-output name=skip::true"")\nelse:\n  print(""::set-output name=skip::false"")\n', 'echo ::set-output name=url::$(curl --location --request POST ""https://api.imgur.com/3/image"" \\\n  --fail \\\n  --header ""Authorization: Client-ID $IMGUR_CLIENT_ID"" \\\n  --form ""image=@screenshot.png;filename=screenshot.png;type=image/png"" \\\n  | jq  --raw-output \'.data.link\')\n', 'exit 1', 'python -m pip install pip --upgrade\npip install poetry\npip install githubrelease\npip install autopub\npip install httpx\n', 'set +e\necho ::set-output name=release::$(autopub check)\n', 'git remote set-url origin https://${{ secrets.BOT_TOKEN }}@github.com/${{ github.repository }}\nautopub prepare\npoetry build\nautopub commit\nautopub githubrelease\npoetry publish --username __token__\n', 'import os\nfrom pathlib import Path\nfrom autopub.base import get_project_version\n\nwith Path(os.environ[""GITHUB_OUTPUT""]).open(\'a\') as f:\n    f.write(f""version={get_project_version()}\\n"")\n', 'pip install httpx', 'import os\n\nimport httpx\n\n\ntag = os.environ[""TAG""]\ncontributor_username = os.environ[""CONTRIBUTOR_USERNAME""]\npr_number = os.environ[""PR_NUMBER""]\n\n\nresponse = httpx.get(\n    url=f""https://api.github.com/repos/strawberry-graphql/strawberry/releases/tags/{tag}"",\n    headers={\n        ""Accept"": ""application/vnd.github.v3+json"",\n    },\n)\n\nresponse.raise_for_status()\ndata = response.json()\nrelease_id = data[""id""]\nrelease_body = data[""body""].strip()\n\nrelease_footer = f""""""\nReleases contributed by @{contributor_username} via #{pr_number}\n"""""".strip()\n\nupdated_release_body = f""{release_body}\\n\\n{release_footer}""\n\nresponse = httpx.patch(\n    url=f""https://api.github.com/repos/strawberry-graphql/strawberry/releases/{release_id}"",\n    json={""body"": updated_release_body},\n    headers={\n        ""Accept"": ""application/vnd.github.v3+json"",\n        ""Authorization"": f""token {os.environ[\'GITHUB_TOKEN\']}"",\n    },\n)\n\nresponse.raise_for_status()\n', 'pip install tweepy==4.0.0', 'import os\nimport base64\nimport tweepy\n\nauth = tweepy.OAuthHandler(\n    os.environ[""TWITTER_CONSUMER_KEY""],\n    os.environ[""TWITTER_CONSUMER_SECRET""],\n)\nauth.set_access_token(\n    os.environ[""TWITTER_ACCESS_TOKEN""],\n    os.environ[""TWITTER_ACCESS_TOKEN_SECRET""],\n)\n\napi = tweepy.API(auth)\n\ntweet = base64.b64decode(os.environ[""TWEET""]).decode(""utf-8"")\n\napi.update_status_with_media(filename=""screenshot.png"", status=tweet)\n', 'git config --global user.name \'Strawberry GraphQL Bot\'\ngit config --global user.email \'bot@strawberry.rocks\'\ngit remote set-url origin https://${{ secrets.BOT_TOKEN }}@github.com/${{ github.repository }}\ngit pull\ngit rm TWEET.md\ngit commit -m ""Remove TWEET.md""\ngit push\n', 'pipx install poetry', 'poetry env use ${{ matrix.python-version }}', 'poetry install', 'poetry run pytest tests/mypy', 'pipx install poetry', 'npm install -g --no-package-lock --no-save pyright', 'poetry env use ${{ matrix.python-version }}', 'poetry install', 'poetry run pytest tests/pyright', 'pipx install poetry', 'poetry env use ${{ matrix.python-version }}', 'poetry install', 'poetry run pytest --cov=strawberry --cov-append --cov-report=xml -n auto --showlocals -vv -m ""not starlette"" -m ""not django"" -m ""not starlite"" --ignore=tests/mypy --ignore=tests/pyright', 'pipx install poetry', 'poetry install', 'poetry run pytest --cov=strawberry --cov-append --cov-report=xml -n auto --showlocals -vv -m ""not django"" --ignore=tests/mypy --ignore=tests/pyright', 'pipx install poetry', 'poetry env use python3.10', 'poetry install', 'poetry add --python ^3.10 django@^${{ matrix.django }}', 'poetry run pytest --cov=strawberry --cov-append --cov-report=xml -n auto --showlocals -vv -m django', 'pipx install poetry', 'poetry env use python3.10', 'poetry install', 'poetry run pip install starlette==${{ matrix.starlette }}', 'poetry run pip install ""fastapi<0.92.0""', 'poetry run pip install ""fastapi==0.92.0""', 'poetry run pytest --cov=strawberry --cov-append --cov-report=xml -n auto --showlocals -vv -m starlette', 'pipx install poetry', 'poetry env use ${{ matrix.python-version }}', 'poetry install', 'poetry run coverage run -m pytest --showlocals -vv -m starlite', 'poetry run coverage xml -i']"
"['source ./ci/install-conda.sh\npython -m pip install --upgrade pip setuptools wheel coverage;\n', 'source ./ci/reload-env.sh\nexport DEFAULT_VENV=$VIRTUAL_ENV\npip install numpy scipy cython oss2 asv\npip install -e "".[dev,extra]""\n', 'source ./ci/reload-env.sh\nunset CI\ncd benchmarks/asv_bench\ngit config --global user.email ""mars_asv_benchmark@outlook.com""\ngit config --global user.name ""Mars ASV Benchmark""\nasv check -E existing\ngit remote add upstream https://github.com/mars-project/mars.git\ngit fetch upstream\ngit merge upstream/master\nasv machine --yes\nasv continuous -e -f 1.1 --strict upstream/master HEAD\n', 'source ./ci/install-conda.sh\npython -m pip install --upgrade pip setuptools wheel coverage;\n', 'source ./ci/reload-env.sh\nexport DEFAULT_VENV=$VIRTUAL_ENV\n\nif [[ ! ""$PYTHON"" =~ ""3.6"" ]]; then\n  conda install -n test --quiet --yes -c conda-forge python=$PYTHON numba\nfi\n\n# todo remove this when fastparquet release new version\nif [[ ""$PYTHON"" =~ ""3.6"" ]]; then\n  pip install numpy\\<1.20.0\nfi\n\nsource ./ci/rewrite-cov-config.sh\n\npip install numpy scipy cython oss2\npip install -e "".[dev,extra]""\npip install virtualenv flaky\n\nif [ -z ""$NO_COMMON_TESTS"" ]; then\n  if [[ ! ""$PYTHON"" =~ ""3.6"" ]] && [[ ! ""$PYTHON"" =~ ""3.9"" ]]; then\n    pip install h5py zarr matplotlib prometheus-client requests\n    conda install -n test --quiet --yes -c conda-forge python=$PYTHON \\\n      ""tiledb-py>=0.4.3,<0.6.0"" ""tiledb<2.0.0"" || true\n  fi\n\n  conda install -n test --quiet --yes -c pkgs/main python=$PYTHON certifi\nfi\nconda list -n test\n', 'source ./ci/reload-env.sh\nsource ./ci/run-tests.sh\ncoverage xml\n', 'bash <(curl -s https://codecov.io/bash)\n', 'source ./ci/reload-env.sh\n\nif [[ -n ""$GIT_TAG"" ]]; then\n  BRANCHES=""$GIT_TAG""\n  echo ""Will handle tag $BRANCHES""\nelse\n  MAINBRANCH=$(git rev-parse --abbrev-ref HEAD)\n  BRANCHES=$(git branch -r --list \'origin/v*\' | sed \'s/ *origin\\///g\')\n  BRANCHES=""$MAINBRANCH $BRANCHES""\n\n  echo ""Will handle branches:""\n  for branch in $BRANCHES; do\n    echo ""  $branch""\n  done\nfi\n\nif [[ ""$DOCKER_ORG"" == ""marsuploader"" ]]; then\n  export DOCKER_ORG=""marsproject""\nfi\n\nfor branch in $BRANCHES; do\n  if [[ -n ""$GIT_TAG"" ]]; then\n    export IMAGE_TAG=""$GIT_TAG""\n  else\n    echo """"\n    git checkout $branch\n    git log --pretty=format:""%h - %an, %cd : %s"" --since=25.hours\n\n    # consider schedule delay of Github Actions, some margin needed.\n    if [[ ! ""$(git log --since=25.hours)"" ]]; then\n      echo ""No recent commits on branch $branch found, will skip building image.""\n      continue\n    fi\n\n    export IMAGE_TAG=""nightly-$branch""\n  fi\n  bash bin/kube-image-tool.sh -o ""$DOCKER_ORG"" -t ""$IMAGE_TAG"" build\n  docker push ""$DOCKER_ORG/mars:$IMAGE_TAG""\ndone\n', 'source ./ci/install-conda.sh\npython -m pip install --upgrade pip setuptools wheel coverage;\n', 'source ./ci/reload-env.sh\nexport DEFAULT_VENV=$VIRTUAL_ENV\n\nsource ./ci/rewrite-cov-config.sh\n\npip install numpy scipy cython oss2\npip install -e "".[dev,extra]""\npip install virtualenv flaky\nconda list -n test\n', 'source ./ci/reload-env.sh\n\nmkdir -p build\nif [[ $UNAME == ""darwin"" ]]; then\n  python -m pytest $PYTEST_CONFIG --forked --timeout=1500 \\\n    mars/oscar mars/services mars/storage mars/lib mars/tests\n  mv .coverage build/.coverage.dist.file\nelif [[ $UNAME == ""windows"" ]]; then\n  python -m pytest $PYTEST_CONFIG --force-flaky --max-runs=10 --timeout=1500 \\\n    mars/oscar\n  mv .coverage build/.coverage.oscar.file\n\n  python -m pytest $PYTEST_CONFIG --force-flaky --max-runs=10 --timeout=1500 \\\n    mars/services\n  mv .coverage build/.coverage.services.file\n\n  # skip mars/tests temporarily, as it may cause test stuck and needs more inspection\n  python -m pytest $PYTEST_CONFIG --force-flaky --max-runs=10 --timeout=1500 \\\n    mars/storage mars/lib  # mars/tests\n  mv .coverage build/.coverage.misc.file\nfi\ncoverage combine build/ && coverage report\ncoverage xml\n', 'bash <(curl -s https://codecov.io/bash)\n', 'source ./ci/install-conda.sh\npython -m pip install --upgrade pip setuptools wheel coverage\n', 'source ./ci/reload-env.sh\nexport DEFAULT_VENV=$VIRTUAL_ENV\n\nsource ./ci/rewrite-cov-config.sh\n\npip install numpy scipy cython\n\npip install -e "".[dev,extra]""\n\nif [[ $UNAME == ""windows"" ]]; then\n  pip install virtualenv flaky\nelse\n  pip install virtualenv flaky\n  if [ -n ""$WITH_KUBERNETES"" ]; then\n    pip install kubernetes ""pandas<1.5.0""\n    kubectl get pods -A\n  fi\n  if [ -n ""$WITH_HADOOP"" ]; then\n    ./ci/install-hadoop.sh\n    echo ""import coverage; coverage.process_startup()"" > \\\n      $(python -c ""import site; print(site.getsitepackages()[-1])"")/coverage.pth\n    conda install -n test --quiet --yes -c conda-forge python=$PYTHON skein libffi conda-pack\n  fi\n  if [ -n ""$WITH_VINEYARD"" ]; then\n    pip install vineyard -i https://pypi.org/simple\n\n    mkdir -p /tmp/etcd-download-test\n    export ETCD_VER=v3.4.13\n    export ETCD_DOWNLOAD_URL=https://github.com/etcd-io/etcd/releases/download\n    curl -L $ETCD_DOWNLOAD_URL/$ETCD_VER/etcd-$ETCD_VER-linux-amd64.tar.gz -o /tmp/etcd-$ETCD_VER-linux-amd64.tar.gz\n    tar xzvf /tmp/etcd-$ETCD_VER-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1\n    sudo mv /tmp/etcd-download-test/etcd /usr/local/bin/\n    sudo mv /tmp/etcd-download-test/etcdctl /usr/local/bin/\n    rm -fr /tmp/etcd-$ETCD_VER-linux-amd64.tar.gz /tmp/etcd-download-test\n  fi\n  if [ -n ""$WITH_RAY"" ] || [ -n ""$WITH_RAY_DAG"" ] || [ -n ""$WITH_RAY_DEPLOY"" ]; then\n    pip install ""ray>=1.8.0,<2.4.0""\n    pip install ""xgboost_ray<0.1.14"" ""protobuf<4""\n    # Ray Datasets need pyarrow>=6.0.1\n    pip install ""pyarrow>=6.0.1""\n    pip install lightgbm\n  fi\n  if [ -n ""$RUN_DASK"" ]; then\n    pip install ""dask[complete]"" ""mimesis<9.0.0"" scikit-learn\n  fi\nfi\nconda list -n test\n', 'source ./ci/reload-env.sh\n\nif [ -n ""$WITH_HADOOP"" ]; then\n  source $CONDA/bin/activate test\n  source ./ci/reload-env.sh\n  pytest $PYTEST_CONFIG -m hadoop\n  coverage report\nfi\nif [ -n ""$WITH_KUBERNETES"" ]; then\n  pytest $PYTEST_CONFIG --forked mars/deploy/kubernetes\n  coverage report\nfi\nif [ -n ""$WITH_VINEYARD"" ]; then\n  pytest $PYTEST_CONFIG mars/storage/tests/test_libs.py\n  mv .coverage build/.coverage.test_lib.file\n\n  pytest $PYTEST_CONFIG mars/deploy/oscar/tests/test_local.py\n  mv .coverage build/.coverage.test_local.file\n\n  pytest $PYTEST_CONFIG -k ""vineyard"" \\\n      mars/tensor/datastore/tests/test_datastore_execution.py \\\n      mars/dataframe/datastore/tests/test_datastore_execution.py\n  mv .coverage build/.coverage.test_vineyard_op.file\n\n  coverage combine build/ && coverage report\nfi\nif [ -n ""$WITH_RAY"" ]; then\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s --ignore=mars/deploy/oscar/ -m ray \n  coverage report\nfi\nif [ -n ""$WITH_RAY_DEPLOY"" ]; then\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray.py -m ray\n  mv .coverage build/.coverage.test_ray.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_load_modules.py -m ray\n  mv .coverage build/.coverage.test_ray_load_modules.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_cluster_standalone.py -m ray\n  mv .coverage build/.coverage.test_ray_cluster_standalone.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_client.py -m ray\n  mv .coverage build/.coverage.test_ray_client.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_fault_injection.py -m ray\n  mv .coverage build/.coverage.test_ray_fault_injection.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_scheduling.py -m ray\n  mv .coverage build/.coverage.test_ray_scheduling.file\n\n  coverage combine build/ && coverage report\nfi\nif [ -n ""$WITH_RAY_DAG"" ]; then\n  export MARS_CI_BACKEND=ray\n  export RAY_idle_worker_killing_time_threshold_ms=60000\n  pytest $PYTEST_CONFIG --durations=0 --timeout=500 mars/dataframe -v -s -m ""not skip_ray_dag"" --ignore=mars/dataframe/contrib/raydataset\n  pytest $PYTEST_CONFIG --durations=0 --timeout=500 mars/dataframe/contrib/raydataset -v -s -m ""not skip_ray_dag"" \n  pytest $PYTEST_CONFIG --durations=0 --timeout=500 mars/tensor -v -s -m ""not skip_ray_dag""\n  pytest $PYTEST_CONFIG --durations=0 --timeout=500 mars/learn --ignore mars/learn/contrib --ignore mars/learn/utils/tests/test_collect_ports.py -v -s -m ""not skip_ray_dag""\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s -m ray_dag\n  mv .coverage build/.coverage.ray_dag.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_dag.py\n  mv .coverage build/.coverage.test_ray_dag.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_dag_failover.py\n  mv .coverage build/.coverage.test_ray_dag_failover.file\n  pytest $PYTEST_CONFIG --durations=0 --timeout=200 -v -s mars/deploy/oscar/tests/test_ray_dag_oscar.py -m ray\n  mv .coverage build/.coverage.test_ray_dag_oscar.file\n  \n  coverage combine build/ && coverage report\nfi\nif [ -n ""$RUN_DASK"" ]; then\n  pytest $PYTEST_CONFIG mars/contrib/dask/tests/test_dask.py\n  coverage report\nfi\ncoverage xml\n', 'sudo docker stop vineyard || true\n', 'bash <(curl -s https://codecov.io/bash)\n', 'git reset --hard\npip install -r ci/requirements-wheel.txt\npython setup.py build_web\n', 'git reset --hard\npip install -r ci/requirements-wheel.txt\npython setup.py sdist --formats=gztar --dist-dir=./wheelhouse\n', 'if [[ ""$GITHUB_REPOSITORY"" == ""mars-project/mars"" ]]; then\n  PYPI_REPO=""https://upload.pypi.org/legacy/""\nelse\n  PYPI_REPO=""https://test.pypi.org/legacy/""\nfi\necho ""[distutils]""             > ~/.pypirc\necho ""index-servers =""        >> ~/.pypirc\necho ""    pypi""               >> ~/.pypirc\necho ""[pypi]""                 >> ~/.pypirc\necho ""repository=$PYPI_REPO""  >> ~/.pypirc\necho ""username=$PYPI_USR""     >> ~/.pypirc\necho ""password=$PYPI_PWD""     >> ~/.pypirc\npython -m pip install twine\npython -m twine upload -r pypi --skip-existing wheelhouse/*\n']"
"['docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker build -t diyhue/diyhue:ci -f ./.build/Dockerfile --platform=${{ matrix.arch }} --build-arg TARGETPLATFORM=${{ matrix.arch }} .\n', 'docker save --output output.tar diyhue/diyhue:ci\n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker load --input output.tar\n', 'docker run -d --name diyhue --network=host -v /mnt/hue-emulator/export:/opt/hue-emulator/export -e MAC=b8:27:eb:d4:dc:11 -e IP=192.168.1.123 -e DECONZ=192.168.1.111 -e IP_RANGE=5,6 -e DEBUG=true diyhue/diyhue:ci\nsleep 15\ndocker logs diyhue\n', 'docker kill diyhue\ndocker rm diyhue\n', 'if [ ${GITHUB_REPOSITORY} == ""diyhue/diyHue"" ]; then\n  if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n    export DOCKER_REPO=""diyhue/core""\n  else\n    export DOCKER_REPO=""${{ matrix.registry }}/diyhue/core""\n  fi\nelse\n  export DOCKER_REPO=""${{ matrix.registry }}/$GITHUB_REPOSITORY""\nfi\necho ""DOCKER_REPO=${DOCKER_REPO}"" >> $GITHUB_ENV\necho Repository set as: ${DOCKER_REPO}\n', 'if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n  docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\nelse\n  echo ""${{ secrets.CR_PAT }}"" | docker login ${{ matrix.registry }} -u ""${{ secrets.CR_USER }}"" --password-stdin\nfi\n', 'docker load --input output.tar\n', 'docker tag diyhue/diyhue:ci ${DOCKER_REPO}:${{ matrix.arch }}-${{ github.run_number }}\ndocker push ${DOCKER_REPO}:${{ matrix.arch }}-${{ github.run_number }}\necho ${DOCKER_REPO}:${{ matrix.arch }}-${{ github.run_number }} published\n', 'if [ ${GITHUB_REPOSITORY} == ""diyhue/diyHue"" ]; then\n  if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n    export DOCKER_REPO=""diyhue/core""\n  else\n    export DOCKER_REPO=""${{ matrix.registry }}/diyhue/core""\n  fi\nelse\n  export DOCKER_REPO=""${{ matrix.registry }}/$GITHUB_REPOSITORY""\nfi\necho ""DOCKER_REPO=${DOCKER_REPO}"" >> $GITHUB_ENV\necho Repository set as: ${DOCKER_REPO}\n', 'export DOCKER_CLI_EXPERIMENTAL=""enabled""\necho ""DOCKER_CLI_EXPERIMENTAL=${DOCKER_CLI_EXPERIMENTAL}"" >> $GITHUB_ENV\n', 'if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n  docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\nelse\n  echo ""${{ secrets.CR_PAT }}"" | docker login ${{ matrix.registry }} -u ""${{ secrets.CR_USER }}"" --password-stdin\nfi\n', 'docker load --input output.tar\n', 'docker tag diyhue/diyhue:ci ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}-$GITHUB_SHA\ndocker push ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}-$GITHUB_SHA\necho ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}-$GITHUB_SHA published\n', 'docker tag diyhue/diyhue:ci ${DOCKER_REPO}:${{ matrix.arch }}-latest\ndocker push ${DOCKER_REPO}:${{ matrix.arch }}-latest\necho ${DOCKER_REPO}:${{ matrix.arch }}-latest published\n', 'docker tag diyhue/diyhue:ci ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}\ndocker push ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}\necho ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/} published\n', 'docker tag diyhue/diyhue:ci ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}\ndocker push ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/}\necho ${DOCKER_REPO}:${{ matrix.arch }}-${GITHUB_REF##*/} published\n', 'if [ ${GITHUB_REPOSITORY} == ""diyhue/diyHue"" ]; then\n  if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n    export DOCKER_REPO=""diyhue/core""\n  else\n    export DOCKER_REPO=""${{ matrix.registry }}/diyhue/core""\n  fi\nelse\n  export DOCKER_REPO=""${{ matrix.registry }}/$GITHUB_REPOSITORY""\nfi\necho ""DOCKER_REPO=${DOCKER_REPO}"" >> $GITHUB_ENV\necho Repository set as: ${DOCKER_REPO}\n', 'export DOCKER_CLI_EXPERIMENTAL=""enabled""\necho ""DOCKER_CLI_EXPERIMENTAL=${DOCKER_CLI_EXPERIMENTAL}"" >> $GITHUB_ENV\n', 'if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n  docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\nelse\n  echo ""${{ secrets.CR_PAT }}"" | docker login ${{ matrix.registry }} -u ""${{ secrets.CR_USER }}"" --password-stdin\nfi\n', 'docker manifest create \\\n  ${DOCKER_REPO}:${GITHUB_REF##*/}-$GITHUB_SHA \\\n  ${DOCKER_REPO}:amd64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv7-${{ github.run_number }} \\\n  ${DOCKER_REPO}:arm64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv6-${{ github.run_number }}\ndocker manifest annotate ${DOCKER_REPO}:${GITHUB_REF##*/}-$GITHUB_SHA ${DOCKER_REPO}:armv6-${{ github.run_number }} --arch arm --variant v6\ndocker manifest annotate ${DOCKER_REPO}:${GITHUB_REF##*/}-$GITHUB_SHA ${DOCKER_REPO}:armv7-${{ github.run_number }} --arch arm --variant v7\ndocker manifest push ${DOCKER_REPO}:${GITHUB_REF##*/}-$GITHUB_SHA\necho ${DOCKER_REPO}:$GITHUB_SHA published\n', 'docker manifest create \\\n  ${DOCKER_REPO}:$GITHUB_RUN_NUMBER \\\n  ${DOCKER_REPO}:amd64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv7-${{ github.run_number }} \\\n  ${DOCKER_REPO}:arm64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv6-${{ github.run_number }}\ndocker manifest annotate ${DOCKER_REPO}:$GITHUB_RUN_NUMBER ${DOCKER_REPO}:armv6-${{ github.run_number }} --arch arm --variant v6\ndocker manifest annotate ${DOCKER_REPO}:$GITHUB_RUN_NUMBER ${DOCKER_REPO}:armv7-${{ github.run_number }} --arch arm --variant v7\ndocker manifest push ${DOCKER_REPO}:$GITHUB_RUN_NUMBER\necho ${DOCKER_REPO}:$GITHUB_RUN_NUMBER published\n', 'docker manifest create \\\n  ${DOCKER_REPO}:latest \\\n  ${DOCKER_REPO}:amd64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv7-${{ github.run_number }} \\\n  ${DOCKER_REPO}:arm64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv6-${{ github.run_number }}\ndocker manifest annotate ${DOCKER_REPO}:latest ${DOCKER_REPO}:armv6-${{ github.run_number }} --arch arm --variant v6\ndocker manifest annotate ${DOCKER_REPO}:latest ${DOCKER_REPO}:armv7-${{ github.run_number }} --arch arm --variant v7\ndocker manifest push ${DOCKER_REPO}:latest\necho ${DOCKER_REPO}:latest published\n', 'docker manifest create \\\n  ${DOCKER_REPO}:${GITHUB_REF##*/} \\\n  ${DOCKER_REPO}:amd64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv7-${{ github.run_number }} \\\n  ${DOCKER_REPO}:arm64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv6-${{ github.run_number }}\ndocker manifest annotate ${DOCKER_REPO}:${GITHUB_REF##*/} ${DOCKER_REPO}:armv6-${{ github.run_number }} --arch arm --variant v6\ndocker manifest annotate ${DOCKER_REPO}:${GITHUB_REF##*/} ${DOCKER_REPO}:armv7-${{ github.run_number }} --arch arm --variant v7\ndocker manifest push ${DOCKER_REPO}:${GITHUB_REF##*/}\necho ${DOCKER_REPO}:${GITHUB_REF##*/} published\n', 'docker manifest create \\\n  ${DOCKER_REPO}:${GITHUB_REF##*/} \\\n  ${DOCKER_REPO}:amd64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv7-${{ github.run_number }} \\\n  ${DOCKER_REPO}:arm64-${{ github.run_number }} \\\n  ${DOCKER_REPO}:armv6-${{ github.run_number }}\ndocker manifest annotate ${DOCKER_REPO}:${GITHUB_REF##*/} ${DOCKER_REPO}:armv6-${{ github.run_number }} --arch arm --variant v6l\ndocker manifest annotate ${DOCKER_REPO}:${GITHUB_REF##*/} ${DOCKER_REPO}:armv7-${{ github.run_number }} --arch arm --variant v7l\ndocker manifest push ${DOCKER_REPO}:${GITHUB_REF##*/}\necho ${DOCKER_REPO}:${GITHUB_REF##*/} published\n', 'if [ ${GITHUB_REPOSITORY} == ""diyhue/diyHue"" ]; then\n  if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n    export DOCKER_REPO=""diyhue/core""\n  else\n    export DOCKER_REPO=""${{ matrix.registry }}/diyhue/core""\n  fi\nelse\n  export DOCKER_REPO=""${{ matrix.registry }}/$GITHUB_REPOSITORY""\nfi\necho ""DOCKER_REPO=${DOCKER_REPO}"" >> $GITHUB_ENV\necho Repository set as: ${DOCKER_REPO}\n', 'export DOCKER_CLI_EXPERIMENTAL=""enabled""\necho ""DOCKER_CLI_EXPERIMENTAL=${DOCKER_CLI_EXPERIMENTAL}"" >> $GITHUB_ENV\n', 'if [ ${{ matrix.registry }} == ""registry.hub.docker.com"" ]; then\n  docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\nelse\n  echo ""${{ secrets.CR_PAT }}"" | docker login ${{ matrix.registry }} -u ""${{ secrets.CR_USER }}"" --password-stdin\nfi\n', 'docker manifest create ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}-$GITHUB_SHA ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}-$GITHUB_SHA\ndocker manifest annotate ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}-$GITHUB_SHA ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}-$GITHUB_SHA --arch arm --variant ${{ matrix.version }}\ndocker manifest push ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}-$GITHUB_SHA\n', 'docker manifest create ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${{ github.run_number }} ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${{ github.run_number }}\ndocker manifest annotate ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${{ github.run_number }} ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${{ github.run_number }} --arch arm --variant ${{ matrix.version }}\ndocker manifest push ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${{ github.run_number }}\n', 'docker manifest create ${DOCKER_REPO}:${{ matrix.arch_friendly }}-latest ${DOCKER_REPO}:${{ matrix.arch_friendly }}-latest\ndocker manifest annotate ${DOCKER_REPO}:${{ matrix.arch_friendly }}-latest ${DOCKER_REPO}:${{ matrix.arch_friendly }}-latest --arch arm --variant ${{ matrix.version }}\ndocker manifest push ${DOCKER_REPO}:${{ matrix.arch_friendly }}-latest\n', 'docker manifest create ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/} ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}\ndocker manifest annotate ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/} ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/} --arch arm --variant ${{ matrix.version }}\ndocker manifest push ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}\n', 'docker manifest create ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/} ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}\ndocker manifest annotate ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/} ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/} --arch arm --variant ${{ matrix.version }}\ndocker manifest push ${DOCKER_REPO}:${{ matrix.arch_friendly }}-${GITHUB_REF##*/}\n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker build -t diyhue/core:ci -f ./.build/Dockerfile --platform=${{ matrix.arch }} --build-arg TARGETPLATFORM=${{ matrix.arch }} .\n', 'docker run -d --name diyHue --network=host -v /mnt/hue-emulator/export:/opt/hue-emulator/export -e MAC=b8:27:eb:d4:dc:11 -e IP=192.168.1.123 -e DECONZ=192.168.1.111 -e IP_RANGE=5,6 -e DEBUG=true diyhue/core:ci\nsleep 15\ndocker logs diyHue\n', 'docker kill diyHue\ndocker rm diyHue\n']"
"['python3 -m pip install --upgrade build\npython3 -m build\n', 'find ./dist/*.whl | xargs pip install\n', 'pip install pytest', 'pytest\n', 'python3 -m pip install --upgrade build\npython3 -m build\n', 'find ./dist/*.whl | xargs pip install\n', 'pip install pytest', 'pytest\n', 'python3 -m pip install --upgrade build\npython3 -m build\n', 'find ./dist/*.whl | xargs pip install\n', 'pip install pytest', 'pytest\n', 'python3 -m pip install --upgrade build\npython3 -m build\n', 'find ./dist/*.whl | xargs pip install\n', 'pip install pytest', 'pytest\n', 'python3 -m pip install --upgrade build\npython3 -m build\n', 'find ./dist/*.whl | xargs pip install\n', 'pip install pytest', 'pytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install .[dev,dev-postgres,dev-mysql,dev-sqlite,dev-duckdb]\n', 'make VENV= INSTALL= check.mypy', 'make VENV= INSTALL= check.black', 'make VENV= INSTALL= check.flake8', 'make VENV= INSTALL= check.pytest', 'make VENV= INSTALL= check.coverage', 'make VENV= INSTALL= check.rstcheck', 'python -m pip install -U pip\npython -m pip install sphinx sphinx-rtd-theme\npython -m pip install .\n', '.github/scripts/docs.sh']"
"['poetry install --no-interaction', 'make lint', 'make types', 'poetry run python scripts/validate_version.py', 'poetry install --no-interaction\npoetry run pip install prompt_toolkit==${{ matrix.promttoolkit }}\n', 'make test', 'poetry run coveralls', 'pip3 install --upgrade coveralls\ncoveralls --service=github --finish\n', 'poetry install --no-interaction --with=docs', 'make docs', 'poetry build\npoetry publish -u __token__ -p ${{ secrets.PYPI_TOKEN }}\n']"
"['./scripts/check-changelog.sh\n', 'python -m pip install --upgrade pip\npip install -r requirements-testing.txt\n', 'flake8 . --append-config=.flake8.soft --count --show-source --statistics\n', 'flake8 . --count --show-source --statistics --exit-zero\n', 'python3 -m coverage run --source minigalaxy -m unittest discover -v tests && python3 -m coverage report -m\n', 'sudo apt-get update\nsudo apt-get install -y debhelper-compat dh-sequence-python3 python3-all python3-setuptools help2man devscripts gettext lsb-release xmlstarlet git\n', './scripts/create-release.sh\n', 'dpkg-buildpackage -us -uc\n', 'git config --global user.name \'Wouter Wijsman\'\ngit config --global user.email \'sharkwouter@users.noreply.github.com\'\ngit add pyproject.toml data/io.github.sharkwouter.Minigalaxy.metainfo.xml debian/changelog minigalaxy/version.py\ngit commit -m ""Add new release""\ngit push\n']"
"['echo ""The GITHUB_REF: $GITHUB_REF""\n#First check to see if the release is a tag\nif [[ $GITHUB_REF =~ refs/tags/* ]]; then\n  #Yes, this is a tag, so we need to test to make sure that the tag\n  #is in the correct format (like v1.10.20)\n  if [[ $GITHUB_REF =~ refs/tags/v[0-9]+.[0-9]+.[0-9]+ ]]; then\n    echo ""PASS: Tagged release with good format""\n    exit 0\n  else\n    echo ""FAIL: Tagged release with bad format""\n    exit 1\n  fi\nelse\necho ""PASS: Not a tagged release""\nexit 0\nfi                          \n', 'echo ""The GITHUB_REF: $GITHUB_REF""\n#First check to see if the release is a tag\nif [[ $GITHUB_REF =~ refs/tags/* ]]; then\n  #Yes, this is a tag, so we need to test to make sure that the tag\n  #is in the correct format (like v1.10.20)\n  if [[ $GITHUB_REF =~ refs/tags/v[0-9]+.[0-9]+.[0-9]+ ]]; then\n    echo ""PASS: Tagged release with good format""\n    exit 0\n  else\n    echo ""FAIL: Tagged release with bad format""\n    exit 1\n  fi\nelse\necho ""PASS: Not a tagged release""\nexit 0\nfi\n', 'sudo apt update -qq\nsudo apt install jq -qq\n', '#Get the virtualenv set up\nrm -rf venv\npython3 -m venv --clear venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install wheel\npython3 -m pip install -q -r requirements.txt\n', 'source venv/bin/activate\npython3 contentctl.py --path . --verbose validate\n', 'ls -lah\n\n#Enter the virtualenv and run the docgen\nsource venv/bin/activate\npython3 bin/doc_gen.py --path . --output docs -v\n\n#Now generate the spec docs\nnpm install -g @adobe/jsonschema2md\njsonschema2md -d spec -o docs/spec -f yaml -e spec.json -x -\n\n#Clean up extra properties on docs\nrm -rf docs/spec/*-*.md\n\necho ""****** BRANCH INFORMATION ******""\ngit branch\ngit branch --show-current\n', 'sudo apt update -qq\nsudo apt install jq -qq\n', '#Get the virtualenv set up\nrm -rf venv\npython3 -m venv --clear venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install wheel\npython3 -m pip install -q -r requirements.txt\n', 'source venv/bin/activate\npython3 contentctl.py --path . --verbose generate --product ESCU --output dist/escu\npython3 contentctl.py --path . --verbose generate --product SSA --output dist/ssa\n', '# clean up current lookups\nrm -rf dist/escu/lookups\nmkdir dist/escu/lookups\n#copy over lookups\ncd lookups\ncp -rv *.csv ../dist/escu/lookups\n', 'cd lookups\ncount=`ls -1 *.mlmodel 2>/dev/null | wc -l`\nif [ $count != 0 ]\nthen cp -rv *.mlmodel ../dist/escu/lookups\nelse echo ""No mlmodel files to copy""\nfi\n', 'if [ $(echo ${GITHUB_REF} | grep ""^refs/tags/*"") ]; then\n  #failed to find the refs/tags/ beginning, grab and set the tag\n  echo ""Release is TAGGED!""\n  echo ""::set-output name=tag::${GITHUB_REF#refs/tags/}""\nelse\n  #Not a tagged relese\n  echo ""Release is NOT TAGGED!""\n  echo ""::set-output name=tag::""\nfi\n', '# check if tag is set, get build number from the tag if set\nif [ -z ""${{ steps.vars.outputs.tag }}"" ]; then\n    CONTENT_VERSION=$(grep -oP ""(\\d+.\\d+.\\d+$)"" dist/escu/default/content-version.conf)\n    echo ""detected content version: $CONTENT_VERSION""\nelse\n    CONTENT_VERSION=$(echo ${{ steps.vars.outputs.tag }} | grep -oP ""\\d+.\\d+.\\d+"")\n    echo ""content version: $CONTENT_VERSION, set by tag: ${{ steps.vars.outputs.tag }}""\nfi\n# update build number and version for ESCU\nsed -i ""s/build = .*$/build = ${{ github.run_number }}/g"" dist/escu/default/app.conf\nsed -i ""s/^version = .*$/version = $CONTENT_VERSION/g"" dist/escu/default/app.conf\nsed -i ""s/\\""version\\"": .*$/\\""version\\"": \\""$CONTENT_VERSION\\""/g"" dist/escu/app.manifest\nsed -i ""s/version = .*$/version = $CONTENT_VERSION/g"" dist/escu/default/content-version.conf\ntar -czf content-pack-build-escu.tar.gz dist/escu/*\n\n# update build number and version for ssa\ntar -czf content-pack-build-ssa.tar.gz dist/ssa/*\n', 'sudo apt install virtualenv\n', 'curl -Ls https://download.splunk.com/misc/packaging-toolkit/splunk-packaging-toolkit-0.9.0.tar.gz -o splunk-packaging-toolkit-latest.tar.gz\nmkdir slim-latest\ntar -zxf splunk-packaging-toolkit-latest.tar.gz -C slim-latest --strip-components=1\n', 'cd slim-latest\nvirtualenv --python=/usr/bin/python2.7 --clear venv\nsource venv/bin/activate\npython2 -m pip install --upgrade pip\npython2 -m pip install wheel\npython2 -m pip install semantic_version\npython2 -m pip install .\n', 'source slim-latest/venv/bin/activate\ncd build\ntar -zxf content-pack-build-escu.tar.gz\ntar -zxf content-pack-build-ssa.tar.gz\n\nmv dist/escu      DA-ESS-ContentUpdate\nmv dist/ssa       SSA_Content\n\nslim package -o upload DA-ESS-ContentUpdate\n\ncp upload/DA-ESS-ContentUpdate-*.tar.gz DA-ESS-ContentUpdate-latest.tar.gz\nsha256sum DA-ESS-ContentUpdate-latest.tar.gz > checksum.txt\n\n#Do this copy so that we conform as much as possible, and have to make \n#as few changes as possible, once we start generating this as a real,\n#properly packaged app\ntar -zcf upload/SSA_Content-NO_SLIM.tar.gz SSA_Content  \ncp upload/SSA_Content-*.tar.gz SSA_Content-latest.tar.gz\nsha256sum SSA_Content-latest.tar.gz >> checksum.txt\n', 'sudo apt update -qq\nsudo apt install jq -qq\n', 'cd bin\n#Enclose in quotes in case there are any special characters in the username/password\n#Better not to pass these arguments on the command line, if possible\n./appinspect.sh ../ DA-ESS-ContentUpdate-latest.tar.gz ""$APPINSPECT_USERNAME"" ""$APPINSPECT_PASSWORD""\n', '#Always create this, regardless of whether success or failure above\ntar -cvzf report.tar.gz report/\n', 'sudo apt update -qq\nsudo apt install jq -qq\n', '#Get the virtualenv set up\nrm -rf venv\npython3 -m venv --clear venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install wheel\npython3 -m pip install -q -r requirements.txt\n', 'source venv/bin/activate\npython3 bin/reporting.py\n', 'aws s3 cp bin/reporting s3://security-content/reporting --recursive --exclude ""*"" --include ""*.svg""\n', 'mkdir latest-escu\ntar -zxf DA-ESS-ContentUpdate-latest.tar.gz -C latest-escu --strip-components=1\nmkdir latest-ssa\ntar -zxf SSA_Content-latest.tar.gz -C latest-ssa --strip-components=1\n', '#Get the virtualenv set up\nrm -rf venv\npython3 -m venv --clear venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install wheel\npython3 -m pip install -q -r requirements.txt\n', 'echo ""::set-output name=branch::${GITHUB_REF#refs/heads/}""\n', 'source venv/bin/activate\npython3 bin/doc_gen.py --path . --output docs -v\n', 'source venv/bin/activate\npython3 bin/pretty_yaml.py --path . -v\n', 'source venv/bin/activate\npython3 bin/generate-coverage-map.py --projects_path . --output docs/mitre-map\n', 'rm -rf dist\nmkdir dist\nmv latest-escu dist/escu\nmv latest-ssa dist/ssa\n# configure git to prep for commit\ngit config user.email ""research@splunk.com""\ngit config user.name ""research bot""\ngit config --global push.default simple\ngit add dist/*\ngit add docs/*\ngit add detections/*\ngit commit --allow-empty -m ""updating docs and package bits [ci skip]""\n# Push quietly to prevent showing the token in log\n#No need to provide any credentials\ngit push\n', 'echo ""::set-output name=tag::${GITHUB_REF#refs/*/}""', 'cp DA-ESS-ContentUpdate-latest.tar.gz DA-ESS-ContentUpdate-${{ steps.vars.outputs.tag }}.tar.gz\ncp SSA_Content-latest.tar.gz SSA_Content-${{ steps.vars.outputs.tag }}.tar.gz\n\n#No checksum on the reports\ncp report.tar.gz report-${{ steps.vars.outputs.tag }}.tar.gz\n\ncp checksum.txt checksum-${{ steps.vars.outputs.tag }}.txt\n', 'aws s3 cp DA-ESS-ContentUpdate-latest.tar.gz s3://attack-range-appbinaries/\n# make the file public since it is not by default\naws s3api put-object-acl --bucket attack-range-appbinaries --key DA-ESS-ContentUpdate-latest.tar.gz --acl public-read\n', '#Get the virtualenv set up\nrm -rf venv\npython3 -m venv --clear venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install wheel\npython3 -m pip install -q -r requirements.txt\n', 'source venv/bin/activate\npython3 bin/create_baseline_folder.py\n', 'source venv/bin/activate\npython3 contentctl.py --path . --verbose generate --product API --output dist/api\n', 'aws s3 rm s3://security-content --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp stories s3://security-content/stories --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp baselines s3://security-content/baselines --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp detections s3://security-content/detections --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp playbooks s3://security-content/playbooks --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp lookups s3://security-content/lookups --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp lookups s3://security-content/lookups --recursive --exclude ""*"" --include ""*.csv""\naws s3 cp lookups s3://security-content/lookups --recursive --exclude ""*"" --include ""*.mlmodel""\naws s3 cp macros s3://security-content/macros --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp deployments s3://security-content/deployments --recursive --exclude ""*"" --include ""*.yml""\naws s3 cp dist/api s3://security-content/json --recursive --exclude ""*"" --include ""*.json""\n', 'API_URL=\'https://content.splunkresearch.com/detections\'\nAPI_STATUS=$(curl -s -o /dev/null -w ""%{http_code}"" $API_URL)\necho ""Security Content API Status: $API_STATUS""\nif [ ""$API_STATUS"" != ""200"" ]; then\n  echo ""Error [Security Content API status: $API_STATUS]""\n  exit 1\nfi\n', 'echo ""The GITHUB_REF: $GITHUB_REF""\n#First check to see if the release is a tag\nif [[ $GITHUB_REF =~ refs/tags/* ]]; then\n  #Yes, this is a tag, so we need to test to make sure that the tag\n  #is in the correct format (like v1.10.20)\n  if [[ $GITHUB_REF =~ refs/tags/v[0-9]+.[0-9]+.[0-9]+ ]]; then\n    echo ""PASS: Tagged release with good format""\n    exit 0\n  else\n    echo ""FAIL: Tagged release with bad format""\n    exit 1\n  fi\nelse\necho ""PASS: Not a tagged release""\nexit 0\nfi                    \n', 'echo ""yes it ran""\n', 'echo ""::set-output name=branch::${GITHUB_REF#refs/heads/}""\n', 'cd bin/automated_detection_testing/ci/detection_testing_batch\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install wheel\npython3 -m pip install -r requirements.txt\n', 'cd bin/automated_detection_testing/ci/detection_testing_batch\nsource .venv/bin/activate\necho ""github.event.issue.pull_request    : [${{ github.event.issue.pull_request }}]""\necho ""github.event.pull_request.number   : [${{ github.event.pull_request.number }}]""\necho ""steps.vars.outputs.branch          : [${{ steps.vars.outputs.branch }}]""\necho ""github.event.pull_request.head.ref : [${{ github.event.pull_request.head.ref }}]""\necho ""github.event_name                  : [${{ github.event_name }}]""\n\n\nif [[ ${{ github.event_name }} == schedule ]]; then\n  # Note that scheduled actions ONLY run on the default branch, so it won\'t run on all other branches!\n  echo ""Running a nightly test on all detections OR a commit was made directly to develop""\n  python detection_testing_execution.py run --branch develop --mode all --mock --config_file test_config_github_actions.json  \nelif [[ ! -z ""${{  github.event.pull_request.head.ref }}"" &&  ! -z ""${{ github.event.pull_request.number }}"" ]]; then\n  echo ""Pull request from source branch [${{ github.event.pull_request.head.ref }}] for PR number [${{ github.event.issue.number }}]""\n  python detection_testing_execution.py run --branch ${{ github.event.pull_request.head.ref }} --pr_number ${{ github.event.pull_request.number }}  --mode changes --mock --config_file test_config_github_actions.json  \nelse\n  echo ""Push from branch [${{ steps.vars.outputs.branch }}]""\n  python detection_testing_execution.py run --branch  ${{ steps.vars.outputs.branch }}  --mode changes --mock --config_file test_config_github_actions.json\nfi\n\nmv *-test-run.json replicate_test.json\n', 'echo ""::set-output name=branch::${GITHUB_REF#refs/heads/}""\n', 'cd bin/automated_detection_testing/ci/detection_testing_batch\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install wheel\npython3 -m pip install -r requirements.txt\n', 'cd bin/automated_detection_testing/ci/detection_testing_batch\nsource .venv/bin/activate\n\npython3 detection_testing_execution.py run -c prior_config/${{ matrix.manifest_filename}} --splunkbase_username ${{ secrets.SPLUNKBASE_TESTING_USERNAME }} --splunkbase_password ${{ secrets.SPLUNKBASE_TESTING_KEY }} \n        \n', 'echo ""::set-output name=branch::${GITHUB_REF#refs/heads/}""\n', 'cd bin/automated_detection_testing/ci/detection_testing_batch\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install wheel\npython3 -m pip install -r requirements.txt\n', 'cd bin/automated_detection_testing/ci/detection_testing_batch\nsource .venv/bin/activate\npython summarize_json.py --files results_*/summary.json --output_filename summary_test_results.json\n', 'echo ""The GITHUB_REF: $GITHUB_REF""\n#First check to see if the release is a tag\nif [[ $GITHUB_REF =~ refs/tags/* ]]; then\n  #Yes, this is a tag, so we need to test to make sure that the tag\n  #is in the correct format (like v1.10.20)\n  if [[ $GITHUB_REF =~ refs/tags/v[0-9]+.[0-9]+.[0-9]+ ]]; then\n    echo ""PASS: Tagged release with good format""\n    exit 0\n  else\n    echo ""FAIL: Tagged release with bad format""\n    exit 1\n  fi\nelse\necho ""PASS: Not a tagged release""\nexit 0\nfi                    \n']"
[]
"['poetry install --no-dev ${{ matrix.extras }} && poetry run pip install pytest', 'poetry run pytest']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip poetry\npoetry config installer.modern-installation false\npoetry install\n', 'poetry run make docs\n', ""echo 'uniswap-python.com' > docs/_build/html/CNAME\n"", 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'python -m pip install --upgrade pip poetry\npoetry config virtualenvs.in-project true\npoetry config installer.modern-installation false\n', 'timeout 10s poetry run pip --version || rm -rf .venv', 'poetry install\nnpm install -g ganache@7.5.0\n', 'make test\n', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'python -m pip install --upgrade pip poetry\npoetry config virtualenvs.in-project true\npoetry config installer.modern-installation false\n', 'timeout 10s poetry run pip --version || rm -rf .venv', 'python -m pip install --upgrade pip poetry\npoetry install\n', 'make typecheck\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['chmod +x ""${{ github.workspace }}/update_readme.sh""\n""${{ github.workspace }}/update_readme.sh""\n']"
"['python -m pip install --upgrade pip\npython -m pip install pyyaml\n', 'python build.py --version ${{ matrix.py_version }}', 'pip install emoji\ncat << EOF >> cl.md\n${{ steps.draft.outputs.body }}\nEOF\nTAG=""${{ steps.draft.outputs.tag_name }}""\necho ""VERSION=${TAG#v}"" >> $GITHUB_ENV\necho ""YAML_CHANGELOG<<EOF"" >> $GITHUB_ENV\ncat cl.md | python .github/tools/reformat_changelog.py --no-emoji >> $GITHUB_ENV\necho ""EOF"" >> $GITHUB_ENV\necho ""CHANGELOG<<EOF"" >> $GITHUB_ENV\ncat cl.md | python .github/tools/reformat_changelog.py --emoji --format=\'+ #{issue} by @{username}\' >> $GITHUB_ENV\necho ""EOF"" >> $GITHUB_ENV\nrm cl.md\n', 'yq eval \'.version = env(VERSION) | .changelog = strenv(YAML_CHANGELOG) | .changelog style=""literal""\' -i release.yaml\n', 'git config user.name ""jellyfin-bot""\ngit config user.email ""team@jellyfin.org""\n\ngit checkout -b prepare-${{ env.VERSION }}\ngit commit -am ""bump version to ${{ env.VERSION }}""\n\nif [[ -z ""$(git ls-remote --heads origin prepare-${{ env.VERSION }})"" ]]; then\n  git push origin prepare-${{ env.VERSION }}\nelse\n  git push -f origin prepare-${{ env.VERSION }}\nfi\n', 'python -m pip install --upgrade pip\npython -m pip install pyyaml\n', 'python build.py --version ${{ matrix.py_version }}', 'python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --output-file=flake8.output\ncat flake8.output\n', 'coverage run\ncoverage xml\ncoverage report\n']"
"['poetry install', 'poetry run python -m tests minimal']"
"['rustc --version > .rustc-version\n', 'cargo build --release\n', 'export APPLE_HOST_SDK_PATH=/Applications/Xcode_12.5.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.3.sdk\n\nif [ ""${{ matrix.build.target_triple }}"" = ""aarch64-apple-darwin"" ]; then\n  export APPLE_SDK_PATH=/Applications/Xcode_12.5.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.3.sdk\nelif [ ""${{ matrix.build.target_triple }}"" = ""aarch64-apple-ios"" ]; then\n  export APPLE_SDK_PATH=/Applications/Xcode_12.5.1.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS14.5.sdk\nelif [ ""${{ matrix.build.target_triple }}"" = ""x86_64-apple-darwin"" ]; then\n  export APPLE_SDK_PATH=/Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk\nelif [ ""${{ matrix.build.target_triple }}"" = ""x86_64-apple-ios"" ]; then\n  export APPLE_SDK_PATH=/Applications/Xcode_12.4.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator14.4.sdk\nelse\n  echo ""unhandled target triple: ${{ matrix.build.target_triple }}""\n  exit 1\nfi\n\n./build-macos.py --target-triple ${{ matrix.build.target_triple }} --python ${{ matrix.build.py }} --optimizations ${{ matrix.build.optimizations }}\n', 'chmod +x build/pythonbuild\n\nif [ ""${{matrix.build.target_triple }}"" = ""x86_64-apple-darwin"" ]; then\n  EXTRA_ARGS=""--run""\nfi\n\nbuild/pythonbuild validate-distribution --macos-sdks-path macosx-sdks ${EXTRA_ARGS} dist/*.tar.zst\n', 'sudo apt update\nsudo apt install -y --no-install-recommends libssl-dev pkg-config\n', 'rustc --version > .rustc-version\n', 'cargo build --release\n', './build-linux.py --make-target toolchain-image-${{ matrix.image }}\nzstd -v -T0 -6 --rm build/image-*.tar\n', 'for f in build/image-*.tar.zst; do\n  echo ""decompressing $f""\n  zstd -d --rm ${f}\ndone\n\nfor f in build/image-*.tar; do\n  echo ""loading $f""\n  docker load --input $f\ndone\n', '# Do empty target so all generated files are touched.\n./build-linux.py --make-target empty\n\n# Touch mtimes of all images so they are newer than autogenerated files above.\ntouch build/image-*\n\n./build-linux.py --target-triple ${{ matrix.build.target_triple }} --python ${{ matrix.build.py }} --optimizations ${{ matrix.build.optimizations }}\n', 'chmod +x build/pythonbuild\n\nif [ -n ""${{matrix.build.run}}"" ]; then\n  EXTRA_ARGS=""--run""\nfi\n\nbuild/pythonbuild validate-distribution ${EXTRA_ARGS} dist/*.tar.zst\n', 'rustc --version > .rustc-version\n', 'cargo build --release\n', 'py.exe -3.9 build-windows.py --help\n', 'call ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Auxiliary\\Build\\${{ matrix.vcvars }}""\npy.exe -3.9 build-windows.py --python ${{ matrix.py }} --sh c:\\cygwin\\bin\\sh.exe --profile ${{ matrix.profile }}\n', '$Dists = Resolve-Path -Path ""dist/*.tar.zst"" -Relative\n.\\pythonbuild.exe validate-distribution --run $Dists\n']"
"['sudo rm -rf /usr/share/dotnet\nsudo rm -rf /usr/local/lib/android\nsudo rm -rf /opt/ghc\n', 'make version', 'echo ""SHORT_SHA=${GITHUB_SHA::7}"" >> $GITHUB_ENV', 'gh pr merge --auto --squash ""$PR_URL""', 'npm install --global @devcontainers/cli', 'devcontainer build --workspace-folder .', 'npm install', 'npm run lint', 'npm install', 'npm run test', 'pip install pip\npip install -r requirements-dev.txt\n', 'python3 -m black frigate --check\n', 'npm install', 'npm run build', 'make', 'docker run --rm --entrypoint=python3 frigate:latest -u -m mypy --config-file frigate/mypy.ini frigate', 'docker run --rm --entrypoint=python3 frigate:latest -u -m unittest', ""echo ${{ join(steps.stale.outputs.*, ',') }}""]"
"['python -m pip install build --user', './build.sh', 'git fetch origin master --depth 1\ngit symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master\n', './local/tests/ci_tests.bash']"
"['exit 1', 'pip install -U twine\ntwine check dist/*\n', 'pip install -U pip\npip install --find-link=dist/ fate_crypto\n', 'twine upload --repository testpypi dist/* --verbose\n', 'twine upload dist/* --verbose\n', 'sudo apt-get install -y gcc g++ make openssl supervisor libgmp-dev  libmpfr-dev libmpc-dev libaio1 libaio-dev numactl autoconf automake libtool libffi-dev libssl-dev liblz4-1 liblz4-dev liblz4-tool zlib1g-dev\npython -m pip install --upgrade pip\nif [ -f python/requirements.txt ];\nthen python -m pip install -r python/requirements.txt;\nfi\n# Set the `CODEQL-PYTHON` environment variable to the Python executable\n# that includes the dependencies\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'cd python/fate_client\nrm -f setup.py\n# clear README.rst\necho ""# fate client"" > README.rst\n# bump fate client version\npoetry version ${{github.event.inputs.target}}\n# build package, saved in dist/\npoetry build\n', 'ls -lh python/fate_client/dist/ python/fate_client/dist/', 'pip install -U twine\ntwine check python/fate_client/dist/*\n', 'pip install -U pip\npip install python/fate_client/dist/fate_client-${{github.event.inputs.target}}.tar.gz\n', 'twine upload --repository testpypi python/fate_client/dist/* --verbose\n', 'twine upload python/fate_client/dist/* --verbose\n']"
""
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'python -m twine upload dist/*\n']"
"['git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'pip install -U pip setuptools wheel', ""pip install -U --pre bokeh pandas 'numpy<1.24' && pip install -U .[dev]"", 'pip install -e .[doc]', 'pip install -U .[test]', 'ruff backtesting', 'mypy backtesting', 'time catchsegv coverage run -m backtesting.test', 'bash <(curl -s https://codecov.io/bash)', 'time catchsegv python -m backtesting.test', 'time catchsegv doc/build.sh', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'pip install -U pip setuptools wheel\npip install -U -e .[doc,test]\n', 'time catchsegv doc/build.sh', '.github/deploy-gh-pages.sh']"
"['sudo apt-get install xclip\n# It fails due to missing \'bdist_wheel\' for some reasons. Explicitly installs it.\n# See also https://github.com/palantir/python-language-server/issues/524#issuecomment-477068693.\npip install wheel setuptools\npython setup.py bdist_wheel\n# Currently PIP with Python 3.5 removes Black in the requirements-dev.txt file\n# as Black only works with Python 3.6+. This is hacky but we will drop\n# Python 3.5 soon so it\'s fine.\nif [[ ""$PYTHON_VERSION"" < ""3.6"" ]]; then sed -i \'/black/d\' requirements-dev.txt; fi\n# sphinx-plotly-directive supports Python 3.6+\nif [[ ""$PYTHON_VERSION"" < ""3.6"" ]]; then sed -i \'/sphinx-plotly-directive/d\' requirements-dev.txt; fi\n# Disable mypy check for PySpark 3.1\nif [[ ""$SPARK_VERSION"" > ""3.1"" ]]; then sed -i \'/mypy/d\' requirements-dev.txt; fi\npip install -r requirements-dev.txt\npip install pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION pyspark==$SPARK_VERSION numpy==$NUMPY_VERSION\n# matplotlib dropped Python 3.5 support from 3.1.x; however, 3.0.3 only supports sphinx 2.x.\n# It forces the sphinx version to 2.x.\nif [[ ""$PYTHON_VERSION"" < ""3.6"" ]]; then pip install ""sphinx<3.0.0""; fi\npip list\n', '# lint-python uses python3 as default. Seems python3 could pick a different Python.\n# Looks a side effect from manual Python 3.5 installation.\nPYTHON_EXECUTABLE=python ./dev/lint-python\n./dev/pytest\n', '# See also https://github.com/conda/conda/issues/7980\nsource ""$CONDA_PREFIX/etc/profile.d/conda.sh""\nconda update -q conda\nconda create -c conda-forge -q -n test-environment python=$PYTHON_VERSION\nconda activate test-environment\nconda install -c conda-forge --yes codecov\nconda config --env --add pinned_packages python=$PYTHON_VERSION\nconda config --env --add pinned_packages pandas==$PANDAS_VERSION\nconda config --env --add pinned_packages pyarrow==$PYARROW_VERSION\nconda config --env --add pinned_packages numpy==$NUMPY_VERSION\nconda config --env --add pinned_packages pyspark==$SPARK_VERSION\nif [[ ""$SPARK_VERSION"" < ""3.0"" ]]; then\n  pip install pyspark==$SPARK_VERSION\nelse\n  conda install -c conda-forge --yes pyspark==$SPARK_VERSION\nfi\nconda install -c conda-forge --yes pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION numpy==$NUMPY_VERSION\nsed -i -e ""/pandas/d"" -e ""/pyarrow/d"" -e ""/numpy>=/d"" requirements-dev.txt\n# Disable mypy check for PySpark 3.1\nif [[ ""$SPARK_VERSION"" > ""3.1"" ]]; then sed -i \'/mypy/d\' requirements-dev.txt; fi\n# sphinx-plotly-directive is not available on Conda.\nsed -i \'/sphinx-plotly-directive/d\' requirements-dev.txt\nconda install -c conda-forge --yes --file requirements-dev.txt\npip install sphinx-plotly-directive  # pip-only dependency\nconda list\n', '# See also https://github.com/conda/conda/issues/7980\nsource ""$CONDA_PREFIX/etc/profile.d/conda.sh""\nconda activate test-environment\n./dev/lint-python\n./dev/pytest\n']"
"['sudo xargs apt-get update\nsudo xargs apt-get -y install < requirements.unix\n', 'python -m pip install --upgrade pip\npip install -U -r requirements.txt\npip install -U pytest coveralls\npip install -U flake8\npip install -U setuptools wheel\n', 'bazel build tensorflow_graphics/... --define=BASEDIR=$(pwd) --sandbox_writable_path=$(pwd)\nbazel clean --expunge\n', 'coverage run --source tensorflow_graphics -m py.test\ncoveralls --service=github\n', 'flake8 --config=.flake8 tensorflow_graphics/\n', 'python setup.py sdist bdist_wheel\npip install dist/*.whl\n', ""cd $(mktemp -d) && python -c 'import tensorflow_graphics as tfg'\n"", 'sudo xargs apt-get update\nsudo xargs apt-get -y install < requirements.unix\n', 'python -m pip install --upgrade pip\npip install -U -r requirements.txt\npip install -U pytest\npip install -U setuptools wheel\npip install -U twine\n', 'bazel build tensorflow_graphics/... --define=BASEDIR=$(pwd) --sandbox_writable_path=$(pwd)\nbazel clean --expunge\n', 'pytest tensorflow_graphics\n', 'python setup.py sdist bdist_wheel\npip install dist/*.whl\n', ""cd $(mktemp -d) && python -c 'import tensorflow_graphics as tfg'\n"", 'twine upload dist/*\n', 'sudo xargs apt-get update\nsudo xargs apt-get -y install < requirements.unix\n', 'python -m pip install --upgrade pip\npip install -U -r requirements.txt\npip install -U pytest\npip install -U setuptools wheel\npip install -U twine\n', 'bazel build tensorflow_graphics/... --define=BASEDIR=$(pwd) --sandbox_writable_path=$(pwd)\nbazel clean --expunge\n', 'pytest tensorflow_graphics\n', 'python setup.py sdist bdist_wheel --nightly\npip install dist/*.whl\n', ""cd $(mktemp -d) && python -c 'import tensorflow_graphics as tfg'\n"", 'twine upload dist/*\n', 'sudo xargs apt-get update\nsudo xargs apt-get -y install < requirements.unix\n', 'python -m pip install --upgrade pip\npip install -U -r requirements.txt\npip install -U pytest\npip install -U setuptools wheel\npip install -U twine\n', 'bazel build tensorflow_graphics/... --define=BASEDIR=$(pwd) --sandbox_writable_path=$(pwd)\nbazel clean --expunge\n', 'pytest tensorflow_graphics\n', 'python setup.py sdist bdist_wheel --nightly\npip install dist/*.whl\n', ""cd $(mktemp -d) && python -c 'import tensorflow_graphics as tfg'\n"", 'twine upload --repository testpypi dist/*\n']"
"['curl -sSL ""https://install.python-poetry.org"" | python\n\n# Adding `poetry` to `$PATH`:\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry config virtualenvs.in-project true\npoetry install\n\npoetry run pip install -U pip\n', 'poetry run flake8 .\n\n# In order to make `exclude` option work, we need to separate the checks\n# of returns and its tests into two separated commands\npoetry run mypy --enable-error-code=unused-awaitable returns\npoetry run mypy tests\n\n# Different python versions are covered differently:\npoetry run pytest returns docs/pages tests\n\npoetry run doc8 -q docs\npoetry run codespell returns tests docs typesafety README.md CONTRIBUTING.md CHANGELOG.md\n\npoetry run poetry check\npoetry run pip check\npoetry run safety check --full-report\npoetry run python -m slotscheck returns --verbose\n\n# We do this to speed up the build:\npoetry run pytest typesafety -p no:cov -o addopts="""" --mypy-ini-file=setup.cfg\n']"
"['docker build . --file Dockerfile -t m2cgen-docker --build-arg python=${{ matrix.python }}', 'if [[ ""${{ matrix.lang }}"" == ""API"" ]]; then\n  export TEST=""API"";\nelse\n  export TEST=""E2E"";\nfi\ndocker run \\\n  --ulimit stack=-1 \\\n  -v ""$GITHUB_WORKSPACE"":""/m2cgen"" \\\n  -e TEST \\\n  -e LANG=""${{ matrix.lang }}"" \\\n  -e GITHUB_ACTIONS \\\n  -e GITHUB_WORKFLOW \\\n  -e GITHUB_RUN_ID \\\n  -e GITHUB_SHA \\\n  -e GITHUB_REF \\\n  -e GITHUB_HEAD_REF \\\n  -e GITHUB_REPOSITORY \\\n  -e GITHUB_SERVER_URL \\\n  -e GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }} \\\n  m2cgen-docker \\\n  bash /m2cgen/.ci/test.sh\n', 'echo ""ðŸŽ‰""', 'docker build . --file Dockerfile -t m2cgen-docker', 'docker run \\\n  -v ""$GITHUB_WORKSPACE"":""/m2cgen"" \\\n  -e RELEASE=""true"" \\\n  m2cgen-docker \\\n  bash /m2cgen/.ci/test.sh\n']"
"['poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'poetry install --no-interaction', 'poetry run alembic upgrade head\npoetry run uvicorn app.main:app &\nAPIURL=http://localhost:8000/api ./postman/run-api-tests.sh\npoetry run alembic downgrade base\n', 'docker build -t $IMAGE_NAME:latest .\necho $DOCKER_PASSWORD | docker login -u $DOCKER_USER --password-stdin\ndocker push $IMAGE_NAME:latest\n', 'poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'poetry install --no-interaction', 'poetry run ./scripts/lint', 'poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'poetry install --no-interaction', 'poetry run alembic upgrade head\npoetry run ./scripts/test\n']"
"['python3 -m venv venv\n. venv/bin/activate\npip install -U pip\npip install poetry\npoetry install\npython -c ""import sys; print(sys.version)""\npip list\n', '. venv/bin/activate\npoetry build\n', 'python3 -m venv fresh_env\n. fresh_env/bin/activate\npip install dist/*.whl\n\niredis -h\niredis help GET\n', '. venv/bin/activate\npoetry publish --username __token__ --password ${PASSWORD}\n', 'python3 -m venv venv\n. venv/bin/activate\npip install pip\npip install poetry\npoetry install\npython -c ""import sys; print(sys.version)""\npip list\n', '. venv/bin/activate\npoetry build\n', 'python3 -m venv fresh_env\n. fresh_env/bin/activate\npip install dist/*.whl\n\niredis -h\niredis help GET\n', ""# pyoxidizer doesn't know the wheel path, and it doesn't support passing env vars\nexport WHEEL_PATH=`ls ./dist/iredis*.whl`\nenvsubst '$WHEEL_PATH' < pyoxidizer.template.bzl > pyoxidizer.bzl\npip install pyoxidizer\npyoxidizer build --release install\ncd ./build/x86*/release/install\ntar -zcf ../../../iredis.tar.gz lib/ iredis\ncd -\n"", './build/x86*/release/install/iredis -h\n./build/x86*/release/install/iredis help GET\n', 'python3 -m venv venv\n. venv/bin/activate\npip install pip\npip install poetry\npoetry install\npython -c ""import sys; print(sys.version)""\npip list\n', '. venv/bin/activate\npoetry build\n', 'python3 -m venv fresh_env\n. fresh_env/bin/activate\npip install dist/*.whl\n\niredis -h\niredis help GET\n', ""# pyoxidizer doesn't know the wheel path, and it doesn't support passing env vars\nexport WHEEL_PATH=`ls ./dist/iredis*.whl`\nenvsubst '$WHEEL_PATH' < pyoxidizer.template.bzl > pyoxidizer.bzl\npip install pyoxidizer\npyoxidizer build --release install\ncd ./build/x86*/release/install\ntar -zcf ../../../iredis.tar.gz lib/ iredis\ncd -\n"", './build/x86*/release/install/iredis -h\n./build/x86*/release/install/iredis help GET\n', 'python3 -m venv venv\n. venv/bin/activate\npip install -U pip setuptools\npip install poetry\npoetry install\npython -c ""import sys; print(sys.version)""\npip list\n', '. venv/bin/activate\npytest\n', 'python3 -m venv venv\n. venv/bin/activate\npip install -U pip flake8 black\n', '. venv/bin/activate\nflake8 .\n', '. venv/bin/activate\nblack --check .\n']"
"['pip install ogb matplotlib\n', 'pip install .\n', ""sed -i -e 's/epochs = /epochs = 1 #/g' examples/node_prediction/*.py\nsed -i -e 's/epochs = /epochs = 1 #/g' examples/graph_prediction/*.py\nsed -i -e 's/epochs = /epochs = 1 #/g' examples/other/*.py\n"", 'cd examples/node_prediction/\nfor f in *.py; do\n  echo ""##### $f #####""\n  python $f\ndone\ncd ..\ncd graph_prediction/\nfor f in *.py; do\n  echo ""##### $f #####""\n  python $f\ndone\ncd ..\ncd other/\nfor f in *.py; do\n  echo ""##### $f #####""\n  python $f\ndone\ncd ..\n', 'pip install flake8\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'pip install black\nblack --diff --check .\n', 'pip install isort\nisort -rc --diff --check-only .\n', 'python -m pip install --upgrade pip\npython -m pip install pytest coverage codecov\n', 'python -m pip install .\n', 'python -m coverage run --source=. -m pytest\n']"
"['python3.7 -m pip install ""docker>=4.4.4,<5.0.0"" ""pytest>=6.2.4,<7.0.0""', 'bash scripts/build-push.sh', 'python3.7 -m pip install ""docker>=4.4.4,<5.0.0"" ""pytest>=6.2.4,<7.0.0""', 'bash scripts/test.sh']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m tox -q -e flake8,py', 'python -m pylint resources/lib/ tests/', 'sudo apt-get update\nsudo apt-get install libxml2-utils\nmake build release=1\n', 'echo ::set-output name=zip-filename::$(cd ..;ls plugin.video.netflix*.zip | head -1)\n', 'description=$(sed \'/v[0-9\\.]*\\s([0-9-]*)/d;/^$/,$d\' changelog.txt)\necho $description\ndescription=""${description//\'%\'/\'%25\'}""\ndescription=""${description//$\'\\n\'/\'%0A\'}""\ndescription=""${description//$\'\\r\'/\'%0D\'}""\necho ::set-output name=body::$description\n', 'sudo apt-get install gettext\n', 'make check-translations']"
"['echo python_version: ${{ matrix.python_version }}\necho arch: ${{ matrix.arch }}\necho build_platform: ${{ matrix.build_platform }}\n', ""if [ ${{ github.ref }} == refs/heads/container_builds-default ]\nthen\n  echo AWS_ENV=Klayers-defaultp38 >> $GITHUB_ENV\nelif [ ${{ github.ref }} == refs/heads/container_builds-dev ]\nthen\n  echo AWS_ENV=Klayers-devp38 >> $GITHUB_ENV\nelif [ ${{ github.ref }} == refs/heads/container_builds-prod ]\nthen\n  echo AWS_ENV=Klayers-prodp38 >> $GITHUB_ENV            \nelse\n  exit 1\nfi\n\nAPP_NAME=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r '.app_name')\necho APP_NAME=$APP_NAME >> $GITHUB_ENV\n"", ""GITHUB_ROLE_ARN=$(cat ./.github/workflows/role_arns.json | jq -r --arg arg $AWS_ENV '.github_role_arn | .[$arg]')\nAWS_REGION=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r --arg arg $AWS_ENV '.aws_region | .[$arg]')\n\necho AWS_ROLE_ARN=$GITHUB_ROLE_ARN >> $GITHUB_ENV\necho AWS_DEFAULT_REGION=$AWS_REGION >> $GITHUB_ENV\n"", ""PARAM_PREFIX=build/${{ matrix.python_version }}/${{ matrix.arch }}\nREPO_PARAM=/kl/$AWS_ENV/$PARAM_PREFIX/repo\nREPO_URL=$(aws ssm get-parameter --name $REPO_PARAM | jq -r '.Parameter.Value')\nREPO_NAME=$(echo $REPO_URL | cut -d'/' -f2)\nBUILD_DIR=pipeline/container_images/build_images/${{ matrix.python_version }}_${{ matrix.arch }}\n\necho REPO_NAME=$REPO_NAME >> $GITHUB_ENV\necho REPO_URL=$REPO_URL >> $GITHUB_ENV\necho PARAM_PREFIX=$PARAM_PREFIX >> $GITHUB_ENV\necho BUILD_DIR=$BUILD_DIR >> $GITHUB_ENV\n\ncp ./pipeline/container_images/build_images/common/build.py ./$BUILD_DIR\n"", ""DIGEST=$(aws ecr describe-images --repository-name $REPO_NAME --image-ids imageTag=latest | jq -r '.imageDetails[0].imageDigest')\naws ssm put-parameter --name /kl/$AWS_ENV/$PARAM_PREFIX/digest --value $DIGEST --overwrite --type String | jq '.'"", 'echo A comment on PR $NUMBER from $USER at $HTML_URL, content $CONTENT\n', ""AWS_ENV=Klayers-defaultp38 >> $GITHUB_ENV\nGITHUB_ROLE_ARN=$(cat ./.github/workflows/role_arns.json | jq -r --arg arg $AWS_ENV '.github_role_arn | .[$arg]')\nAWS_REGION=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r --arg arg $AWS_ENV '.aws_region | .[$arg]')\n\necho AWS_ROLE_ARN=$GITHUB_ROLE_ARN >> $GITHUB_ENV\necho AWS_DEFAULT_REGION=$AWS_REGION >> $GITHUB_ENV\n"", ""if [ ${{ github.ref }} == refs/heads/klayers-default ]\nthen\n  echo AWS_ENV=Klayers-defaultp38 >> $GITHUB_ENV\nelif [ ${{ github.ref }} == refs/heads/klayers-dev ]\nthen\n  echo AWS_ENV=Klayers-devp38 >> $GITHUB_ENV\nelif [ ${{ github.ref }} == refs/heads/master ]\nthen\n  echo AWS_ENV=Klayers-prodp38 >> $GITHUB_ENV            \nelse\n  exit 1\nfi\n\nAPP_NAME=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r '.app_name')\necho APP_NAME=$APP_NAME >> $GITHUB_ENV\n"", ""GITHUB_ROLE_ARN=$(cat ./.github/workflows/role_arns.json | jq -r --arg arg $AWS_ENV '.github_role_arn | .[$arg]')\nAWS_REGION=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r --arg arg $AWS_ENV '.aws_region | .[$arg]')\n\necho AWS_ROLE_ARN=$GITHUB_ROLE_ARN >> $GITHUB_ENV\necho AWS_DEFAULT_REGION=$AWS_REGION >> $GITHUB_ENV\n"", ""BUCKET_PARAMETER_SUFFIX=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r '.s3bucket_config_parameter_name_suffix')\necho CONFIG_BUCKET_PARAMETER_NAME=/$APP_NAME/$AWS_ENV/$BUCKET_PARAMETER_SUFFIX >> $GITHUB_ENV\n"", 'export CONFIG_BUCKET_NAME=$(aws ssm get-parameter --name $CONFIG_BUCKET_PARAMETER_NAME | jq -r "".Parameter.Value"")\necho CONFIG_BUCKET_NAME=$CONFIG_BUCKET_NAME >> $GITHUB_ENV\n', ""CONFIG_DIR=$(cat ./pipeline/Terraform/terraform.tfvars.json | jq -r --arg arg $AWS_ENV '.config_dir | .[$arg]')\necho $CONFIG_DIR\naws s3 cp $CONFIG_DIR s3://$CONFIG_BUCKET_NAME --recursive\n"", 'STATE_MACHINE_ARN=$(aws ssm get-parameter --name ""/gh-push/$AWS_ENV/PipelineArn"" | jq -r "".Parameter.Value"")\naws stepfunctions start-execution --state-machine-arn $STATE_MACHINE_ARN --input file://${{ github.event_path }}']"
"['python -m pip install black\npython --version\npython -m black --version\necho $PWD ; ls\npython -m black ./exps -l 88 --check --diff --verbose\npython -m black ./tests -l 88 --check --diff --verbose\npython -m black ./xautodl/x* -l 88 --check --diff --verbose\npython -m black ./xautodl/spaces -l 88 --check --diff --verbose\npython -m black ./xautodl/trade_models -l 88 --check --diff --verbose\npython -m black ./xautodl/procedures -l 88 --check --diff --verbose\npython -m black ./xautodl/config_utils -l 88 --check --diff --verbose\npython -m black ./xautodl/log_utils -l 88 --check --diff --verbose\n', 'pip install .\n', 'python -m pip install pytest\npython -m pip install torch torchvision\npython -m pip install parameterized\necho $PWD\necho ""Show what we have here:""\nls\npython --version\npython -m pytest ./tests/test_import.py -s\npython -m pytest ./tests/test_basic_space.py -s\n', 'python -m pytest ./tests/test_math*.py -s\n', 'python -m pytest ./tests/test_synthetic*.py -s\n', 'pip install .\n', 'python -m pip install pytest\npython -m pip install torch torchvision\npython -m pip install parameterized\necho $PWD\necho ""Show what we have here:""\nls\npython --version\npython -m pytest ./tests/test_misc* -s\n', 'pip install .\n', 'python -m pip install pytest\npython -m pip install parameterized\npython -m pip install torch torchvision\npython -m pytest ./tests/test_super_*.py\n', 'python -m pytest ./tests/test_tas.py\n']"
"['$ProgressPreference = \'SilentlyContinue\'\nInvoke-WebRequest -Uri ""https://boostorg.jfrog.io/artifactory/main/release/1.77.0/source/boost_1_77_0.zip"" -UseBasicParsing -OutFile $HOME/boost.zip\nExpand-Archive $HOME/boost.zip -DestinationPath $HOME/boost\n', '.\\tools\\install_windows_cuda.ps1\n', 'python -m pip install build --user\npython -m pip install --upgrade pip twine wheel\npython -m pip install pytest setuptools\n', '$Env:CUMM_CUDA_ARCH_LIST = ""all""\n$Env:SPCONV_DISABLE_JIT = ""1""\npip install pccm pybind11\n# download boost header only\n$Env:BOOST_ROOT = ""$HOME/boost/boost_1_77_0""\n# ls ""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v${{ matrix.cuda-version }}\\include\\thrust""\npython -m build --wheel --outdir dist/ .\n', '$Env:TWINE_USERNAME = ""__token__""\n$Env:TWINE_PASSWORD = ""${{ secrets.pypi_password }}""\ntwine upload dist/*\n', 'python -m pip install build --user\npython -m pip install --upgrade pip twine wheel\npython -m pip install pytest setuptools\nmkdir -p third_party\nwget https://boostorg.jfrog.io/artifactory/main/release/1.77.0/source/$BOOST_VERSION.zip -O third_party/boost.zip\nunzip third_party/boost.zip -d third_party/boost\n', 'chmod +x tools/build-wheels.sh\ndocker run --rm -e PLAT=$PLAT -e CUMM_CUDA_VERSION=${{ matrix.cuda-version }} \\\n -e SPCONV_PYTHON_LIST=${{env.PYTHON_VERSION}} \\\n -e BOOST_ROOT=/io/third_party/boost/$BOOST_VERSION \\\n -v `pwd`:/io $DOCKER_IMAGE bash -c ""source /etc/bashrc && /io/tools/build-wheels.sh""\n', 'chmod +x tools/build-wheels.sh\ndocker run --rm -e PLAT=$PLAT -e CUMM_CUDA_VERSION=${{ matrix.cuda-version }} \\\n  -e SPCONV_PYTHON_LIST=${{env.PYTHON_VERSION}} \\\n  -e BOOST_ROOT=/io/third_party/boost/$BOOST_VERSION \\\n  -v `pwd`:/io $DOCKER_IMAGE bash -c ""source /etc/bashrc && /io/tools/build-wheels.sh""\n']"
"['pip install -U -r requirements.txt\npip install setuptools wheel twine\n', 'scripts/test', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install -U -r requirements.txt\n', 'scripts/test\n', 'scripts/lint\n', 'codecov']"
['python3 -m pip install --upgrade build\npython3 -m build\n']
"['poetry config experimental.new-installer false\npoetry config virtualenvs.in-project true\npoetry install --no-interaction\n', 'poetry run coverage run -m ward\npoetry run coverage xml -i\n', 'poetry publish --build --username __token__ --password ${{ secrets.pypi_api_token }}\n']"
"['pip install -e .[full,tests]', 'python -m unittest discover -v -f -s ./tests/test_unit', 'pip install -U setuptools', 'python setup.py sdist']"
"['echo ""TMPDIR=$env:USERPROFILE\\AppData\\Local\\Temp"" >> $env:GITHUB_ENV\necho ""TEMP=$env:USERPROFILE\\AppData\\Local\\Temp"" >> $env:GITHUB_ENV\necho ""TMP=$env:USERPROFILE\\AppData\\Local\\Temp"" >> $env:GITHUB_ENV\n', 'python -m pip install flake8 pytest parameterized\npython -m pip install -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest -v\n']"
"['python -m pip install --upgrade pip setuptools wheel\npip install flake8 pytest black\n', 'python -m pip install -e "".[base]""\n', 'python tests/scripts/check_pip.py missing cvxpy\npython tests/scripts/check_pip.py contains scikit-learn\npython tests/scripts/import_all.py\n', 'python -m pip install -e "".[cvxpy]""\n', 'python tests/scripts/check_pip.py contains cvxpy scikit-learn\npython tests/scripts/import_all.py\n', 'python -m pip install -e "".[all]""\n', 'python tests/scripts/check_pip.py contains cvxpy scikit-learn\n', 'sudo apt-get update && sudo apt-get install pandoc\npython -m pip install -e "".[docs]""\nsphinx-build doc docs\n', 'sudo apt-get update && sudo apt-get install pandoc\npython -m pip install --upgrade pip\npip install -e "".[docs]""\n', 'sphinx-build doc docs', 'python -m pip install --upgrade pip\npython -m pip install wheel\npip install ${{ matrix.pre-release-dependencies }} scikit-lego\npip freeze\n', 'pip install -e "".[test]""\npytest\n', 'python -m pip install --upgrade pip\npip install -e "".[test]""\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install -e "".[test]""\n', 'pytest -n 2 tests\n', 'python -m pip install --upgrade pip\npip install -e "".[test]""\n', 'pytest -n 2 tests\n']"
"['git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'pip install -U pip setuptools wheel\npip install -U .\n', 'pip install flake8 coverage mypy types-Markdown\nsudo apt update && sudo apt-get install \\\n  texlive-xetex lmodern texlive-fonts-recommended  # test_pdf_pandoc\nwget -O/tmp/pandoc.deb https://github.com/jgm/pandoc/releases/download/2.10/pandoc-2.10-1-amd64.deb && sudo dpkg -i /tmp/pandoc.deb\n', 'pip install -e .', ""find -name '*.md' | xargs .github/lint-markdown.sh\nflake8\nmypy -p pdoc\ntime coverage run -m unittest -v pdoc.test\nPDOC_TEST_PANDOC=1 time catchsegv python -m unittest -v pdoc.test.CliTest.test_pdf_pandoc\nbash <(curl -s https://codecov.io/bash)\n"", 'time python -m unittest -v pdoc.test', 'time doc/build.sh', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'pip install -U pip setuptools wheel\npip install -e .\n', 'time doc/build.sh', '.github/deploy-gh-pages.sh']"
"['bash -c ""tests/scripts_/download_test_files.sh""\n', '[[ ! -z `sudo find /var/cache/apt/archives -type f -name ""ffmpeg*.deb""` ]] && echo ""no update"" || sudo apt-get update\nsudo apt-get install -yq --no-install-recommends --download-only ffmpeg\n', 'sudo apt-get install -yq --no-install-recommends ffmpeg\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'echo ""Using test videos:""\nls -l ./tests/support_/videos/*/\npytest -m integrationtest\n', 'git fetch --prune --unshallow --tags\necho exit code $?\ngit tag --list\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'echo ""Short version:""\npython ./setup.py  --quiet --version\necho ""Long version:""\npython ./setup.py  --quiet fullversion\npython ./setup.py sdist bdist_wheel\n', 'PY_VERSION=$(python ./setup.py  --quiet --version)\nPY_BDIST_PATH=$(ls  dist/*.whl | head -n1)\nPY_BDIST_FILE=${PY_BDIST_PATH#*/}\necho ::set-output name=py_version::${PY_VERSION}\necho ::set-output name=py_bdist_file::${PY_BDIST_FILE}\necho ::set-output name=py_bdist_path::${PY_BDIST_PATH}\necho ${PY_VERSION} > dist/VERSION.txt\n', 'mkdir -p ./dist\nfind ./artifacts/ -type f -name ""*.whl"" -exec cp -n {} ./dist/ \\;\nfind ./artifacts/ -type f -name ""VERSION.txt"" -exec cp -n {} ./dist/ \\;\nls -l ./dist/\nPY_VERSION=$(cat ./dist/VERSION.txt)\necho ::set-output name=py_version::${PY_VERSION}\n', 'echo ${{ steps.buildx.outputs.platforms }}', 'echo  ""GITHUB_REF:${GITHUB_REF}""\necho  ""GITHUB_REPOSITORY:${GITHUB_REPOSITORY}""\nDOCKER_IMAGE=docker.io/josh5/unmanic\nVERSION_TAG=${GITHUB_REF#refs/*/}\n\nDOCKER_TAGS=""""\nif [[ ${VERSION_TAG%/merge} == \'master\' ]]; then\n  DOCKER_TAGS=""${DOCKER_TAGS}${DOCKER_IMAGE}:latest,""\nelif [[ ${VERSION_TAG%/merge} == \'staging\' ]]; then\n  DOCKER_TAGS=""${DOCKER_TAGS}${DOCKER_IMAGE}:staging,""\nelif [[ ${VERSION_TAG%/merge} =~ ""dev-""* ]]; then\n  DOCKER_TAGS=""${DOCKER_TAGS}${DOCKER_IMAGE}:${VERSION_TAG%/merge},""\nfi\nif [[ ${GITHUB_REF} == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/}\n  if [[ ${VERSION} =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}[-\\w]*$ ]]; then\n    DOCKER_TAGS=""${DOCKER_TAGS}${DOCKER_IMAGE}:${VERSION},""\n    DOCKER_TAGS=""${DOCKER_TAGS}${DOCKER_IMAGE}:latest,""\n  fi\nfi\n\nDOCKER_PUSH=""true""\nif [[ ${DOCKER_IMAGE} != \'docker.io/josh5/unmanic\' ]]; then\n  DOCKER_PUSH=""false""\nfi\nif [[ ${VERSION_TAG%/merge} =~ ""pr-""* ]]; then\n  DOCKER_PUSH=""false""\nfi\nif [[ ${VERSION_TAG%/merge} =~ ^[0-9]+$ ]]; then\n  DOCKER_PUSH=""false""\nfi\n\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=docker_tags::$(echo ${DOCKER_TAGS} | sed \'s/,$//\')\necho ::set-output name=docker_platforms::linux/amd64,linux/arm64,linux/arm/v7\necho ::set-output name=docker_push::${DOCKER_PUSH}\n', 'if [[ -e /tmp/.buildx-cache-new ]]; then\n  echo ""Cleaning up old cache...""\n  rm -rf /tmp/.buildx-cache\n  mv -v /tmp/.buildx-cache-new /tmp/.buildx-cache\nfi\n', 'mkdir -p ./dist\nfind ./artifacts/ -type f -name ""*.whl"" -exec cp -n {} ./dist/ \\;\nfind ./artifacts/ -type f -name ""*.tar.gz"" -exec cp -n {} ./dist/ \\;\nls -l ./dist/\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-line-length=127 --statistics\n', 'pytest -m unittest\n']"
"['conda install pytorch torchvision -c pytorch-nightly\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\npython collect_env.py\n', 'pip install -U mock hypothesis parameterized pytest scipy\npip install -e .\n', 'python -m unittest discover -v -s .\n']"
['./scripts/metrics-regress.py $METRICS_REGRESSION_NAME $METRICS_PROJECT_ID']
"['pip install poetry && poetry config virtualenvs.create false && poetry install --no-interaction --no-ansi\n', 'sh check.sh\n', 'git_hash=$(git rev-parse --short ""$GITHUB_SHA"")\ngit_branch=""$(echo ${GITHUB_REF} | cut -d\'/\' -f3)""\necho ${PKG_TOKEN} | docker login ghcr.io -u ${USERNAME} --password-stdin\ndocker build -t ghcr.io/${USERNAME}/tg:${git_branch}-${git_hash} -t ghcr.io/${USERNAME}/tg:latest .\ndocker push ghcr.io/${USERNAME}/tg:${git_branch}-${git_hash}\ndocker push ghcr.io/${USERNAME}/tg:latest\n']"
"['python -m pip install -U build pip setuptools\npython -m pip install -U -r requirements.txt\npython -m build --sdist\n', 'python -m mypy $MODULE_NAME\n', 'rm -rf $MODULE_NAME\n', 'python -m pip freeze > installed.txt\npython -m pip uninstall -y -r installed.txt\n', 'SDIST=$(python -c ""import os;print(os.listdir(\'./dist\')[-1])"" 2>&1)\npython -m pip install dist/$SDIST\n', 'python -c ""import $MODULE_NAME"" -Werror\n', 'python -m pip install -U -r requirements.txt\n', 'python -m pytest tests -Werror\n']"
"['echo ""date=$(/bin/date -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade pytest\npip install . -r doc/requirements.txt\n[[ -e cache ]] && mkdir -p .cache && mv -f cache .cache/cve-bin-tool\nsleep 5\n', 'pytest test/test_requirements.py\n', 'python -m pip install --upgrade wheel\npython -m pip install .\n', 'NO_EXIT_CVE_NUM=1 python -m cve_bin_tool.cli -u now test/assets/test-kerberos-5-1.15.1.out\n', 'python -m cve_bin_tool.cli --export-json exported_data\n', 'python -m pip install --upgrade wheel\npython -m pip install .\n', 'python cve_bin_tool/format_checkers.py\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade pre-commit\npre-commit install\n', 'pre-commit run ${{ matrix.tool }} --all-files\n', 'python -m pip install --upgrade gitlint\necho ""$TITLE"" | gitlint\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade sbom4python\npip install . --upgrade --upgrade-strategy=eager\n', 'sbom4python --module cve-bin-tool --output cve-bin-tool-py${{ matrix.python }}.spdx\nsbom4python --module cve-bin-tool --sbom cyclonedx --format json --output cve-bin-tool-py${{ matrix.python }}.json\n', '/bin/tail -n +10 sbom/cve-bin-tool-py${{ matrix.python }}.spdx > orig\n/bin/tail -n +10 cve-bin-tool-py${{ matrix.python }}.spdx > new\necho ""changed=$(/bin/diff -q orig new)"" >> $GITHUB_OUTPUT\n', '/bin/cat cve-bin-tool-py${{ matrix.python }}.spdx\n', 'cp cve-bin-tool-py${{ matrix.python }}.spdx sbom/cve-bin-tool-py${{ matrix.python }}.spdx\ncp cve-bin-tool-py${{ matrix.python }}.json sbom/cve-bin-tool-py${{ matrix.python }}.json\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade -r doc/requirements.txt\n', 'cd doc/\nsphinx-build -b html . _build\n', 'echo ""date=$(/bin/date -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\necho ""yesterday=$(/bin/date -d ""-1 day"" -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\n', 'echo ""Today\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.date }}""\necho ""Yesterday\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.yesterday }}""\n', 'sudo apt-get update && sudo apt-get install cabextract', 'sudo apt-get install build-essential libpoppler-cpp-dev pkg-config python3-dev', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade pdftotext\npython -m pip install --upgrade reportlab\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install --upgrade .\n', '[[ -e cache ]] && mkdir -p .cache && mv cache ~/.cache/cve-bin-tool\nNO_EXIT_CVE_NUM=1 python -m cve_bin_tool.cli test/assets/test-kerberos-5-1.15.1.out -n json\ncp -r ~/.cache/cve-bin-tool cache\n', 'pytest -n 4 -v --ignore=test/test_cli.py --ignore=test/test_cvedb.py --ignore=test/test_requirements.py --ignore=test/test_html.py --ignore=test/test_json.py\n', 'pytest -v test/test_cli.py test/test_cvedb.py\n', 'echo ""date=$(/bin/date -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\necho ""yesterday=$(/bin/date -d ""-1 day"" -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\n', 'echo ""Today\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.date }}""\necho ""Yesterday\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.yesterday }}""\n', 'sudo apt-get update && sudo apt-get install cabextract', 'sudo apt-get install build-essential libpoppler-cpp-dev pkg-config python3-dev', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade pdftotext\npython -m pip install --upgrade reportlab\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install --editable .\n', '[[ -e cache ]] && mkdir -p .cache && mv cache ~/.cache/cve-bin-tool\nNO_EXIT_CVE_NUM=1 python -m cve_bin_tool.cli test/assets/test-kerberos-5-1.15.1.out -n json\ncp -r ~/.cache/cve-bin-tool cache\n', 'pytest --cov --cov-append -n 4 -v --ignore=test/test_cli.py --ignore=test/test_cvedb.py --ignore=test/test_requirements.py --ignore=test/test_html.py --ignore=test/test_json.py\n', 'pytest -v --cov --cov-append --cov-report=xml test/test_cli.py test/test_cvedb.py\n', 'echo ""date=$(/bin/date -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\necho ""yesterday=$(/bin/date -d ""-1 day"" -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\n', 'echo ""Today\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.date }}""\necho ""Yesterday\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.yesterday }}""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install --editable .\n', 'python -m playwright install chromium --with-deps\n', '[[ -e cache ]] && mkdir -p .cache && mv cache ~/.cache/cve-bin-tool\nNO_EXIT_CVE_NUM=1 python -m cve_bin_tool.cli test/assets/test-kerberos-5-1.15.1.out -n json\ncp -r ~/.cache/cve-bin-tool cache\n', 'pytest -v test/test_source_osv.py test/test_source_gad.py test/test_source_nvd.py test/test_nvd_api.py test/test_cvedb.py test/test_available_fix.py\n', 'pytest -v -n auto test/test_html.py', 'pytest -v test/test_json.py\n', 'echo ""DATE=$(get-date -format ""yyyyMMdd"")"" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\necho ""YESTERDAY=$(get-date (get-date).addDays(-1)  -format ""yyyyMMdd"")"" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n', 'echo ""Today\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.DATE }}""\necho ""Yesterday\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.YESTERDAY }}""\n', ""mkdir '~\\.cache'\nif (Test-Path -Path cache) { mv cache '~\\.cache\\cve-bin-tool' }\n"", 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install --upgrade .\n', 'python -m cve_bin_tool.cli test/assets/test-kerberos-5-1.15.1.out -n json\n', 'pytest -n 4 -v --ignore=test/test_cli.py --ignore=test/test_cvedb.py --ignore=test/test_requirements.py --ignore=test/test_html.py --ignore=test/test_json.py\n', 'pytest -v test/test_cli.py test/test_cvedb.py\n', 'echo ""DATE=$(get-date -format ""yyyyMMdd"")"" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\necho ""YESTERDAY=$(get-date (get-date).addDays(-1)  -format ""yyyyMMdd"")"" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n', 'echo ""Today\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.DATE }}""\necho ""Yesterday\'s Cache Key: Linux-cve-bin-tool-${{ steps.get-date.outputs.YESTERDAY }}""\n', ""mkdir '~\\.cache'\nif (Test-Path -Path cache) { mv cache '~\\.cache\\cve-bin-tool' }\n"", 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install --upgrade .\n', 'python -m cve_bin_tool.cli test/assets/test-kerberos-5-1.15.1.out -n json\n', 'pytest --cov --cov-append -n 4 -v --ignore=test/test_cli.py --ignore=test/test_cvedb.py --ignore=test/test_requirements.py --ignore=test/test_html.py --ignore=test/test_json.py\n', 'pytest -v --cov --cov-append --cov-report=xml test/test_cli.py test/test_cvedb.py\n', 'conda install -c conda-forge python=3.9 poppler pdftotext\npython -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --upgrade reportlab\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install --upgrade .\n', 'pytest test/test_output_engine.py -k test_output_pdf --cov --cov-append --cov-report=xml', 'echo ""date=$(/bin/date -u ""+%Y%m%d"")"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npython -m pip install --upgrade wheel\npython -m pip install --editable .\n', '[[ -e cache ]] && mkdir -p .cache && mv cache ~/.cache/cve-bin-tool\npython -m cve_bin_tool.cli test/assets/test-kerberos-5-1.15.1.out -u now\ncp -r ~/.cache/cve-bin-tool cache\n', 'python .github/workflows/update_js_dependencies.py', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel\npython -m pip install --upgrade . pytest-xdist pytest-playwright\npython -m playwright install chromium --with-deps\n', 'python -m pytest -v -n auto test/test_html.py', 'python -c \'from test.test_output_engine import TestOutputEngine; \\\nfrom cve_bin_tool.output_engine.html import output_html; \\\noutput_html(TestOutputEngine.MOCK_OUTPUT, None, """", """", """", 3, 3, 0, None, None, open(""test.html"", ""w""))\'\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade pre-commit\npre-commit install\n', 'pre-commit autoupdate\npre-commit uninstall\n', 'python .github/workflows/update-dev-requirements.py\n', 'sed -i ""s/[0-9]\\{4\\}/$(date +%Y)/"" spdx_header.txt\n']"
""
"['pip install .\npip install deepdiff numpy\nmkdir -p .github/linters\ncp pyproject.toml .github/linters\n', 'python3 setup.py develop          \nsolc-select install all\n', 'solc-select use 0.3.6\n', 'solc-select use 0.8.0', 'pip install .\npip install deepdiff numpy\n\nmkdir -p .github/linters\ncp pyproject.toml .github/linters\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo python3 setup.py develop\nsolc-select install all\nsolc-select install all\n', 'solc-select use 0.4.0\n', 'solc-select use 0.8.0']"
"['echo ""${{ env.CONFIG_POETRY_PATH }}"" >> $GITHUB_PATH\n', 'poetry config cache-dir ${{ env.POETRY_CACHE_DIR }}', 'sudo apt update\nsudo apt install -y libomp-dev\n', 'poetry install --no-interaction --no-root\n', 'poetry run pip install .\nrm -r ./probreg\n', 'poetry run python -m unittest discover']"
"['python -m pip install --upgrade pip\npip install .[dev]\n', 'pip install pytest\npytest -vv\n', 'pip install flake8 flake8-docstrings\n', 'flake8 . --count --show-source --statistics\n', 'git fetch --no-tags --prune --unshallow\n', 'yarn install\n', 'yarn format -- --check --branch origin/master\n', 'yarn install\n', 'yarn build\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller\n', 'pyinstaller --noconfirm --distpath %cd%\\ --onefile --console --icon %cd%\\Dashboard\\static\\img\\aniGamerPlus.ico --clean --add-data %cd%;aniGamerPlus/  %cd%\\aniGamerPlus.py', 'chcp 65001\necho (""RELEASE_VERSION="" + $env:GITHUB_REF.replace(\'refs/tags/\', \'\')) >> $env:GITHUB_ENV\necho (""RELEASE_TITLE=aniGamerPlus_"" + $env:GITHUB_REF.replace(\'refs/tags/\', \'\') + ""_windows_64bit"") >> $env:GITHUB_ENV\necho (""RELEASE_FILENAME=aniGamerPlus_"" + $env:GITHUB_REF.replace(\'refs/tags/\', \'\') + ""_windows_64bit.zip"") >> $env:GITHUB_ENV\ngit fetch --tags --force\n', ""echo $RELEASE_VERSION\necho ${{ env.RELEASE_VERSION }}\necho $RELEASE_TITLE\necho ${{ env.RELEASE_TITLE }}\necho $RELEASE_FILENAME\necho ${{ env.RELEASE_FILENAME }}\necho $TAG_MESSAGE\necho '${{ steps.tag_data.outputs.git-tag-annotation }}'\n"", 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller\n', 'pyinstaller --noconfirm --distpath %cd%\\ --onefile --console --icon %cd%\\Dashboard\\static\\img\\aniGamerPlus.ico --clean --add-data %cd%;aniGamerPlus/  %cd%\\aniGamerPlus.py', '7z a -tzip ${{ env.RELEASE_FILENAME }} aniGamerPlus.exe Dashboard DanmuTemplate.ass config-sample.json sn_list-sample.txt LICENSE README.md']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python3.7 -m pip install docker pytest', 'bash scripts/build-push.sh', 'python3.7 -m pip install docker pytest', 'bash scripts/test.sh']"
""
[]
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -e .\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\n#flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n#flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nflake8 . --exit-zero  \n', ""pytest --cov-config=.coveragerc --cov-report= --cov=ffsubsync -v -m 'not integration' tests/\n"", ""pytest --cov-config=.coveragerc --cov-report=xml:cov.xml --cov=ffsubsync -v -m 'not integration' tests/\n"", ""INTEGRATION=1 pytest --cov-config=.coveragerc --cov-report=xml:cov.xml --cov=ffsubsync -v -m 'integration' tests/\n""]"
"['echo ""========== categorization of changed files ==========""\nfor file in $(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }})\ndo\n  echo -n ""$file => ""\n\n  case $file in\n  csharp/*)\n    echo ""C#""\n    echo ""::set-output name=has_csharp_changes::true""\n    ;;\n\n  go/*)\n    echo ""Go""\n    echo ""::set-output name=has_go_changes::true""\n    ;;\n\n  java/*)\n    echo ""Java""\n    echo ""::set-output name=has_java_changes::true""\n    ;;\n\n  python/*)\n    echo ""Python""\n    echo ""::set-output name=has_python_changes::true""\n    ;;\n\n  typescript/*)\n    echo ""TypeScript""\n    echo ""::set-output name=has_typescript_changes::true""\n    ;;\n\n  *)\n    echo ""<unmatched>""\n    ;;\n  esac\ndone\n', 'HAS_CSHARP_CHANGES=""${{needs.check_file_changes.outputs.has_csharp_changes}}""\nHAS_GO_CHANGES=""${{needs.check_file_changes.outputs.has_go_changes}}""\nHAS_JAVA_CHANGES=""${{needs.check_file_changes.outputs.has_java_changes}}""\nHAS_PYTHON_CHANGES=""${{needs.check_file_changes.outputs.has_python_changes}}""\nHAS_TYPESCRIPT_CHANGES=""${{needs.check_file_changes.outputs.has_typescript_changes}}""\n\nif [ ""${HAS_CSHARP_CHANGES}"" == ""true"" ]; then\n  docker run --rm --net=host -t -v $PWD:$PWD -w $PWD jsii/superchain:1-buster-slim-node14 /bin/bash -c ""scripts/build-csharp.sh""\nfi\n\nif [ ""${HAS_GO_CHANGES}"" == ""true"" ]; then\n  docker run --rm --net=host -t -v $PWD:$PWD -w $PWD jsii/superchain:1-buster-slim-node14 /bin/bash -c ""scripts/build-go.sh""\nfi\n\nif [ ""${HAS_JAVA_CHANGES}"" == ""true"" ]; then\n  docker run --rm --net=host -t -v $PWD:$PWD -w $PWD jsii/superchain:1-buster-slim-node14 /bin/bash -c ""scripts/build-java.sh""\nfi\n\nif [ ""${HAS_PYTHON_CHANGES}"" == ""true"" ]; then\n  docker run --rm --net=host -t -v $PWD:$PWD -w $PWD jsii/superchain:1-buster-slim-node14 /bin/bash -c ""scripts/build-python.sh""\nfi\n\nif [ ""${HAS_TYPESCRIPT_CHANGES}"" == ""true"" ]; then\n  docker run --rm --net=host -t -v $PWD:$PWD -w $PWD jsii/superchain:1-buster-slim-node14 /bin/bash -c ""scripts/build-typescript.sh""\nfi\n', 'bash .github/extract-snippets/extract-snippets.sh dryrun', 'bash .github/extract-snippets/extract-snippets.sh']"
"['curl \\\n  -H ""Content-Type: application/json"" \\\n  -d \'{""username"": ""GitHub action failure"", ""content"": ""[Scheduled action failed!](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})""}\' \\\n  ${{ secrets.DISCORD_WEBHOOK_ACTION_FAILURE }}', 'scripts/install/frontend\nscripts/install/pip_packages --requirement requirements_generate_data.txt\n', 'python3 -m scripts.data.generate_category_data ${{ matrix.category }}', 'if test -f ""outputdata/${{ matrix.category }}/data.json""; then\n  echo ""updated=true"" >> ""$GITHUB_OUTPUT""\nelse\n  echo ""updated=false"" >> ""$GITHUB_OUTPUT""\nfi\n', 'jq -c . outputdata/${{ matrix.category }}/data.json\njq -c . outputdata/${{ matrix.category }}/repositories.json\n', 'aws s3 sync \\\n  outputdata/${{ matrix.category }} \\\n  s3://data-v2/${{ matrix.category }} \\\n  --endpoint-url ${{ secrets.CF_R2_ENDPOINT_DATA }}\n', 'curl --silent --show-error --fail -X POST \\\n  ""https://api.cloudflare.com/client/v4/zones/${{ secrets.CF_ZONE_ID }}/purge_cache"" \\\n  -H ""Authorization: Bearer ${{ secrets.CF_BUST_CACHE_TOKEN }}"" \\\n  -H ""Content-Type: application/json"" \\\n  --data \'{""files"": [""https:\\/\\/data-v2.hacs.xyz\\/${{ matrix.category }}\\/data.json"", ""https:\\/\\/data-v2.hacs.xyz\\/${{ matrix.category }}\\/repositories.json""]}\'\n', 'curl \\\n  -H ""Content-Type: application/json"" \\\n  -d \'{""username"": ""GitHub action failure"", ""content"": ""[Scheduled action failed!](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})""}\' \\\n  ${{ secrets.DISCORD_WEBHOOK_ACTION_FAILURE }}\n', 'scripts/install/pip_packages pre-commit\npre-commit install-hooks --config .github/pre-commit-config.yaml\n', 'pre-commit run --hook-stage manual ${{ matrix.check }} --all-files --config .github/pre-commit-config.yaml', 'scripts/install/pip_packages --requirement requirements_lint.txt', 'echo ""::add-matcher::.github/matchers/bellybutton.json""', 'bellybutton lint', 'jq -r -e -c . tests/fixtures/*.json', 'sed -i ""/MINIMUM_HA_VERSION = /c\\MINIMUM_HA_VERSION = \\""$(jq .homeassistant -r ${{ github.workspace }}/hacs.json)\\"""" ${{ github.workspace }}/custom_components/hacs/const.py\npython3 ${{ github.workspace }}/scripts/update/manifest.py --version ${{ steps.version.outputs.version }}\n', '${{ github.workspace }}/scripts/install/frontend', 'cd ${{ github.workspace }}/custom_components/hacs\nzip hacs.zip -r ./\n', 'scripts/install/frontend\nscripts/install/pip_packages --requirement requirements_test.txt\n', 'python3 -m pytest\n', 'scripts/coverage\ncurl -sfSL https://codecov.io/bash | bash -\n', 'scripts/install/pip_packages --requirement requirements_test.txt\nscripts/install/core\nscripts/install/frontend\n', 'python3 -m pytest\n', 'curl \\\n  -H ""Content-Type: application/json"" \\\n  -d \'{""username"": ""GitHub action failure"", ""content"": ""[Scheduled action failed!](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})""}\' \\\n  ${{ secrets.DISCORD_WEBHOOK_ACTION_FAILURE }}\n', 'curl \\\n  -H ""Content-Type: application/json"" \\\n  -d \'{""username"": ""GitHub action failure"", ""content"": ""[Scheduled action failed!](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})""}\' \\\n  ${{ secrets.DISCORD_WEBHOOK_ACTION_FAILURE }}\n', 'mkdir ./test_configuration\nbash scripts/install/frontend\ncp -r ./custom_components ./test_configuration\necho ""default_config:"" >> ./test_configuration/configuration.yaml\necho ""hacs:"" >> ./test_configuration/configuration.yaml\necho ""  token: CHANGE_ME"" >> ./test_configuration/configuration.yaml\n', 'curl \\\n  -H ""Content-Type: application/json"" \\\n  -d \'{""username"": ""GitHub action failure"", ""content"": ""[Scheduled action failed!](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})""}\' \\\n  ${{ secrets.DISCORD_WEBHOOK_ACTION_FAILURE }}']"
"['python -m pip install --upgrade pip\npython -m pip install bandit\n', 'bandit -r -lll -ii .', 'npm install -g aws-cdk\ncdk --version\n', 'cd test_infra\ncat <<EOT >> cdk.context.json\n{\n  ""availability-zones:account=111111111111:region=us-east-1"": [\n    ""us-east-1a"",\n    ""us-east-1b"",\n    ""us-east-1c"",\n    ""us-east-1d"",\n    ""us-east-1e"",\n    ""us-east-1f""\n  ]\n}\nEOT\npython -m pip install --upgrade pip\npython -m pip install poetry\npoetry env use python\npoetry env info\nsource $(poetry env info --path)/bin/activate\npoetry install -vvv\n', 'cd test_infra\nsource $(poetry env info --path)/bin/activate\ncdk synth\n', 'if grep -ro ""@pytest.mark.xfail()"" tests/; then echo ""xfails must catch a specific error, e.g. \'@pytest.mark.xfail(raises=NotImplementedError)\'"" && exit 1; else echo ""success"" && exit 0; fi', 'python -m pip install --upgrade pip\npython -m pip install poetry\npoetry config virtualenvs.create false --local\npoetry install -vvv\n', 'pytest tests/unit/test_metadata.py', 'pytest tests/unit/test_session.py', 'pytest tests/unit/test_utils.py', 'pytest -n 4 tests/unit/test_moto.py', 'python -m pip install --upgrade pip\npython -m pip install poetry\npoetry config virtualenvs.create false --local\npoetry install --all-extras -vvv\n', 'black --check .', 'ruff . --ignore ""PL"" --ignore ""D""\nruff awswrangler\n', 'mypy --install-types --non-interactive awswrangler', 'pylint -j 0 --disable=all --enable=R0913,R0915 awswrangler', 'doc8 --max-line-length 120 docs/source']"
""
"['scripts/install', 'scripts/build', 'scripts/publish', 'scripts/install', 'scripts/check', 'scripts/build', 'scripts/test']"
"['pip3 install -q tensorflow==${{ matrix.tf-version }}\npip install -q protobuf==3.19.0\npip install -q requests\npip install -e .\n', 'pip install -q pytest\npip install -q pytest-cov\npip install -q python-coveralls\npytest --cov=ge --cov-report=xml\n']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', './docs/build.sh\n', 'set -x\ngit fetch\n( git branch gh-pages remotes/origin/gh-pages && git clone . --branch=gh-pages _gh-pages/ ) || mkdir _gh-pages\nrm -rf _gh-pages/.git/\nmkdir -p _gh-pages/branch/\n', ""set -x\n# Delete everything under _gh-pages/ that is from the\n# primary branch deployment.  Eicludes the other branches\n# _gh-pages/branch-* paths, and not including\n# _gh-pages itself.\nfind _gh-pages/ -mindepth 1 ! -path '_gh-pages/branch*' -delete\nrsync -a generated/docs/ _gh-pages/\n"", 'set -x\nbrname=""${{github.ref}}""\nbrname=""${brname##refs/heads/}""\nbrdir=${brname//\\//--}   # replace \'/\' with \'--\'\nrm -rf   _gh-pages/branch/${brdir}\nrsync -a generated/docs/ _gh-pages/branch/${brdir}\n', 'set -x\nfor brdir in `ls _gh-pages/branch/` ; do\n    brname=${brdir//--/\\/}   # replace \'--\' with \'/\'\n    if ! git show-ref remotes/origin/$brname ; then\n        echo ""Removing $brdir""\n        rm -r _gh-pages/branch/$brdir/\n    fi\ndone\n', 'touch _gh-pages/.nojekyll\n', 'docker run --detach \\\n  --name neo4j-4 \\\n  --env NEO4J_AUTH=none \\\n  --publish 7474:7474 \\\n  --publish 7473:7473 \\\n  --publish 7687:7687 \\\n  neo4j:4.4-community\n', 'pip install -e .\npip install -r test-requirements.txt\n', '(docker logs -f neo4j-4 & ) | grep -q Started', 'make test_unit', 'make test_integration', 'docker run --detach \\\n  --name neo4j-5 \\\n  --env NEO4J_AUTH=none \\\n  --publish 7474:7474 \\\n  --publish 7473:7473 \\\n  --publish 7687:7687 \\\n  neo4j:5\n', 'pip install -e .\npip install -r test-requirements.txt\n', '(docker logs -f neo4j-5 & ) | grep -q Started', 'make test_unit', 'make test_integration']"
""
""
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt\npython -m pip install --upgrade --upgrade-strategy eager .[tensorflow,torch,shap]\nif [ ""$RUNNER_OS"" != ""Windows"" ]; then  \n# Windows support for ray is experimental (https://docs.ray.io/en/latest/installation.html#windows-support)\n  python -m pip install --upgrade --upgrade-strategy eager .[tensorflow,torch,shap,ray]  # include other deps so that they are taking into account during ray install\nfi\npython -m spacy download en_core_web_md\npython -m pip freeze\n', 'pytest -m tf1 alibi\npytest -m ""not tf1"" alibi\n', 'make build_pypi\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt\npython -m pip install --upgrade --upgrade-strategy eager .[all]\n', 'flake8 alibi\n', 'mypy alibi\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements/docs.txt\npython -m pip freeze\n', 'make build_docs\n', 'make build_latex\n', 'pip install ""tox>=3.21.0,<4.0.0""\nmake licenses\nmake check_licenses\n', 'pip install ""tox>=3.21.0,<4.0.0""\ntox\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt -r testing/requirements.txt\npython -m pip install --upgrade --upgrade-strategy eager .[shap,tensorflow]\nif [ ""$RUNNER_OS"" != ""Windows"" ]; then  \n# Windows support for ray is experimental (https://docs.ray.io/en/latest/installation.html#windows-support)\n  python -m pip install --upgrade --upgrade-strategy eager .[shap,tensorflow,ray]\nfi\npython -m spacy download en_core_web_md\npython -m pip freeze\n', 'pytest --suppress-no-test-exit-code --no-cov -rA --durations=0 -vv testing/test_notebooks.py\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt -r testing/requirements.txt\npython -m pip install --upgrade --upgrade-strategy eager .[shap,tensorflow]\nif [ ""$RUNNER_OS"" != ""Windows"" ]; then  \n# Windows support for ray is experimental (https://docs.ray.io/en/latest/installation.html#windows-support)\n  python -m pip install --upgrade --upgrade-strategy eager .[shap,tensorflow,ray]\nfi\npython -m spacy download en_core_web_md\npython -m pip freeze\n', 'tests=""test_notebook_execution[$(echo ${FILES} | sed \'s|doc/source/examples/||g\' | sed \'s| | or |g\')]"" &&\npytest --suppress-no-test-exit-code --no-cov -rA --durations=0 -vv testing/test_notebooks.py -k ""$tests""\n']"
"[""# Filter out non-python files.\n(cat $HOME/files_added.txt; echo; cat $HOME/files_modified.txt) | tr ' ' '\\n' | grep '\\.py$' > py_files.txt || true\n# Filter out non-test python files and e2e or integration tests.\ncat py_files.txt | grep '_test\\.py$' | grep -v _e2e_ | grep -v integration | grep -v 'examples/' > py_test_files.txt || true\n# Select proto files.\n(cat $HOME/files_added.txt; echo; cat $HOME/files_modified.txt) | tr ' ' '\\n' | grep '\\.proto$' > proto_files.txt || true\n"", '# Instruction from https://docs.bazel.build/versions/master/install-ubuntu.html\ncurl -sSL https://github.com/bazelbuild/bazel/releases/download/3.4.1/bazel-3.4.1-installer-linux-x86_64.sh -o bazel_installer.sh\nchmod +x bazel_installer.sh\nsudo ./bazel_installer.sh\n', 'python -m pip install --upgrade pip wheel\n# TODO(b/232490018): Cython need to be installed separately to build pycocotools.\npython -m pip install Cython -c ./test_constraints.txt\nTFX_DEPENDENCY_SELECTOR=NIGHTLY pip install -c ./test_constraints.txt --extra-index-url https://pypi-nightly.tensorflow.org/simple --pre --editable .[all]\n', '[ ! -s ""py_test_files.txt"" ] || cat py_test_files.txt | xargs -I {} python {}\n', 'curl -sSOL https://github.com/yoheimuta/protolint/releases/download/v${PROTOLINT_VERSION}/protolint_${PROTOLINT_VERSION}_Linux_x86_64.tar.gz\ntar zxf protolint_${PROTOLINT_VERSION}_Linux_x86_64.tar.gz\necho ""[NOTE] This linter is currently EXPERIMENTAL.=======================================""\necho ""Please contact reviewers for existing lint errors or false negative errors.""\necho ""====================================================================================""\n[ ! -s ""proto_files.txt"" ] || cat proto_files.txt | xargs -I {} ./protolint {}\n', 'pip install pylint\necho ""[NOTE] This linter is currently EXPERIMENTAL.=======================================""\necho ""Please contact reviewers for existing lint errors or false negative errors.""\necho ""Feel free to send PRs for pylintrc in the root directory of the repository if needed.""\necho ""====================================================================================""\n[ ! -s ""py_files.txt"" ] || pylint $(cat py_files.txt | tr \'\\n\' \' \')\n']"
"['scripts/install', 'scripts/build', 'scripts/publish', 'scripts/install', 'scripts/check', 'scripts/build', 'scripts/test', 'scripts/test', 'scripts/test', 'scripts/coverage']"
"['sudo apt install -y pandoc gsfonts\npython -m pip install --upgrade pip\npip install jaxlib\npip install jax\npip install black\npip install .[doc,test]\npip install https://github.com/pyro-ppl/funsor/archive/master.zip\npip install -r docs/requirements.txt\npip freeze\n', 'make lint\n', 'make docs\n', 'make doctest\npython -m doctest -v README.md\n', 'sudo apt install -y graphviz\npython -m pip install --upgrade pip\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install jaxlib\npip install jax\npip install https://github.com/pyro-ppl/funsor/archive/master.zip\npip install .[dev,test]\npip freeze\n', 'CI=1 pytest -vs -k ""not test_example"" --durations=100 --ignore=test/infer/ --ignore=test/contrib/\n', 'python -m pip install --upgrade pip\n# Keep track of pyro-api master branch\npip install https://github.com/pyro-ppl/pyro-api/archive/master.zip\npip install jaxlib\npip install jax\npip install https://github.com/pyro-ppl/funsor/archive/master.zip\npip install .[dev,test]\npip freeze\n', 'pytest -vs --durations=20 test/infer/test_mcmc.py\npytest -vs --durations=20 test/infer --ignore=test/infer/test_mcmc.py\npytest -vs --durations=20 test/contrib\n', 'JAX_ENABLE_X64=1 pytest -vs test/infer/test_mcmc.py -k x64\n', 'XLA_FLAGS=""--xla_force_host_platform_device_count=2"" pytest -vs test/infer/test_mcmc.py -k ""chain or pmap or vmap""\nXLA_FLAGS=""--xla_force_host_platform_device_count=2"" pytest -vs test/contrib/test_tfp.py -k ""chain""\nXLA_FLAGS=""--xla_force_host_platform_device_count=2"" pytest -vs test/infer/test_hmc_gibbs.py -k ""chain""\n', 'python -m pip install --upgrade pip\npip install jaxlib\npip install jax\npip install https://github.com/pyro-ppl/funsor/archive/master.zip\npip install .[dev,examples,test]\npip freeze\n', 'CI=1 XLA_FLAGS=""--xla_force_host_platform_device_count=2"" pytest -vs -k test_example\n']"
"['python -m pip install -e .', 'python -m pip freeze', 'python -m pip install tensorflow\npython -c ""import deepxde""\n', 'python -m pip install tensorflow-probability\npython -c ""import deepxde""\n', 'python -m pip install torch\npython -c ""import deepxde""\n', 'python -m pip install jax flax optax\npython -c ""import deepxde""\n', 'python -m pip install paddlepaddle==0.0.0 -f https://www.paddlepaddle.org.cn/whl/mac/cpu/develop.html\npython -c ""import deepxde""\n', 'python -m pip install paddlepaddle==0.0.0 -f https://www.paddlepaddle.org.cn/whl/linux/cpu-mkl/develop.html\npython -c ""import deepxde""\n', 'python -m pip install paddlepaddle==0.0.0 -f https://www.paddlepaddle.org.cn/whl/windows/cpu-mkl-avx/develop.html\npython -c ""import deepxde""\n', 'pipx run build', 'pipx run twine check dist/*']"
"['cd resource\nwget https://github.com/KUR-creative/SickZil-Machine/releases/download/v0.1.1-pre0/model.zip\nunzip model.zip\n', 'python -m pip install --upgrade pip\npip install -r deps/requirements_cpu.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\ncd test\npytest\n', 'Set-Location resource\nInvoke-WebRequest https://github.com/KUR-creative/SickZil-Machine/releases/download/v0.1.1-pre0/model.zip -Outfile model.zip\nUnZip model.zip\n', 'python -m pip install --upgrade pip\npip install -r deps/requirements_cpu.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\ncd test\npytest\n']"
[]
"['pip install . -r requirements/doc.txt\nmake -C docs html SPHINXOPTS=-W\n', 'pip install black mypy ruff types-certifi types-cryptography types-pyopenssl', 'ruff examples src tests\nblack --check --diff examples src tests\nmypy src tests\n', 'sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate off\necho ""AIOQUIC_SKIP_TESTS=chacha20"" >> $GITHUB_ENV\necho ""CFLAGS=-I/usr/local/opt/openssl/include"" >> $GITHUB_ENV\necho ""LDFLAGS=-L/usr/local/opt/openssl/lib"" >> $GITHUB_ENV\n', 'choco install openssl --no-progress\necho ""INCLUDE=C:\\Progra~1\\OpenSSL-Win64\\include"" >> $GITHUB_ENV\necho ""LIB=C:\\Progra~1\\OpenSSL-Win64\\lib"" >> $GITHUB_ENV\n', 'python -m pip install -U pip setuptools wheel\npip install .[dev]\ncoverage run -m unittest discover -v\ncoverage xml\n', 'pip install -U build\npython -m build --sdist\n', 'pip install cibuildwheel\ncibuildwheel --output-dir dist\n']"
"['echo ""$GITHUB_CONTEXT""', 'echo ""$JOB_CONTEXT""', 'python -m pip install --upgrade pip wheel setuptools\nif [ -f requirements-all.txt ]; then\n  python -m pip install -r requirements-all.txt\nelif [ -f requirements.txt ]; then\n  python -m pip install -r requirements.txt;\nfi\npython -m pip install -e .\n', 'if [ -f requirements-dev.txt ]; then\n  python -m pip install -r requirements-dev.txt\nelse\n  echo ""Missing requirements-dev.txt. Installing minimal requirements for testing.""\n  python -m pip install pytest pytest-cov pytest-xdist pytest-check aiohttp nbconvert jupyter_contrib_nbextensions\n  python -m pip install Pygments respx pytest-xdist markdown beautifulsoup4 Pillow async-cache lxml\nfi\npython -m pip install ""pandas>=1.3.0"" ""pygeohash>=1.2.0""\n', 'mkdir ~/.msticpy\nmkdir ~/.msticpy/mordor\ncp ./tests/testdata/geolite/GeoLite2-City.mmdb ~/.msticpy\ntouch ~/.msticpy/GeoLite2-City.mmdb\ncp -r ./tests/testdata/mordor/* ~/.msticpy/mordor\ntouch ~/.msticpy/mordor/mitre_tact_cache.pkl\ntouch ~/.msticpy/mordor/mitre_tech_cache.pkl\ntouch ~/.msticpy/mordor/mordor_cache.pkl\n', 'pytest tests -n auto --junitxml=junit/test-${{ matrix.python-version }}-results.xml --cov=msticpy --cov-report=xml\n', 'python -m pip install --upgrade pip wheel setuptools\nif [ -f requirements-all.txt ]; then\n  python -m pip install -r requirements-all.txt\nelif [ -f requirements.txt ]; then\n  python -m pip install -r requirements.txt;\nfi\npython -m pip install -e .\n', 'if [ -f requirements-dev.txt ]; then\n  python -m pip install -r requirements-dev.txt\nelse\n  echo ""Missing requirements-dev.txt. Installing minimal requirements for testing.""\n  python -m pip install flake8 black bandit mypy pylint types-attrs pydocstyle pyroma\nfi\n', 'black --diff --check --exclude venv msticpy\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 msticpy --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 --max-line-length=90 --exclude=tests* . --ignore=E501,W503 --jobs=auto\n', 'pylint msticpy --disable=duplicate-code --disable=E1135,E1101,E1133\n', 'mypy --ignore-missing-imports --follow-imports=silent --show-column-numbers --show-error-end --show-error-context --disable-error-code annotation-unchecked --junit-xml junit/mypy-test-${{ matrix.python-version }}-results.xml msticpy\n', 'bandit -x tests -r -s B303,B404,B603,B607,B608 msticpy\n', 'flake8 --max-line-length=90 --exclude=tests* . --ignore=E501,W503 --jobs=auto\n', 'pydocstyle --convention=numpy msticpy\n', 'pyroma --min 10 .\n', 'python -m pip install --upgrade pip\npython -m pip install build\n', 'python -m build --sdist --wheel --outdir dist/']"
"['pip install -r requirements.txt', 'mkdocs gh-deploy --force']"
"['pip install torch==1.13.1+cpu torchvision==0.14.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install .[dev,extras,test]\n', 'cd guide && python artwork.py', 'cd guide && python download_data.py', 'cd guide && cp _config_dev.yml _config.yml && jupyter-book build -W --keep-going .', 'pip install torch==1.13.1+cpu torchvision==0.14.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install .[dev,extras,test]\n', 'cd guide && python artwork.py', 'cd guide && python download_data.py', 'cd guide && jupyter-book build -W --keep-going .', 'pip install -e .', 'python -c ""import openpifpaf; print(openpifpaf.__version__)""', 'git status', 'git diff', 'python -c ""import openpifpaf; assert \'dirty\' not in openpifpaf.__version__""', 'python setup.py sdist', 'ls dist', 'ls -n /Applications/ | grep Xcode*', 'conda --version', 'which python', 'python --version', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install torch==${{ matrix.torch }} torchvision==${{ matrix.torchvision }} -f ${{ matrix.torch-source }}\n', 'python -m pip install -e "".[test]"" --no-build-isolation', 'python -m pip install -e "".[onnx]"" --no-build-isolation', 'python -m pip install -e "".[extras,train]"" --no-build-isolation', 'ffmpeg -codecs', 'python -m pip install -e "".[coreml]"" --no-build-isolation', 'sudo apt-get update && sudo apt-get --no-install-recommends install python3-opencv libopencv-dev', 'wget https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.13.1%2Bcpu.zip\nunzip libtorch-cxx11-abi-shared-with-deps-1.13.1+cpu.zip\nls\nls libtorch\nls libtorch/lib\n', 'python -m pip freeze\npython --version\npython -c ""import openpifpaf; print(openpifpaf.__version__)""\n', 'pylint openpifpaf --disable=fixme\n', 'pylint tests/*.py --disable=fixme\n', 'python -m pycodestyle src/openpifpaf\n', 'python -m pycodestyle tests/*.py\n', 'cpplint --recursive --exclude=src/openpifpaf/csrc/build --linelength=120 --filter=-legal,-whitespace/parens,-whitespace/braces,-whitespace/semicolon,-readability/todo src/openpifpaf/csrc', 'cd guide\npython nb_cell_tags.py  # runs all notebooks through nbformat\ngit status\ngit diff\ngit diff-index --quiet HEAD  # exit code 1 when files were changed\n', 'cd guide\nnbstripout *.ipynb\ngit status\ngit diff\ngit diff-index --quiet HEAD  # exit code 1 when files were changed\n', 'mkdir guide_py\njupyter nbconvert --to=script --output-dir=./guide_py ./guide/*.ipynb\npylint ./guide_py/*.py --disable=trailing-whitespace,trailing-newlines,line-too-long,pointless-statement,undefined-variable,expression-not-assigned,wrong-import-position,ungrouped-imports,wrong-import-order,invalid-name\npycodestyle guide_py/*.py --ignore=W291,W391,E302,E305,E402,E501\n', 'pytest -vv -s -m slow\n', 'pytest -vv -m ""not slow""\n', 'cd guide && python download_data.py && cd ..\n', 'cd guide && pytest -v -s --nbval-lax --current-env *.ipynb && cd ..\n', 'python setup.py sdist', 'mkdir sdist_test_dir\ncd sdist_test_dir\npython -m pip uninstall -y openpifpaf\npython -m pip install --no-build-isolation ../dist/openpifpaf*\npython -c ""import openpifpaf""\n', 'mkdir sdist_with_build_isolation_test_dir\ncd sdist_with_build_isolation_test_dir\npython -m pip uninstall -y openpifpaf\npython -m pip install ../dist/openpifpaf*\npython -c ""import openpifpaf""\n', 'git pull\ngit checkout stable\ngit merge main -m ""merge main""\ngit push\n']"
"['docker-compose up --build -d\n', 'python -m pip install --upgrade pip\npython -m pip install $REQUIREMENTS\npython -m pip install -r requirements-test.txt\n# step to test duckdb\n# TODO: move these requirements into the test matrix\npip install duckdb_engine\npython -m pip install .\n', 'make test-travis\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-test.txt\npython -m pip install pytest-parallel\npython -m pip install sqlalchemy-bigquery==1.4 pandas-gbq==0.17\npython -m pip install .\n', '# tests are mostly waiting on http requests to bigquery api\n# note that test backends can cache data, so more processes\n# is not always faster\npytest siuba -m bigquery --workers 2 --tests-per-worker 20\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-test.txt\npython -m pip install pytest-parallel\npython -m pip install snowflake-sqlalchemy\npython -m pip install .\n', 'pytest siuba -m snowflake --workers 2 --tests-per-worker 20\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\npython -m pip install .\n', 'make docs-build\n', 'pip install wheel\npython setup.py build sdist bdist_wheel\n', 'python -m pip install -e . \npython -m pip install airtable-python-wrapper==0.15.0\n', 'echo ""TODO: skipping upload for now""\n#python docs/scripts/upload_airtable.py\n', 'python -m pip install --upgrade pip\npython -m pip install psycopg2~=2.8.4\npython -m pip install -r requirements-test.txt\npython -m pip install .\n', 'make test-travis\n']"
"[""git config --global user.name 'github-actions[bot]'\ngit config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'\n"", 'python3 convert.py categories\n', 'python3 convert.py adguard\npython3 convert.py adguard_important\n', 'python3 convert.py pihole\n', 'python3 convert.py unbound\n', 'git add . && git commit -am ""Automated update\n- Categories lists update\n- AdGuard list update\n- Parsed list update\n- Unbound list update""\ngit push', 'pip install pytest', 'cd scripts\npython3 tests.py --type duplicates\n', 'pip install pytest', 'cd scripts\npython3 tests.py --type regex']"
"['python -m pip install --upgrade pip\npip install poetry\n', './build.sh\n']"
""
"['mkdir dist\necho ""::set-env name=VERSION::$(python scripts/get_version.py)""\necho ""Building branch ${{env.GITHUB_REF}} - version ${{env.VERSION}}""\n', 'sudo apt update\nsudo apt install libopengl0 freeglut3 freeglut3-dev libxcb-icccm4 libxkbcommon-x11-0 libxcb-xkb1 libxcb-render-util0 libxcb-randr0 libxcb-keysyms1 libxcb-image0 libxcb-shape0-dev libxcb-cursor-dev -y\n', 'python -m pip install --upgrade pip setuptools --ignore-installed\npip install .\npip install .[develop]\n', 'cp $(python -c ""import iso639; print(iso639.mapping.TABLE_PATH)"") iso-639-3.tab\ncp $(python -c ""import iso639; print(iso639.mapping.MAPPING_PATH)"") iso-639-3.json\n', 'pyinstaller FastFlix_Nix_OneFile.spec', 'cp docs/build-licenses.txt dist/LICENSE\n', 'chmod +x dist/FastFlix\ndist/FastFlix --version\ndist/FastFlix --test\n', 'mkdir dist\nNew-Item -Path Env: -Name VERSION -Value $(python.exe scripts\\get_version.py)\necho ""Building branch $env:GITHUB_REF - version $env:VERSION""\necho ""::set-env name=VERSION::$env:VERSION""\n', 'python -m pip install --upgrade pip setuptools --ignore-installed\npip install .\npip install .[develop]\n', 'copy $(python -c ""import iso639; print(iso639.mapping.TABLE_PATH)"") iso-639-3.tab\ncopy $(python -c ""import iso639; print(iso639.mapping.MAPPING_PATH)"") iso-639-3.json\n', 'pyinstaller FastFlix_Windows_OneFile.spec', 'pyinstaller FastFlix_Windows_Installer.spec', 'makensis.exe FastFlix.nsi\nmove FastFlix_installer.exe dist\\FastFlix_${{ env.VERSION }}_installer.exe\n', 'dist\\FastFlix.exe --version\ndist\\FastFlix.exe --test\n', 'move dist\\*.exe .\nmove docs\\build-licenses.txt LICENSE\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine build --upgrade\n', 'pip install .\npip install .[develop]\npython -m build --wheel\ntwine upload dist/*.whl\n', 'pip install black==23.1.0', 'python -m black --check .', 'sudo apt update\nsudo apt install libopengl0 freeglut3 freeglut3-dev -y\n', 'python -m pip install --upgrade pip setuptools\npip install .\npip install .[develop]\n', 'pytest tests']"
[]
"['make dev-setup', 'make black-check', 'make flake8', 'make isort', 'pip install -r requirements.txt -r requirements_build.txt\n', 'python -m PyInstaller tdmgr.spec\n', 'pip install -r requirements.txt -r requirements_build.txt\n', 'python -m PyInstaller tdmgr32.spec\n', 'pip install -r requirements.txt -r requirements_build.txt\n', 'python -m PyInstaller tdmgr.spec\n', 'ls dist', 'gzip dist/tdmgr_*', 'ls dist', 'pip install -r requirements.txt -r requirements_build.txt\nnpm install --global create-dmg\n', 'python -m PyInstaller tdmgr_mac.spec\n', 'ls dist', 'mv dist/tdmgr*.app dist/tdmgr-macOS.app\n', 'tar -cvf dist.tar dist/*']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\n', 'pip install -e "".[release]""\npython setup.py sdist\n', 'cd docs && make clean && make html\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\n', 'pip install -e "".[release]""\n', 'cd docs && make clean && make html\n', 'ghp-import -f -n -c pysimdjson.tkte.ch -p docs/_build/html\n', 'sudo apt-get install -y build-essential clang-6.0\n', ""python -m pip install --upgrade pip wheel\npip install cython\npip install -e '.[test]'\nrm simdjson/csimdjson.cpp\npython setup.py develop\n"", 'coverage run -m pytest\n', 'coverage report -m\n', 'flake8\n', 'sudo apt-get install -y build-essential clang-6.0\n', ""python -m pip install --upgrade pip wheel\npip install cython\npip install -e '.[test]'\nrm simdjson/csimdjson.cpp\npython setup.py develop\n"", 'pytest\n']"
"['nvm install', 'make init', 'tox --version\ntox\n']"
""
"['echo ""dir=$(npm config get cache)"" >> $GITHUB_OUTPUT\n', 'npm run install:all', 'npm run package\nmv pyright-*.vsix ${{ env.VSIX_NAME }}\n', 'unzip diff.zip', 'cat diff_*.txt | tee fulldiff.txt\n', 'python -m pip install -U pip\npip install git+https://github.com/hauntsaninja/mypy_primer.git\n', 'cd pyright_to_test\necho ""new commit""\ngit rev-list --format=%s --max-count=1 $GITHUB_SHA\n\nMERGE_BASE=$(git merge-base $GITHUB_SHA origin/$GITHUB_BASE_REF)\ngit checkout -b base_commit $MERGE_BASE\necho ""base commit""\ngit rev-list --format=%s --max-count=1 base_commit\n\necho \'\'\ncd ..\n# fail action if exit code isn\'t zero or one\n(\n  mypy_primer \\\n  --repo pyright_to_test \\\n  --type-checker pyright \\\n  --new $GITHUB_SHA --old base_commit \\\n  --num-shards 1 --shard-index ${{ matrix.shard-index }} \\\n  --debug \\\n  --output concise \\\n  | tee diff_${{ matrix.shard-index }}.txt\n) || [ $? -eq 1 ]\n', 'echo ${{ github.event.pull_request.number }} | tee pr_number.txt\n', 'python -m pip install -U pip\npip install git+https://github.com/hauntsaninja/mypy_primer.git\n', 'cd pyright_to_test\necho ""new commit""\ngit rev-list --format=%s --max-count=1 $GITHUB_SHA\n\ncd ..\n# fail action if exit code isn\'t zero or one\n(\n  mypy_primer \\\n  --repo pyright_to_test \\\n  --type-checker pyright \\\n  --new $GITHUB_SHA \\\n  --debug \\\n  --output concise \\\n  | tee diff.txt\n) || [ $? -eq 1 ]\n', 'npm install', 'npm install -g ""vsce@$(jq -r \'.dependencies.vsce.version\' < packages/vscode-pyright/package-lock.json)""\nnpx vsce --version\n', 'npx vsce publish --packagePath ${{ env.VSIX_NAME }} --pat ${{ secrets.VSCE_TOKEN }} --noVerify', 'echo ""::set-output name=dir::$(npm config get cache)""\n', 'npm install', 'npx lerna exec --stream --no-bail -- tsc --noEmit', 'echo ""::set-output name=dir::$(npm config get cache)""\n', 'npm run install:all', 'git diff --exit-code --name-only', 'npm run check', 'echo ""::set-output name=dir::$(npm config get cache)""\n', 'npm install', 'npm test', 'echo ""::set-output name=dir::$(npm config get cache)""\n', 'npm install', 'npm publish --dry-run', 'npm run package', 'echo All required jobs succeeded.']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine mock\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install torch==1.9.0+cpu torchvision==0.10.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\nmake install_dev\n', 'make flake8', 'make black', 'python -m pip install --upgrade pip\npip install torch==1.9.0+cpu torchvision==0.10.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\nmake install_dev\n', 'make flake8', 'make test']"
"['pip install --upgrade pip poetry', 'pip install bandit black codespell flake8 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety requests', 'bandit --recursive --skip B101 . || true', 'black --check . || true', 'codespell --skip=""./autorecon/wordlists""', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check -r requirements.txt', 'python3 autorecon.py 127.0.0.1 || true']"
['git checkout HEAD^2']
""
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python git_search.py\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python git_status.py\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python wiki_gen.py\n']"
"['pip install flake8\n# https://michaelcurrin.github.io/dev-cheatsheets/cheatsheets/python/linting/flake8.html\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --ignore=C901,W503,W504,E741 --statistics > current.txt\ngit fetch origin\ngit checkout origin/""$GITHUB_BASE_REF""\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --ignore=C901,W503,W504,E741 --statistics > base.txt\nif diff base.txt current.txt | grep ""^> ./""; then\n  false\nfi\n', 'set -x\npip install pylint\npip install --upgrade -r requirements.txt\n# TODO: donot ignore serialization.py\npylint --exit-zero --errors-only --ignore=serialization.py pocsuite3 > current.txt\ngit fetch origin\ngit checkout origin/""$GITHUB_BASE_REF""\npylint --exit-zero --errors-only --ignore=serialization.py pocsuite3 > base.txt\nif diff base.txt current.txt | grep ""^> ""; then\n  false\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\npip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'echo ""$AUR_SSH_KEY"" > ~/aur_ssh_key\nchmod 600 ~/aur_ssh_key\ngit config --global core.sshCommand ""ssh -i ~/aur_ssh_key -o \'StrictHostKeyChecking=no\'""\ngit clone ""aur@aur.archlinux.org:pocsuite3.git"" .\n', 'export VERSION=$(echo $GH_REF | sed \'s:refs/tags/v::\')\nsed -i ""s/^pkgver=.*\\$/pkgver=${VERSION}/g"" PKGBUILD\nsed -i ""s/^pkgrel=.*\\$/pkgrel=1/g"" PKGBUILD\n', 'export VERSION=$(echo $GH_REF | sed \'s:refs/tags/v::\')\ngit config --global user.email ""abcnsxyz@gmail.com""\ngit config --global user.name \'Tian Qiao\'\ngit commit -a -m ""Version ${VERSION} (automated version bump)""\ngit push origin master\n', ""export VERSION=$(echo $GH_REF | sed 's:refs/tags/v::')\ndocker build --build-arg version=${VERSION} \\\n--tag pocsuite3/pocsuite3:v${VERSION} \\\n--tag pocsuite3/pocsuite3:latest \\\n.\n"", 'docker push -a pocsuite3/pocsuite3\n', 'pip install --upgrade -r requirements.txt\npython setup.py install\npython test.py\n']"
"['python -m pip install --upgrade pip\npip install -e .[dev]\n', 'pip install pandas==${{ matrix.pandas-version }}', 'pytest --cov=pandarallel\n']"
"['sudo apt-get update -qq\nsudo apt-get install -qq unzip wget -y\nsudo apt-get install -qq dos2unix -y\nsudo apt-get install ffmpeg v4l2loopback-dkms v4l2loopback-utils linux-modules-extra-$(uname -r) -y\n', 'dos2unix scripts/bash/prepare_dataset.sh\nchmod +x scripts/bash/prepare_dataset.sh\ndos2unix scripts/bash/install_opencv.sh\nchmod +x scripts/bash/install_opencv.sh\n', 'pip install -U pip wheel numpy\npip install -U .[asyncio]\npip uninstall opencv-python -y\npip install -U flake8 six codecov httpx pytest pytest-asyncio pytest-cov yt_dlp mpegdash paramiko m3u8 async-asgi-testclient\npip install -U deffcode\npip install cryptography==38.0.4\n', 'bash scripts/bash/prepare_dataset.sh', 'bash scripts/bash/install_opencv.sh', 'pip install .[core]', 'mkdir -p $HOME/logs\nexport VIDGEAR_LOGFILE=""$HOME/logs""\ntimeout 1200 pytest --verbose --cov=vidgear --cov-report=xml  --cov-report term-missing vidgear/tests/ || code=$?; if [[ $code -ne 124 && $code -ne 0 ]]; then exit $code; else echo ""EXIT_CODE=$code"" >>$GITHUB_ENV; fi\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'sudo modprobe v4l2loopback devices=1 video_nr=0 exclusive_caps=1 card_label=\'VCamera\'\ntimeout 1200 pytest --verbose --cov=vidgear --cov-report=xml  --cov-report term-missing vidgear/tests/ || code=$?; if [[ $code -ne 124 && $code -ne 0 ]]; then exit $code; else echo ""EXIT_CODE=$code"" >>$GITHUB_ENV; fi\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'echo ""Exit code was = $EXIT_CODE""', 'python -m pip install --upgrade pip wheel setuptools\npip install .[asyncio]\n', 'pip install -U mkdocs mkdocs-material mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin mkdocs-exclude mike mkdocstrings mkdocstrings-python-legacy          \npip install jinja2==3.0.*\n', 'REMOTE=""https://${GIT_TOKEN}@github.com/${GITHUB_REPOSITORY}""\ngit config --global user.name ""${GIT_NAME}""\ngit config --global user.email ""${GIT_EMAIL}""\ngit remote set-url origin ${REMOTE}\n', 'echo ""RELEASE_NAME=$(python -c \'import vidgear; print(vidgear.__version__)\')"" >>$GITHUB_ENV\n', 'echo ""${{ env.NAME_RELEASE }}""\nmike deploy --push --update-aliases --no-redirect ${{ env.NAME_RELEASE }} ${{ env.RELEASE_NAME }} --title=${{ env.RELEASE_NAME }} \n', 'python -m pip install --upgrade pip wheel setuptools\npip install .[asyncio]\n', 'pip install -U mkdocs mkdocs-material mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin mkdocs-exclude mike mkdocstrings mkdocstrings-python-legacy\npip install jinja2==3.0.*\n', 'REMOTE=""https://${GIT_TOKEN}@github.com/${GITHUB_REPOSITORY}""\ngit config --global user.name ""${GIT_NAME}""\ngit config --global user.email ""${GIT_EMAIL}""\ngit remote set-url origin ${REMOTE}\n', 'echo ""RELEASE_NAME=$(python -c \'import vidgear; print(vidgear.__version__)\')"" >>$GITHUB_ENV\n', 'echo ""${{ env.NAME_STABLE }}""\nmike deploy --push --update-aliases --no-redirect ${{ env.NAME_STABLE }} latest --title=latest\nmike set-default --push latest\n', 'git checkout testing', 'python -m pip install --upgrade pip wheel setuptools\npip install .[asyncio]\n', 'pip install -U mkdocs mkdocs-material mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin mkdocs-exclude mike mkdocstrings mkdocstrings-python-legacy          \npip install jinja2==3.0.*\n', 'REMOTE=""https://${GIT_TOKEN}@github.com/${GITHUB_REPOSITORY}""\ngit config --global user.name ""${GIT_NAME}""\ngit config --global user.email ""${GIT_EMAIL}""\ngit remote set-url origin ${REMOTE}\n', 'echo ""RELEASE_NAME=$(python -c \'import vidgear; print(vidgear.__version__)\')"" >>$GITHUB_ENV\n', 'echo ""Releasing ${{ env.NAME_DEV }}""\nmike deploy --push --update-aliases --no-redirect ${{ env.NAME_DEV }} dev --title=dev\n']"
"['python -m pip install --upgrade pip\npip install -r .devcontainer/requirements-dev.txt\n', 'sudo apt-get install fping', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-line-length=300 --statistics\n', 'pip install pytest\npip install pytest-cov\npytest hassio-google-drive-backup/tests --junitxml=junit/test-results.xml --cov=hassio-google-drive-backup/backup --cov-report=xml --cov-report=html\n', 'docker build -f hassio-google-drive-backup/Dockerfile-server --tag $IMAGE_NAME --label ""runnumber=${GITHUB_RUN_ID}"" hassio-google-drive-backup/.', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=ghcr.io/${{ github.repository_owner }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\ndocker tag $IMAGE_NAME $IMAGE_ID:run-$GITHUB_RUN_NUMBER\ndocker push $IMAGE_ID:run-$GITHUB_RUN_NUMBER\n', 'python3 staging/update.py dev staging\n', 'python3 staging/update.py dev staging\n', 'cd staging\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""Updating staging addon config""\ngit push\n']"
"['if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  docker buildx build . --file agent.Dockerfile\nfi\n', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho VERSION=$VERSION >> $GITHUB_ENV\necho IMAGE_ID=$IMAGE_ID >> $GITHUB_ENV\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file alibiexplainer.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file alibiexplainer.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file artexplainer.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file artexplainer.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file custom_model_grpc.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file custom_model_grpc.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', './test/scripts/gh-actions/build-images.sh\ndocker image ls\ncat ./config/overlays/test/configmap/inferenceservice.yaml\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/check-poetry-lockfile.sh', './test/scripts/gh-actions/build-server-runtimes.sh predictor,transformer\ndocker image ls\ncat ./config/overlays/test/configmap/inferenceservice.yaml\n', './test/scripts/gh-actions/build-server-runtimes.sh explainer\ndocker image ls\ncat ./config/overlays/test/configmap/inferenceservice.yaml\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\n\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh ""fast or pmml or graph""\n\nkubectl get pods -n kserve\n', 'kubectl patch configmaps -n kserve inferenceservice-config --patch-file config/overlays/test/configmap/inferenceservice-ingress.yaml\nkubectl describe configmaps -n kserve inferenceservice-config\n', './test/scripts/gh-actions/run-e2e-tests.sh fast\n\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\n\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh slow\n\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\n\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh explainer\n\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\n\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh ""transformer or mms""\n\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/build-qpext-image.sh\ndocker image ls\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', 'kubectl patch configmaps -n knative-serving config-deployment --patch \'{""data"": {""queue-sidecar-image"": ""kserve/qpext:${{ github.sha }}""}}\'\nkubectl describe configmaps -n knative-serving config-deployment\n', './test/scripts/gh-actions/run-qpext-test.sh\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh ""grpc""\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-modelmesh-dep.sh\n./test/scripts/gh-actions/setup-kserve-helm.sh\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh ""helm""\nkubectl get pods -n kserve\n', 'kubectl get pods --all-namespaces\n./test/scripts/gh-actions/status-check.sh\n', './test/scripts/gh-actions/setup-poetry.sh', './test/scripts/gh-actions/setup-kserve.sh\n\nkubectl get pods -n kserve\nkubectl describe pods -n kserve\n', './test/scripts/gh-actions/run-e2e-tests.sh raw\n\nkubectl get pods -n kserve\n', './test/scripts/gh-actions/status-check.sh\n', 'go get -v -t -d ./...\n', 'make fmt\n', ""export GOPATH=/home/runner/go\nexport PATH=$PATH:/usr/local/kubebuilder/bin:/home/runner/go/bin\nwget -O $GOPATH/bin/yq https://github.com/mikefarah/yq/releases/download/v4.28.1/yq_linux_amd64\nchmod +x $GOPATH/bin/yq\nmake test\n./coverage.sh\necho ::set-output name=coverage::$(./coverage.sh | tr -s '\\t' | cut -d$'\\t' -f 3)\n"", 'echo ""Coverage output is ${{ steps.test.outputs.coverage }}""\n', 'for dir in ./*/; do helm package $dir; done', 'for filename in *.tgz; do mv ""$filename"" ""helm-chart-$filename""; done', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  docker buildx build . --file Dockerfile\nfi\n', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho VERSION=$VERSION >> $GITHUB_ENV\necho IMAGE_ID=$IMAGE_ID >> $GITHUB_ENV\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file lgb.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file lgb.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file paddle.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file paddle.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file pmml.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file pmml.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', './test/scripts/gh-actions/setup-poetry.sh', 'cd python/kserve\npoetry build\npoetry publish\n', './test/scripts/gh-actions/setup-poetry.sh', 'cd python/kserve\nmake install_dependencies\n', 'cd python/kserve\nmake dev_install\n', 'cd python\nsource kserve/.venv/bin/activate\npytest --cov=kserve ./kserve\n', 'cd python/sklearnserver\nmake install_dependencies\n', 'cd python/sklearnserver\nmake dev_install\n', 'cd python\nsource sklearnserver/.venv/bin/activate\npytest --cov=sklearnserver ./sklearnserver\n', 'cd python/xgbserver\nmake install_dependencies\n', 'cd python/xgbserver\nmake dev_install\n', 'cd python\nsource xgbserver/.venv/bin/activate\npytest --cov=xgbserver ./xgbserver\n', 'cd python/pmmlserver\nmake install_dependencies\n', 'cd python/pmmlserver\nmake dev_install\n', 'cd python\nsource pmmlserver/.venv/bin/activate\npytest --cov=pmmlserver ./pmmlserver\n', 'cd python/lgbserver\nmake install_dependencies\n', 'cd python/lgbserver\nmake dev_install\n', 'cd python\nsource lgbserver/.venv/bin/activate\npytest --cov=lgbserver ./lgbserver\n', 'echo ""python version ${{ steps.setup-python.outputs.python-version }}""\ncd python/paddleserver\nmake install_dependencies\n', 'cd python/paddleserver\nmake dev_install\n', 'cd python\nsource paddleserver/.venv/bin/activate\npytest --cov=paddleserver ./paddleserver\n', 'cd python/alibiexplainer\nmake install_dependencies\n', 'cd python/alibiexplainer\nmake dev_install\n', 'cd python\nsource alibiexplainer/.venv/bin/activate\npytest --cov=alibiexplainer ./alibiexplainer\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd qpext\n  docker buildx build . --file qpext.Dockerfile\nfi\n', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho VERSION=$VERSION >> $GITHUB_ENV\necho IMAGE_ID=$IMAGE_ID >> $GITHUB_ENV\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  docker buildx build . --file router.Dockerfile\nfi\n', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho VERSION=$VERSION >> $GITHUB_ENV\necho IMAGE_ID=$IMAGE_ID >> $GITHUB_ENV\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file sklearn.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file sklearn.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file storage-initializer.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file storage-initializer.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  docker buildx build . --file tools/tf2openapi/Dockerfile\nfi\n', 'docker buildx build . --file tools/tf2openapi/Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file custom_transformer.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file custom_transformer.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file custom_transformer_grpc.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file custom_transformer_grpc.Dockerfile --tag $GRPC_IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$GRPC_IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $GRPC_IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  cd python\n  docker buildx build . --file xgb.Dockerfile\nfi\n', 'cd python\ndocker buildx build . --file xgb.Dockerfile --tag $IMAGE_NAME\n', 'docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASSWORD }}', 'IMAGE_ID=kserve/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n# [[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n']"
"['python -m pip install --upgrade pip', 'pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu', 'pip install .[${{ matrix.pytorch-toolbelt-version }}]', 'pip install flake8==5', 'pytest', 'flake8', 'python -m pip install --upgrade pip', 'pip install black==22.8.0', 'black --config=pyproject.toml --check .', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --progress-bar off --upgrade pip\npip install --progress-bar off torch==${{ matrix.torch-version }}\npip install --progress-bar off .[extras]\nif [[ ${{ matrix.torch-version }} == ""1.4.0"" ]]; then pip install pylint==2.4.4 flake8==3.7.9; fi\nif [[ ${{ matrix.torch-version }} != ""1.0.1"" ]]; then pip install mypy==0.790; fi\npip install pytest\npip install --progress-bar off coverage codecov\npip install --progress-bar off --upgrade numpy==${{ matrix.numpy-version }}\n', 'if [[ ${{ matrix.torch-version }} == ""1.4.0"" ]]; then pylint texar/ examples/; fi\nif [[ ${{ matrix.torch-version }} == ""1.4.0"" ]]; then flake8 texar/ examples/; fi\n', 'if [[ ${{ matrix.numpy-version }} == ""1.21"" && ${{ matrix.torch-version }} != ""1.0.1"" && ${{ matrix.torch-version }} != ""1.7.1"" && ${{ matrix.torch-version }} != ""1.8.1"" ]]; then mypy .; fi\nif [[ ${{ matrix.numpy-version }} == ""1.21"" && ${{ matrix.torch-version }} != ""1.0.1"" && ${{ matrix.torch-version }} != ""1.7.1"" && ${{ matrix.torch-version }} != ""1.8.1"" ]]; then _rc=0; for dir in `echo examples/**/`; do mypy $dir || _rc=$?; done && [[ $_rc == 0 ]]; fi\n', 'coverage run -m pytest\n', 'codecov\n', 'python -m pip install --progress-bar off --upgrade pip\npip install --progress-bar off -r requirements.txt\npip install --progress-bar off .[extras]\npip install --progress-bar off -r docs/requirements.txt\npip install --progress-bar off --upgrade numpy==1.21\n', 'cd docs\nsphinx-build -W -b html -d _build/doctrees . _build/html\nsphinx-build -W -b spelling -d _build/doctrees . _build/spelling\n', 'python -m pip install build --user\n', 'pwd\nls\n', 'python -m build --sdist --wheel --outdir dist/ .\n']"
"['python -m pip install --upgrade pip setuptools wheel\npip install -r requirements.txt\npip install certifi pyinstaller --no-binary pyinstaller\n', 'pyinstaller -y -F -i CurseBreaker.ico -n CurseBreaker -c --noupx --exclude-module FixTk --exclude-module tcl --exclude-module tk --exclude-module _tkinter --exclude-module tkinter --exclude-module Tkinter --additional-hooks-dir=. CurseBreaker.py', 'python -m pip install --upgrade pip setuptools wheel\npip install -r requirements.txt\npip install certifi pyinstaller\n', 'pyinstaller -y -F -n CurseBreaker -c -s --noupx --exclude-module FixTk --exclude-module tcl --exclude-module tk --exclude-module _tkinter --exclude-module tkinter --exclude-module Tkinter --additional-hooks-dir=. CurseBreaker.py', 'python3 -m pip install --upgrade pip setuptools wheel\npip3 install -r requirements.txt\npip3 install certifi pyinstaller\n', 'pyinstaller -y -F -n CurseBreaker -c -s --noupx --exclude-module FixTk --exclude-module tcl --exclude-module tk --exclude-module _tkinter --exclude-module tkinter --exclude-module Tkinter --additional-hooks-dir=. CurseBreaker.py']"
""
[]
"['pip install -r dev-requirements.txt', 'git config user.name ""GitHub Actions""\ngit config user.email ""actions@users.noreply.github.com""\n', 'coverage run -m pytest', 'make lint', 'coverage run -a ./scripts/update.py --release', 'coverage run -a ./scripts/update.py']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install -U pip', 'python -m pip install . pytest pytest-mock ja-ginza ja-ginza-electra', 'pytest']"
"['pip install tensorflow-cpu==2.11.1\npip install -e .[lint,test]\n', 'flake8', ""black --check --target-version py37 --exclude 'build/|\\.mypy_cache/|\\.venv/|env/|larq/snapshots/|.pytype/' ."", 'isort . --check --diff', 'pytype --jobs auto', 'pip install --upgrade setuptools wheel', 'python setup.py sdist bdist_wheel', ""pip install 'protobuf < 3.20'\npip install tensorflow-cpu==${{matrix.tf-version}} || pip install tensorflow==${{matrix.tf-version}}\npip install -e .[test]\n"", 'pytest . -n auto --cov=larq --cov-report=xml --cov-config=.coveragerc', 'bash <(curl -s https://codecov.io/bash) -f ./coverage.xml -F unittests']"
"['./scripts/py_dep_install.sh\n', './lintme\n', 'mindmeld num-parse\n', 'mkdir ~/test-reports\npytest --junitxml=~/test-reports/junit.xml --cov-report html --cov=mindmeld --ignore=examples/\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install coveralls\n', 'coverage run --source unicorn_binance_websocket_api unittest_binance_websocket_api.py']"
"['echo ""${DOCKER_PASSWORD}"" | docker login --username ${DOCKER_USERNAME} --password-stdin\n', 'docker buildx build \\\n--platform linux/amd64,linux/arm64 \\\n--output ""type=image,push=true"" \\\n--file ./Dockerfile . \\\n--tag $(echo ""${DOCKER_USERNAME}"" | tr \'[:upper:]\' \'[:lower:]\')/webmonitor:latest\n', 'docker buildx build \\\n--platform=linux/amd64 \\\n--output ""type=image,push=false"" \\\n--file ./Dockerfile .\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m unittest discover tests']"
"['node --version', 'npm ci', 'npm install --global rollup', 'rollup -c']"
"['pip install build', 'python -m build', 'pip install --user ruff', 'ruff --format=github .', 'pip install -e .[dev]', 'pytest test']"
"['python -m pip install ./ --user\n', 'pip install -U jsoncomparison', 'PYTHONPATH=$(pwd) python3 ci/generate_combinations.py --write --no-tqdm', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'pip install -U pip\npip install tox tox-gh-actions coveralls coverage[toml]\n', '! python2 -m aqt help\n[[ $(python2 -m aqt help) == ""aqtinstall requires python 3!"" ]]\n', 'tox', 'coveralls --service=github', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'python -m pip install build twine --user', 'python -m build ./', 'python -m twine check dist/*', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'python -m pip install tox-gh-actions build\ntox\n', 'python -m pip install ./ --user\n', 'python -m venv venv\nsource venv/bin/activate\npython -m pip install -U pip wheel setuptools setuptools_scm pyinstaller\npython -m pip install .\npython tools/build_standalone.py\ndeactivate\nrm -rf venv\n', 'python -m venv venv\nvenv/Scripts/activate.ps1\npython -m pip install -U pip wheel setuptools setuptools_scm pyinstaller\npython -m pip install .\npython tools/build_standalone.py\ndeactivate\nRemove-Item venv -Recurse -Force\n', 'import os\nimport pathlib\nimport subprocess\ntimeout = 300\nos.mkdir(""Qt"")\nos.chdir(""Qt"")\nartifact = ""${{ matrix.artifact }}""\nplatform = ""${{ matrix.os }}""\nqtver = ""${{ matrix.qtver }}""\nenv = os.environ.copy()\ngithub_workspace = pathlib.Path(env[""GITHUB_WORKSPACE""])\nif artifact == ""binary"":\n  if platform.startswith(""windows""):\n    bin_path = str(github_workspace / ""dist"" / ""aqt.exe"")\n  else:\n    bin_path = (github_workspace / ""dist"" / ""aqt"").as_posix()\n  prefix = [bin_path, ""install""]\nelse:\n  prefix = [""python"", ""-m"", ""aqt"", ""install""]\ncommand_line = []\ncommand_line.extend(prefix)\nif platform == ""windows-latest"":\n  if qtver.startswith(\'5.15\'):\n    args = [qtver, ""windows"", ""desktop"", ""win64_msvc2019_64""]\n  elif qtver.startswith(\'5.14\'):\n    args = [qtver, ""windows"", ""desktop"", ""win64_msvc2017_64""]\n  elif qtver.startswith(\'6\'):\n    args = [qtver, ""windows"", ""desktop"", ""win64_mingw81""]\n  else:\n    args = [qtver, ""windows"", ""desktop"", ""win64_msvc2015_64""]\nelif platform == ""macOS-latest"":\n  args = [qtver, ""mac"", ""desktop"", ""clang_64""]\nelse:\n  args = [qtver, ""linux"", ""desktop"", ""gcc_64""]\ncommand_line.extend(args)\ncommand_line.extend([""--archives"", ""qtbase"", ""icu"", ""qt""])\nenv[""AQT_CONFIG""] = (github_workspace / ""ci"" / ""settings.ini"").as_posix()\nenv[""LOG_CFG""] = (github_workspace / ""ci"" / ""logging.ini"").as_posix()\nprint(""Execute: {}"".format(command_line))\ntry:\n  res = subprocess.run(command_line, timeout=timeout, check=True, env=env)\nexcept subprocess.CalledProcessError as cpe:\n  exit(cpe.returncode)\nassert res.returncode == 0\nif qtver.startswith(\'6\'):\n  command_line6 = []\n  command_line6.extend(prefix)\n  if platform == \'ubuntu-20.04\':\n    command_line6.extend([qtver, ""linux"", ""android"", ""android_armv7""])\n    timeout = 360\n  elif platform == ""macOS-latest"":\n    command_line6.extend([qtver, ""mac"", ""ios"", ""ios""])\n    timeout = 360\n  else:\n    command_line6.extend([qtver, ""windows"", ""android"", ""android_armv7""])\n    timeout = 360\n  try:\n    res = subprocess.run(command_line6, timeout=timeout, check=True)\n  except subprocess.CalledProcessError as cpe:\n    exit(cpe.returncode)\n  assert res.returncode == 0\n', 'import os\nimport pathlib\nfrom subprocess import CalledProcessError, PIPE, run\nos.chdir(""Qt"")\nplatform = ""${{ matrix.os }}""\nqtver = ""${{ matrix.qtver }}""\nif platform == ""windows-latest"":\n  if qtver.startswith(\'5.15\'):\n    arch_dir = \'msvc2019_64\'\n  elif qtver.startswith(\'5.14\'):\n    arch_dir = \'msvc2017_64\'\n  elif qtver.startswith(\'6\'):\n    arch_dir = \'mingw81_64\'\n  else:\n    arch_dir = \'msvc2015_64\'\nelif platform == ""macOS-latest"":\n  arch_dir = \'clang_64\'\nelse:\n  arch_dir = \'gcc_64\'\ntry:\n  res = run([f""{qtver}/{arch_dir}/bin/qmake"", ""-query""], timeout=15, check=True, stdout=PIPE)\nexcept CalledProcessError as cpe:\n  exit(cpe.returncode)\nif res.returncode == 0:\n  qt_prefix_path = pathlib.Path.cwd() / qtver / arch_dir\n  for line in res.stdout.splitlines():\n    if line.startswith(b\'QT_INSTALL_PREFIX\'):\n      result = line[18:].decode(\'UTF-8\')\n      assert qt_prefix_path.samefile(result)\n  print(\'PREFIX {}\'.format(result))\n  if qtver.startswith(\'6\'):\n    if platform == ""windows-latest"" and qtver.startswith(\'6\'):\n      qmake = os.path.join(qtver, \'android_armv7\', \'bin\', \'qmake.bat\')\n    elif platform == ""macOS-latest"" and qtver.startswith(\'6\'):\n      qmake = os.path.join(qtver, \'ios\', \'bin\', \'qmake\')\n    else:\n      qmake = os.path.join(qtver, \'android_armv7\', \'bin\', \'qmake\')\n    try:\n      res = run([qmake, ""-query""], timeout=15, check=True, stdout=PIPE)\n    except CalledProcessError as cpe:\n      exit(cpe.returncode)\n    assert res.returncode == 0\n    for line in res.stdout.splitlines():\n      if line.startswith(b\'QT_INSTALL_PREFIX\'):\n        result = line[18:].decode(\'UTF-8\')\n    print(\'PREFIX {}\'.format(result))\n', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'python -m venv venv\nvenv/Scripts/activate.ps1\npython -m pip install -U pip wheel setuptools setuptools_scm pyinstaller\npython -m pip install .\npython tools/build_standalone.py ${{ matrix.arch }}\ndeactivate\nRemove-Item venv -Recurse -Force\n']"
"['python -m pip install urllib3 && python -m pip install -r requirements.txt --no-cache-dir', ""python .github/workflows/license_checker_v2.py --dependencies $(cut -d '=' -f 1 <<< $(pip freeze))"", 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 neuraxle testing_neuraxle --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 neuraxle testing_neuraxle --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python setup.py test\n']"
"['mdbook build', 'poetry install', 'poetry run pytest --cov=./\n', 'rustup component add clippy']"
"['TASKS=$(echo $(cat .github/workflows/matrix.json) | sed \'s/ //g\' )\necho ""matrix=$TASKS"" >> $GITHUB_OUTPUT\n', 'which python\npython --version\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\nwhich pip\npip --version\n', 'python -m pip install -U pip setuptools wheel\npython -m pip install -U cachetools pefile\n', 'python setup.py sdist', 'dir dist\nGet-ChildItem dist -File | Foreach-Object {$sdistname = $_.Name}\necho $sdistname\necho ""SDISTNAME=$sdistname"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'which python\npython --version\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\nwhich pip\npip --version\n', 'python -m pip install -U pip setuptools wheel\npython -m pip install -U cachetools pefile\n', 'python setup.py bdist_wheel\n', 'dir dist\n$ver = (findstr __version__ .\\py2exe\\version.py).split(""\'"")[1]\necho $ver\necho ""VER=$ver"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n$wheelname = python .github\\helpers\\get_wheel_name.py $ver\necho $wheelname\necho ""WHEELNAME=$wheelname"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'which python\npython --version\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\nwhich pip\npip --version\n', 'python -m pip install -U pip setuptools wheel\n', '$ver = (findstr __version__ .\\py2exe\\version.py).split(""\'"")[1]\necho $ver\necho ""VER=$ver"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n$wheelname = python .github\\helpers\\get_wheel_name.py $ver\necho $wheelname\necho ""WHEELNAME=$wheelname"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'dir\npip install ${{ env.WHEELNAME }}\npython -c ""import py2exe""\n', 'cd tests\npython test_modulefinder.py\n', 'cd tests\ncd functional\npython .\\enable_tests.py\n.\\runner_freeze.ps1\n']"
"['GITEE_GIT_ADDR=""git@gitee.com:Sipeed/maixpy_scripts.git""\ngit fetch --unshallow\nSSHPATH=""$HOME/.ssh""\nrm -rf ""$SSHPATH""\nmkdir -p ""$SSHPATH""\necho ""${{ secrets.GITEE_SYNC_ACCESSS_KEY }}"" > ""$SSHPATH/id_rsa""\nchmod 600 ""$SSHPATH/id_rsa""\nsudo sh -c ""echo StrictHostKeyChecking no >>/etc/ssh/ssh_config""\ngit remote add upstream $GITEE_GIT_ADDR\ngit push upstream --all --force\n']"
"['scripts/install', 'scripts/build', 'scripts/publish', 'scripts/install', 'scripts/check', 'scripts/build', 'scripts/test', 'scripts/coverage']"
"['echo ${{ env.GIT_DIFF }}\necho ""::set-output name=is_set::true""\n', 'source $CONDA/etc/profile.d/conda.sh\nconda activate hummingbot\npre-commit run --files $(git diff --name-only origin/$GITHUB_BASE_REF)\n', 'source $CONDA/etc/profile.d/conda.sh\nconda activate hummingbot\nmake test\n', 'source $CONDA/etc/profile.d/conda.sh\nconda activate hummingbot\nmake report_coverage\n', 'source $CONDA/etc/profile.d/conda.sh\nconda activate hummingbot\ngit fetch --all -q\ngit checkout -b $GITHUB_SHA\ncoverage xml\ndiff-cover --compare-branch=origin/$GITHUB_BASE_REF --fail-under=80 coverage.xml\n', 'git clone --depth 1 https://github.com/DiscordHooks/github-actions-discord-webhook.git webhook\nbash webhook/send.sh $JOB_STATUS $WEBHOOK_URL\n']"
""
"['echo ::set-output name=dir::$(pip cache dir)\n', 'git fetch origin ""$GITHUB_BASE_REF""', 'npm install -g npm@^7 pyright\n', 'pip install .\npip install pre-commit\n', 'pre-commit run --all-files', 'git diff --name-only --diff-filter=AM ""origin/$GITHUB_BASE_REF"" -z -- \'*.py{,i}\' | xargs -0 --no-run-if-empty pyright', 'buf lint --config ""src/bentoml/grpc/buf.yaml"" --error-format msvs src\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install -r requirements/docs-requirements.txt\n', 'sudo apt-get update && sudo apt-get install -y libenchant-2-dev\n', 'make spellcheck-docs', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install "".[grpc]""\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov=src/bentoml --cov-append)\nif [ ""${{ matrix.os }}"" != \'windows-latest\' ]; then\n  # we will use pytest-xdist to improve tests run-time.\n  OPTS=(${OPTS[@]} --dist loadfile -n auto --run-grpc-tests)\nfi\n# Now run the unit tests\ncoverage run -m pytest tests/unit ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', ""pip install -e '.[monitor-otlp]' -r requirements/tests-requirements.txt\n"", 'pip install -r requirements.txt\nmkdir -p monitoring && pytest . -vvv --capture=tee-sys\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install -r requirements/tests-requirements.txt\nif [ ""${{ matrix.server_type }}"" == \'grpc\' ]; then\n  pip install -e "".[grpc]""\nelse\n  pip install -e .\nfi\nif [ -f ""tests/e2e/bento_server_${{ matrix.server_type }}/requirements.txt"" ]; then\n  pip install -r tests/e2e/bento_server_${{ matrix.server_type }}/requirements.txt\nfi\n', 'OPTS=(--cov-config pyproject.toml --cov=src/bentoml --cov-append)\ncoverage run -m pytest tests/e2e/bento_server_${{ matrix.server_type }} ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install catboost\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework catboost)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install diffusers torch transformers\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework diffusers)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'sudo apt-get install -y git\n\npip install .\npip install torch requests\npip install git+https://github.com/facebookresearch/detectron2.git\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework detectron)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install easyocr torch requests Pillow\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework easyocr)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install flax jax jaxlib chex tensorflow\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework flax)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install fastai ""torch<1.12"" torchvision pandas scikit-learn\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework fastai)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py tests/integration/frameworks/test_fastai_unit.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install ""keras<2.12"" ""tensorflow>=2.7.3""\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework keras)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install lightgbm\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework lightgbm)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install mlflow scikit-learn\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append)\ncoverage run -m pytest tests/integration/frameworks/mlflow ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install onnx onnxruntime skl2onnx\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework onnx)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework picklable_model)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install torch torchvision\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework pytorch)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py tests/integration/frameworks/test_pytorch_unit.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install torch torchvision lightning\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework pytorch_lightning)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install torch torchvision\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework torchscript)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install joblib scikit-learn\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework sklearn)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install ""tensorflow>=2.7.3""\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework tensorflow)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py tests/integration/frameworks/test_tensorflow_unit.py ""${OPTS[@]}""\n', 'coverage xml', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --disable-tf-eager-execution --cov-report=xml:no_eager_execution.xml)\ncoverage run -m pytest tests/integration/frameworks/test_tensorflow_unit.py ""${OPTS[@]}""\n', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install tensorflow tensorflow_hub datasets transformers jax jaxlib flax torch\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework transformers)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py tests/integration/frameworks/test_transformers_unit.py ""${OPTS[@]}""\n', 'coverage xml', 'echo ::set-output name=dir::$(pip cache dir)\n', 'pip install .\npip install xgboost\npip install -r requirements/tests-requirements.txt\n', 'OPTS=(--cov-config pyproject.toml --cov src/bentoml --cov-append --framework xgboost)\ncoverage run -m pytest tests/integration/frameworks/test_frameworks.py ""${OPTS[@]}""\n']"
"['pip install mkdocs mkdocs-material\ncd mkdocs\nsh build_docs.sh\n', 'python -m pip install --upgrade pip\npip install ""cython>=0.29""\npip install -e "".[tests, docs]""\n', 'pip install pytest\npytest -vs --show-capture=no tests']"
""
"['pip install -r requirements.txt\n', 'python preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path bert_dataset.pt --processes_num 8 --seq_length 64 --data_processor bert\npython pretrain.py --dataset_path bert_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/bert_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2\nmv models/bert_model.bin-10 models/bert_model.bin\npython preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path roberta_dataset.pt --processes_num 8 --dynamic_masking --seq_length 64 --data_processor mlm\npython pretrain.py --dataset_path roberta_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/roberta_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2 --data_processor mlm --target mlm\nmv models/roberta_model.bin-10 models/roberta_model.bin\npython preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path albert_dataset.pt --processes_num 8 --seq_length 64 --data_processor albert\npython pretrain.py --dataset_path albert_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/albert/base_config.json --output_model_path models/albert_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2\nmv models/albert_model.bin-10 models/albert_model.bin\npython preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path gpt2_dataset.pt --processes_num 8 --seq_length 64 --data_processor lm\npython pretrain.py --dataset_path gpt2_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/gpt2/config.json --output_model_path models/gpt2_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2\nmv models/gpt2_model.bin-10 models/gpt2_model.bin\npython preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path spanbert_dataset.pt --processes_num 8 --dynamic_masking --span_masking --seq_length 64 --data_processor mlm\npython pretrain.py --dataset_path spanbert_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/spanbert_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2 --data_processor mlm --target mlm\nmv models/spanbert_model.bin-10 models/spanbert_model.bin\npython preprocess.py --corpus_path corpora/book_review_cls.txt --vocab_path models/google_zh_vocab.txt --dataset_path cls_dataset.pt --processes_num 8 --seq_length 64 --data_processor cls\npython pretrain.py --dataset_path cls_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/cls_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2 --labels_num 2 --data_processor cls --target cls\nmv models/cls_model.bin-10 models/cls_model.bin\npython preprocess.py --corpus_path corpora/parallel_corpus_en_zh.txt --vocab_path models/google_uncased_en_vocab.txt --tgt_vocab_path models/google_zh_vocab.txt --dataset_path mt_dataset.pt --processes_num 8 --seq_length 64 --tgt_seq_length 64 --data_processor mt\npython pretrain.py --dataset_path mt_dataset.pt --vocab_path models/google_uncased_en_vocab.txt --tgt_vocab_path models/google_zh_vocab.txt --config_path models/encoder_decoder_config.json --output_model_path models/mt_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2\nmv models/mt_model.bin-10 models/mt_model.bin\npython preprocess.py --corpus_path corpora/CLUECorpusSmall_5000_lines_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path pegasus_dataset.pt --processes_num 8 --seq_length 128 --tgt_seq_length 128 --dup_factor 1 --sentence_selection_strategy random --data_processor gsg\npython pretrain.py --dataset_path pegasus_dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/pegasus/base_config.json --output_model_path models/pegasus_model.bin --total_steps 10 --save_checkpoint_steps 10 --report_steps 2 --batch_size 2\nmv models/pegasus_model.bin-10 models/pegasus_model.bin\npython finetune/run_classifier.py --pretrained_model_path models/bert_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/classifier_model.bin --train_path datasets/test_data/chnsenticorp_test/train.tsv --dev_path datasets/test_data/chnsenticorp_test/dev.tsv --epochs_num 3 --batch_size 2\npython inference/run_classifier_infer.py --load_model_path models/classifier_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --test_path datasets/test_data/chnsenticorp_test/test_nolabel.tsv --prediction_path datasets/test_data/chnsenticorp_test/prediction.tsv --labels_num 2\npython finetune/run_classifier.py --pretrained_model_path models/albert_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/albert/base_config.json --output_model_path models/classifier_model.bin --train_path datasets/test_data/chnsenticorp_test/train.tsv --dev_path datasets/test_data/chnsenticorp_test/dev.tsv --learning_rate 4e-5 --epochs_num 3 --batch_size 2\npython finetune/run_classifier_mt.py --pretrained_model_path models/bert_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --dataset_path_list datasets/test_data/douban_test/ datasets/test_data/chnsenticorp_test/ --epochs_num 1 --batch_size 2\npython finetune/run_ner.py --pretrained_model_path models/bert_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/ner_model.bin --train_path datasets/test_data/msra_ner_test/train.tsv --dev_path datasets/test_data/msra_ner_test/dev.tsv --label2id_path datasets/msra_ner/label2id.json --epochs_num 2 --batch_size 2\npython inference/run_ner_infer.py --load_model_path models/ner_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --test_path datasets/test_data/msra_ner_test/test_nolabel.tsv --prediction_path datasets/test_data/msra_ner_test/prediction.tsv --label2id_path datasets/msra_ner/label2id.json\npython finetune/run_cmrc.py --pretrained_model_path models/bert_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --output_model_path models/cmrc_model.bin --train_path datasets/test_data/cmrc_test/train.json --dev_path datasets/test_data/cmrc_test/dev.json --epochs_num 2 --batch_size 2 --seq_length 128\npython inference/run_cmrc_infer.py --load_model_path models/cmrc_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/bert/mini_config.json --test_path datasets/test_data/cmrc_test/test.json --prediction_path datasets/test_data/cmrc_test/prediction.json --seq_length 128\n']"
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pip install pytest\npip install pytest-cov\npytest ./tests --cov-config=.coveragerc --cov-report=xml --cov=./src/zvt --ignore=tests/recorders/ --ignore=tests/domain/\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements_travis.txt\npip install pylint pytest-cov codecov\n', 'pylint tensornetwork\n', 'pytest --cov=./\n', 'codecov\n']"
"['echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'echo ""$HOME/Library/Python/${{ matrix.python-version }}/bin"" >> $GITHUB_PATH', 'bash .ci/run', 'docker build -f .ci/end2end_tests.Dockerfile . -t promnesia_end2end_tests\ndocker run -v /dev/shm:/dev/shm -e CI promnesia_end2end_tests -- -k chrome\n', 'docker build -f .ci/end2end_tests.Dockerfile . -t promnesia_end2end_tests\ndocker run -v /dev/shm:/dev/shm -e CI promnesia_end2end_tests -- -k firefox\n', '.ci/github-ci-compat', '# --use-pep517 is a work around for some breaking change in setuptools 66.0.0\n# see https://github.com/pypa/setuptools/issues/3772#issuecomment-1384671296\npython3 -m pip install --use-pep517 .\nexport PATH=.ci/fake-systemd:$PATH\ntests/install_and_run\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'pip3 install --user wheel twine && .ci/release --test', 'pip3 install --user wheel twine && .ci/release', 'extension/.ci/build --lint', 'extension/.ci/build --lint --release']"
"['conda install pytorch=${{ matrix.pytorch-version }} torchvision cpuonly python=${{ matrix.python-version }} -c pytorch', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install pytest pytest-cov\npytest --cov . --cov-report=xml\n', '# Replace `linux` below with the appropriate OS\n# Options are `alpine`, `linux`, `macos`, `windows`\ncurl -Os https://uploader.codecov.io/latest/linux/codecov\nchmod +x codecov\n./codecov -t ${CODECOV_TOKEN}']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install poetry\npoetry install\n', 'poetry run tox\n']"
"['nix-shell --run ""poetry install""', 'nix-shell --run ""poetry run make style_check""', 'nix-shell --run ""poetry install""', 'nix-shell --run ""poetry run make defs_check""', 'nix-shell --run ""poetry install""', 'nix-shell --run ""poetry run make gen_check""', './ci/check_changelog.sh']"
[]
"['python -m pip install --upgrade pip setuptools wheel', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install -e "".""', 'sudo apt-get update\nsudo apt-get install libcurl4-gnutls-dev libgnutls28-dev\n', 'python -m pip install -e "".[all]""', 'python -m pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install pytest pytest-cov\npytest --cov=./ --cov-report=xml\n']"
""
"['python -m pip install --upgrade pip wheel\npython -m pip install --upgrade flake8 wemake-python-styleguide\npython -m pip install --upgrade flake8-quotes\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'flake8 . --inline-quotes \'""\' --count --exit-zero --max-complexity=15 --max-line-length=90 --statistics --select=I,P,WPS305,C812,E203,W503,E800\n', 'flake8 . --inline-quotes \'""\' --count --exit-zero --max-complexity=15 --max-line-length=90 --statistics --select=D,DAR\n', 'echo ""Some stuff may not be used, but is used in commented out code.""\necho ""Make sure you check with the find command before you remove anything!""\nflake8 . --inline-quotes \'""\' --count --exit-zero --max-complexity=15 --max-line-length=90 --statistics --select=F\necho ""Some stuff may not be used, but is used in commented out code.""\necho ""Make sure you check with the find command before you remove anything!""\n', 'flake8 . --inline-quotes \'""\' --count --exit-zero --max-complexity=15 --max-line-length=90 --statistics --ignore=I,P,WPS305,C812,E203,W503,E800,D,DAR,F\n']"
"['python -m pip install --upgrade pre-commit\n', 'pre-commit run --show-diff-on-failure --files  skmob/tessellation/*', 'python -m pip install --upgrade pip\npython -m pip install --upgrade poetry tox tox-gh-actions\n', 'python3 -m poetry run tox\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade poetry\npoetry install\n', 'poetry run sphinx-build docs static', '\nmv .nojekyll static\nmv logo_skmob.png static\nmkdir static/coverage\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
""
""
"['pip install --upgrade pip\npip install --upgrade pytest coverage  # pytest-cov\npip install --upgrade importlib-metadata  # Solves a python 3.7 install bug\n', 'pip install .\npip install .[contrib]  # Optional requirements for contrib module\n', 'pip freeze --all', 'coverage run --source=. --omit=*__init__.py,setup.py -m pytest\ncoverage xml\n# pytest tests.py --doctest-modules --junitxml=junit/test-results.xml --cov=com --cov-report=xml --cov-report=html\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload --repository-url https://test.pypi.org/legacy/ dist/*\n', 'python -m pip install --upgrade pip\n', 'pip install -e .\n', 'pip install isort\nisort -c opendrop/**.py\n', 'pip install black\nblack . --check --diff\n', 'pip install flake8\nflake8 . --count --show-source --statistics\n', 'pip install pylint\npylint --rcfile=setup.cfg opendrop\n', 'pip install pytest\npytest\n']"
"['pip install twine', 'python setup.py sdist']"
"['upx --version', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install face_recognition --no-deps\npip install pyinstaller\n', 'python number_parser.py -v\n', 'pyinstaller \\\n  --onefile Movie_Data_Capture.py \\\n  --python-option u \\\n  --hidden-import ""ImageProcessing.cnn"" \\\n  --add-data ""$(python -c \'import cloudscraper as _; print(_.__path__[0])\' | tail -n 1):cloudscraper"" \\\n  --add-data ""$(python -c \'import opencc as _; print(_.__path__[0])\' | tail -n 1):opencc"" \\\n  --add-data ""$(python -c \'import face_recognition_models as _; print(_.__path__[0])\' | tail -n 1):face_recognition_models"" \\\n  --add-data ""Img:Img"" \\\n  --add-data ""scrapinglib:scrapinglib"" \\\n  --add-data ""config.ini:."" \\\n', 'pyinstaller `\n  --onefile Movie_Data_Capture.py `\n  --python-option u `\n  --hidden-import ""ImageProcessing.cnn"" `\n  --add-data ""$(python -c \'import cloudscraper as _; print(_.__path__[0])\' | tail -n 1);cloudscraper"" `\n  --add-data ""$(python -c \'import opencc as _; print(_.__path__[0])\' | tail -n 1);opencc"" `\n  --add-data ""$(python -c \'import face_recognition_models as _; print(_.__path__[0])\' | tail -n 1);face_recognition_models"" `\n  --add-data ""Img;Img"" `\n  --add-data ""scrapinglib;scrapinglib"" `\n  --add-data ""config.ini;."" `\n', 'cp config.ini dist/\n', 'echo ""VERSION=$(python Movie_Data_Capture.py --version)"" >> $GITHUB_ENV\n', 'echo ""VERSION=$(python Movie_Data_Capture.py --version)"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'cd dist\ntouch IPX-292.mp4\ntouch STAR-437-C.mp4\ntouch 122922_001.mp4\n./Movie_Data_Capture\n']"
"['make install_deps && pip3 uninstall -y amundsen-common', 'make test', 'make install_deps && pip3 uninstall -y amundsen-common && pip3 install -e ../common', 'make test', 'git config --file .gitmodules --get-regexp url | while read url; do\n  git config --file=.gitmodules $(echo ""$url"" | sed -E ""s/git@github.com:|https:\\/\\/github.com\\//https:\\/\\/${{ secrets.CI_PAT }}:${{ secrets.CI_PAT }}@github.com\\//"")\ndone\ngit submodule sync\ngit submodule update --init --recursive\n', 'echo ""::set-output name=module_folder::$(awk \'{print $1}\' <<< ""${{ github.event.release.name }}"")""', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}', 'make build-push-image-latest', 'make build-push-image-version', 'make install_deps && pip3 uninstall -y amundsen-common && pip3 install -e ../common', 'make test', 'npm install\nnpm install codecov -g\nnpm run lint\n', 'npm run betterer\nnpm run build --if-present\n', 'npm run test', 'export PATH=${PATH}:`go env GOPATH`/bin\ngo get -v -u github.com/google/addlicense\n', 'export PATH=${PATH}:`go env GOPATH`/bin\naddlicense -check -l mit -c ""Amundsen"" $(find $PWD -type f -name \'*.py\')\n', 'export PATH=${PATH}:`go env GOPATH`/bin\naddlicense -check -l mit -c ""Amundsen"" $(find $PWD -type f -name \'*.tsx\')\n', 'make install_deps && pip3 uninstall -y amundsen-common && pip3 install -e ../common', 'make test', ""pip install python-semantic-release\ngit config user.name github-actions\ngit config user.email github-actions@github.com\nsemantic-release version\necho ::set-output name=version::$(semantic-release print-version --current)\nsemantic-release changelog > './CHANGELOG.md'\n"", 'exit 0', 'exit 0', 'make install_deps && pip3 uninstall -y amundsen-common && pip3 install -e ../common', 'make test']"
"['git checkout HEAD^2', 'python -m pip install -U pip\npip install "".[arrow]""\npip install -r requirements/requirements-docs.txt\npip install -r requirements/requirements-mxnet.txt\npip install -r requirements/requirements-pytorch.txt\npip install -r requirements/requirements-rotbaum.txt\npip install -r requirements/requirements-extras-statsforecast.txt\n', 'sudo apt-get install pandoc\n', 'SKIP_NOTEBOOKS=${{ github.event_name == \'pull_request\' && !contains(github.event.pull_request.labels.*.name, \'pr:docs-build-notebook\') }}\nif [ $SKIP_NOTEBOOKS = true ]; then\n  DOCS_BUILD_MODE=skip\nelse\n  DOCS_BUILD_MODE=release\nfi\necho ""Running in \'$DOCS_BUILD_MODE\' mode""\njust mode=$DOCS_BUILD_MODE docs\n', 'aws s3 sync docs/_build/html s3://gluonts-website/${GITHUB_REF#refs/heads/} --delete --acl bucket-owner-full-control\n', 'python -m pip install -U pip\npip install -e .\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-arrow.txt\n', 'pytest -n2 --doctest-modules \\\n  src/gluonts/core \\\n  src/gluonts/dataset \\\n  src/gluonts/model \\\n  src/gluonts/time_feature \\\n  src/gluonts/zebras \\\n  src/gluonts/maybe.py \\\n  src/gluonts/itertools.py\n', 'python -m pip install -U pip\npip install ruff\npip install docformatter[tomli]==1.5.0\n', 'ruff src\n', 'docformatter -r src/gluonts\n', 'python -m pip install pip==20.2\npip install --pre ""mxnet<2"" -f https://dist.mxnet.io/python\npip install torch==1.10.0+cpu --no-cache-dir -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install pytorch-lightning~=1.5\npip install -e "".[shell]""\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-sagemaker-sdk.txt\npip install -r requirements/requirements-extras-m-competitions.txt\npip install -r requirements/requirements-rotbaum.txt\npip install -r requirements/requirements-extras-anomaly-evaluation.txt\npip install -r requirements/requirements-extras-autogluon.txt\n', ""pytest -m 'not (gpu or serial)' --cov src/gluonts --cov-report=term --cov-report xml test\n"", 'python -m pip install -U pip\npython -m pip install setuptools wheel\n', 'python setup.py sdist bdist_wheel\n', 'python -m pip install -U pip\npip install .\npip install click black mypy\npip install types-python-dateutil\n', 'just black\njust mypy\n', 'just license', 'git clone https://github.com/awslabs/gluonts --branch $(curl https://api.github.com/repos/awslabs/gluonts/releases/latest | grep tag_name | cut -d : -f 2,3 | tr -d \\""\\ | tr -d \\,\\ )\ncd gluon-ts\npython -m pip install pip==20.2\npip install mxnet~=1.8.0\npip install torch==1.10.0+cpu --no-cache-dir -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install pytorch-lightning~=1.5\npip install -e "".[shell]""\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-sagemaker-sdk.txt\npip install -r requirements/requirements-extras-m-competitions.txt\npip install -r requirements/requirements-rotbaum.txt\npip install -r requirements/requirements-extras-anomaly-evaluation.txt\npip install -r requirements/requirements-extras-autogluon.txt\n', ""cd gluon-ts\npytest -m 'not (gpu or serial)' --cov src/gluonts --cov-report=term --cov-report xml test\n"", '$tmp=(Invoke-WebRequest -Uri https://api.github.com/repos/awslabs/gluonts/releases/latest).Content | ConvertFrom-Json | Select-Object tag_name\n$tmp=$tmp.psobject.properties.value.trim() \ngit clone https://github.com/awslabs/gluonts --branch $tmp\ncd gluon-ts\npython -m pip install -U pip\npip install mxnet~=1.7.0\npip install torch\npip install pytorch-lightning~=1.5\npip install -e "".[shell]""\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-sagemaker-sdk.txt\npip install -r requirements/requirements-extras-m-competitions.txt\npip install -r requirements/requirements-rotbaum.txt\npip install -r requirements/requirements-extras-anomaly-evaluation.txt\npip install -r requirements/requirements-extras-autogluon.txt\n', ""cd gluon-ts\npytest -m 'not (gpu or serial)' --cov src/gluonts --cov-report=term --cov-report xml test\n"", 'python -m pip install -U pip\npip install .\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-statsforecast.txt\npip install -r requirements/requirements-extras-hierarchicalforecast.txt\n', 'pytest -n2 --doctest-modules test/ext/statsforecast\npytest -n2 --doctest-modules test/ext/hierarchicalforecast\n', 'pip install mxnet~=1.8.0', 'pip install mxnet~=1.7.0', 'python -m pip install -U pip\npip install torch==1.10.0+cpu --no-cache-dir -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install pytorch-lightning~=1.5\npip install -e "".[shell]""\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-sagemaker-sdk.txt\npip install -r requirements/requirements-extras-m-competitions.txt\npip install -r requirements/requirements-extras-anomaly-evaluation.txt\npip install -r requirements/requirements-extras-autogluon.txt\n', 'pytest -n2 --doctest-modules test/nursery\n', 'python -m pip install -U pip\npip install .\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-prophet.txt\n', 'pytest -n2 --doctest-modules test/ext/prophet\n', 'wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\nsudo add-apt-repository ""deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/""\nsudo apt-get install -y \\\n  libcairo-dev \\\n  libedit-dev \\\n  libnlopt-dev \\\n  libxml2-dev \\\n  libcurl4-openssl-dev\nRscript -e \'install.packages(c(""forecast"", ""nnfor"", ""hts""), repos=""https://cloud.r-project.org"")\'\n', 'python -m pip install -U pip\npip install .\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-r.txt\n', 'pytest -n2 test/ext/r_forecast\n', 'python -m pip install -U pip\npip install "".[arrow]""\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-m-competitions.txt\npip install -r requirements/requirements-pytorch.txt\npip install -r requirements/requirements-extras-cpflows.txt\n', 'pytest -n2 --doctest-modules --ignore test/nursery test\n', 'python -m pip install -U pip setuptools wheel\npip install .\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-rotbaum.txt\npip install -r requirements/requirements-rotbaum-extra-methods.txt\n', 'pytest -n2 --timeout 120 --doctest-modules test/ext/rotbaum\n', 'python -m pip install -U pip\npip install "".[mxnet,arrow,shell]""\npip install -r requirements/requirements-test.txt\npip install -r requirements/requirements-extras-m-competitions.txt\n', 'pytest -n2 --doctest-modules --ignore test/nursery test\n']"
"['python -m pip install --upgrade pip\npip install -r tests/actions_requirements.txt\n', 'python3 --version || python --version\necho ""import tests.actions_test"" > test.py\ncoverage run --source models,examples test.py && coverage report\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install -r tests/actions_requirements.txt\n', 'python3 --version || python --version\necho ""import tests.actions_test"" > test.py\ncoverage run --source models,examples test.py && coverage report\n', 'python -m pip install --upgrade pip\npip install -r tests/actions_requirements.txt\n', 'python3 --version || python --version\necho ""import tests.actions_test"" > test.py\ncoverage run --source models,examples test.py && coverage report\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npython -m playwright install\n', 'pytest tests', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -c ""import sys; print(sys.version)""', ""sed 's/>/=/g' requirements.txt | sed 's/$/\\.*/g' > requirements.min.txt"", 'python -m pip install --upgrade -r requirements.min.txt', 'python -m pip install --editable .[ci]', 'black --check --diff ./', 'flake8 ./', 'if [ ""$RUNNER_OS"" == ""macOS"" ]; then\n     brew link --force libomp\nfi\n', 'python -m numba -s', './test.sh unit', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --editable .[ci]', 'black --check --diff ./', 'flake8 ./', 'if [ ""$RUNNER_OS"" == ""macOS"" ]; then\n     echo ""Linking OpenMP into /usr/local/lib""\n     brew link --force libomp\n     echo ""Checking /usr/local/lib for OpenMP""\n     ls /usr/local/lib | grep libomp.dylib\n     echo ""Changing  @rpath for the omppool.cpython-x-darwin.so shared object to look in /usr/local/lib""\n     ls ""$(python -c \'import site; print(site.getsitepackages()[0])\')""/numba/np/ufunc/omppool.*.so | xargs install_name_tool -change @rpath/libomp.dylib /usr/local/lib/libomp.dylib \n     echo ""Checking Numba OpenMP Access""\n     python -c ""from numba.np.ufunc import omppool"" \nfi\n', 'python -m numba -s', './test.sh unit', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --editable .[ci]', 'black --check --diff ./', 'flake8 ./', 'if [ ""$RUNNER_OS"" == ""macOS"" ]; then\n     brew link --force libomp\nfi\n', 'python -m numba -s', './test.sh coverage', 'coverage report -m --fail-under=100 --skip-covered --omit=setup.py,docstring.py,stumpy/cache.py']"
"['pip install poetry\npoetry config virtualenvs.in-project true\npoetry install\npoetry run pip install pytest-benchmark\n', 'poetry run pytest dataprep/tests/benchmarks/eda.py --benchmark-json benchmark.json', 'echo ""Cache Version ${{ secrets.CACHE_VERSION }}""\npip install poetry\npoetry install\npoetry config --list\n', 'poetry run mypy --version \npoetry run pylint --version \npoetry run pytest --version \npoetry run black --version\n', 'poetry run black --check --quiet dataprep', 'poetry run mypy dataprep', 'poetry run pytest --cov-report xml --cov=dataprep dataprep/tests', 'poetry run pylint dataprep', 'pip install poetry\ncurl -L https://github.com/jgm/pandoc/releases/download/2.11.2/pandoc-2.11.2-1-amd64.deb -o /tmp/pandoc.deb && sudo dpkg -i /tmp/pandoc.deb\n', 'pip install poetry\npoetry install\n', 'poetry run sphinx-build -M html docs/source docs/build', ""echo 'docs.dataprep.ai' > docs/build/html/CNAME""]"
[]
"['pip install -r requirements-docs.txt\n', 'mkdocs build --verbose --clean --strict', 'python -m pip install poetry poetry-dynamic-versioning\npoetry install\npoetry build\n', 'poetry install --no-interaction -E http', 'poetry run pip install isort==""$ISORT""\n', 'poetry run pip install pydantic==""1.5.1""\n', 'poetry run pip install black==""22.1.0""\n', 'source $VENV\n./scripts/lint.sh\n', 'source $VENV\n./scripts/test.sh\n']"
""
"['echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', ""sudo apt-get -q update\nsudo env DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y -q \\\n     apt-transport-https \\\n     bc \\\n     build-essential \\\n     ca-certificates \\\n     curl \\\n     debhelper-compat \\\n     debian-archive-keyring \\\n     debootstrap \\\n     genisoimage \\\n     gettext \\\n     git \\\n     gnupg2 \\\n     imagemagick \\\n     jq \\\n     po4a \\\n     rsync \\\n     software-properties-common \\\n     squashfs-tools \\\n     virt-what \\\n     xorriso\n  git clone --depth=1 --single-branch --recurse-submodules --shallow-submodules --branch='debian/1%20210407' 'https://salsa.debian.org/live-team/live-build.git' /tmp/live-build\n  cd /tmp/live-build\n  dpkg-buildpackage -b -uc -us\n  cd /tmp\n  sudo dpkg -i /tmp/live-build*.deb\n  rm -rf /tmp/live-build\n"", 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""sha=$(git rev-parse --short HEAD)"" >> $GITHUB_OUTPUT', 'echo ""mversion=$(grep -P ""^\\s+image:\\s*malcolm"" docker-compose.yml | awk \'{print $2}\' | cut -d\':\' -f2 | uniq -c | sort -nr | awk \'{print $2}\' | head -n 1)"" >> $GITHUB_OUTPUT', 'IMAGES=( $(grep image: docker-compose.yml | awk \'{print $2}\' | sort -u) )\nfor IMAGE in ""${IMAGES[@]}""; do\n  REPO_IMAGE=""$(echo ""$IMAGE"" | sed ""s@^.*\\(malcolm\\)@ghcr.io/${{ github.repository_owner }}/\\1@"" | sed ""s/:.*/:${{ steps.extract_branch.outputs.branch }}/"")""\n  docker pull ""$REPO_IMAGE"" && \\\n    docker tag ""$REPO_IMAGE"" ""$IMAGE"" && \\\n    docker rmi ""$REPO_IMAGE""\ndone\nDEST_IMAGES_TGZ=$(pwd)/malcolm_""$(date +%Y.%m.%d_%H:%M:%S)""_${{ steps.extract_commit_sha.outputs.sha }}_images.tar.gz\ndocker save ""${IMAGES[@]}"" | gzip --best > ""$DEST_IMAGES_TGZ""\nfor IMAGE in ""${IMAGES[@]}""; do\n  docker rmi ""$IMAGE""\ndone\npushd ./malcolm-iso\nmkdir -p ./shared\necho ""GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}"" > ./shared/environment.chroot\nsudo /usr/bin/env bash ./build.sh -d ""$DEST_IMAGES_TGZ""\nrm -rf ./shared/\nsudo chmod 644 ./malcolm-*.*\npopd\n', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""sha=$(git rev-parse --short HEAD)"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', ""sudo apt-get -q update\nsudo env DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y -q \\\n     apt-transport-https \\\n     bc \\\n     build-essential \\\n     ca-certificates \\\n     curl \\\n     debhelper-compat \\\n     debian-archive-keyring \\\n     debootstrap \\\n     genisoimage \\\n     gettext \\\n     git \\\n     gnupg2 \\\n     imagemagick \\\n     jq \\\n     po4a \\\n     rsync \\\n     software-properties-common \\\n     squashfs-tools \\\n     virt-what \\\n     xorriso\n  git clone --depth=1 --single-branch --recurse-submodules --shallow-submodules --branch='debian/1%20210407' 'https://salsa.debian.org/live-team/live-build.git' /tmp/live-build\n  cd /tmp/live-build\n  dpkg-buildpackage -b -uc -us\n  sudo dpkg -i /tmp/live-build*.deb\n"", 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""sha=$(git rev-parse --short HEAD)"" >> $GITHUB_OUTPUT', 'echo ""mversion=$(grep -P ""^\\s+image:\\s*malcolm"" docker-compose.yml | awk \'{print $2}\' | cut -d\':\' -f2 | uniq -c | sort -nr | awk \'{print $2}\' | head -n 1)"" >> $GITHUB_OUTPUT', 'cp -r ./shared ./docs ./_config.yml ./_includes ./_layouts ./Gemfile ./README.md ./sensor-iso\ncp ./scripts/malcolm_utils.py ./sensor-iso/shared/bin/\ncp ./scripts/documentation_build.sh ./sensor-iso/docs/\ncp -r ./arkime/patch ./sensor-iso/shared/arkime_patch\npushd ./sensor-iso\necho ""${{ steps.extract_malcolm_version.outputs.mversion }}"" > ./shared/version.txt\necho ""${{ secrets.MAXMIND_GEOIP_DB_LICENSE_KEY }}"" > ./shared/maxmind_license.txt\necho ""GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}"" > ./shared/environment.chroot\necho ""VCS_REVSION=${{ steps.extract_commit_sha.outputs.sha }}"" > ./shared/environment.chroot\nsudo /usr/bin/env bash ./build.sh\nrm -rf ./shared/ ./docs/ ./_config.yml ./_includes ./_layouts /Gemfile ./README.md\nsudo chmod 644 ./hedgehog-*.*\npopd\n', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT']"
"['python3 -m pip install PyQt5==5.14\npython3 -c ""from PyQt5.QtCore import QSettings; print(\'done\')""\nmake install\n', 'make test', 'wifipumpkin3 -h']"
"['set PYTHONENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nset PYTHONUTF8=1\npython packaging/package2.py app\n', ""git fetch --tags\n$ErrorActionPreference = 'continue'\ngh release delete bootstrapper-package-app -y\ngit push --delete origin bootstrapper-package-app\nsleep 5\n"", 'set PYTHONENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nset PYTHONUTF8=1\npython -m pip install --upgrade pip\npython -m venv venv\nvenv\\Scripts\\python -m pip install --upgrade pip wheel setuptools\nvenv\\Scripts\\python -m pip install -r requirements.txt\n', 'where curl\npython packaging/package2.py runtime\n', ""git fetch --tags\n$ErrorActionPreference = 'continue'\ngh release delete bootstrapper-package-runtime -y\ngit push --delete origin bootstrapper-package-runtime\nsleep 5\n"", 'set PYTHONENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nset PYTHONUTF8=1\npython packaging/package2.py template\n', ""git fetch --tags\n$ErrorActionPreference = 'continue'\ngh release delete bootstrapper-package-template -y\ngit push --delete origin bootstrapper-package-template\nsleep 5\n"", 'set PYTHONENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nset PYTHONUTF8=1\npython packaging/package2.py vendor\n', ""git fetch --tags\n$ErrorActionPreference = 'continue'\ngh release delete bootstrapper-package-vendor -y\ngit push --delete origin bootstrapper-package-vendor\nsleep 5\n"", 'dotnet publish -o build\\bootstrapper packaging\\bootstrapper.sln\ntar caf build\\ArknightsAutoHelper.zip -C build\\bootstrapper *.exe *.toml\n', ""git fetch --tags\n$ErrorActionPreference = 'continue'\ngh release delete bootstrapper-release -y\ngit push --delete origin bootstrapper-release\nsleep 5\n"", 'pip install -r requirements.txt', 'python -m Arknights.addons.riic.riic_resource cache/riic_pack.xz\n', 'if [ -f cache/riic_pack.xz ]\nthen\n  gh release create riic_pack --prerelease || true\n  gh release upload riic_pack cache/riic_pack.xz --clobber\nfi\n', 'set PYTHONENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nset PYTHONUTF8=1\npython -c ""print(\'\\u4f60\\u597d\')""\n']"
"['if [[ ! $GITHUB_REF_NAME =~ ^release/v[0-9]+\\.[0-9]+\\.x-0\\.[0-9]+bx$ ]]; then\n  echo this workflow should only be run against long-term release branches\n  exit 1\nfi\n', '.github/scripts/use-cla-approved-github-bot.sh', 'commit=$(gh pr view $NUMBER --json mergeCommit --jq .mergeCommit.oid)\ntitle=$(gh pr view $NUMBER --json title --jq .title)\n\nbranch=""opentelemetrybot/backport-${NUMBER}-to-${GITHUB_REF_NAME//\\//-}""\n\ngit cherry-pick $commit\ngit push origin HEAD:$branch\ngh pr create --title ""[$GITHUB_REF_NAME] $title"" \\\n             --body ""Clean cherry-pick of #$NUMBER to the \\`$GITHUB_REF_NAME\\` branch."" \\\n             --head $branch \\\n             --base $GITHUB_REF_NAME\n', '# Only the latest commit of the feature branch is available\n# automatically. To diff with the base branch, we need to\n# fetch that too (and we only need its latest commit).\ngit fetch origin ${{ github.base_ref }} --depth=1\nif [[ $(git diff --name-only FETCH_HEAD | grep CHANGELOG) ]]\nthen\n  echo ""A CHANGELOG was modified. Looks good!""\nelse\n  echo ""No CHANGELOG was modified.""\n  echo ""Please add a CHANGELOG entry, or add the \\""Skip Changelog\\"" label if not required.""\n  false\nfi', 'echo ""md=$(git diff --name-only --diff-filter=ACMRTUXB $(git merge-base origin/main ${{ github.event.pull_request.head.sha }}) ${{ github.event.pull_request.head.sha }} | grep .md$ | xargs)"" >> $GITHUB_OUTPUT\n', 'npm install -g markdown-link-check', 'markdown-link-check \\\n  --verbose \\\n  --config .github/workflows/check_links_config.json \\\n  ${{needs.changedfiles.outputs.md}} \\\n  || { echo ""Check that anchor links are lowercase""; exit 1; }', 'if [[ ! $GITHUB_REF_NAME =~ ^release/v[0-9]+\\.[0-9]+\\.x-0\\.[0-9]+bx$ ]]; then\n  echo this workflow should only be run against long-term release branches\n  exit 1\nfi\n\nif ! grep --quiet ""^## Unreleased$"" CHANGELOG.md; then\n  echo the change log is missing an \\""Unreleased\\"" section\n  exit 1\nfi\n', 'stable_version=$(./scripts/eachdist.py version --mode stable)\nunstable_version=$(./scripts/eachdist.py version --mode prerelease)\n\nif [[ $stable_version =~ ^([0-9]+\\.[0-9]+)\\.([0-9]+)$ ]]; then\n  stable_major_minor=""${BASH_REMATCH[1]}""\n  stable_patch=""${BASH_REMATCH[2]}""\nelse\n  echo ""unexpected stable_version: $stable_version""\n  exit 1\nfi\n\nif [[ $unstable_version =~ ^0\\.([0-9]+)b([0-9]+)$ ]]; then\n  unstable_minor=""${BASH_REMATCH[1]}""\n  unstable_patch=""${BASH_REMATCH[2]}""\nelse\n  echo ""unexpected unstable_version: $unstable_version""\n  exit 1\nfi\n\nstable_version=""$stable_major_minor.$((stable_patch + 1))""\nunstable_version=""0.${unstable_minor}b$((unstable_patch + 1))""\n\necho ""STABLE_VERSION=$stable_version"" >> $GITHUB_ENV\necho ""UNSTABLE_VERSION=$unstable_version"" >> $GITHUB_ENV\n', '.github/scripts/update-version.sh $STABLE_VERSION $UNSTABLE_VERSION', 'date=$(date ""+%Y-%m-%d"")\nsed -Ei ""s/^## Unreleased$/## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} ($date)/"" CHANGELOG.md\n', '.github/scripts/use-cla-approved-github-bot.sh', 'message=""Prepare release ${STABLE_VERSION}/${UNSTABLE_VERSION}""\nbranch=""opentelemetrybot/prepare-release-${STABLE_VERSION}-${UNSTABLE_VERSION}""\n\ngit commit -a -m ""$message""\ngit push origin HEAD:$branch\ngh pr create --title ""[$GITHUB_REF_NAME] $message"" \\\n             --body ""$message."" \\\n             --head $branch \\\n             --base $GITHUB_REF_NAME\n', 'if [[ $GITHUB_REF_NAME != main ]]; then\n  echo this workflow should only be run against main\n  exit 1\nfi\n\nif ! grep --quiet ""^## Unreleased$"" CHANGELOG.md; then\n  echo the change log is missing an \\""Unreleased\\"" section\n  exit 1\nfi\n\nif [[ ! -z $PRERELEASE_VERSION ]]; then\n  stable_version=$(./scripts/eachdist.py version --mode stable)\n  stable_version=${stable_version//.dev/}\n  if [[ $PRERELEASE_VERSION != ${stable_version}* ]]; then\n    echo ""$PRERELEASE_VERSION is not a prerelease for the version on main ($stable_version)""\n    exit 1\n  fi\nfi\n', 'if [[ -z $PRERELEASE_VERSION ]]; then\n  stable_version=$(./scripts/eachdist.py version --mode stable)\n  stable_version=${stable_version//.dev/}\nelse\n  stable_version=$PRERELEASE_VERSION\nfi\n\nunstable_version=$(./scripts/eachdist.py version --mode prerelease)\nunstable_version=${unstable_version//.dev/}\n\nif [[ $stable_version =~ ^([0-9]+)\\.([0-9]+)\\.0$ ]]; then\n  stable_version_branch_part=$(echo $stable_version | sed -E \'s/([0-9]+)\\.([0-9]+)\\.0/\\1.\\2.x/\')\n  unstable_version_branch_part=$(echo $unstable_version | sed -E \'s/0\\.([0-9]+)b0/0.\\1bx/\')\n  release_branch_name=""release/v${stable_version_branch_part}-${unstable_version_branch_part}""\nelif [[ $stable_version =~ ^([0-9]+)\\.([0-9]+)\\.0 ]]; then\n  # pre-release version, e.g. 1.9.0rc2\n  release_branch_name=""release/v$stable_version-$unstable_version""\nelse\n  echo ""unexpected version: $stable_version""\n  exit 1\nfi\n\ngit push origin HEAD:$release_branch_name\n\necho ""STABLE_VERSION=$stable_version"" >> $GITHUB_ENV\necho ""UNSTABLE_VERSION=$unstable_version"" >> $GITHUB_ENV\necho ""RELEASE_BRANCH_NAME=$release_branch_name"" >> $GITHUB_ENV\n', '.github/scripts/update-version.sh $STABLE_VERSION $UNSTABLE_VERSION', 'date=$(date ""+%Y-%m-%d"")\nsed -Ei ""s/^## Unreleased$/## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} ($date)/"" CHANGELOG.md\n', '.github/scripts/use-cla-approved-github-bot.sh', 'message=""Prepare release ${STABLE_VERSION}/${UNSTABLE_VERSION}""\nbranch=""opentelemetrybot/prepare-release-${STABLE_VERSION}-${UNSTABLE_VERSION}""\n\ngit commit -a -m ""$message""\ngit push origin HEAD:$branch\ngh pr create --title ""[$RELEASE_BRANCH_NAME] $message"" \\\n             --body ""$message."" \\\n             --head $branch \\\n             --base $RELEASE_BRANCH_NAME\n', 'if [[ -z $PRERELEASE_VERSION ]]; then\n  stable_version=$(./scripts/eachdist.py version --mode stable)\n  stable_version=${stable_version//.dev/}\nelse\n  stable_version=$PRERELEASE_VERSION\nfi\n\nunstable_version=$(./scripts/eachdist.py version --mode prerelease)\nunstable_version=${unstable_version//.dev/}\n\nif [[ $stable_version =~ ^([0-9]+)\\.([0-9]+)\\.0$ ]]; then\n  stable_major=""${BASH_REMATCH[1]}""\n  stable_minor=""${BASH_REMATCH[2]}""\n  stable_next_version=""$stable_major.$((stable_minor + 1)).0""\nelif [[ $stable_version =~ ^([0-9]+)\\.([0-9]+)\\.0 ]]; then\n  # pre-release version, e.g. 1.9.0rc2\n  stable_major=""${BASH_REMATCH[1]}""\n  stable_minor=""${BASH_REMATCH[2]}""\n  stable_next_version=""$stable_major.$stable_minor.0""\nelse\n  echo ""unexpected stable_version: $stable_version""\n  exit 1\nfi\n\nif [[ $unstable_version =~ ^0\\.([0-9]+)b[0-9]+$ ]]; then\n  unstable_minor=""${BASH_REMATCH[1]}""\nelse\n  echo ""unexpected unstable_version: $unstable_version""\n  exit 1\nfi\n\nunstable_next_version=""0.$((unstable_minor + 1))b0""\n\necho ""STABLE_VERSION=${stable_version}"" >> $GITHUB_ENV\necho ""STABLE_NEXT_VERSION=${stable_next_version}.dev"" >> $GITHUB_ENV\n\necho ""UNSTABLE_VERSION=${unstable_version}"" >> $GITHUB_ENV\necho ""UNSTABLE_NEXT_VERSION=${unstable_next_version}.dev"" >> $GITHUB_ENV\n', '.github/scripts/update-version.sh $STABLE_NEXT_VERSION $UNSTABLE_NEXT_VERSION', '# the actual release date on main will be updated at the end of the release workflow\ndate=$(date ""+%Y-%m-%d"")\nsed -Ei ""s/^## Unreleased$/## Unreleased\\n\\n## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} ($date)/"" CHANGELOG.md\n', '.github/scripts/use-cla-approved-github-bot.sh', 'message=""Update version to ${STABLE_NEXT_VERSION}/${UNSTABLE_NEXT_VERSION}""\nbody=""Update version to \\`${STABLE_NEXT_VERSION}/${UNSTABLE_NEXT_VERSION}\\`.""\nbranch=""opentelemetrybot/update-version-to-${STABLE_NEXT_VERSION}-${UNSTABLE_NEXT_VERSION}""\n\ngit commit -a -m ""$message""\ngit push origin HEAD:$branch\ngh pr create --title ""$message"" \\\n             --body ""$body"" \\\n             --head $branch \\\n             --base main\n', 'git checkout main', 'git pull --rebase=false origin main', 'git checkout ${{ github.event.pull_request.head.sha }}', 'pip install tox==3.27.1 -U tox-factor', 'tox -e public-symbols-check', 'if [[ $GITHUB_REF_NAME != release/* ]]; then\n  echo this workflow should only be run against release branches\n  exit 1\nfi\n', 'stable_version=$(./scripts/eachdist.py version --mode stable)\nunstable_version=$(./scripts/eachdist.py version --mode prerelease)\n\nif [[ $stable_version =~ ^([0-9]+)\\.([0-9]+)\\.([0-9]+) ]]; then\n  stable_major=""${BASH_REMATCH[1]}""\n  stable_minor=""${BASH_REMATCH[2]}""\n  stable_patch=""${BASH_REMATCH[3]}""\nelse\n  echo ""unexpected stable_version: $stable_version""\n  exit 1\nfi\nif [[ $stable_patch != 0 ]]; then\n  if [[ $unstable_version =~ ^0\\.([0-9]+)b([0-9]+)$ ]]; then\n    unstable_minor=""${BASH_REMATCH[1]}""\n    unstable_patch=""${BASH_REMATCH[2]}""\n  else\n    echo ""unexpected unstable_version: $unstable_version""\n    exit 1\n  fi\n  if [[ $unstable_patch != 0 ]]; then\n    prior_version_when_patch=""$stable_major.$stable_minor.$((stable_patch - 1))/0.${unstable_minor}b$((unstable_patch - 1))""\n  fi\nfi\n\necho ""STABLE_VERSION=$stable_version"" >> $GITHUB_ENV\necho ""UNSTABLE_VERSION=$unstable_version"" >> $GITHUB_ENV\n\necho ""PRIOR_VERSION_WHEN_PATCH=$prior_version_when_patch"" >> $GITHUB_ENV\n', 'if [[ -z $PRIOR_VERSION_WHEN_PATCH ]]; then\n  # not making a patch release\n  if ! grep --quiet ""^## Version ${STABLE_VERSION}/${UNSTABLE_VERSION} "" CHANGELOG.md; then\n    echo the pull request generated by prepare-release-branch.yml needs to be merged first\n    exit 1\n  fi\nfi\n', './scripts/build.sh', 'pip install twine\n', 'twine upload --repository testpypi --skip-existing --verbose dist/*\n', 'twine upload --skip-existing --verbose dist/*\n', '# conditional block not indented because of the heredoc\nif [[ ! -z $PRIOR_VERSION_WHEN_PATCH ]]; then\ncat > /tmp/release-notes.txt << EOF\nThis is a patch release on the previous $PRIOR_VERSION_WHEN_PATCH release, fixing the issue(s) below.\n\nEOF\nfi\n\n# CHANGELOG_SECTION.md is also used at the end of the release workflow\n# for copying the change log updates to main\nsed -n ""0,/^## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} /d;/^## Version /q;p"" CHANGELOG.md \\\n  > /tmp/CHANGELOG_SECTION.md\n\n# the complex perl regex is needed because markdown docs render newlines as soft wraps\n# while release notes render them as line breaks\nperl -0pe \'s/(?<!\\n)\\n *(?!\\n)(?![-*] )(?![1-9]+\\. )/ /g\' /tmp/CHANGELOG_SECTION.md \\\n  >> /tmp/release-notes.txt\n', 'gh release create --target $GITHUB_REF_NAME \\\n                  --title ""Version ${STABLE_VERSION}/${UNSTABLE_VERSION}"" \\\n                  --notes-file /tmp/release-notes.txt \\\n                  --discussion-category announcements \\\n                  v$STABLE_VERSION\n', 'if [[ -z $PRIOR_VERSION_WHEN_PATCH ]]; then\n  # this was not a patch release, so the version exists already in the CHANGELOG.md\n\n  # update the release date\n  date=$(gh release view v$STABLE_VERSION --json publishedAt --jq .publishedAt | sed \'s/T.*//\')\n  sed -Ei ""s/## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} .*/## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} ($date)/"" CHANGELOG.md\n\n  # the entries are copied over from the release branch to support workflows\n  # where change log entries may be updated after preparing the release branch\n\n  # copy the portion above the release, up to and including the heading\n  sed -n ""0,/^## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} ($date)/p"" CHANGELOG.md > /tmp/CHANGELOG.md\n\n  # copy the release notes\n  cat /tmp/CHANGELOG_SECTION.md >> /tmp/CHANGELOG.md\n\n  # copy the portion below the release\n  sed -n ""0,/^## Version ${STABLE_VERSION}\\/${UNSTABLE_VERSION} /d;0,/^## Version /{/^## Version/!d};p"" CHANGELOG.md \\\n    >> /tmp/CHANGELOG.md\n\n  # update the real CHANGELOG.md\n  cp /tmp/CHANGELOG.md CHANGELOG.md\nelse\n  # this was a patch release, so the version does not exist already in the CHANGELOG.md\n\n  # copy the portion above the top-most release, not including the heading\n  sed -n ""0,/^## Version /{ /^## Version /!p }"" CHANGELOG.md > /tmp/CHANGELOG.md\n\n  # add the heading\n  date=$(gh release view v$STABLE_VERSION --json publishedAt --jq .publishedAt | sed \'s/T.*//\')\n  echo ""## Version ${STABLE_VERSION}/${UNSTABLE_VERSION} ($date)"" >> /tmp/CHANGELOG.md\n\n  # copy the release notes\n  cat /tmp/CHANGELOG_SECTION.md >> /tmp/CHANGELOG.md\n\n  # copy the portion starting from the top-most release\n  sed -n ""/^## Version /,\\$p"" CHANGELOG.md >> /tmp/CHANGELOG.md\n\n  # update the real CHANGELOG.md\n  cp /tmp/CHANGELOG.md CHANGELOG.md\nfi\n', '.github/scripts/use-cla-approved-github-bot.sh', 'message=""Copy change log updates from $GITHUB_REF_NAME""\nbody=""Copy log updates from \\`$GITHUB_REF_NAME\\`.""\nbranch=""opentelemetrybot/copy-change-log-updates-from-${GITHUB_REF_NAME//\\//-}""\n\nif [[ -z $PRIOR_VERSION_WHEN_PATCH ]]; then\n  if git diff --quiet; then\n    echo there are no updates needed to the change log on main, not creating pull request\n    exit 0 # success\n  fi\nfi\n\ngit commit -a -m ""$message""\ngit push origin HEAD:$branch\ngh pr create --title ""$message"" \\\n             --body ""$body"" \\\n             --head $branch \\\n             --base main\n', 'pip install tox==3.27.1 -U tox-factor', 'git config --system core.longpaths true', 'tox -f ${{ matrix.python-version }}-${{ matrix.package }} -- --benchmark-json=${{ env.RUN_MATRIX_COMBINATION }}-benchmark.json', 'pip install tox==3.27.1', 'tox -e ${{ matrix.tox-environment }}', 'pip install tox==3.27.1 -U tox-factor', 'tox -f ${{ matrix.python-version }}-${{ matrix.package }}']"
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'docker build -t local-test .', 'docker run --rm local-test:latest bash -i -c ""pytest""']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['python -m pip install --upgrade pip\n', 'pip install -r requirements.txt\n', 'pip install -r requirements-deeplearning.txt\n', 'pip install -r requirements-test.txt\n', '# pytest\npytest tests/ -m ""notebook_tests"" --durations=10 --doctest-modules --junitxml=junit/test-results.xml --cov=dice_ml --cov-report=xml --cov-report=html\n', 'python -m pip install --upgrade pip\npip install -r requirements-linting.txt\n', 'isort . -c\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# The GitHub editor is 127 chars wide.\nflake8 . --count --max-complexity=30 --max-line-length=127 --statistics\n# Check for cyclometric complexity for specific files where this metric has been \n# reduced to ten and below\nflake8 dice_ml/data_interfaces/ --count --max-complexity=10 --max-line-length=127\n', '# stop the build if there are flake8 errors in notebooks\nflake8_nb docs/source/notebooks/ --statistics --max-line-length=127\n', '# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', 'conda env update --file environment.yml --name base\n', 'conda env update --file environment-deeplearning.yml --name base\n', 'conda install pytest ipython jupyter nbformat pytest-mock\n\npytest\n', 'python -m pip install --upgrade pip\n', 'pip install -r requirements.txt\n', 'pip install -r requirements-deeplearning.txt\n', 'pip install -r requirements-test.txt\n', '# pytest\npytest tests/ -m ""not notebook_tests"" --durations=10 --doctest-modules --junitxml=junit/test-results.xml --cov=dice_ml --cov-report=xml --cov-report=html\n', 'python setup.py check sdist bdist_wheel\ntwine check dist/* \n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install pip --upgrade\npip install build\npython -m build\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-test.txt\n', 'coverage run --append --source=benedict -m unittest\ncoverage report --show-missing\ncoverage xml -o ./coverage.xml\n']"
"['pip install black==22.3.0 flake8', 'python -m black --config=pyproject.toml --check openunmix tests scripts', 'python -m flake8  openunmix tests  --show-source --statistics  --select=F6,F7,F82,F52\npython -m flake8 --config .flake8 --exit-zero openunmix tests  --statistics\n', 'sudo apt update\nsudo apt install libsndfile1-dev libsndfile1 ffmpeg sox\n', 'python -m pip install --upgrade --user pip --quiet\npython -m pip install .[""stempeg""]\npython --version\npip --version\npython -m pip list\n', 'umx https://samples.ffmpeg.org/A-codecs/wavpcm/test-96.wav --audio-backend stempeg\n', ""python -m pip install -e .['tests']\npython --version\npip --version\npython -m pip list\n"", 'conda list', 'py.test tests/test_model.py -v', 'sudo apt update\nsudo apt install libsndfile1-dev libsndfile1 ffmpeg sox\n', 'python -m pip install --upgrade --user pip --quiet\npython -m pip install numpy Cython --upgrade-strategy only-if-needed --quiet\npython -m pip install coverage codecov --upgrade-strategy only-if-needed --quiet\nif [ $TORCH_INSTALL == ""1.8.0"" ]; then\n  INSTALL=""torch==1.8.0+cpu torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html""\nelif [ $TORCH_INSTALL == ""1.9.0"" ]; then\n  INSTALL=""torch==1.9.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html""\nelse\n  INSTALL=""--pre torch torchaudio -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html""\nfi\npython -m pip install $INSTALL\npython -m pip install -e .[\'tests\']\npython --version\npip --version\npython -m pip list\n', 'chmod +x tests/create_dummy_datasets.sh\n./tests/create_dummy_datasets.sh\n', 'coverage run -a -m py.test tests\n# chmod +x ./tests/cli_test.sh\n# ./tests/cli_test.sh\n', 'chmod +x ./tests/cli_test.sh\n./tests/cli_test.sh\n', 'coverage report -m\ncoverage xml -o coverage.xml\n']"
['pip install --upgrade pip\npip install --upgrade setuptools setuptools_scm pep517\npip install .[tests]\npytest -vv\n']
"['pip install pre-commit\npre-commit run --all-files\n', 'pip install --upgrade pip\npip install .[dev,miosr,cvxpy]\n', 'sudo apt-get update -y\nsudo apt-get install pandoc\npip install .[docs]\ncd docs\npython -m sphinx -T -E -W -b html -d _build/doctrees . _build/html\ncd ..\n', 'coverage run --source=pysindy -m pytest test  && coverage xml\n', 'pip install papermill\ncd examples\npapermill --report-mode 1_feature_overview.ipynb out.json\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build .']"
"['python -m pip install --upgrade pip\n# Until the next xdis release\npip install git+https://github.com/rocky/python-xdis#egg=xdis\npip install -e .\npip install -r requirements-dev.txt\n', 'make check\n', 'python -m pip install --upgrade pip\n# Until the next xdis release\npip install git+https://github.com/rocky/python-xdis#egg=xdis\npip install -e .\npip install -r requirements-dev.txt\n', 'make check\n', 'python -m pip install --upgrade pip\n# Until the next xdis release\npip install git+https://github.com/rocky/python-xdis#egg=xdis\npip install -e .\npip install -r requirements-dev.txt\n', 'make check\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install tox==3.12.1 pre-commit==1.17.0\n', 'wget https://s3.amazonaws.com/argoai-argoverse/hd_maps.tar.gz\ntar -xvzf hd_maps.tar.gz\n', 'tox\npre-commit run --all-files\n']"
"['pip install black', 'black --config=black.toml --check tests', 'black --config=black.toml --check yandex_music', 'pip install -r requirements-dev.txt', 'pytest -v --cov=yandex_music', 'codecov -F github -t $CODECOV_TOKEN --name ""GHA""', 'pip install twine\npython setup.py sdist\ntwine upload dist/*\n', 'pip install -r requirements-dev.txt', 'pytest -vv', 'pip install -r requirements-dev.txt', 'pytest -vv']"
"['pip install -r requirements_dev.txt', 'isort --diff --check-only tests custom_components', 'pip install -r tests/requirements_${{ matrix.ha-version }}.txt \n', 'pytest \\\n  -qq \\\n  --timeout 9 \\\n  --durations 10 \\\n  -n auto \\\n  --cov custom_components.yandex_smart_home \\\n  --cov-report xml \\\n  -o console_output_style=count \\\n  -p no:sugar \\\n  tests\n']"
"['poetry install', 'poetry run flake8', 'poetry install', 'poetry run pytest']"
""
""
"['pip install tox codecov', 'tox -e py', 'codecov', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get install libpq-dev -y\npython3 -m pip install pipenv wheel twine\n', 'pipenv install pip==20.1.1 setuptools==47.3.1\n', 'python setup.py sdist\ntwine upload --skip-existing dist/webull-*.tar.gz -u ted_chou12 -p ${{ secrets.PYPI_PASSWORD }}\n']"
"['pip3 install podman-compose', 'cd docker\npodman-compose -f docker-compose.yml build\npodman-compose -f docker-compose.yml push\n', 'pip install -r requirements-dev.txt', 'make html', 'echo ""/usr/local/cuda/bin"" >> $GITHUB_PATH', 'python -m venv env', 'source ../env/bin/activate\npip install -r requirements-dev.txt\n', 'source ../env/bin/activate\npip install -r tests/requirements-tests.txt\n', 'source ../env/bin/activate\npip install -e .\n', 'source ../env/bin/activate\npip uninstall -y bddl\n', 'source ../env/bin/activate\npip install -e .\n', 'ln -s /scr/ig-data igibson/data', 'source ../env/bin/activate\npython tests/create_tests_of_examples.py\n', 'source ../env/bin/activate\npytest /tmp/tests_of_examples\n', 'rm -rf /tmp/tests_of_examples || true', 'rm -rf env/', 'echo ""/usr/local/cuda/bin"" >> $GITHUB_PATH', 'python -m venv env', 'source ../env/bin/activate\npip install -r requirements-dev.txt\n', 'source ../env/bin/activate\npip install -r tests/requirements-tests.txt\n', 'source ../env/bin/activate\npip install -e .\n', 'source ../env/bin/activate\npip uninstall -y bddl\n', 'source ../env/bin/activate\npip install -e .\n', 'ln -s /scr/ig-data igibson/data', 'source ../env/bin/activate\npython tests/create_tests_of_examples.py --exhaustive\n', 'source ../env/bin/activate\npytest /tmp/tests_of_examples\n', 'rm -rf /tmp/tests_of_examples || true', 'rm -rf env/', 'python -m pip install build --user', 'python -m build --sdist --outdir dist/ .', 'echo ""/usr/local/cuda/bin"" >> $GITHUB_PATH', 'python -m venv env', 'source ../env/bin/activate\npip install -r requirements-dev.txt\n', 'source ../env/bin/activate\npip install -e .\n', 'source ../env/bin/activate\npip uninstall -y bddl\n', 'source ../env/bin/activate\npip install -e .\n', 'ln -s /scr/ig-data igibson/data', 'source ../env/bin/activate\npytest\n', 'rm -rf env/']"
"['echo ""DOCKER_TAG=$(echo ${GITHUB_REF#refs/tags/})"" >> $GITHUB_ENV\n', 'python3 $(pwd)/label_studio/core/version.py\ncat $(pwd)/label_studio/core/version_.py\n', 'cd docs && npm install', 'cd docs && touch db.json && npm run publish', '! grep -Eo \'(&quot;|"")isSharedInviteError(&quot;|"")\\s*:\\s*true\' <(\n    curl --silent --location ${{ env.INVITE_LINK }} -H \'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0\' -H ""Accept: application/json""\n)\n', 'python -m pip install --upgrade pip setuptools\npip install --upgrade cython\npip install -U pip==20.2\npip install -r deploy/requirements.txt -r deploy/requirements-test.txt\npip install -e .\n', 'echo ""::set-output name=dir::$(npm config get cache)""', './deploy/prebuild_wo_frontend.sh', 'python label_studio/manage.py migrate', 'cd label_studio/\npytest --cov=. --cov-report=xml -vv -n auto\n', 'sudo apt-get update\nsudo apt-get install virtualenv libsasl2-dev python-dev libldap2-dev libssl-dev libxml2-dev libxslt-dev python-dev\n', 'pip install -U pip==20.2\npip install -r deploy/requirements.txt -r deploy/requirements-test.txt\npip install -e .\n', 'echo ""::set-output name=dir::$(npm config get cache)""', './deploy/prebuild_wo_frontend.sh', 'python label_studio/manage.py migrate', 'cd label_studio/\npytest --cov=. --cov-report=xml -vv -n auto\n', ""python -m pip install --upgrade pip setuptools\npip install --upgrade cython\nif (Test-Path -Path '.\\deploy\\requirements.txt' -PathType Leaf)\n{pip install -r deploy\\requirements.txt}\nif (Test-Path -Path '.\\deploy\\requirements-test.txt' -PathType Leaf)\n{pip install -r deploy/requirements-test.txt}\npip install -e .\n"", 'set PYTHONIOENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nlabel-studio init my_project --agree-fix-sqlite --force-fix-sqlite\ncp sqlite3.dll %pythonLocation%/DLLs/sqlite3.dll\n', 'set PYTHONIOENCODING=utf-8\nset PYTHONLEGACYWINDOWSSTDIO=utf-8\nlabel-studio init my_project --username test@test.com --password testpwd\n', 'cd label_studio/\npython -m pytest -vv -n auto\n', 'pip install twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['SUBDIR=website\ngit fetch\nREV=$(git log -5 --pretty=oneline origin/gh-pages | grep ""Deploy website"" |  awk \'NF>1{print $NF}\'  | head -1)\n# This condition passes in 2 conditions:\n# 1. The revision does not exist (squash/force push happened)\n# 2. There are changes between last deployed revision and HEAD\nif [[ ! $(git rev-parse --verify -q ""$REV^{commit}"") ||  $(git diff-index $REV -- $SUBDIR) ]]; then\n  echo ""Changes detected in directory $SUBDIR between origin/main and this diff""\n  cd $SUBDIR\n  yarn --no-progress\n  git config --global user.email omry@users.noreply.github.com\n  git config --global user.name omry\n  echo ""machine github.com login docusaurus-bot password $WEBSITE_GITHUB_TOKEN"" > ~/.netrc\n  yarn install && GIT_USER=docusaurus-bot yarn deploy\nelse\n  echo ""No changes detected in directory $SUBDIR between origin/main and this diff""\nfi\n']"
"['pip install bandit black codespell flake8 isort mypy pytest pyupgrade safety', 'bandit -r . || true', 'black --check . || true', 'codespell --ignore-words-list=""followings"" --quiet-level=2', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt', 'mypy --ignore-missing-imports .', 'pytest . || true', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
[]
""
"['brew install python3 cmake sdl2 sdl2_image sdl2_ttf sdl2_gfx boost boost-python3', 'python3 -m pip install --upgrade pip setuptools wheel build psutil\npython3 -m pip install -r requirements.txt\n', 'python3 -m build --no-isolation --wheel', 'python -m pip install --upgrade pip\npython -m pip install -U build setuptools psutil wheel\npython -m pip install -r requirements.txt\n', 'python -m build', 'python -m pip install gfootball --no-index --find-links=dist', 'FOR %%f IN (gfootball\\env\\*test.py) DO ( call python %%f & if errorlevel 1 exit /B 1 )', 'python -m pip install --upgrade pip setuptools wheel psutil build', 'python -m build --sdist', 'mv artifacts/**/*.whl dist']"
"['npm install', 'pip install -r requirements.txt', 'python manage.py test bookmarks.tests', 'npm install', 'pip install -r requirements.txt\nplaywright install chromium\npython manage.py compilescss\npython manage.py collectstatic --ignore=*.scss\n', 'python manage.py test bookmarks.e2e --pattern=""e2e_test_*.py""']"
"['python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -e "".[tensorflow-cpu,tests]"" --progress-bar off --upgrade\n', 'pytest --cov=keras_tuner --cov-report xml:coverage.xml\n', 'python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install protobuf==3.20.1\npip install ""tensorflow<2.1""\npip install -e "".[tensorflow-cpu,tests]"" --progress-bar off --upgrade\n', 'pytest keras_tuner/integration_tests/legacy_import_test.py\n', 'python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -e "".[tensorflow-cpu,tests]"" --progress-bar off --upgrade\n', 'bash shell/lint.sh', 'python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -e "".[tensorflow-cpu,tests]"" --progress-bar off --upgrade\n', 'bash shell/run_guides.sh', 'python -m pip install --upgrade pip setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -e "".[tensorflow-cpu,tests]"" --progress-bar off --upgrade\n', 'bash shell/run_guides.sh', 'python -m pip install --upgrade pip setuptools\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install -e "".[tests]"" --progress-bar off --upgrade\npip uninstall tensorflow keras -y\npip install tf-nightly --progress-bar off --upgrade\n', 'pytest\n']"
"['which python\npython -m venv venv\nsource venv/bin/activate\npython -m pip install --constraint=.github/workflows/constraints.txt --upgrade pip\nwhich python\n', 'curl -sSL https://install.python-poetry.org | python3 -\nexport PATH=""/Users/runner/.local/bin:$PATH""\npoetry --version\npoetry config virtualenvs.in-project true\npoetry config virtualenvs.create false\npoetry config virtualenvs.path venv\nsource venv/bin/activate\nwhich python\n', 'source venv/bin/activate\nexport PATH=""/Users/runner/.local/bin:$PATH""\nrm poetry.lock\npoetry update isaacgym\npoetry lock --no-update\npoetry export -f requirements.txt --output requirements.txt\n', 'which python\npython -m venv venv\nsource venv/bin/activate\npython -m pip install --constraint=.github/workflows/constraints.txt --upgrade pip\nwhich python\n', 'curl -sSL https://install.python-poetry.org | python3 -\nexport PATH=""/Users/runner/.local/bin:$PATH""\npoetry --version\npoetry config virtualenvs.in-project true\npoetry config virtualenvs.create false\npoetry config virtualenvs.path venv\nsource venv/bin/activate\nwhich python\n', 'source venv/bin/activate\nexport PATH=""/Users/runner/.local/bin:$PATH""\nrm poetry.lock\npoetry update isaacgym\npoetry lock --no-update\npoetry export -f requirements.txt --output requirements.txt\n', 'poetry install -E pytest', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_classic_control.py', 'poetry install -E ""pytest jax""', 'poetry run pip install ""stable_baselines3==2.0.0a1""', 'poetry run pytest tests/test_classic_control_gymnasium.py', 'poetry run pytest tests/test_classic_control_jax.py', 'poetry run pytest tests/test_jax_compute_gae.py', 'poetry install -E ""pytest optuna""', 'poetry run pytest tests/test_tuner.py', 'poetry install -E ""pytest atari""', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_atari.py', 'poetry install -E ""pytest atari jax""', 'poetry run pytest tests/test_atari_jax.py', 'poetry run pip install ""stable_baselines3==2.0.0a1"" ""gymnasium[atari,accept-rom-license]==0.28.1""  ""ale-py==0.8.1""', 'poetry run pytest tests/test_atari_gymnasium.py', 'poetry run pytest tests/test_atari_jax_gymnasium.py', 'poetry install -E ""pytest procgen""', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_procgen.py', 'poetry install -E ""pytest mujoco dm_control""', 'poetry run pip install setuptools==59.5.0', 'sudo apt-get update && sudo apt-get -y install libgl1-mesa-glx libosmesa6 libglfw3\n', 'poetry run pytest tests/test_mujoco.py', 'poetry install -E ""pytest mujoco dm_control""', 'poetry run pip install setuptools==59.5.0', 'poetry run pip install ""stable_baselines3==2.0.0a1""', 'sudo apt-get update && sudo apt-get -y install libgl1-mesa-glx libosmesa6 libglfw3\n', 'poetry run pytest tests/test_mujoco_gymnasium.py', 'poetry install -E ""pytest mujoco dm_control jax""', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_mujoco.py', 'poetry install -E ""pytest mujoco dm_control jax""', 'poetry run pip install setuptools==59.5.0', 'poetry run pip install ""stable_baselines3==2.0.0a1""', 'poetry run pytest tests/test_mujoco_gymnasium.py', 'poetry install -E ""pytest mujoco_py mujoco jax""', 'poetry run pip install setuptools==59.5.0', 'sudo apt-get update && sudo apt-get -y install wget unzip software-properties-common \\\n  libgl1-mesa-dev \\\n  libgl1-mesa-glx \\\n  libglew-dev \\\n  libosmesa6-dev patchelf\n', 'poetry run pytest tests/test_mujoco_py.py', 'poetry install -E ""pytest mujoco_py mujoco jax""', 'poetry run pip install ""stable_baselines3==2.0.0a1""', 'poetry run pip install setuptools==59.5.0', 'sudo apt-get update && sudo apt-get -y install wget unzip software-properties-common \\\n  libgl1-mesa-dev \\\n  libgl1-mesa-glx \\\n  libglew-dev \\\n  libosmesa6-dev patchelf\n', 'poetry run pytest tests/test_mujoco_py_gymnasium.py', 'poetry install -E ""pytest envpool jax ppo_atari_envpool_xla_jax_scan""', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_envpool.py', 'poetry install -E ""pytest atari""', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_atari_multigpu.py', 'poetry install -E ""pytest pettingzoo atari""', 'poetry run pip install setuptools==59.5.0', 'poetry run AutoROM --accept-license', 'poetry run pytest tests/test_pettingzoo_ma_atari.py', 'poetry install -E pytest', 'poetry install -E ""pytest cloud""', 'poetry run pip install setuptools==59.5.0', 'poetry run pytest tests/test_utils.py']"
"['sudo apt update', 'sudo apt install -y devscripts debhelper gcc-aarch64-linux-gnu', 'make package-debian', 'sudo apt update', 'make package-tar', 'sudo chown -R build .', 'sudo -u build make package-arch', 'sudo dnf update -y', 'sudo dnf install -y \\\n  @development-tools \\\n  @rpm-development-tools \\\n  python-pip \\\n  gcc-aarch64-linux-gnu\n', 'make package-fedora RPMBUILD_TARGET=x86_64\nmake package-fedora RPMBUILD_TARGET=aarch64\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine pandas pyecharts mplfinance tushare matplotlib\npip install -r requirements.txt\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\n# cd ..\npytest test\n']"
"['python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n', 'python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n']"
"['python -m pip install --upgrade pip\npip install pytest\nsudo apt-get install xvfb\n', 'xvfb-run pytest tests/unit\n', 'pip install pytest\n', 'pytest tests/unit\n', 'pip install pytest\n', '\n$env:PYGLET_DEBUG_LIB = ""True""\n$env:PYGLET_DEBUG_GL_TRACE = ""True""\n\nmkdir D:\\a\\pyglet\\pyglet\\lib\ncopy C:\\msys64\\mingw64\\bin\\opengl32.dll D:\\a\\pyglet\\pyglet\\lib\\\n\nmkdir D:\\a\\pyglet\\lib\ncopy C:\\msys64\\mingw64\\bin\\opengl32.dll D:\\a\\pyglet\\lib\\\n\n# pytest tests/unit\npython -m pyglet.info\n']"
"['docker build . --file Dockerfile --tag localbuild/testimage:latest', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install sphinx sphinx_rtd_theme\npip install -r requirements.txt\npython setup.py develop\n', 'echo ${GITHUB_REF#refs/tags/v} > version.txt\nmake doc\n', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install setuptools wheel pytest\n', 'echo ${GITHUB_REF#refs/tags/v} > version.txt\npython setup.py sdist --formats=gztar,zip\npython setup.py bdist_wheel\n', 'pip install ./dist/*.whl\ncd test\npytest -v test.py::TestConversion\n', 'rm ./dist/*.zip', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest pytest-cov codecov\npython setup.py develop\n', 'pytest -v ./test/test.py::TestConversion --cov=./pdf2docx --cov-report=xml\n', 'codecov\n', 'cd test\\outputs\n$files = Get-ChildItem "".""\nfor ($i=0; $i -lt $files.Count; $i++) { \n  $name = $files[$i].name;\n  echo ""Converting $name to pdf..."";\n  OfficeToPDF $files[$i] \n}\ndel *.docx\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npython setup.py develop\n', 'pytest -sv ./test/test.py::TestQuality']"
"['pip install --user ruff', 'ruff --format=github --line-length=7228 --target-version=py39 --ignore=E402,E712,E721,E722,E741,F401,F403,F405,F541,F601,F811,F822,F841 .', 'ruff --format=github --line-length=366 --target-version=py39 --exclude=tests/ --ignore=E402,E722,E741,F401,F403,F405,F541,F822,F841 .', 'python -m pip install --upgrade pip\npip install -r dev_requirements.txt\npip install -r requirements.txt\n', 'python -m pytest -v tests/\n']"
"['python -m pip install --upgrade pip\npython -m pip install pylint==2.15.2 pycodestyle==2.9.1 pytest-cov\npython -m pip install .\npython -m pip list\n', 'pytest --cov-report=xml --cov=diffprivlib --cov-append\n', 'pycodestyle --max-line-length=120 diffprivlib', 'pylint --fail-under=9.5 -rn diffprivlib', 'python -m pip install --upgrade pip', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\npython -m pip install pytest\npython -m pip install .\npython -m pip list\n', 'pytest\n', 'python -m pip install --upgrade pip\npython -m pip install pytest\npython -m pip install .\n', 'python -m pip install numpy==${{ matrix.version }}\n', 'python -m pip install scikit-learn==${{ matrix.version }}\n', 'python -m pip install scipy==${{ matrix.version }}\n', 'python -m pip install crlibm\n', 'python -m pip list\n', 'pytest\n']"
"['sudo apt-get install libportaudio2', 'pytest tests/ --cov=core/ --cov-report=xml', ""sed -i 's/\\/home\\/runner\\/work\\/ProjectAlice\\/ProjectAlice\\//\\/github\\/workspace\\//g' coverage.xml""]"
[]
"['sudo apt-get update -qq\nsudo apt-get install -y xvfb qtbase5-dev qtdeclarative5-dev libqt5webkit5-dev libsqlite3-dev qt5-default qttools5-dev-tools\n# start xvfb in the background\nsudo /usr/bin/Xvfb $DISPLAY -screen 0 1280x1024x24 &\ncur=`pwd`\nwget http://www.coppeliarobotics.com/files/CoppeliaSim_Edu_V4_1_0_Ubuntu18_04.tar.xz\ntar -xf CoppeliaSim_Edu_V4_1_0_Ubuntu18_04.tar.xz\nexport COPPELIASIM_ROOT=""$cur/CoppeliaSim_Edu_V4_1_0_Ubuntu18_04""\necho $COPPELIASIM_ROOT\nls -al\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT:$COPPELIASIM_ROOT/platforms\nexport QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT\npip3 install -r requirements.txt\npip3 install setuptools\npip3 install .\nmv pyrep _pyrep\npython3 -m unittest discover tests\n']"
""
[]
"['# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', ""pip install -e '.[atari, mujoco, envpool]'\nconda list\n"", 'pip install pytest\npip install pytest-cov\n', 'pytest --cov=./ --cov-config=./.core-coveragerc --cov-report=xml -v\nls -al\n', 'pip install mkdocs-material\npip install mkdocs-minify-plugin\npip install mkdocs-redirects\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-committers-plugin-2\npip install mkdocs-git-authors-plugin\npip install -e .\n', 'bash ./docs/cfg-params.sh', 'mkdocs gh-deploy --force', '# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', ""conda install python=${{ matrix.python-version }}\npip install -e '.[atari, mujoco, envpool]'\nconda list\n"", 'pip install pytest\n', '# run all tests\npytest -s -k ""not torch_tensor_share""\n', '# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', ""conda install python=${{ matrix.python-version }}\npip install -e '.[atari, mujoco]'\nconda list\n"", 'pip install pytest\n', '# run all tests\npytest -s -k ""not torch_tensor_share""\n']"
"['pip3 install nox==2019.11.9', 'pip3 install poetry==1.0.5', 'nox --sessions tests coverage', 'pip install nox', 'pip install poetry', 'nox', 'poetry build', 'poetry publish --username=__token__ --password=${{ secrets.PYPI_TOKEN }}', 'pip install poetry', ""poetry version patch && version=$(poetry version | awk '{print $2}') && poetry version $version.dev.$(date +%s)"", 'poetry build', 'python -m pip install poetry\npoetry install\n', 'plain_text=""hello. Testing Ciphey.""\nciphey_out=$(poetry run ciphey -q -t ""$plain_text"")\nif [ ""$ciphey_out"" == ""$plain_text"" ]\nthen\n  exit 0\nelse\n  echo ""Ciphey decryption on plain text failed""\n  exit 1\nfi\n', 'plain_text=""hello. Testing Ciphey.""\nbase64_encoded=$(echo -n ""$plain_text"" | base64)\nciphey_out=$(poetry run ciphey -q -t ""$base64_encoded"")\nif [ ""$ciphey_out"" == ""$plain_text"" ]\nthen\n  exit 0\nelse\n  echo ""Ciphey decryption on base64 encoded string failed""\n  exit 1\nfi \n', 'python -m pip install --upgrade pip', 'pip install codespell flake8 nox poetry', 'codespell --ignore-words-list=""nd,te"" --skip=""translations,*.archive""', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'python -m nox']"
""
"['echo ""PY=$(python --version --version | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'echo ${{ steps.docker_build.outputs.digest }}']"
"['sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker system prune -af\ndf -h\n', 'cd CI/docker\nchmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.cpu-training -t autogluon-nightly-training:cpu-latest .\ndocker tag autogluon-nightly-training:cpu-latest 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-training:cpu-latest\ndocker push 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-training:cpu-latest\n', 'sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker system prune -af\ndf -h\n', 'cd CI/docker\nchmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.cpu-inference -t autogluon-nightly-inference:cpu-latest .\ndocker tag autogluon-nightly-inference:cpu-latest 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-inference:cpu-latest\ndocker push 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-inference:cpu-latest\n', 'sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker system prune -af\ndf -h\n', 'cd CI/docker\nchmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.gpu-training -t autogluon-nightly-training:gpu-latest .\ndocker tag autogluon-nightly-training:gpu-latest 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-training:gpu-latest\ndocker push 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-training:gpu-latest\n', 'sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker system prune -af\ndf -h\n', 'cd CI/docker\nchmod +x ./login_ecr.sh; ./login_ecr.sh\ndocker build -f Dockerfile.gpu-inference -t autogluon-nightly-inference:gpu-latest .\ndocker tag autogluon-nightly-inference:gpu-latest 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-inference:gpu-latest\ndocker push 369469875935.dkr.ecr.us-east-1.amazonaws.com/autogluon-nightly-inference:gpu-latest\n', 'echo It appears that you have modified multimodal unit tests/docs. Please make sure to update \\""multimodal/tests/hf_model_list.yaml\\"" to include any model changes and label this PR with \\""model list checked\\"".\nexit 1\n', 'echo This is a restricted branch reserved for certain modules. Please use another branch instead\nexit 1\n', ""chmod +x ./.github/workflow_scripts/copy_docs.sh\n./.github/workflow_scripts/copy_docs.sh '${{ github.ref }}' '${{ github.repository }}' '${{ env.SHORT_SHA }}'\n"", 'chmod +x ./.github/workflow_scripts/copy_docs.sh\n./.github/workflow_scripts/copy_docs.sh ""$branch"" \'${{ github.event.pull_request.head.repo.full_name }}\' \'${{ env.SHORT_SHA }}\' PR-\'${{ github.event.number }}\'\n', 'echo ::set-output name=run-url::https://github.com/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID', 'chmod +x ./.github/workflow_scripts/test_common.sh && ./.github/workflow_scripts/test_common.sh\n', 'chmod +x ./.github/workflow_scripts/test_core.sh && ./.github/workflow_scripts/test_core.sh\n', 'chmod +x ./.github/workflow_scripts/test_features.sh && ./.github/workflow_scripts/test_features.sh\n', 'chmod +x ./.github/workflow_scripts/test_eda.sh && ./.github/workflow_scripts/test_eda.sh\n', 'wget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew unlink libomp\nbrew install libomp.rb\nrm libomp.rb\n', 'conda install --channel conda-forge pygraphviz\nchmod +x ./.github/workflow_scripts/test_tabular.sh && ./.github/workflow_scripts/test_tabular.sh ""-m not gpu"" ""true""\n', 'chmod +x ./.github/workflow_scripts/test_install.sh && ./.github/workflow_scripts/test_install.sh\n', 'chmod +x ./.github/workflow_scripts/test_install_windows.sh && ./.github/workflow_scripts/test_install_windows.sh\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc packaging\n', 'for v in common core features tabular multimodal timeseries autogluon eda\ndo\n  cd ""$v""/\n  python setup.py sdist bdist_wheel\n  twine upload dist/* --verbose\n  cd ..\ndone\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc packaging\n', 'for v in common core features tabular multimodal timeseries autogluon eda\ndo\n  cd ""$v""/\n  python setup.py sdist bdist_wheel\n  twine upload dist/* --verbose\n  cd ..\ndone\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc packaging\n', 'for v in common core features tabular multimodal timeseries autogluon eda\ndo\n  cd ""$v""/\n  python setup.py sdist bdist_wheel\n  twine upload --repository testpypi dist/* --verbose\n  cd ..\ndone\n']"
""
"['python -m pip install --upgrade pip\npip install -U setuptools wheel twine build\npip install -r requirements.txt\n', 'python setup.py build_ext --inplace\npython -m build\npython -m twine upload dist/*.tar.gz\n', 'python -c ""import sys; print(sys.version)""', 'pip install -U pip\npip install -U setuptools\npip install -r requirements.txt\npip install -r requirements-test.txt\npython setup.py build_ext --inplace\npython setup.py install\n', 'pytest -vs tests/ --cov causalml/']"
"['pip install -r requirements.txt\n', 'pip install .\npython examples/example_bit.py\npython examples/example_img.py\npython examples/example_no_writing.py\npython examples/example_str.py\npython examples/example_str_multi.py\n']"
"['make install\n', 'make test\n', 'make install\n', 'make test\n']"
"['python -m pip install --upgrade pip\npip install pytest\npip install -r requirements.txt\npip install onnxruntime\npip install -e .\n', 'cd test && pytest -m ""conversion""', 'cd test &&  pytest -m ""not conversion""', 'echo ""Deploying...""\nRUNNER_LABELS=""cml,aws""\nRUNNER_REPO=""https://github.com/${GITHUB_REPOSITORY}""\nMACHINE=""cml$(date +%s)""\ndocker-machine create \\\n  --driver amazonec2 \\\n  --amazonec2-instance-type p3.8xlarge \\\n  --amazonec2-vpc-id $VPC \\\n  --amazonec2-region us-east-1 \\\n  --amazonec2-zone c \\\n  --amazonec2-ssh-user ubuntu \\\n  --amazonec2-ami ami-06a25ee8966373068 \\\n  --amazonec2-root-size 150 \\\n  $MACHINE\neval ""$(docker-machine env --shell sh $MACHINE)""\n\n(\ndocker-machine ssh $MACHINE ""sudo mkdir -p \\\n  /docker_machine && \\\nsudo chmod 777 /docker_machine"" && \\\ndocker-machine scp -r -q ~/.docker/machine/ \\\n  $MACHINE:/docker_machine && \\\ndocker run --name runner -d \\\n  --gpus all \\\n  -v /docker_machine/machine:/root/.docker/machine \\\n  --net host \\\n  --ipc host \\\n  -e DOCKER_MACHINE=$MACHINE \\\n  -e repo_token=$repo_token \\\n  -e RUNNER_LABELS=$RUNNER_LABELS \\\n  -e RUNNER_REPO=$RUNNER_REPO \\\n  -e RUNNER_IDLE_TIMEOUT=120 \\\n  dvcorg/cml-py3:latest && \\\nsleep 20 && echo ""Deployed $MACHINE""\n) || (echo ""Shut down machine"" && docker-machine rm -y -f $MACHINE && exit 1)\n', 'apt-get update -y\napt-get install python3-dev -y\npip install -r requirements.txt\npip install .\ncd test/benchmarks && python question_answering_accuracy.py\necho -en ""## Benchmarks: QA Accuracy\\n"" >> accuracy_report.md\ncat results_accuracy.md >> accuracy_report.md\ncml-send-comment accuracy_report.md\npython question_answering_components.py\necho -en ""## Benchmarks: QA per component\\n"" >> components_report.md\ncat results_per_component.md >> components_report.md\ncml-send-comment components_report.md\n']"
"['sudo apt-get install -y libsndfile1-dev sox git git-lfs', 'pip3 install --upgrade pip wheel', 'pip3 install -r requirements/dev.txt', './ci/format.py --check\n', 'tox -e common_upstream-audio${{ matrix.torchaudio-version }} -- -n 2\n', 'tox -e all_others-deps_all-audio${{ matrix.torchaudio-version }} -- -n 2\n', 'python3 -m venv ""$HOME/s3prl_env""\necho ""$HOME/s3prl_env/bin"" >> $GITHUB_PATH\n', 'sudo apt-get install -y libsndfile1-dev sox', 'pip3 install --upgrade pip wheel', 'pip install -e "".[dev]""\n', 'cd docs/\n./rebuild_docs.sh\n']"
""
"['make check\nmake mypy\n', 'poetry build\npoetry config pypi-token.pypi ${{ secrets.PYPI_TOKEN }}\npoetry publish\n', 'make change-version\n', 'poetry build\npoetry config pypi-token.testpypi ${{ secrets.TEST_PYPI_TOKEN }}\npoetry config repositories.testpypi ""https://test.pypi.org/legacy/""\npoetry publish --repository testpypi\n', 'poetry install', 'make test\n']"
"['pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[contrastive]', 'python ./tests/contrastive/test_CEM_MAF.py', 'python ./tests/contrastive/test_CEM.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .', 'python ./tests/ted/test_TED_Cartesian.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[matching]', 'python ./tests/matching/test_order_constraints.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[dipvae]', 'python ./tests/dipvae/test_DIPVAE.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[cofrnet]', 'python ./tests/cofrnet/cofrnet_test.py', 'pytest --nbmake ./examples/cofrnet/cofrnet_example.ipynb', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[protodash]', 'python ./tests/protodash/test_PDASH.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[rbm]', 'python ./tests/rbm/test_Boolean_Rule_CG.py', 'python ./tests/rbm/test_Linear_Rule_Regression.py', 'python ./tests/rbm/test_Logistic_Rule_Regression.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[nncontrastive]', 'python ./tests/nncontrastive/test_nncontrastive.py', 'pip3 install pytest nbmake wheel --upgrade setuptools', 'pip3 install .[tsice]', 'python ./tests/tsice/test_tsice.py']"
"['echo ""$GITHUB_CONTEXT""', 'pip install git+https://${{ secrets.EXPLOSIONBOT_TOKEN }}@github.com/explosion/explosion-bot\npython -m explosionbot\n', 'python -m pip install ""torch==1.8.1+cpu"" -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -m pip install -U build pip setuptools wheel\npython -m pip install -r requirements.txt\n', 'python -m build --sdist\n', 'python -m mypy $MODULE_NAME\n', 'rm -rf $MODULE_NAME\n', 'python -m pip freeze --exclude pywin32 --exclude torch\npython -m pip freeze --exclude pywin32 --exclude torch > installed.txt\npython -m pip uninstall -y -r installed.txt\n', 'python -m pip install ""torch==1.8.1+cpu"" -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -m pip install torch --index-url https://download.pytorch.org/whl/cpu\n', 'SDIST=$(python -c ""import os;print(os.listdir(\'./dist\')[-1])"" 2>&1)\npython -m pip install dist/$SDIST\n', 'python -m pip install -r requirements.txt\npython -m pytest --pyargs $MODULE_NAME --cov=$MODULE_NAME\n', 'python -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0-py3-none-any.whl --no-deps\npython -c ""import spacy; nlp = spacy.load(\'en_core_web_trf\'); doc = nlp(\'test\')""\n', 'python -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.4.0/en_core_web_trf-3.4.0-py3-none-any.whl --no-deps\npython -c ""import spacy; nlp = spacy.load(\'en_core_web_trf\'); doc = nlp(\'test\')""\n']"
""
"['make dev_install', 'pip install --upgrade pip setuptools codecov', 'pip install --upgrade --user pip setuptools codecov', 'make dev_install', 'make -e test\ncodecov\n']"
""
"['python -m pip install --upgrade pip', 'pip install -e "".[tests]"" && pip install tensorflow==2.2.0', 'make lint', 'black tf_explain tests --check', 'python -m pip install --upgrade pip', 'pip install -e "".[tests]"" && pip install opencv-python && pip install tensorflow==${{ matrix.tensorflow-version }}', 'make test']"
"['chmod +x ghostwriter-cli-linux\n./ghostwriter-cli-linux install --dev\n', 'docker-compose -f local.yml run django coverage run manage.py test --exclude-tag=GitHub\ndocker-compose -f local.yml run django coverage xml -o ""coverage/reports/coverage.xml""\n']"
""
"['pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --max-complexity 20 --max-line-length 127 --statistics\n', 'make install-keras\n', 'make install-pytorch\n', 'make test\n']"
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'make tests']"
"['black --check ipycanvas\nblacken-docs docs/*.rst\n', ""jupytext examples/*.ipynb --check 'black --check {}'"", 'python -m build\ncd dist\nsha256sum * | tee SHA256SUMS\n', 'cd dist\npip install -vv ${{ matrix.dist }}\n', 'ls $CONDA_PREFIX/share/jupyter/nbextensions\nls $CONDA_PREFIX/share/jupyter/labextensions\n', 'test -d $CONDA_PREFIX/share/jupyter/nbextensions/ipycanvas\ntest -f $CONDA_PREFIX/share/jupyter/nbextensions/ipycanvas/extension.js\ntest -f $CONDA_PREFIX/share/jupyter/nbextensions/ipycanvas/index.js\ntest -d $CONDA_PREFIX/share/jupyter/labextensions/ipycanvas\ntest -f $CONDA_PREFIX/share/jupyter/labextensions/ipycanvas/package.json\ntest -d $CONDA_PREFIX/share/jupyter/labextensions/ipycanvas/static\n', 'jupyter nbextension list 2>&1 | grep ""ipycanvas/extension""', 'jupyter labextension list 2>&1 | grep ipycanvas', 'yarn install\nyarn playwright install chromium\n', 'yarn run start:detached', 'yarn run test', 'python setup.py sdist bdist_wheel', 'twine upload dist/ipycanvas*', 'yarn install\nnpm publish\n', 'git config --global hub.protocol https', 'hub pr checkout ${{ github.event.issue.number }}', 'pip install -vv .']"
"['npm install -g yarn', 'yarn install --network-timeout 1000000', '${{ matrix.python }} -m pip install -r requirements.txt\n${{ matrix.python }} -m pip install pyinstaller\n', '${{ matrix.python }} -m pip install -r requirements-gpu.txt -f https://download.pytorch.org/whl/torch_stable.html --user', '${{ matrix.python }} -m pip install -r requirements-cpu.txt -f https://download.pytorch.org/whl/torch_stable.html --user', 'brew install p7zip libomp\n${{ matrix.python }} -m pip install -r requirements-gpu.txt --user\n', '${{ matrix.python }} build.py --debug', 'node deploy.js', '${{ matrix.python }} -m pip install -r requirements.txt\n${{ matrix.python }} -m pip install pyinstaller\n', 'brew install p7zip libomp', '${{ matrix.python }} -m pip install --pre -r requirements-gpu.txt -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html --user', '${{ matrix.python }} -m pip install --pre -r requirements-cpu.txt -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --user', '${{ matrix.python }} build.py --debug']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install coverage\npip install coveralls\n', 'coverage run --source=combo -m pytest\n', 'coveralls --service=github\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install coverage\npip install coveralls\n', 'coverage run --source=combo -m pytest\n', 'coveralls --service=github\n']"
"['gh auth status\ngh pr list --repo ""${{ github.repository }}"" --assignee ""machineFL"" --base main --state open --search ""status:success review:required"" --limit 1 --json number > dep_PRs_waiting_approval.json\ndep_pull_request=$(cat dep_PRs_waiting_approval.json | grep -Eo ""[0-9]*"")\necho ::set-output name=dep_pull_request::${dep_pull_request}\n', 'gh pr review --repo ""${{ github.repository }}"" --comment --body ""auto approve"" ${{ steps.find_prs.outputs.dep_pull_request }}\ngh pr review --repo ""${{ github.repository }}"" --approve ${{ steps.find_prs.outputs.dep_pull_request }}\ngh pr merge --repo ""${{ github.repository }}"" --auto --squash --delete-branch ${{ steps.find_prs.outputs.dep_pull_request }}\n', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmake installdeps-test\n', 'git clone -b main --single-branch https://github.com/conda-forge/evalml-core-feedstock\ncp .github/meta.yaml ./evalml-core-feedstock/recipe/\npip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmkdir evalml-core-feedstock/evalml\ncp -r `ls -A | grep -v ""evalml-core-feedstock""` ./evalml-core-feedstock/evalml/\npython .github/conda_config.py ""$(python -c ""import evalml; print(evalml.__version__)"")""\ncd evalml-core-feedstock\necho ""Pre docker login""\necho ""${DOCKERHUB_PASSWORD}"" | docker login -u ""${DOCKERHUB_USER}"" --password-stdin\necho ""Post docker login""\nexport DOCKER_CONTAINERID=""$(docker run -td condaforge/linux-anvil-cos7-x86_64)""\necho ""Created container ${DOCKER_CONTAINERID}""\nchmod -R 777 ./\ndocker cp . ""${DOCKER_CONTAINERID}"":/home/conda/feedstock_root/\ndocker cp ./recipe/. ""${DOCKER_CONTAINERID}"":/home/conda/recipe_root/\necho ""COMMITING UPDATED IMAGE""\ndocker commit ""${DOCKER_CONTAINERID}"" psalter/build:latest\ndocker stop ""${DOCKER_CONTAINERID}""\nexport CONFIG=linux_64_\nexport UPLOAD_PACKAGES=False\nexport HOST_USER_ID=$(id -u)\nexport FEEDSTOCK_NAME=evalml-core-feedstock\necho ""Running docker""\ndocker run -t -e CONFIG -e HOST_USER_ID -e UPLOAD_PACKAGES -e GIT_BRANCH -e UPLOAD_ON_BRANCH -e CI -e FEEDSTOCK_NAME -e CPU_COUNT -e BINSTAR_TOKEN -e FEEDSTOCK_TOKEN -e STAGING_BINSTAR_TOKEN psalter/build:latest bash /home/conda/feedstock_root/.scripts/build_steps.sh\n', 'make installdeps-test\n', 'python .github/conda_version_check.py\n', 'make installdeps-test\n', 'mkdir /tmp/dependencies_updated_artifacts\nexport DEPENDENCY_FILE_PATH=/tmp/dependencies_updated_artifacts/current_dependencies.txt\nevalml/tests/dependency_update_check/make_deps_diff.sh\ndiff evalml/tests/dependency_update_check/latest_dependency_versions.txt /tmp/dependencies_updated_artifacts/current_dependencies.txt > /tmp/dependencies_updated_artifacts/diff.txt\n', 'echo Displaying dependencies which have changed, with main on the left and the new branch on the right:\ncat /tmp/dependencies_updated_artifacts/diff.txt\nexit 1\n', 'echo ""DEFAULT_TIMESTAMP=$(date +""%Y-%m-%dT%H:%M:%S.%3NZ"")"" >> $GITHUB_ENV\n', 'echo ""CURRENT_HASH=$(git rev-parse --short HEAD)"" >> $GITHUB_ENV\necho ""Latest commit hash: ${{ env.CURRENT_HASH }}""\necho ""PREVIOUS_HASH=$(git rev-parse --short HEAD~1)"" >> $GITHUB_ENV\necho ""Previous commit hash: ${{ env.PREVIOUS_HASH }}""\n', 'curl --location --request POST \'${{ secrets.AIRFLOW_BASE_URL }}dags/evalml_automl_run_tests_generate_report/dagRuns\' \\\n-u \'${{ secrets.AIRFLOW_USER }}:${{ secrets.AIRFLOW_PASS }}\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n  ""conf"": {\n        ""description"": ""${{ env.CURRENT_HASH }}_default )"",\n        ""n_trials"": 1,\n        ""pytest_args"": {\n            ""automl-algo"": ""default"",\n            ""ensembling"": false,\n            ""max-batches"": 0,\n            ""max-iterations"": 0,\n            ""holdout-size"": 0.5,\n            ""pred-vs-actual"": false\n        },\n        ""python_version"": ""3.8"",\n        ""scenarios_yaml"": ""release.yaml"",\n        ""evalml_branch_previous"": ""${{ env.PREVIOUS_HASH }}"",\n        ""evalml_branch_new"": ""${{ env.CURRENT_HASH }}"",\n        ""username"": ""${{ secrets.AIRFLOW_USER }}"",\n        ""author"": ""${{ github.event.head_commit.author.name }}""\n      },\n  ""logical_date"": ""${{ env.DEFAULT_TIMESTAMP }}"",\n  ""dag_run_id"": ""api_evalml_automl_run_tests_generate_report_default_${{ env.DEFAULT_TIMESTAMP }}""\n}\'\n', 'echo ""ITERATIVE_TIMESTAMP=$(date +""%Y-%m-%dT%H:%M:%S.%3NZ"")"" >> $GITHUB_ENV\n', 'curl --location --request POST \'${{ secrets.AIRFLOW_BASE_URL }}dags/evalml_automl_run_tests_generate_report/dagRuns\' \\\n-u \'${{ secrets.AIRFLOW_USER }}:${{ secrets.AIRFLOW_PASS }}\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n  ""conf"": {\n        ""description"": ""${{ env.CURRENT_HASH }}_iterative )"",\n        ""n_trials"": 1,\n        ""pytest_args"": {\n            ""automl-algo"": ""iterative"",\n            ""ensembling"": false,\n            ""max-batches"": 0,\n            ""max-iterations"": 0,\n            ""holdout-size"": 0.5,\n            ""pred-vs-actual"": false\n        },\n        ""python_version"": ""3.8"",\n        ""scenarios_yaml"": ""release.yaml"",\n        ""evalml_branch_previous"": ""${{ env.PREVIOUS_HASH }}"",\n        ""evalml_branch_new"": ""${{ env.CURRENT_HASH }}"",\n        ""username"": ""${{ secrets.AIRFLOW_USER }}"",\n        ""author"": ""${{ github.event.head_commit.author.name }}""\n      },\n  ""logical_date"": ""${{ env.ITERATIVE_TIMESTAMP }}"",\n  ""dag_run_id"": ""api_evalml_automl_run_tests_generate_report_iterative_${{ env.ITERATIVE_TIMESTAMP }}""\n}\'\n', 'current_hash=$(git rev-parse --short HEAD)\necho ""Latest commit hash: $current_hash""\necho ""::set-output name=current_hash::$current_hash""\nprevious_hash=$(git rev-parse --short HEAD~1)\necho ""Previous commit hash: $previous_hash""\necho ""::set-output name=previous_hash::$previous_hash""\n', 'make installdeps\nmake installdeps-test\n', 'response=$( looking-glass run-evalml-automl --username machineFL --automl-algo default --scenarios-yaml release.yaml --evalml-branch ${{ steps.get_hashes.outputs.current_hash }} --job-desc ${{ steps.get_hashes.outputs.current_hash }}_default )\necho ""::set-output name=job_id::$(echo $response | sed \'s/.*Job ID: \\([^ ]*\\).*/\\1/\')""\n', 'response=$( looking-glass run-evalml-automl --username machineFL --automl-algo iterative --scenarios-yaml release.yaml --evalml-branch ${{ steps.get_hashes.outputs.current_hash }} --job-desc ${{ steps.get_hashes.outputs.current_hash }}_iterative )\necho ""::set-output name=job_id::$(echo $response | sed \'s/.*Job ID: \\([^ ]*\\).*/\\1/\')""\n', 'response=$( looking-glass run-evalml-automl --username machineFL --automl-algo default --scenarios-yaml release.yaml --evalml-branch ${{ steps.get_hashes.outputs.previous_hash }} --job-desc ${{ steps.get_hashes.outputs.previous_hash }}_default )\necho ""::set-output name=job_id::$(echo $response | sed \'s/.*Job ID: \\([^ ]*\\).*/\\1/\')""\n', 'response=$( looking-glass run-evalml-automl --username machineFL --automl-algo iterative --scenarios-yaml release.yaml --evalml-branch ${{ steps.get_hashes.outputs.previous_hash }} --job-desc ${{ steps.get_hashes.outputs.previous_hash }}_iterative )\necho ""::set-output name=job_id::$(echo $response | sed \'s/.*Job ID: \\([^ ]*\\).*/\\1/\')""\n', 'for id in ${{ steps.current_default.outputs.job_id }} ${{ steps.current_iterative.outputs.job_id }} ${{ steps.previous_default.outputs.job_id }} ${{ steps.previous_iterative.outputs.job_id }}; do\n  echo ""Waiting for job id: $id""\n  result=\'\'\n  sleep_time=0\n  while [ -z ""$result""  ]\n  do\n    sleep $sleep_time\n    result=$(looking-glass get-job --job-id $id | grep -n \'COMPLETED\' || :;)\n    sleep_time=60\n  done\n  looking-glass get-results --job-id $id --file-path $id\ndone\n', 'filename_base=${{ steps.get_hashes.outputs.current_hash }}_v_${{ steps.get_hashes.outputs.previous_hash }}\nlooking-glass run-local-report --previous-path ${{ steps.previous_default.outputs.job_id }} -n ${{ steps.current_default.outputs.job_id }} --output-name ${filename_base}_default --output-type html\nlooking-glass run-local-report --previous-path ${{ steps.previous_iterative.outputs.job_id }} -n ${{ steps.current_iterative.outputs.job_id }} --output-name ${filename_base}_iterative --output-type html\n', 'filename_base=${{ steps.get_hashes.outputs.current_hash }}_v_${{ steps.get_hashes.outputs.previous_hash }}\ndefault_url=s3://evalml-main-looking-glass-reports/${filename_base}_default.html\naws s3 cp ./${filename_base}_default.html $default_url\necho ""::set-output name=default_report_url::$default_url""\necho ""Default Algorith Report: $default_url""\niterative_url=s3://evalml-main-looking-glass-reports/${filename_base}_iterative.html\naws s3 cp ./${filename_base}_iterative.html $iterative_url\necho ""::set-output name=iterative_report_url::$iterative_url""\necho ""Iterative Algorith Report: $iterative_url""\n', 'filename_base=${{ steps.get_hashes.outputs.current_hash }}_v_${{ steps.get_hashes.outputs.previous_hash }}\npresigned_default_url=$( aws s3 presign ${{ steps.s3_upload.outputs.default_report_url }} --expires-in 604800 )\npresigned_iterative_url=$( aws s3 presign ${{ steps.s3_upload.outputs.iterative_report_url }} --expires-in 604800 )\nresponse=$(\ncurl -X POST https://slack.com/api/chat.postMessage -H \'Content-type: application/json;charset=UTF-8\' -H \'Authorization: Bearer ${{ secrets.LG_SLACK_TOKEN }}\' \\\n--data-binary @- << EOF\n{\n  ""channel"": ""C042B5JBBPF"",\n  ""blocks"": [\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""<!subteam^${{ secrets.LG_SLACK_NOTIFICATION_USER_GROUP }}>, performance tests for latest commit on evalml are complete""\n      }\n    },\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""*Author*: ${{ github.event.head_commit.author.name }}""\n      }\n    },\n    {\n      ""type"": ""section"",\n      ""fields"": [\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Current Commit*: <${{ github.event.head_commit.url }}|${{ steps.get_hashes.outputs.current_hash }}>""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Previous Commit*: <https://github.com/alteryx/evalml/commit/${{ steps.get_hashes.outputs.previous_hash }}|${{ steps.get_hashes.outputs.previous_hash }}>""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Current Default Algo Job*: ${{ steps.current_default.outputs.job_id }}""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Previous Default Algo Job*: ${{ steps.previous_default.outputs.job_id }}""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Current Iterative Algo Job*: ${{ steps.current_iterative.outputs.job_id }}""\n        },\n        {\n          ""type"": ""mrkdwn"",\n          ""text"": ""*Previous Iterative Algo Job*: ${{ steps.previous_iterative.outputs.job_id }}""\n        },\n      ]\n    },\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""*Default Algorithm Report*: <$presigned_default_url|${{ steps.s3_upload.outputs.default_report_url }}>""\n      }\n    },\n    {\n      ""type"": ""section"",\n      ""text"": {\n        ""type"": ""mrkdwn"",\n        ""text"": ""*Iterative Algorithm Report*: <$presigned_iterative_url|${{ steps.s3_upload.outputs.iterative_report_url }}>""\n      }\n    }\n  ]\n}\nEOF\n)\necho $response\nts=$(echo $response | jq .ts)\ncurl -F ""file=@${filename_base}_default.html"" -F channels=C042B5JBBPF -F \'thread_ts=$ts\' -H \'Authorization: Bearer ${{ secrets.LG_SLACK_TOKEN }}\'  https://slack.com/api/files.upload\ncurl -F ""file=@${filename_base}_iterative.html"" -F channels=C042B5JBBPF -F \'thread_ts=$ts\' -H \'Authorization: Bearer ${{ secrets.LG_SLACK_TOKEN }}\'  https://slack.com/api/files.upload\n', 'make package', 'python -m pip install ""unpacked_sdist/.""\n', 'python -m pip install ""unpacked_sdist/."" --no-deps\n', 'python -c ""import evalml""\npython -c ""from evalml.demos import load_fraud; load_fraud(n_rows=10)""\npython -c ""from evalml.demos import load_churn; load_churn(n_rows=10)""\n', 'python -m pip check\n', 'python -m pip install ""unpacked_sdist/[updater]""\n', 'python -c ""import alteryx_open_src_update_checker""\n', 'make installdeps-test\nexport DEPENDENCY_FILE_PATH=evalml/tests/dependency_update_check/latest_dependency_versions.txt\nevalml/tests/dependency_update_check/make_deps_diff.sh\ncat evalml/tests/dependency_update_check/latest_dependency_versions.txt\n', 'python -m pip install -e .[dev]\n', 'python -m pip install -e .[dev] --no-deps\n', 'make lint', 'sudo apt update && sudo apt install -y graphviz', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmake installdeps\nmake installdeps-test\npip freeze\n', 'source test_python/bin/activate\ncoverage erase\n', 'source test_python/bin/activate\nmake ${{matrix.command}}\n', 'pip install ""coverage[toml]""', 'sudo apt update && sudo apt install -y graphviz', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\npython -m pip install --upgrade pip -q\n', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmake installdeps-prophet\nmake installdeps-test\npip freeze\n', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmake installdeps\nmake installdeps-test\npip freeze\n', 'source test_python/bin/activate\nmake ${{matrix.command}}\n', 'sudo apt update && sudo apt install -y graphviz', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmake installdeps\nmake installdeps-test\npip freeze\n', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\nmake installdeps-prophet\nmake installdeps-test\npip freeze\n', 'source test_python/bin/activate\ncoverage erase\n', 'source test_python/bin/activate\nmake ${{matrix.command}}\n', 'pip install ""coverage[toml]""', 'sudo apt update && sudo apt install -y graphviz', 'pip install virtualenv\nvirtualenv test_python -q\nsource test_python/bin/activate\npython -m pip install --upgrade pip -q\n', 'source test_python/bin/activate\npip install prophet-prebuilt==1.0.2\npip install -e . --no-dependencies\npip install -r evalml/tests/dependency_update_check/minimum_test_requirements.txt\npip install -r evalml/tests/dependency_update_check/minimum_requirements.txt\n', 'source test_python/bin/activate\npip install -e . --no-dependencies\npip install -r evalml/tests/dependency_update_check/minimum_test_requirements.txt\npip install -r evalml/tests/dependency_update_check/minimum_requirements.txt\n', 'source test_python/bin/activate\nmake ${{ matrix.command }}\n', 'mkdir /tmp/dependencies_updated_artifacts\nprintf ""${{ steps.min_dep_gen_test.outputs.min_reqs }}"" > /tmp/minimum_test_requirements.txt\ndiff /tmp/minimum_test_requirements.txt evalml/tests/dependency_update_check/minimum_test_requirements.txt\n', 'printf ""${{ steps.min_dep_gen_test.outputs.min_reqs }}"" > evalml/tests/dependency_update_check/minimum_test_requirements.txt\n', 'printf ""${{ steps.min_dep_gen_reqs.outputs.min_reqs }}"" > /tmp/minimum_requirements.txt\ndiff /tmp/minimum_requirements.txt evalml/tests/dependency_update_check/minimum_requirements.txt\n', 'printf ""${{ steps.min_dep_gen_reqs.outputs.min_reqs }}"" > evalml/tests/dependency_update_check/minimum_requirements.txt\n', 'rm -rf docs/', 'if [[ $(expr match ""${{ github.event.pull_request.head.ref }}"" ""release_v[0-9.]\\+"") -gt 0 ]]; then\n  echo This is a release PR;\nelif [[ $(expr match ""${{ github.event.pull_request.head.ref }}"" ""latest-dep-update-[a-zA-Z0-9]*"") -gt 0 ]]; then\n  echo This is a latest dependency update PR;\nelif [[ $(expr match ""${{ github.event.pull_request.head.ref }}"" ""min-dep-update-[a-zA-Z0-9]*"") -gt 0 ]]; then\n  echo This is a minimum dependency update PR;\nelse\n  echo This is a regular PR;\nfi\n  echo ""PR #: ${{ github.event.number }}""\n', 'if [[ $(expr match ""${{ github.event.pull_request.head.ref }}"" ""release_v[0-9.]\\+"") -gt 0 ]]; then\n  exit 0;\nelif [[ $(expr match ""${{ github.event.pull_request.head.ref }}"" ""latest-dep-update-[a-zA-Z0-9]*"") -gt 0 ]]; then\n  exit 0;\nelif [[ $(expr match ""${{ github.event.pull_request.head.ref }}"" ""min-dep-update-[a-zA-Z0-9]*"") -gt 0 ]]; then\n  exit 0;\nfi\ncat docs/source/release_notes.rst | grep "":pr:\\`${{ github.event.number }}\\`""\n', 'sudo apt update && sudo apt install -y graphviz', 'pip config --site set global.progress_bar off\npython -m pip install -U pip\n', 'make installdeps-prophet\nmake installdeps-test\npython -m pip uninstall -y featuretools\npython -m pip install https://github.com/alteryx/featuretools/archive/main.zip\n', 'echo ""Run unit tests without code coverage for ${{ matrix.python_version }} and ${{ matrix.libraries }}""\necho ""Testing with EvalML version:"" `python -c ""import evalml; print(evalml.__version__)""`\npytest evalml/ -n 2 --ignore=evalml/tests/automl_tests/parallel_tests\npip check\n', 'sudo apt update && sudo apt install -y graphviz', 'pip config --site set global.progress_bar off\npython -m pip install -U pip\n', 'make installdeps-prophet\nmake installdeps-test\npython -m pip uninstall -y woodwork\npython -m pip install https://github.com/alteryx/woodwork/archive/main.zip\n', 'echo ""Run unit tests without code coverage for ${{ matrix.python_version }} and ${{ matrix.libraries }}""\necho ""Testing with EvalML version:"" `python -c ""import evalml; print(evalml.__version__)""`\npytest evalml/ -n 2 --ignore=evalml/tests/automl_tests/parallel_tests\npip check\n', '$File = ""Miniconda3-4.7.12.1-Windows-x86_64.exe""\n$Uri = ""https://repo.anaconda.com/miniconda/$File""\n$ProgressPreference = ""silentlyContinue""\nInvoke-WebRequest -Uri $Uri -Outfile ""$env:USERPROFILE/$File""\n$hashFromFile = Get-FileHash ""$env:USERPROFILE/$File"" -Algorithm SHA256\n$hashFromUrl = ""f18060cc0bb50ae75e4d602b7ce35197c8e31e81288d069b758594f1bb46ab45""\nif ($hashFromFile.Hash -ne ""$hashFromUrl"") {\n  Throw ""File hashes did not match!""\n}\n', 'start /wait """" %UserProfile%\\Miniconda3-4.7.12.1-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /AddToPath=1 /S /D=%UserProfile%\\Miniconda3', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda create -n curr_py python=${{ matrix.python_version }}\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda config --add channels conda-forge\nconda activate curr_py\nconda install make -q -y\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\nconda config --add channels conda-forge\nconda install -q -y -c conda-forge python-graphviz graphviz\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\nconda install numba -q -y\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\npython -m pip install --upgrade pip\npython -m pip install .[test]\npython -m pip install .[prophet]\npip freeze\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\npython -m pip install --upgrade pip\npython -m pip install .[test]\npip freeze\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\nmake ${{matrix.command}}\n', '$File = ""Miniconda3-4.7.12.1-Windows-x86_64.exe""\n$Uri = ""https://repo.anaconda.com/miniconda/$File""\n$ProgressPreference = ""silentlyContinue""\nInvoke-WebRequest -Uri $Uri -Outfile ""$env:USERPROFILE/$File""\n$hashFromFile = Get-FileHash ""$env:USERPROFILE/$File"" -Algorithm SHA256\n$hashFromUrl = ""f18060cc0bb50ae75e4d602b7ce35197c8e31e81288d069b758594f1bb46ab45""\nif ($hashFromFile.Hash -ne ""$hashFromUrl"") {\n  Throw ""File hashes did not match!""\n}\n', 'start /wait """" %UserProfile%\\Miniconda3-4.7.12.1-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /AddToPath=1 /S /D=%UserProfile%\\Miniconda3', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda create -n curr_py python=${{ matrix.python_version }}\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda config --add channels conda-forge\nconda activate curr_py\nconda install make -q -y\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\nconda config --add channels conda-forge\nconda install -q -y -c conda-forge python-graphviz graphviz\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\nconda install numba -q -y\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\npython -m pip install --upgrade pip\npython -m pip install .[test]\npython -m pip install .[prophet]\npip freeze\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\npython -m pip install --upgrade pip\npython -m pip install .[test]\npip freeze\n', '. $env:USERPROFILE\\Miniconda3\\shell\\condabin\\conda-hook.ps1\nconda activate curr_py\nmake ${{matrix.command}}\n']"
"['# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', './build_tools/build.sh', './build_tools/test.sh']"
""
""
"['bash scripts/pre_build.sh', 'sudo apt update && sudo apt install -y wget\nwget -q https://fastdl.mongodb.org/tools/db/mongodb-database-tools-ubuntu2204-x86_64-100.6.1.deb\nsudo dpkg -i mongodb-database-tools-ubuntu2204-x86_64-100.6.1.deb\nmongorestore --gzip --archive=assets/ci.gz\nmake ci-test\n', 'rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', 'curl ""https://api.telegram.org/bot$TOKEN/sendMessage?chat_id=260260121&text=Normal%20Build%20complete!""\necho ""YYeTsBot Build complete!""\n', 'rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', 'sudo apt update && sudo apt install -y wget\nwget -q https://fastdl.mongodb.org/tools/db/mongodb-database-tools-ubuntu2204-x86_64-100.6.1.deb\nsudo dpkg -i mongodb-database-tools-ubuntu2204-x86_64-100.6.1.deb\nmongorestore --gzip --archive=assets/ci.gz\nmake ci-test\n', 'wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -\necho deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list\nsudo apt-get update\nsudo apt-get install trivy -y\n', 'sudo apt-get update\nsudo apt-get install -y libgtk-3-dev libwebkit2gtk-4.0-dev libappindicator3-dev librsvg2-dev patchelf\n', 'rustup target add aarch64-apple-darwin\n', 'bash scripts/pre_build.sh\necho ""REACT_APP_TAURI=https://yyets.dmesg.app"" >> YYeTsFE/.env\n', 'npm install -g yarn\ncd YYeTsFE\nyarn --network-timeout 1000000\ncd ${{ env.GITHUB_WORKSPACE }}\n', 'echo ""BUILD_ARGS=--target universal-apple-darwin"" >> $GITHUB_ENV', 'npm install -g yarn\nmake all\nls yyetsweb/builds/\n']"
""
""
""
"['python -m pip install --upgrade pip\npip install -r requirements/test.txt\npip install -e .\n', 'pytest -vv\n']"
""
"['powershell Start-Process -PassThru -Wait PowerShell -ArgumentList ""\'-Command Set-MpPreference -DisableArchiveScanning \\$true\'""\npowershell Start-Process -PassThru -Wait PowerShell -ArgumentList ""\'-Command Set-MpPreference -DisableBehaviorMonitoring \\$true\'""\npowershell Start-Process -PassThru -Wait PowerShell -ArgumentList ""\'-Command Set-MpPreference -DisableRealtimeMonitoring \\$true\'""\npowershell Start-Process -PassThru -Wait PowerShell -ArgumentList ""\'-Command  Add-MpPreference -ExclusionPath $GITHUB_WORKSPACE\'""\npip3 install setuptools wheel\npip3 install .\ncd examples\nrm -rf rootfs\ncurl -LJk -o master.zip https://github.com/qilingframework/rootfs/archive/refs/heads/master.zip && unzip master.zip\nmv rootfs-master rootfs\ncd $GITHUB_WORKSPACE\ncmd.exe //C \'examples\\scripts\\dllscollector.bat\'\ncd $GITHUB_WORKSPACE/examples/rootfs/x86_windows/bin\nunzip -Pinfected wannacry.bin.zip\nunzip -Pinfected UselessDisk.bin.zip\nunzip -Pinfected GandCrab502.bin.zip\nunzip -Pinfected al-khaser.bin.zip\nunzip -Pinfected sality.dll.zip\ncd $GITHUB_WORKSPACE/tests\ncmd.exe //C \'.\\test_pe.bat\'\n', 'cd examples\nrm -rf rootfs\nwget https://github.com/qilingframework/rootfs/archive/refs/heads/master.zip\nunzip master.zip && mv rootfs-master rootfs\ncd ../qiling\ncd ../examples/rootfs/x86_linux/kernel && unzip -P infected m0hamed_rootkit.ko.zip\ncd ../../../../\npip3 install -e .[evm,RE]\n\nif [ ${{ matrix.os }} == \'ubuntu-18.04\' ] and [ ${{ matrix.python-version }} == \'3.9\' ]; then\n  docker run -it --rm -v ${GITHUB_WORKSPACE}:/qiling qilingframework/qiling:dev bash -c ""cd tests && ./test_onlinux.sh""\nelse\n  pip3 install setuptools wheel\n  cd tests && ./test_onlinux.sh\nfi\n', 'pip install setuptools wheel\n', 'pip install .\npython setup.py sdist bdist_wheel\n']"
""
"['python -m pip install --upgrade pip\npip install bs4 requests\n', 'python scripts/check_pull_request.py --check_solution --pr_number ${{ github.event.number }}\necho ""SOLUTION_LANG=`head -1 result`"" >> $GITHUB_ENV\n', 'python scripts/check_pull_request.py --pr_number ${{ github.event.number }}\n', 'python -m pip install --upgrade pip\npip install bs4 requests\n', 'python scripts/check_pull_request.py --check_solution --pr_number ${{ github.event.number }}\n', 'python scripts/check_pull_request.py --pr_number ${{ github.event.number }}\n', 'python -m pip install --upgrade pip\npip install pytz\n', 'python scripts/pick_problem.py\n', 'git config --local user.email ""tony9402@naver.com""\ngit config --local user.name ""tony9402""\ngit add .\ngit commit -m ""Today Problem""\n', 'python -m pip install --upgrade pip\npip install pytz\n', 'python scripts/auto_update.py --push\n', 'git config --local user.email ""tony9402@naver.com""\ngit config --local user.name ""tony9402""\ngit add .\ngit commit -m ""Auto Update (PUSH)""\ngit push\n', 'python -m pip install --upgrade pip\npip install pytz\n', 'python scripts/auto_update.py --all\n', 'git config --local user.email ""tony9402@naver.com""\ngit config --local user.name ""tony9402""\ngit add .\ngit commit -m ""Auto UPDATE (ALL)""\n']"
""
"['python3 --version\npython3 -m pip install -U pip\npip3 install cython pybind11\npip3 install scipy==1.10.1\npip3 install torch==${{ matrix.pytorch }}\npip3 install torchvision==${{ matrix.torchvision }}\npip3 install chardet==3.0.4  # can be remove when fix in: https://github.com/aio-libs/aiohttp/issues/5366\npip3 install requests numpy gym gsutil tqdm\nmake dev\n', 'make lint', 'make tests']"
""
[]
"['pip install pre-commit', 'pre-commit run --all-files', 'pip install tox tox-gh-actions', 'python -m tox -e ${{ matrix.tox_env }}', 'python -m tox', 'pip install -r example/requirements.txt', 'python example/main.py &', 'echo ""buster_tag=$(echo -n ${GITHUB_REF} | sed -E \'s/refs\\/(heads|tags)\\///g\' | sed -e \'s/\\//-/g\' | cat - <(echo ""-buster""))"" >> $GITHUB_ENV', 'pip install ""hatchling==1.12.*""', 'hatchling build', 'sudo apt-get update && sudo apt-get install -y --no-install-recommends sqlite3', 'pip install mutmut', 'pip install "".[tests]""', 'mutmut run --paths-to-mutate ""src/"" --tests-dir ""test/"" --paths-to-exclude=definitions.py --runner ""pytest -n auto -x -q test""', 'while read -r line; do mutmut show $line; done <<< $( sqlite3 .mutmut-cache ""select id from Mutant where status in (\'bad_survived\', \'bad_timeout\', \'ok_suspicious\');"" );', 'pip install "".[tests]""', 'pytest -m hypothesis -m hypothesis_nested --hypothesis-profile CI test', 'pip install "".[tests]""', ""pytest test-corpus -k 'azure.com'"", 'pip install "".[tests]""', ""pytest test-corpus -k 'microsoft.com and graph'"", 'pip install "".[tests]""', ""pytest test-corpus -k 'amazonaws.com'"", 'pip install "".[tests]""', ""pytest test-corpus -k 'googleapis.com'"", 'pip install "".[tests]""', ""pytest test-corpus -k 'not (microsoft.com and graph) and not azure.com and not amazonaws.com and not googleapis.com'""]"
"['pip install --upgrade pip setuptools\npip install .\npip install .[baselines_jax]\npip install .[baselines]\npip install .[testing]\n', 'pytype -j ""$(grep -c ^processor /proc/cpuinfo)"" bsuite\n', 'pytest -n ""$(grep -c ^processor /proc/cpuinfo)"" bsuite\n', 'pip install --upgrade pip setuptools twine\n', 'python setup.py sdist\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload --skip-existing dist/*\n']"
"['pip install -r requirements.txt', 'tox -- -W error', 'tox', 'pip install -r requirements.txt', 'tox -e check', 'pip install -r requirements.txt', 'tox -e mapfiles', 'if git ls-files -m | grep mapping; then\n  echo \'Please run ""tox -e mapfiles"" and add the changes to a commit.\'\n  exit 1\nfi\n', 'pip install -r requirements.txt', 'tox -e regexlint', 'pip install -r requirements.txt', ""tox -e web-doc -- dirhtml\ntouch doc/_build/dirhtml/.nojekyll\necho -e 'pygments.org\\nwww.pygments.org' > doc/_build/dirhtml/CNAME\necho 'Automated deployment of docs for GitHub pages.' > doc/_build/dirhtml/README\n""]"
"['pip3 install -U -r requirements.dev.txt', 'pip3 install -U -r requirements.txt', 'pytest', 'python3 setup.py sdist bdist_wheel', 'python3 -m pip install invoke', 'invoke virtualenv', 'invoke test']"
""
"['conda info\nconda install -c rdkit rdkit -y\n', 'python -m pip install --upgrade pip\npip install pytest\npip install gdown\n', 'wget https://github.com/milvus-io/milvus/releases/download/v2.0.0-rc7/milvus-standalone-docker-compose.yml -O docker-compose.yml\ndocker-compose up -d\n', 'sudo apt-get install -y libsndfile1', 'sudo apt-get install -y ffmpeg', 'docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7', 'cd solutions/${{github.event.label.name}}/server\npip install -r requirements.txt\ncd src && python -m pytest\n', 'cd benchmark_test/scripts\npip install -r requirements.txt\npython -m pytest test_main.py\n', 'python -m pip install --upgrade pip\npip install pytest\npip install gdown\n', 'wget https://github.com/milvus-io/milvus/releases/download/v2.0.0-rc7/milvus-standalone-docker-compose.yml -O docker-compose.yml\ndocker-compose up -d\n', 'sudo apt-get install -y libsndfile1', 'docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7', 'cd solutions/${{github.event.label.name}}/server\npip install -r requirements.txt\ncd src && python -m pytest\n', 'cd benchmark_test/scripts\npip install -r requirements.txt\npython -m pytest test_main.py\n', 'python -m pip install --upgrade pip\npip install pytest\npip install gdown\n', 'mapfile -d \',\' -t added_modified_files < <(printf \'%s,\' \'${{ steps.files.outputs.added_modified }}\')\nfor added_modified_file in ""${added_modified_files[@]}""; do\n  if [[ ${added_modified_file} = *docker-compose.yaml ]]; then\n    mkdir -p `dirname ${added_modified_file}`/data\n    docker-compose -f ${added_modified_file} up -d\n    cd `dirname ${added_modified_file}`\n    pip install -r server/requirements.txt\n    python -m pytest test_docker_compose.py\n  fi\ndone\n', 'python -m pip install --upgrade pip\npip install gdown\npip install pytest nbmake\n', 'sudo apt-get install -y libsndfile1', 'sudo apt-get install -y libglu1-mesa', 'sudo apt install -y libopengl0', 'wget https://github.com/milvus-io/milvus/releases/download/v2.0.0-rc7/milvus-standalone-docker-compose.yml -O docker-compose.yml\ndocker-compose up -d\n', 'mapfile -d \',\' -t added_modified_files < <(printf \'%s,\' \'${{ steps.files.outputs.added_modified }}\')\nfor added_modified_file in ""${added_modified_files[@]}""; do\n  if [[ ${added_modified_file} = *.ipynb ]]; then\n    pytest --nbmake ${added_modified_file}\n  fi\ndone\n    \n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pip install pylint==2.10.2\npylint --rcfile=pylint.conf --output-format=colorized solutions\npylint --rcfile=pylint.conf --output-format=colorized benchmark_test\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install -r ./src/requirements.txt\n', '# Install Apktool.\nsudo wget -q https://raw.githubusercontent.com/iBotPeaches/Apktool/master/scripts/osx/apktool -O /usr/local/bin/apktool\nsudo chmod a+x /usr/local/bin/apktool\nsudo wget -q ""https://bitbucket.org/iBotPeaches/apktool/downloads/apktool_${APKTOOL_VERSION}.jar"" -O /usr/local/bin/apktool.jar\nsudo chmod a+x /usr/local/bin/apktool.jar\n# Install BundleDecompiler.\nsudo wget -q https://raw.githubusercontent.com/TamilanPeriyasamy/BundleDecompiler/master/build/libs/BundleDecompiler-0.0.2.jar -O /usr/local/bin/BundleDecompiler.jar\nsudo chmod a+x /usr/local/bin/BundleDecompiler.jar\n', 'export PATH=""${PATH}:${ANDROID_HOME}/build-tools/${BUILD_TOOLS_VERSION}""\npytest --verbose --cov=./ --cov-report xml\n', 'bash <(curl -Ls https://coverage.codacy.com/get.sh) report -l Python -r ./coverage.xml\n', 'pip install black', 'black .\ngit config --global user.name \'Auto Black Formatter\'\ngit config --global user.email \'claudiugeorgiu@users.noreply.github.com\'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit checkout ${GITHUB_REF#refs/heads/}\ngit diff --quiet || git commit -am ""Format code with Black [auto]"" && git push\n', 'python -m pip install --upgrade pip\npython -m pip install -r ./src/requirements.txt\n', '# Install Apktool.\nsudo wget -q https://raw.githubusercontent.com/iBotPeaches/Apktool/master/scripts/linux/apktool -O /usr/local/bin/apktool\nsudo chmod a+x /usr/local/bin/apktool\nsudo wget -q ""https://bitbucket.org/iBotPeaches/apktool/downloads/apktool_${APKTOOL_VERSION}.jar"" -O /usr/local/bin/apktool.jar\nsudo chmod a+x /usr/local/bin/apktool.jar\n# Install BundleDecompiler.\nsudo wget -q https://raw.githubusercontent.com/TamilanPeriyasamy/BundleDecompiler/master/build/libs/BundleDecompiler-0.0.2.jar -O /usr/local/bin/BundleDecompiler.jar\nsudo chmod a+x /usr/local/bin/BundleDecompiler.jar\n', 'export PATH=""${PATH}:${ANDROID_HOME}/build-tools/${BUILD_TOOLS_VERSION}""\npytest --verbose --cov=./ --cov-report xml\n', 'bash <(curl -Ls https://coverage.codacy.com/get.sh) report -l Python -r ./coverage.xml\n', 'python -m pip install --upgrade pip\npython -m pip install -r ./src/requirements.txt\n', '# Install Apktool.\nwget https://raw.githubusercontent.com/iBotPeaches/Apktool/master/scripts/windows/apktool.bat -OutFile C:\\Windows\\apktool.bat\nwget ""https://bitbucket.org/iBotPeaches/apktool/downloads/apktool_$env:APKTOOL_VERSION.jar"" -OutFile C:\\Windows\\apktool.jar\n', '$env:PATH=""$($env:PATH);$($env:ANDROID_HOME)\\build-tools\\$($env:BUILD_TOOLS_VERSION)""\npytest --verbose --cov=./ --cov-report xml\n', 'python -m pip install codacy-coverage\npython-codacy-coverage\n']"
"['just create-venv\njust install-deps\njust install\n', 'just ci-api-test\n', 'sudo apt update\nsudo apt install ffmpeg\n', 'just create-venv\njust install-deps\njust install\n', 'just run -v\njust run -h\njust run https://www.bilibili.com/video/BV1AZ4y147Yg -w -y\njust clean\n', 'just ci-e2e-test\n', 'npm install -g pnpm', 'pnpm i --frozen-lockfile', 'pnpm build']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade poetry\npoetry install\n', './scripts/lint.sh', 'python -m pip install --upgrade pip\npython -m pip install --upgrade poetry\npoetry install\npoetry run pip install --upgrade tornado --ignore-installed || true\n', 'poetry run pytest tests/ -s --cov=portray/ --cov-report=term-missing ${@-}\npoetry run coverage xml\n']"
"['python -m pip install --upgrade pip\npip install -r requirements/test.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npwd\npytest -x -s -vv --cov=./pgsync --cov-report=xml:tests/coverage.xml --cov-report term-missing\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['echo ""BRANCH=$(echo ${GITHUB_REF#refs/heads/} | sed \'s/\\//-/g\')"" >> $GITHUB_ENV\nREPO_OWNER=${{ github.repository_owner }}\necho ""IMAGE_NAME=${REPO_OWNER,,}/${GITHUB_REPOSITORY#*/}"" >> $GITHUB_ENV\n', 'docker buildx build \\\n--platform linux/amd64,linux/arm64 \\\n--tag ghcr.io/$IMAGE_NAME:$BRANCH \\\n--output ""type=registry"" ./\n', 'echo ""TAG=${GITHUB_REF/refs\\/tags\\//}"" >> $GITHUB_ENV\nREPO_OWNER=${{ github.repository_owner }}\necho ""IMAGE_NAME=${REPO_OWNER,,}/${GITHUB_REPOSITORY#*/}"" >> $GITHUB_ENV\n', 'docker buildx build \\\n--platform linux/amd64,linux/arm64 \\\n--tag ghcr.io/$IMAGE_NAME:$TAG \\\n--output ""type=registry"" ./\n']"
"['pip install ${{ env.REQUIREMENTS }} conan==${{ steps.parse_conan_v1_version.outputs.result }}\n', 'echo \'## Linter summary (recipes)\' >> $GITHUB_STEP_SUMMARY\npylint --rcfile=linter/pylintrc_recipe `ls recipes/*/*/conanfile.py | shuf -n 500` --output-format=json --output=recipes.json --score=y --exit-zero\njq \'[map( select(.type==""error"")) | group_by (.message)[] | {message: .[0].message, length: length}] | sort_by(.length) | reverse\' recipes.json > recipes2.json\njq -r \'.[] | "" * \\(.message): \\(.length)""\' recipes2.json >> $GITHUB_STEP_SUMMARY\n', 'echo \'## Linter summary (test_package)\' >> $GITHUB_STEP_SUMMARY\npylint --rcfile=linter/pylintrc_testpackage `ls recipes/*/*/test_package/conanfile.py | shuf -n 500` --output-format=json --output=recipes.json --exit-zero\njq \'[map( select(.type==""error"")) | group_by (.message)[] | {message: .[0].message, length: length}] | sort_by(.length) | reverse\' recipes.json > recipes2.json\njq -r \'.[] | "" * \\(.message): \\(.length)""\' recipes2.json >> $GITHUB_STEP_SUMMARY\n', 'pip install ${{ env.REQUIREMENTS }} conan==${{ steps.parse_conan_v1_version.outputs.result }}\n', 'echo ""::add-matcher::linter/recipe_linter.json""\nfor file in ${{ steps.changed-files.outputs.all_changed_files }}; do\n  pylint --rcfile=linter/pylintrc_recipe --output-format=parseable ${file}\ndone\n', 'pip install ${{ env.REQUIREMENTS }} conan==${{ steps.parse_conan_v1_version.outputs.result }}\n', 'echo ""::add-matcher::linter/recipe_linter.json""\nfor file in ${{ steps.changed-files.outputs.all_changed_files }}; do\n  pylint --rcfile=linter/pylintrc_testpackage --ignore-paths=""recipes/[^/]*/[^/]*/test_v1[^/]*/conanfile.py"" --output-format=parseable ${file}\ndone\n', 'pip install yamllint strictyaml argparse', 'echo ""::add-matcher::linter/yamllint_matcher.json""\nyamllint --config-file linter/yamllint_rules.yml -f standard ${{ env.CONFIG_FILES_PATH }}\necho ""::remove-matcher owner=yamllint_matcher::""\n', 'for file in ${{ env.CONFIG_FILES_PATH }}; do\n  python3 linter/config_yaml_linter.py ${file}\ndone\n', 'echo ""::add-matcher::linter/yamllint_matcher.json""\nyamllint --config-file linter/yamllint_rules.yml -f standard ${{ env.CONANDATA_FILES_PATH }}\necho ""::remove-matcher owner=yamllint_matcher::""\n', 'for file in ${{ env.CONANDATA_FILES_PATH }}; do\n  python3 linter/conandata_yaml_linter.py ${file}\ndone\n', 'pip install yamllint strictyaml argparse', 'echo ""::add-matcher::linter/yamllint_matcher.json""\nfor file in ${{ steps.changed_files_config.outputs.all_changed_files }}; do\n  yamllint --config-file linter/yamllint_rules.yml -f standard ${file}\ndone\necho ""::remove-matcher owner=yamllint_matcher::""\n\nfor file in ${{ steps.changed_files_conandata.outputs.all_changed_files }}; do\n  python3 linter/config_yaml_linter.py ${file}\ndone\n', 'echo ""::add-matcher::linter/yamllint_matcher.json""\nfor file in ${{ steps.changed_files_conandata.outputs.all_changed_files }}; do\n  yamllint --config-file linter/yamllint_rules.yml -f standard ${file}\ndone\necho ""::remove-matcher owner=yamllint_matcher::""\n\nfor file in ${{ steps.changed_files_conandata.outputs.all_changed_files }}; do\n  python3 linter/conandata_yaml_linter.py ${file}\ndone\n', ""dotnet tool install --global MarkdownSnippets.Tool\nmdsnippets ${GITHUB_WORKSPACE} \\\n  --convention InPlaceOverwrite \\\n  --exclude-directories 'recipes' \\\n  --toc-level 5\n""]"
"['cd docs/scripts && bash generateFromDocstrings.sh', 'python -m pip install --upgrade pip\n', 'pip install .', 'python -m pip install --upgrade pip\n', 'pip install .', 'python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'pip install pytest\npytest\n']"
""
"['pip install -r backend/requirements/frontend_build.txt', 'yarn --cwd frontend install', 'yarn --cwd frontend build', 'pip install -r backend/requirements/frontend_test.txt', 'yarn --cwd frontend install', 'yarn --cwd frontend test', 'pip install -r backend/requirements/backend_test.txt', 'sudo apt-get update', 'sudo apt-get -y install ffmpeg mopidy gstreamer1.0-plugins-bad', 'mopidy -o ""audio/output=fakesink sync=true"" &\n# wait for mopidy so it can handle connections\nsleep 5\n', 'python backend/manage.py migrate\n', 'python backend/manage.py test', 'pip install -r backend/requirements/install.txt', 'sudo apt-get update', 'sudo rm -rf /usr/lib/python3/dist-packages/yaml\nsudo rm -rf /usr/lib/python3/dist-packages/PyYAML-*\n', 'sudo apt-get --reinstall install python-apt\nsudo apt-get --reinstall install apt-transport-https\nsudo apt-get install build-essential libssl-dev libffi-dev python-dev\n', 'sudo pip install -U pip', 'sudo pip install pyopenssl --upgrade', 'sudo systemctl start postgresql', 'bin/raveberry --confirm-config --use-default-password-i-promise-ill-change-it install', 'counter=0\nuntil [[ $(curl -sS http://localhost/api/version/) == ""Raveberry version""* ]] || [ $counter -gt 30 ]\ndo\n  counter=$((counter + 1))\n  sleep 1\ndone\n# exit with failure if curl did not succeed in any iteration\n[ $counter != 31 ]\n', 'echo ""GITHUB_VERSION=$(cat backend/VERSION | tr -d \'[:space:]\')"" >> $GITHUB_ENV', 'echo ""PYPI_VERSION=$(curl -Ls https://pypi.org/pypi/raveberry/json | jq -r .info.version | tr -d \'[:space:]\')"" >> $GITHUB_ENV', 'echo ""DOCKER_VERSION=$(docker pull raveberry/raveberry >/dev/null && docker run raveberry/raveberry /bin/cat /opt/raveberry/VERSION | tr -d \'[:space:]\')"" >> $GITHUB_ENV', 'if [ ""$DOCKER_VERSION"" != ""$GITHUB_VERSION"" ]; then echo ""::set-output name=docker-status::update""; fi\nif [ ""$PYPI_VERSION"" != ""$GITHUB_VERSION"" ]; then echo ""::set-output name=pypi-status::update""; fi\n', 'pip install build --user', 'ln -s backend raveberry', 'python -m build --sdist --wheel --outdir dist/', 'echo ""VERSION=$(cat backend/VERSION | tr -d \'[:space:]\')"" >> $GITHUB_ENV', 'pip install ""django==4.*""', 'cp -r ""$(python -c \'import django, os; print(f""{os.path.dirname(django.__file__)}/contrib/admin/static/admin"")\')"" backend/static/admin', 'docker buildx build --platform linux/amd64,linux/arm/v7 --output type=registry -f docker/Dockerfile -t raveberry/raveberry:$VERSION -t raveberry/raveberry .', 'docker buildx build --platform linux/amd64,linux/arm/v7 --output type=registry -f docker/nginx.Dockerfile -t raveberry/raveberry-nginx:$VERSION -t raveberry/raveberry-nginx .']"
"['pipx run nox -s prepare -- --headers --signatures --tests', 'python -m pip install build\npython -m build -w ./awkward-cpp\nls ./awkward-cpp/dist\n', 'python -m pip install -v ./awkward-cpp/dist/*.whl', 'python -m pip install -v .', 'python -m pip list', 'python -m pip install -v -r requirements-test.txt', 'python -m pytest -vv -rs tests --cov=awkward --cov-report=term --cov-report=xml', '# Find latest unix timestamp in awkward-cpp, and the kernel generation files\nepoch=$( git log -1 --format=%at -- awkward-cpp kernel-specification.yml kernel-test-data.json )\necho ""source-date-epoch=$epoch"" >> $GITHUB_OUTPUT\n', 'pipx run nox -s prepare', 'pipx run build --sdist awkward-cpp', 'pipx run twine check awkward-cpp/dist/*', 'pipx run nox -s prepare', 'import subprocess, glob\nsubprocess.run(\n  [""pipx"", ""run"", ""twine"", ""check"", *glob.glob(""wheelhouse/*.whl"")],\n  check=True\n)\n', 'pipx run nox -s prepare', 'import subprocess, glob\nsubprocess.run(\n  [""pipx"", ""run"", ""twine"", ""check"", *glob.glob(""wheelhouse/*.whl"")],\n  check=True\n)\n', '# Find latest unix timestamp in awkward-cpp, and the kernel generation files\nepoch=$( git log -1 --format=%at -- awkward-cpp kernel-specification.yml kernel-test-data.json )\necho ""source-date-epoch=$epoch"" >> $GITHUB_OUTPUT\n', 'pipx run nox -s check_cpp_constraint', 'pipx run nox -s prepare', 'pipx run build --sdist awkward-cpp', 'pipx run nox -s check_cpp_sdist_released -- awkward-cpp/dist/awkward-cpp*.tar.gz', 'pipx run nox -s prepare', 'pipx run build --sdist --wheel', 'pipx run twine check dist/*', ""# Don't include `header-only` parent directory\nenv -C header-only/ zip -r header-only.zip .\n"", 'aws s3 cp docs/switcher.json ""s3://${S3_BUCKET}/doc/switcher.json""\naws cloudfront create-invalidation --distribution-id ""${CLOUDFRONT_ID}"" \\\n  --paths ""/doc/switcher.json""\n', 'pipx run nox -s prepare -- --headers --signatures', '# pyodide-build doesn\'t work out of the box with pipx\npython3 -m pip install pyyaml pyodide-build==0.21.0\nexport CMAKE_ARGS=""-DEMSCRIPTEN=1""\npyodide build --exports whole_archive\n', 'pipx run --spec cogapp cog -o environment.yml environment.yml.cog', 'pipx run nox -s prepare -- --headers --signatures', 'pipx run build -w ./awkward-cpp', 'pipx run nox -s prepare -- --headers --signatures', 'pipx run build -w', 'pipx run --spec cogapp cog -o environment.yml environment.yml.cog', 'mkdir -p docs/lite/pypi/\ncp dist/awkward*.whl docs/lite/pypi/\n', 'python -m pip install dist/awkward*.whl --force-reinstall --no-deps', 'pipx run nox -s prepare -- --docs --headers', 'doxygen', 'cp -r awkward-cpp/docs/html/ docs/_static/doxygen', 'echo ""DOCS_REPORT_ANALYTICS=1"" >> $GITHUB_ENV\necho ""DOCS_SHOW_VERSION=1"" >> $GITHUB_ENV\n', 'echo ""DOCS_VERSION=main"" >> $GITHUB_ENV\n', 'sphinx-build -M html . _build/ -T', 'aws s3 sync built-docs/ ""s3://${S3_BUCKET}/${{ github.head_ref }}""\n', 'aws s3 sync built-docs/ ""s3://${S3_BUCKET}/doc/main/""\naws cloudfront create-invalidation --distribution-id ""${CLOUDFRONT_ID}"" \\\n  --paths ""/doc/main*""\necho ""path=/doc/main"" >> $GITHUB_OUTPUT\n', '# Take only leading version\nversion=$(echo ""${GITHUB_REF_NAME}"" | sed -n -E ""s/v?([0-9]+\\.[0-9]+)\\.[0-9]+/\\1/p"")\naws s3 cp docs/switcher.json ""s3://${S3_BUCKET}/doc/""\naws s3 sync built-docs/ ""s3://${S3_BUCKET}/doc/$version/""\naws s3 sync built-docs/ ""s3://${S3_BUCKET}/doc/stable/""\naws cloudfront create-invalidation --distribution-id ""${CLOUDFRONT_ID}"" \\\n  --paths ""/doc/$version*"" ""/doc/stable*"" ""/doc/switcher.json""\necho ""path=/doc/stable"" >> $GITHUB_OUTPUT\n', 'cmake -B build -S header-only -DCMAKE_RUNTIME_OUTPUT_DIRECTORY=bin -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=ON\ncmake --build build/\n', 'import os\nimport pathlib\nimport subprocess\nfor path in pathlib.Path(""build/bin"").glob(""test_*""):\n    if path.is_file():\n        print(f""Running {path.name}"", flush=True)\n        print(""::group::Test output"", flush=True)\n        subprocess.run([path], check=True)\n        print(""::endgroup::"", flush=True)\n', 'echo ""::add-matcher::$GITHUB_WORKSPACE/.github/matchers/pylint.json""\npipx run nox -s pylint\n', '# Find latest unix timestamp in awkward-cpp, and the kernel generation files\nepoch=$( git log -1 --format=%at -- awkward-cpp kernel-specification.yml kernel-test-data.json )\necho ""source-date-epoch=$epoch"" >> $GITHUB_OUTPUT\n', 'pipx run nox -s prepare', 'pipx run build --sdist awkward-cpp', 'pipx run nox -s check_cpp_sdist_released -- awkward-cpp/dist/awkward-cpp*.tar.gz', 'pipx run nox -s prepare', 'pipx run build --sdist', 'pipx run build --sdist awkward-cpp', 'pipx run twine check dist/*', 'pipx run nox -s prepare', 'pipx run build --wheel .', 'pipx run nox -s prepare -- --headers --signatures --tests', 'python -m pip install build\npython -m build -w awkward-cpp\n', 'python -m pip install -v @(get-childitem -path awkward-cpp/dist/*.whl)', 'python -m pip install -v .', 'python -m pip list', 'pipx run nox -s diagnostics -- --check-spec-sorted', 'python -m pip install -v -r requirements-test.txt pytest-github-actions-annotate-failures', 'python -m pytest -vv -rs awkward-cpp/tests-spec', 'python -m pytest -vv -rs awkward-cpp/tests-spec-explicit', 'python -m pytest -vv -rs awkward-cpp/tests-cpu-kernels', 'python -m pytest -vv -rs tests', 'pipx run nox -s prepare -- --headers --signatures --tests', 'python -m pip install build\npython -m build -w ./awkward-cpp\n', 'python -m pip install -v ./awkward-cpp/dist/*.whl', 'python -m pip install -v .', 'python -m pip list', 'pipx run nox -s diagnostics -- --check-spec-sorted', 'python -m pip install -v -r requirements-test.txt pytest-github-actions-annotate-failures', 'python -m pytest -vv -rs awkward-cpp/tests-spec', 'python -m pytest -vv -rs awkward-cpp/tests-spec-explicit', 'python -m pytest -vv -rs awkward-cpp/tests-cpu-kernels', 'python -m pytest -vv -rs tests', 'pipx run nox -s prepare -- --headers --signatures --tests', 'python -m pip install build\npython -m build -w ./awkward-cpp\n', 'python -m pip install -v ./awkward-cpp/dist/*.whl ""${{ matrix.numpy-package }}""', 'python -m pip install -v .', 'python -m pip list', 'pipx run nox -s diagnostics -- --check-spec-sorted', 'python -m pip install -v -r requirements-test.txt pytest-github-actions-annotate-failures', 'python -m pytest -vv -rs awkward-cpp/tests-spec', 'python -m pytest -vv -rs awkward-cpp/tests-spec-explicit', 'python -m pytest -vv -rs awkward-cpp/tests-cpu-kernels', 'python -m pytest -vv -rs tests --cov=awkward --cov-report=term --cov-report=xml', 'pipx run nox -s prepare -- --headers --signatures --tests', 'python3 -m pip install build\npython3 -m build -w ./awkward-cpp\n', 'python3 -m pip install -v ./awkward-cpp/dist/*.whl', 'python3 -m pip install -v .', 'python -m pip list', 'python -m pip install -v -r requirements-test.txt pytest-github-actions-annotate-failures', 'python -m pytest -vv -rs tests']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install pylint\npylint --exit-zero NanoVNASaver\n', 'pip install pytest-cov\npytest --cov=NanoVNASaver\n', 'sudo apt-get update\nsudo apt install -y python3.9 python3-pip python3.9-venv \\\n  python3.9-dev \\\n  python3-pyqt5\n', 'python3.9 -m venv build\n. build/bin/activate\npython -m pip install pip==23.0.1 setuptools==67.6.0\npip install -r requirements.txt\npip install PyInstaller==5.9.0\n', '. build/bin/activate\npython setup.py -V\npyinstaller --onefile -p src -n nanovna-saver nanovna-saver.py\n', 'sudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt-get update\nsudo apt install -y python3.11 python3-pip python3.11-venv \\\n  python3.11-dev \\\n  python3-pyqt5\n', 'python3.11 -m venv build\n. build/bin/activate\npython -m pip install pip==23.0.1 setuptools==67.6.0\npip install -r requirements.txt\npip install PyInstaller==5.9.0\n', '. build/bin/activate\npython setup.py -V\npyinstaller --onefile -p src -n nanovna-saver nanovna-saver.py\n', 'python -m pip install pip==23.0.1 setuptools==67.6.0\npip install -r requirements.txt\npip install PyInstaller==5.9.0\n', 'python setup.py -V\npyinstaller --onefile -p src -n nanovna-saver nanovna-saver.py\n', 'echo ""arch=`uname -m`"" >> ""$GITHUB_ENV""\n', 'python -m pip install pip==23.0.1 setuptools==67.6.0\npip install -r requirements.txt\npip install PyInstaller==5.9.0\n', 'python setup.py -V        \npyinstaller --onedir -p src -n NanoVNASaver nanovna-saver.py --window --clean  -y -i icon_48x48.icns\ntar -C dist -zcf ./dist/NanoVNASaver.app-${{ env.arch }}.tar.gz  NanoVNASaver.app\necho ""Created: NanoVNASaver.app-${{ env.arch }}.tar.gz""\n', 'pip install -U pip setuptools setuptools-scm\npip install -r requirements.txt\npip install PyInstaller==5.9.0\n', 'python setup.py -V\npyinstaller --onefile -p src -n nanovna-saver.exe nanovna-saver.py\n']"
"['\npip3 install -q tensorflow==${{ matrix.tf-version }}\npip3 install -q ""keras<=${{ matrix.tf-version }}""\npip list\npip install -q protobuf==3.19.0\npip install -q requests\npip install -e .\n', 'pip install -q pytest\npip install -q pytest-cov\npip install -q python-coveralls\npytest --cov=ge --cov-report=xml\n']"
""
"['pipx install poetry==1.4.2', 'sudo apt-get update\nsudo apt-get install \\\n  python3-pil \\\n  tesseract-ocr \\\n  tesseract-ocr-eng \\\n  tesseract-ocr-jpn \\\n  tesseract-ocr-chi-sim\nsudo apt-get install \\\n  xvfb \\\n  libxkbcommon-x11-0 \\\n  libxcb-icccm4 \\\n  libxcb-image0 \\\n  libxcb-keysyms1 \\\n  libxcb-randr0 \\\n  libxcb-render-util0 \\\n  libxcb-xinerama0 \\\n  libxcb-xfixes0 \\\n  libxcb-shape0 \\\n  libxcb-cursor0 \\\n  libegl1 \\\n  libegl1-mesa\nsudo apt-get install \\\n  ca-certificates\necho ""XDG_SESSION_TYPE=gnome"" >> $GITHUB_ENV\n', 'brew install pkg-config tesseract tesseract-lang dylibbundler\nbrew info tesseract\n""/Library/Application Support/VMware Tools/vmware-resolutionSet"" 1920 1080\n', '$url = ""https://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-w64-setup-v5.0.1.20220118.exe""\nInvoke-WebRequest -Uri $url -OutFile ""tesseract_installer.exe""\n7z x tesseract_installer.exe -O""C:\\Program Files\\Tesseract-OCR""\n$tesseract = ""C:\\Program Files\\Tesseract-OCR\\""\necho ""$tesseract"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\necho ""TESSDATA_PREFIX=$tesseract"" >> $env:GITHUB_ENV\nSet-DisplayResolution -Width 1920 -Height 1080 -Force\n', 'tesseract --version\ntesseract --list-langs\n', 'poetry install', 'poetry run pytest -vv -m ""not skip_on_gh"" --cov --cov-report=xml\npoetry run coverage lcov\n', 'pipx install poetry==1.4.2', 'sudo apt-get update\nsudo apt-get install \\\n  python3-pil \\\n  tesseract-ocr \\\n  tesseract-ocr-eng \\\n  tesseract-ocr-jpn \\\n  tesseract-ocr-chi-sim\nsudo apt-get install \\\n  xvfb \\\n  libxkbcommon-x11-0 \\\n  libxcb-icccm4 \\\n  libxcb-image0 \\\n  libxcb-keysyms1 \\\n  libxcb-randr0 \\\n  libxcb-render-util0 \\\n  libxcb-xinerama0 \\\n  libxcb-xfixes0 \\\n  libxcb-shape0 \\\n  libxcb-cursor0 \\\n  libegl1 \\\n  libegl1-mesa\nsudo apt-get install \\\n  ca-certificates\necho ""XDG_SESSION_TYPE=gnome"" >> $GITHUB_ENV\necho ""QT_DEBUG_PLUGINS=1"" >> $GITHUB_ENV\n', 'brew install pkg-config tesseract tesseract-lang dylibbundler\nbrew info tesseract\n""/Library/Application Support/VMware Tools/vmware-resolutionSet"" 1920 1080\n', '$url = ""https://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-w64-setup-v5.0.1.20220118.exe""\nInvoke-WebRequest -Uri $url -OutFile ""tesseract_installer.exe""\n7z x tesseract_installer.exe -O""C:\\Program Files\\Tesseract-OCR""\n$tesseract = ""C:\\Program Files\\Tesseract-OCR\\""\necho ""$tesseract"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\necho ""TESSDATA_PREFIX=$tesseract"" >> $env:GITHUB_ENV\nSet-DisplayResolution -Width 1920 -Height 1080 -Force\n', 'tesseract --version\ntesseract --list-langs\n', 'poetry install', 'poetry run ruff .', 'poetry run mypy', 'poetry run black --check --diff .', 'bash -c ""poetry run pip-audit --strict -r <(poetry export -f requirements.txt --with=dev)""', 'poetry run pytest -vv -m ""not skip_on_gh"" --cov --cov-report=xml', 'poetry run coverage lcov', 'poetry run python bundle/build.py --framework=briefcase --dev', '(cat logs/briefcase*) || true\nfind build/normcap || true\n', 'poetry build', 'echo ""NOW=$(date +\'%Y-%m-%dT%H:%M:%S\')"" >> $GITHUB_ENV', 'pipx install poetry==1.4.2', 'poetry install', 'poetry run python bundle/build.py --framework=briefcase', 'pipx install poetry==1.4.2', 'poetry publish --build --username __token__ --password ${{ secrets.PYPI_TOKEN }}\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'echo ${{ steps.traffic.outputs.traffic_branch }}\necho ${{ steps.traffic.outputs.traffic_path }}\ncd ${{ steps.traffic.outputs.traffic_path }}\nls -a\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest poetry codespell cpplint poethepoet isort flake8-quotes\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'isort . -p=buffalo --skip=3rd/ --skip=docs/ --skip=build/ --skip=tests \\\n--skip=.pyx --check-only\n', ""flake8 buffalo ./ --filename='*.py' --ignore=E402,E501,E731,E741 --exclude=__init__.py,setup.py,3rd/,build/ --inline-quotes=double\n"", ""flake8 buffalo ./ --filename='*.pyx' --ignore=E225,E226,E227,E402,E501,E741,E901,E999 --exclude=__init__.py,3rd/,build/\n"", 'cpplint --linelength=140 --headers=hpp,h --extensions=cc,cpp \\\n--filter=-runtime/int,-build/include_subdir,-legal/copyright,-readability/namespace,-readability/todo,-build/namespaces,-whitespace/comments,-readability/casting,-runtime/references,-whitespace/braces,-whitespace/forcolon,-build/include_order,-build/c++11 \\\n--exclude=include/buffalo/cuda/als/als.hpp \\\n--exclude=include/buffalo/cuda/bpr/bpr.hpp \\\n--recursive ./lib ./include\n', 'git submodule update --init --recursive --remote\npip install --upgrade pip setuptools wheel\npip install git-lfs pytest\npip install .\ngit-lfs install\ngit lfs pull origin master\ncd tests; pytest ./\n']"
"['git clone https://github.com/szymonmaszke/torchstar-docker', 'docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\n', './torchstar-docker/src/entrypoint nvidia/cuda ${{ matrix.tag }} ${GITHUB_REPOSITORY} ${{ github.event_name }}\n', 'git clone https://github.com/szymonmaszke/torchstar-docker', 'docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\n', './torchstar-docker/src/entrypoint ubuntu ""18.04"" ${GITHUB_REPOSITORY} ${{ github.event_name }}\n', './nightly.sh', 'python -m pip install --upgrade pip\npip install setuptools wheel\n', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e test', 'tox -e publish -- -t ${{ secrets.CODECOV_TOKEN }}']"
"['python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --ignore F722 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=18 --max-line-length=79 --statistics\n', 'py.test --cov=fastapi_contrib --cov-report=term-missing:skip-covered --cov-branch --cov-fail-under=97\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8==3.8.1 isort==4.3.21\npython -m pip install black==22.3.0\nflake8 --version\n', 'echo ""Running isort""\nisort -c -sp .\necho ""Running black""\nblack -l 100 --check .\necho ""Running flake8""\nflake8 .\n', ""python -m pip install -U pip\npython -m pip install ninja opencv-python-headless onnx pytest-xdist\npython -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n# install from github to get latest; install iopath first since fvcore depends on it\npython -m pip install -U 'git+https://github.com/facebookresearch/iopath'\npython -m pip install -U 'git+https://github.com/facebookresearch/fvcore'\n"", 'CC=clang CXX=clang++ python -m pip install -e .[all]\npython -m detectron2.utils.collect_env\n./datasets/prepare_for_tests.sh\n', 'python -m pytest -n 4 --durations=15 -sv tests/']"
"['$CONDA/bin/conda install conda-build\n$CONDA/bin/conda install anaconda-client\n', '$CONDA/bin/conda config --add channels anaconda\n$CONDA/bin/conda config --add channels conda-forge\n$CONDA/bin/conda build --python ${{ matrix.python }} conda/spleeter\n', '$CONDA/bin/anaconda login --username ${{ secrets.ANACONDA_USERNAME }} --password ${{ secrets.ANACONDA_PASSWORD }}\nfor package in /usr/share/miniconda/conda-bld/linux-64/spleeter*.bz2; do\n  $CONDA/bin/anaconda upload $package\ndone\n', 'C:\\Miniconda\\condabin\\conda.bat init powershell\nC:\\Miniconda\\condabin\\conda.bat install conda-build\nC:\\Miniconda\\condabin\\conda.bat install anaconda-client\n', 'C:\\Miniconda\\condabin\\conda.bat config --add channels anaconda\nC:\\Miniconda\\condabin\\conda.bat config --add channels conda-forge\nC:\\Miniconda\\condabin\\conda.bat build --python ${{ matrix.python }} conda\\spleeter\n', 'anaconda login --username ${{ secrets.ANACONDA_USERNAME }} --password ${{ secrets.ANACONDA_PASSWORD }}\n$packages = Get-ChildItem ""C:\\Miniconda\\conda-bld\\win-64\\""\nforeach ($package in $packages){\n  anaconda upload $package.FullName\n}\n', 'docker build \\\n  --build-arg BASE=python:${{ matrix.distribution }} \\\n  -t deezer/python-cuda-10-1:${{ matrix.distribution }} \\\n  -f docker/cuda-10-1.dockerfile .\n', 'echo ${{ secrets.DOCKERHUB_PASSWORD }} | docker login -u ${{ secrets.DOCKERHUB_USERNAME }} --password-stdin', 'docker push deezer/python-cuda-10-1:${{ matrix.distribution }}', 'echo ""base=python:${{ matrix.distribution }}"" >> $GITHUB_ENV\necho ""image=spleeter"" >> $GITHUB_ENV\n', 'echo ""base=deezer/python-cuda-10-1:${{ matrix.distribution }}"" >> $GITHUB_ENV\necho ""image=spleeter-gpu"" >> $GITHUB_ENV\n', 'docker build \\\n  --build-arg BASE=${{ env.base }} \\\n  --build-arg SPLEETER_VERSION=${{ github.event.inputs.version }} \\\n  -t deezer/${{ env.image }}:${{ matrix.distribution }} \\\n  -f docker/spleeter.dockerfile .\n', 'docker tag deezer/${{ env.image }}:${{ matrix.distribution }} deezer/${{ env.image }}:latest\n', 'docker run \\\n  -v $(pwd):/runtime \\\n  deezer/${{ env.image }}:${{ matrix.distribution }} \\\n  separate -o /tmp /runtime/audio_example.mp3\n', 'echo ${{ secrets.DOCKERHUB_PASSWORD }} | docker login -u ${{ secrets.DOCKERHUB_USERNAME }} --password-stdin', 'docker push deezer/${{ env.image }}:${{ matrix.distribution }}', 'docker build -t conda:cpu -f docker/conda.dockerfile .\necho ""image=spleeter"" >> $GITHUB_ENV\n', 'docker build --build-arg BASE=deezer/python-cuda-10-1:3.8 -t conda:gpu -f docker/conda.dockerfile .\necho ""image=spleeter-gpu"" >> $GITHUB_ENV\n', 'docker build \\\n  --build-arg BASE=conda:${{ matrix.platform }} \\\n  --build-arg SPLEETER_VERSION=${{ github.event.inputs.version }} \\\n  -t deezer/${{ env.image }}:conda \\\n  -f docker/spleeter-conda.dockerfile .\n', 'echo ${{ secrets.DOCKERHUB_PASSWORD }} | docker login -u ${{ secrets.DOCKERHUB_USERNAME }} --password-stdin', 'docker push deezer/${{ env.image }}:conda', 'echo ""image=spleeter"" >> $GITHUB_ENV', 'echo ""image=spleeter-gpu"" >> $GITHUB_ENV', 'docker build \\\n  --build-arg BASE=deezer/${{ env.image }}:${{ matrix.distribution }} \\\n  --build-arg MODEL=${{ matrix.model }} \\\n  -t deezer/${{ env.image }}:${{ matrix.distribution }}-${{ matrix.model }} \\\n  -f docker/spleeter-model.dockerfile .\n', 'docker run \\\n  -v $(pwd):/runtime \\\n  deezer/${{ env.image }}:${{ matrix.distribution }} \\\n  separate -o /tmp -p spleeter:${{ matrix.model }} /runtime/audio_example.mp3\n', 'echo ${{ secrets.DOCKERHUB_PASSWORD }} | docker login -u ${{ secrets.DOCKERHUB_USERNAME }} --password-stdin', 'docker push deezer/${{ env.image }}:${{ matrix.distribution }}-${{ matrix.model }}', 'pip install poetry\npoetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry config pypi-token.pypi $PYPI_TOKEN\n', 'poetry build\npoetry publish\n', 'sudo apt-get update && sudo apt-get install -y ffmpeg\n', 'pip install poetry\npoetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install', 'poetry run black spleeter --check\npoetry run isort spleeter --check\n', 'poetry run pytest tests/']"
""
"['pip3 install -q torch==${{ matrix.torch-version }}\npip install -q requests\npip install -e .\n', 'pip install -q pytest\npip install -q pytest-cov\npip install -q python-coveralls\npip install -q sklearn\npytest --cov=deepctr_torch --cov-report=xml\n']"
"['python -m pip install --upgrade pip\npip install python-coveralls\npip install pytest-cover\n', 'pip install -e .[torch]\n', 'py.test tests/ --cov=rlcard\n']"
"['version_file=""policy_sentry/bin/version.py""\n\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch --tags\ngit pull origin master\nlatest_tag=$(git describe --tags `git rev-list --tags --max-count=1`)\necho ""latest tag: $latest_tag""\nnew_tag=$(echo $latest_tag | awk -F. -v a=""$1"" -v b=""$2"" -v c=""$3"" \'{printf(""%d.%d.%d"", $1+a, $2+b , $3+1)}\')\necho ""new tag: $new_tag""\n\nprintf ""# pylint: disable=missing-module-docstring\\n__version__ = \'$new_tag\'"""""" > $version_file\n\ngit commit -m ""Bump to ${new_tag}""  $version_file || echo ""No changes to commit""\ngit push origin\n', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'invoke build.install-package', 'invoke test.format', 'invoke integration.clean', 'invoke integration.version', 'invoke integration.initialize', 'invoke unit.pytest', 'invoke test.security', 'invoke integration.query', 'invoke integration.write-policy', 'invoke build.uninstall-package', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'invoke build.install-package', 'invoke test.format', 'invoke integration.clean', 'invoke integration.version', 'invoke integration.initialize', 'invoke unit.pytest', 'invoke test.security', 'invoke integration.query', 'invoke integration.write-policy', 'invoke build.uninstall-package', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch --tags\ngit pull origin master\npip install setuptools wheel twine\npython -m setup sdist bdist_wheel\n', 'sleep 5m\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\npip install homebrew-pypi-poet\npip install policy_sentry -U\ngit fetch origin\ngit checkout --track origin/master\nlatest_tag=$(git describe --tags `git rev-list --tags --max-count=1`)\necho ""latest tag: $latest_tag""\ngit pull origin $latest_tag\npoet -f policy_sentry > HomebrewFormula/policy_sentry.rb\ngit add .\ngit commit -m ""update brew formula"" policy_sentry/bin/cli.py HomebrewFormula/policy_sentry.rb || echo ""No brew changes to commit""\ngit push -u origin master\n', 'version_file=""policy_sentry/bin/version.py""\n\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch --tags\ngit pull origin master\nlatest_tag=$(git describe --tags `git rev-list --tags --max-count=1`)\necho ""latest tag: $latest_tag""\nnew_tag=$(echo $latest_tag | awk -F. -v a=""$1"" -v b=""$2"" -v c=""$3"" \'{printf(""%d.%d.%d"", $1+a, $2+b , $3+1)}\')\necho ""new tag: $new_tag""\n\nprintf ""# pylint: disable=missing-module-docstring\\n__version__ = \'$new_tag\'"""""" > $version_file\n\ngit commit -m ""Bump to ${new_tag}""  $version_file || echo ""No changes to commit""\ngit push origin\n', ""pip install pyupio\npip install -r requirements.txt\ndefault_branch=`git remote show origin | grep 'HEAD branch' | cut -d' ' -f5`\npyup --provider github --provider_url https://api.github.com --repo=$GITHUB_REPOSITORY --user-token=${{ secrets.PYUP_GITHUB_ACCESS_TOKEN }} --branch $default_branch\n"", 'pip install requests schema PyYAML click click_log beautifulsoup4', 'echo ""PYTHONPATH=$(pwd)"" >> $GITHUB_ENV', 'python .github/scripts/update_data.py\nif [[ $(du -m /tmp/.policy_sentry/iam-definition.json | cut -f1) -lt 3 ]]; then\n  echo ""File size is less than 3 MB, something is wrong with this update""\n  exit 1\nfi\ncp -f /tmp/.policy_sentry/iam-definition.json $(pwd)/policy_sentry/shared/data/iam-definition.json\ncp -rf /tmp/.policy_sentry/data/docs $(pwd)/policy_sentry/shared/data/\n', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'invoke build.install-package', 'invoke test.format', 'invoke integration.clean', 'invoke integration.version', 'invoke integration.initialize', 'invoke unit.pytest', 'invoke test.security', 'invoke integration.query', 'invoke integration.write-policy', 'invoke build.uninstall-package']"
""
"['python -m pip install --upgrade pip\npip install flake8==3.8.1 flake8-bugbear flake8-comprehensions isort==4.3.21\npip install black==22.3.0\nflake8 --version\n', 'echo ""Running isort""\nisort -c -sp .\necho ""Running black""\nblack --check .\necho ""Running flake8""\nflake8 .\n']"
"['python -m pip install --upgrade pip\npip install flake8\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'flake8 . --count --show-source --statistics']"
"['echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'bash scripts/ci/run', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'pip3 install --user wheel twine && scripts/release --test', 'pip3 install --user wheel twine && scripts/release']"
""
"['sed -ri \'s/^(##*)\\s*:.*:\\s*/\\1 /g\' README.md\nawk \'{if (match($0,""## Supporters"")) exit; print}\' README.md > README\nmv -f README README.md\n', 'python3 -m pip install --upgrade build && python3 -m build', 'sudo apt-get install -y pandoc', 'python -m pip install --upgrade pip\npython -m pip install pytest pytest-cov pytest-pythonpath coverage\npip install -r requirements.txt\npip install .\n']"
"['make init\n', 'make format-check type-check lint-check test\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['""${GITHUB_WORKSPACE}/automation/scripts/github_workflow_free_space.sh""\n', 'sudo apt-get install curl jq', 'echo ""mlrun_commit_hash=$(git rev-parse --short=8 $GITHUB_SHA)""  >> $GITHUB_OUTPUT\necho ""unstable_version_prefix=$(cat automation/version/unstable_version_prefix)""  >> $GITHUB_OUTPUT\n', 'export version_suffix=$(echo ""$GITHUB_REF_NAME"" | grep -E ""^[0-9]+\\.[0-9]+\\.x$"" | tr -d \'.\');\nexport unstable_tag=$(if [ -z ""$version_suffix"" ]; then echo ""unstable-cache""; else echo ""unstable-cache-$version_suffix"";fi);\nexport build_from_cache=$(if [ -z ""$INPUT_BUILD_FROM_CACHE"" ]; then echo ""true"" ; else echo ""$INPUT_BUILD_FROM_CACHE"";fi);\nexport no_cache=$(if [ ""$build_from_cache"" = ""false"" ]; then echo ""true"" ; else echo """";fi);\necho ""tag=$(echo $unstable_tag)"" >> $GITHUB_OUTPUT\necho ""no_cache=$(echo $no_cache)"" >> $GITHUB_OUTPUT\n', 'echo ""mlrun_version=$( \\\n  input_mlrun_version=$INPUT_VERSION && \\\n  default_mlrun_version=$(echo ${{ steps.git_info.outputs.unstable_version_prefix }}+${{ steps.git_info.outputs.mlrun_commit_hash }}) && \\\n  echo ${input_mlrun_version:-`echo $default_mlrun_version`})"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_repo=$( \\\n  input_docker_repo=$INPUT_DOCKER_VERSION && \\\n  default_docker_repo=$(echo ${{ github.repository_owner }} | tr \'[:upper:]\' \'[:lower:]\') && \\\n  echo ${input_docker_repo:-`echo $default_docker_repo`})"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_registries=$( \\\n  input_docker_registries=$INPUT_DOCKER_REGISTRIES && \\\n  echo ${input_docker_registries:-ghcr.io/})"" >> $GITHUB_OUTPUT\necho ""mlrun_cache_date=$(date +%s)"" >> $GITHUB_OUTPUT\n', 'echo ${{ secrets.GHCR_DOCKER_REGISTRY_PASSWORD }} | \\\n  docker login ghcr.io -u ${{ secrets.GHCR_DOCKER_REGISTRY_USERNAME }} --password-stdin | true\necho ${{ secrets.DOCKER_HUB_DOCKER_REGISTRY_PASSWORD }} | \\\n  docker login registry.hub.docker.com -u ${{ secrets.DOCKER_HUB_DOCKER_REGISTRY_USERNAME }} \\\n  --password-stdin | true\necho ${{ secrets.QUAY_IO_DOCKER_REGISTRY_PASSWORD }} | \\\n  docker login quay.io -u ${{ secrets.QUAY_IO_DOCKER_REGISTRY_USERNAME }} \\\n  --password-stdin | true\n', 'for registry in $(echo ${{ steps.computed_params.outputs.mlrun_docker_registries }} | sed ""s/,/ /g""); \\\n  do \\\n    MLRUN_CACHE_DATE=${{ steps.computed_params.outputs.mlrun_cache_date }} \\\n    MLRUN_DOCKER_REGISTRY=$registry \\\n    MLRUN_DOCKER_CACHE_FROM_REGISTRY=ghcr.io/ \\\n    MLRUN_DOCKER_REPO=${{ steps.computed_params.outputs.mlrun_docker_repo }} \\\n    MLRUN_VERSION=${{ steps.computed_params.outputs.mlrun_version }} \\\n    MLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} \\\n    MLRUN_NO_CACHE=${{ steps.docker_cache.outputs.no_cache }} \\\n    MLRUN_PUSH_DOCKER_CACHE_IMAGE=""true"" \\\n    MLRUN_PYTHON_VERSION=${{ matrix.python-version }} \\\n    INCLUDE_PYTHON_VERSION_SUFFIX=${{ matrix.include-suffix }} \\\n    make push-${{ matrix.image-name }}; \\\n  done;\n', 'for registry in ""ghcr.io/"" ""quay.io/"" ""registry.hub.docker.com/""; \\\n  do \\\n    MLRUN_CACHE_DATE=${{ steps.computed_params.outputs.mlrun_cache_date }} \\\n    MLRUN_DOCKER_REGISTRY=$registry \\\n    MLRUN_DOCKER_CACHE_FROM_REGISTRY=ghcr.io/ \\\n    MLRUN_DOCKER_REPO=${{ steps.computed_params.outputs.mlrun_docker_repo }} \\\n    MLRUN_VERSION=unstable \\\n    MLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} \\\n    MLRUN_PYTHON_VERSION=${{ matrix.python-version }} \\\n    INCLUDE_PYTHON_VERSION_SUFFIX=${{ matrix.include-suffix }} \\\n    make push-${{ matrix.image-name }}; \\\n  done;\n', 'MLRUN_CACHE_DATE=${{ steps.computed_params.outputs.mlrun_cache_date }} \\\nMLRUN_DOCKER_REGISTRY=ghcr.io/ \\\nMLRUN_DOCKER_CACHE_FROM_REGISTRY=ghcr.io/ \\\nMLRUN_DOCKER_REPO=${{ steps.computed_params.outputs.mlrun_docker_repo }} \\\nMLRUN_VERSION=${{ steps.docker_cache.outputs.tag }} \\\nMLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} \\\nMLRUN_PUSH_DOCKER_CACHE_IMAGE=true \\\nMLRUN_PYTHON_VERSION=${{ matrix.python-version }} \\\nINCLUDE_PYTHON_VERSION_SUFFIX=${{ matrix.include-suffix }} \\\nmake push-${{ matrix.image-name }}\n', 'python -m pip install --upgrade pip~=22.3.0\npip install -r dev-requirements.txt\n', 'make lint', 'echo ""$RUNNER_CONTEXT""', 'sudo apt-get update && sudo apt install -y protobuf-compiler\n\n# version is coupled to `go/cmd/schemas_compiler/docker/Dockerfile`\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28\ngo install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2\n', 'make lint-go', 'echo ""branch=$(echo ${GITHUB_BASE_REF#refs/heads/})"" >> $GITHUB_OUTPUT\n', 'export version_suffix=$(echo ""${{ steps.git_info.outputs.branch }}"" | grep -E ""^[0-9]+\\.[0-9]+\\.x$"" | tr -d \'.\')\nexport unstable_tag=$(if [ -z ""$version_suffix"" ]; then echo ""unstable-cache""; else echo ""unstable-cache-$version_suffix"";fi)\necho ""tag=$(echo $unstable_tag)"" >> $GITHUB_OUTPUT\n', 'MLRUN_DOCKER_REGISTRY=ghcr.io/ MLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} make test-dockerized', 'echo ""branch=$(echo ${GITHUB_BASE_REF#refs/heads/})"" >> $GITHUB_OUTPUT\n', 'export version_suffix=$(echo ""${{ steps.git_info.outputs.branch }}"" | grep -E ""^[0-9]+\\.[0-9]+\\.x$"" | tr -d \'.\')\nexport unstable_tag=$(if [ -z ""$version_suffix"" ]; then echo ""unstable-cache""; else echo ""unstable-cache-$version_suffix"";fi)\necho ""tag=$(echo $unstable_tag)"" >> $GITHUB_OUTPUT\n', 'MLRUN_DOCKER_REGISTRY=ghcr.io/ MLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} make test-integration-dockerized', 'make test-go-unit-dockerized\n', '""${GITHUB_WORKSPACE}/automation/scripts/github_workflow_free_space.sh""\n', 'minikube kubectl -- config view --flatten > kubeconfig_flatten\necho ""MLRUN_TEST_KUBECONFIG=$(pwd)/kubeconfig_flatten"" >> $GITHUB_ENV\n', 'make test-go-integration-dockerized\n', 'set -x\nminikube ip\nminikube logs\nminikube kubectl -- --namespace ${NAMESPACE} get events\nminikube kubectl -- --namespace ${NAMESPACE} logs -l app.kubernetes.io/component=api,app.kubernetes.io/name=mlrun --tail=-1\nminikube kubectl -- --namespace ${NAMESPACE} get pods\nminikube kubectl -- --namespace ${NAMESPACE} get pods -o yaml\nminikube kubectl -- --namespace ${NAMESPACE} describe pods\nset +x\n', 'echo ""branch=$(echo ${GITHUB_BASE_REF#refs/heads/})"" >> $GITHUB_OUTPUT\n', 'export version_suffix=$(echo ""${{ steps.git_info.outputs.branch }}"" | grep -E ""^[0-9]+\\.[0-9]+\\.x$"" | tr -d \'.\')\nexport unstable_tag=$(if [ -z ""$version_suffix"" ]; then echo ""unstable-cache""; else echo ""unstable-cache-$version_suffix"";fi)\necho ""tag=$(echo $unstable_tag)"" >> $GITHUB_OUTPUT\n', 'MLRUN_DOCKER_REGISTRY=ghcr.io/ MLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} make test-migrations-dockerized', 'pip install -r automation/requirements.txt && pip install -e .', 'MLRUN_PYTHON_VERSION=${{ matrix.python-version }} make test-package', 'echo ""branch=$(echo ${GITHUB_BASE_REF#refs/heads/})"" >> $GITHUB_OUTPUT\n', 'export version_suffix=$(echo ""${{ steps.git_info.outputs.branch }}"" | grep -E ""^[0-9]+\\.[0-9]+\\.x$"" | tr -d \'.\')\nexport unstable_tag=$(if [ -z ""$version_suffix"" ]; then echo ""unstable-cache""; else echo ""unstable-cache-$version_suffix"";fi)\necho ""tag=$(echo $unstable_tag)"" >> $GITHUB_OUTPUT\n', 'MLRUN_DOCKER_REGISTRY=ghcr.io/ MLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} make html-docs-dockerized', 'echo ""branch=$(echo ${GITHUB_BASE_REF#refs/heads/})"" >> $GITHUB_OUTPUT\n', 'export version_suffix=$(echo ""${{ steps.git_info.outputs.branch }}"" | grep -E ""^[0-9]+\\.[0-9]+\\.x$"" | tr -d \'.\')\nexport unstable_tag=$(if [ -z ""$version_suffix"" ]; then echo ""unstable-cache""; else echo ""unstable-cache-$version_suffix"";fi)\necho ""tag=$(echo $unstable_tag)"" >> $GITHUB_OUTPUT\n', 'cd head/mlrun\nMLRUN_DOCKER_CACHE_FROM_TAG=${{ steps.docker_cache.outputs.tag }} make test-backward-compatibility-dockerized\n', 'echo ""\nPlease add the following lines to the top of your code:\n\n# Copyright 2018 Iguazio\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""\n', 'make install-requirements', 'make install-complete-requirements', 'make install-all-requirements', 'export tag=$(echo ${GITHUB_REF#refs/tags/})\ngit config user.name ""mlrun-iguazio""\ngit config user.email ""mlrun.iguazio@gmail.com""\nMLRUN_GIT_ORG=${{ github.repository_owner }} MLRUN_VERSION=$tag make release\n', 'sudo apt-get install curl jq', 'export tag=$(echo ${GITHUB_REF#refs/tags/})\nchmod +x ./automation/scripts/pypi_release_waiter.sh\n./automation/scripts/pypi_release_waiter.sh $tag\n', 'pip install twine wheel\nexport TWINE_USERNAME=${{ secrets.PYPI_USERNAME }}\nexport TWINE_PASSWORD=${{ secrets.PYPI_PASSWORD }}\nMLRUN_VERSION=""$INPUT_VERSION"" make publish-package\n', 'python -m pip install --upgrade pip\npip install -r automation/requirements.txt -r dev-requirements.txt && pip install -e .\n', 'make release-notes MLRUN_OLD_VERSION=""v$INPUT_PREVIOUS_VERSION"" MLRUN_VERSION=""v$INPUT_VERSION"" MLRUN_RELEASE_BRANCH=${{ github.ref_name }} MLRUN_RAISE_ON_ERROR=false MLRUN_RELEASE_NOTES_OUTPUT_FILE=release_notes.md MLRUN_SKIP_CLONE=true\n', 'echo ""body<<EOF"" >> $GITHUB_OUTPUT\ncat release_notes.md >> $GITHUB_OUTPUT\necho ""EOF"" >> $GITHUB_OUTPUT\n', 'tar -cvf mlrun-tutorials.tar docs/tutorial\n', '""${GITHUB_WORKSPACE}/automation/scripts/github_workflow_free_space.sh""\n', 'echo ""image_name=$(make pull-${{ matrix.image-name }} | tail -1)"" >> $GITHUB_OUTPUT\n', 'if [[ -n ""${{ github.event.inputs.publish_results }}"" ]]; then \\\n  echo ""format=sarif"" >> $GITHUB_OUTPUT; \\\n  echo ""fail_build=false"" >> $GITHUB_OUTPUT; \\\nelse \\\n  echo ""format=table"" >> $GITHUB_OUTPUT; \\\n  echo ""fail_build=true"" >> $GITHUB_OUTPUT; \\\nfi\n', 'if [[ -n ""${{ github.event.inputs.publish_results }}"" ]]; then \\\n  echo ""format=sarif"" >> $GITHUB_OUTPUT; \\\n  echo ""fail_build=false"" >> $GITHUB_OUTPUT; \\\nelse \\\n  echo ""format=table"" >> $GITHUB_OUTPUT; \\\n  echo ""fail_build=true"" >> $GITHUB_OUTPUT; \\\nfi\n', 'sshpass \\\n  -p ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" \\\n  ssh \\\n  -o StrictHostKeyChecking=no \\\n  -o ServerAliveInterval=180 \\\n  -o ServerAliveCountMax=3 \\\n  ${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}@${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }} \\\n  kubectl -n default-tenant rollout restart deployment docker-registry\n\nsshpass \\\n  -p ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" \\\n  scp \\\n  automation/system_test/cleanup.py \\\n  ${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}@${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }}:/home/iguazio/cleanup.py\n\nsshpass \\\n  -p ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" \\\n  scp \\\n  automation/system_test/dev_utilities.py \\\n  ${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}@${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }}:/home/iguazio/dev_utilities.py\n\nsshpass \\\n  -p ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" \\\n  ssh \\\n  -o StrictHostKeyChecking=no \\\n  -o ServerAliveInterval=180 \\\n  -o ServerAliveCountMax=3 \\\n  ${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}@${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }} \\\n    LC_ALL=en_US.utf8 LANG=en_US.utf8 /bin/python3 \\\n      /home/iguazio/cleanup.py \\\n      docker-images \\\n      http://localhost:8009 \\\n      igz0.docker_registry.0 \\\n      ""ghcr.io/mlrun/mlrun-api,ghcr.io/mlrun/mlrun-ui,ghcr.io/mlrun/mlrun,ghcr.io/mlrun/ml-models,ghcr.io/mlrun/ml-base,ghcr.io/mlrun/log-collector""\n', 'sshpass -p ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" scp -o StrictHostKeyChecking=no   ${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}@${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }}:/tmp/system-tests-branches-list.txt system-tests-branches-list.txt\n', '# Read branches from local file\nbranches=$(cat system-tests-branches-list.txt)\necho ""branches found in system-tests-branches-list.txt: $branches""\n\n# Split branches into an array\nIFS=\',\' read -ra branches_array <<< ""$branches""\n\n# Get the first branch in the list to work on\nfirst_branch=""${branches_array[0]}""\necho ""working on $first_branch""\n\n# Remove the first branch from the list\nbranches_array=(""${branches_array[@]:1}"")\n\n# Add the first branch at the end of the list\nbranches_array+=(""$first_branch"")\n\n# Join branches back into a string\nbranches=$(printf "",%s"" ""${branches_array[@]}"")\nbranches=${branches:1}\n\n# Output the new list of branches\necho ""$branches""\n\n# Write new branches order to a local file\necho ""$branches"" | cat > system-tests-branches-list.txt\n\n# Set output\necho ""name=$(echo $first_branch)"" >> $GITHUB_OUTPUT\n', '# Override the remote file with the new list of branches\nsshpass -p ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" scp -o StrictHostKeyChecking=no system-tests-branches-list.txt ${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}@${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }}:/tmp/\n', 'pip install -r automation/requirements.txt && pip install -e .', 'sudo apt-get install curl jq', 'echo ""mlrun_hash=$(git rev-parse --short=8 $GITHUB_SHA)"" >> $GITHUB_OUTPUT\n', 'echo ""ui_hash=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch ${{ steps.current-branch.outputs.name }} https://github.com/mlrun/ui.git mlrun-ui 2> /dev/null && \\\n  cd mlrun-ui && \\\n  git rev-parse --short=8 HEAD && \\\n  cd .. && \\\n  rm -rf mlrun-ui)"" >> $GITHUB_OUTPUT\n', 'echo ""mlrun_hash=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch ${{ steps.current-branch.outputs.name }} https://github.com/mlrun/mlrun.git mlrun-upstream 2> /dev/null && \\\n  cd mlrun-upstream && \\\n  git rev-parse --short=8 HEAD && \\\n  cd .. && \\\n  rm -rf mlrun-upstream)"" >> $GITHUB_OUTPUT\necho ""ui_hash=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch ${{ steps.current-branch.outputs.name }} https://github.com/mlrun/ui.git mlrun-ui 2> /dev/null && \\\n  cd mlrun-ui && \\\n  git rev-parse --short=8 HEAD && \\\n  cd .. && \\\n  rm -rf mlrun-ui)"" >> $GITHUB_OUTPUT\necho ""unstable_version_prefix=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch ${{ steps.current-branch.outputs.name }} https://github.com/mlrun/mlrun.git mlrun-upstream 2> /dev/null && \\\n  cd mlrun-upstream && \\\n  cat automation/version/unstable_version_prefix && \\\n  cd .. && \\\n  rm -rf mlrun-upstream)"" >> $GITHUB_OUTPUT\n', 'action_mlrun_hash=${{ steps.git_action_info.outputs.mlrun_hash }} && \\\nupstream_mlrun_hash=${{ steps.git_upstream_info.outputs.mlrun_hash }} && \\\nexport mlrun_hash=${upstream_mlrun_hash:-`echo $action_mlrun_hash`}\necho ""mlrun_hash=$(echo $mlrun_hash)"" >> $GITHUB_OUTPUT\naction_mlrun_ui_hash=${{ steps.git_action_ui_info.outputs.ui_hash }} && \\\nupstream_mlrun_ui_hash=${{ steps.git_upstream_info.outputs.ui_hash }} && \\\nexport ui_hash=${upstream_mlrun_ui_hash:-`echo $action_mlrun_ui_hash`}\necho ""ui_hash=$(echo $ui_hash)"" >> $GITHUB_OUTPUT\necho ""mlrun_version=$(echo ${{ steps.git_upstream_info.outputs.unstable_version_prefix }}+$mlrun_hash)"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_tag=$(echo ${{ steps.git_upstream_info.outputs.unstable_version_prefix }}-$mlrun_hash)"" >> $GITHUB_OUTPUT\necho ""mlrun_ui_version=${{ steps.git_upstream_info.outputs.unstable_version_prefix }}-$ui_hash"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_repo=$( \\\n  input_docker_repo=$INPUT_DOCKER_REPO && \\\n  echo ${input_docker_repo:-mlrun})"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_registry=$( \\\n  input_docker_registry=$INPUT_DOCKER_REGISTRY && \\\n  echo ${input_docker_registry:-ghcr.io/})"" >> $GITHUB_OUTPUT\necho ""mlrun_system_tests_clean_resources=$( \\\n  input_system_tests_clean_resources=$INPUT_CLEAN_RESOURCES_IN_TEARDOWN && \\\n  echo ${input_system_tests_clean_resources:-true})"" >> $GITHUB_OUTPUT\necho ""override_iguazio_version=$INPUT_OVERRIDE_IGUAZIO_VERSION"" >> $GITHUB_OUTPUT\n', 'python automation/system_test/prepare.py run \\\n  --mlrun-version ""${{ steps.computed_params.outputs.mlrun_version }}"" \\\n  --data-cluster-ip ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_IP }}"" \\\n  --data-cluster-ssh-username ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_USERNAME }}"" \\\n  --data-cluster-ssh-password ""${{ secrets.LATEST_SYSTEM_TEST_DATA_CLUSTER_SSH_PASSWORD }}"" \\\n  --app-cluster-ssh-password ""${{ secrets.LATEST_SYSTEM_TEST_APP_CLUSTER_SSH_PASSWORD }}"" \\\n  --github-access-token ""${{ secrets.SYSTEM_TEST_GITHUB_ACCESS_TOKEN }}"" \\\n  --provctl-download-url ""${{ secrets.LATEST_SYSTEM_TEST_PROVCTL_DOWNLOAD_PATH }}"" \\\n  --provctl-download-s3-access-key ""${{ secrets.LATEST_SYSTEM_TEST_PROVCTL_DOWNLOAD_URL_S3_ACCESS_KEY }}"" \\\n  --provctl-download-s3-key-id ""${{ secrets.LATEST_SYSTEM_TEST_PROVCTL_DOWNLOAD_URL_S3_KEY_ID }}"" \\\n  --mlrun-dbpath ""${{ secrets.LATEST_SYSTEM_TEST_MLRUN_DB_PATH }}"" \\\n  --webapi-direct-url ""${{ secrets.LATEST_SYSTEM_TEST_WEBAPI_DIRECT_URL }}"" \\\n  --framesd-url ""${{ secrets.LATEST_SYSTEM_TEST_FRAMESD_URL }}"" \\\n  --username ""${{ secrets.LATEST_SYSTEM_TEST_USERNAME }}"" \\\n  --access-key ""${{ secrets.LATEST_SYSTEM_TEST_ACCESS_KEY }}"" \\\n  --iguazio-version ""${{ steps.computed_params.outputs.iguazio_version }}"" \\\n  --spark-service ""${{ secrets.LATEST_SYSTEM_TEST_SPARK_SERVICE }}"" \\\n  --slack-webhook-url ""${{ secrets.LATEST_SYSTEM_TEST_SLACK_WEBHOOK_URL }}"" \\\n  --mysql-user ""${{ secrets.LATEST_SYSTEM_TEST_MYSQL_USER }}"" \\\n  --mysql-password ""${{ secrets.LATEST_SYSTEM_TEST_MYSQL_PASSWORD }}"" \\\n  --purge-db \\\n  --mlrun-commit ""${{ steps.computed_params.outputs.mlrun_hash }}"" \\\n  --override-image-registry ""${{ steps.computed_params.outputs.mlrun_docker_registry }}"" \\\n  --override-image-repo ${{ steps.computed_params.outputs.mlrun_docker_repo }} \\\n  --override-mlrun-images \\\n  ""${{ steps.computed_params.outputs.mlrun_docker_registry }}${{ steps.computed_params.outputs.mlrun_docker_repo }}/mlrun-api:${{ steps.computed_params.outputs.mlrun_docker_tag }},ghcr.io/mlrun/mlrun-ui:${{ steps.computed_params.outputs.mlrun_ui_version }},ghcr.io/mlrun/mlrun:${{ steps.computed_params.outputs.mlrun_docker_tag }},ghcr.io/mlrun/ml-models:${{ steps.computed_params.outputs.mlrun_docker_tag }},ghcr.io/mlrun/ml-base:${{ steps.computed_params.outputs.mlrun_docker_tag }},ghcr.io/mlrun/log-collector:${{ steps.computed_params.outputs.mlrun_docker_tag }}""\n', 'pip install -r automation/requirements.txt && pip install -e .', 'sudo apt-get install curl jq', 'python automation/system_test/prepare.py env \\\n  --mlrun-dbpath ""${{ secrets.LATEST_SYSTEM_TEST_MLRUN_DB_PATH }}"" \\\n  --webapi-direct-url ""${{ secrets.LATEST_SYSTEM_TEST_WEBAPI_DIRECT_URL }}"" \\\n  --framesd-url ""${{ secrets.LATEST_SYSTEM_TEST_FRAMESD_URL }}"" \\\n  --username ""${{ secrets.LATEST_SYSTEM_TEST_USERNAME }}"" \\\n  --access-key ""${{ secrets.LATEST_SYSTEM_TEST_ACCESS_KEY }}"" \\\n  --spark-service ""${{ secrets.LATEST_SYSTEM_TEST_SPARK_SERVICE }}"" \\\n  --slack-webhook-url ""${{ secrets.LATEST_SYSTEM_TEST_SLACK_WEBHOOK_URL }}"" \\\n  --branch ""${{ needs.prepare-system-tests-enterprise-ci.outputs.mlrunBranch }}"" \\\n  --github-access-token ""${{ secrets.SYSTEM_TEST_GITHUB_ACCESS_TOKEN }}""\n', 'MLRUN_SYSTEM_TESTS_CLEAN_RESOURCES=""${{ needs.prepare-system-tests-enterprise-ci.outputs.mlrunSystemTestsCleanResources }}"" \\\nMLRUN_VERSION=""${{ needs.prepare-system-tests-enterprise-ci.outputs.mlrunVersion }}"" \\\nMLRUN_SYSTEM_TESTS_COMPONENT=""${{ matrix.test_component }}"" \\\nMLRUN_SYSTEM_TESTS_BRANCH=""${{ needs.prepare-system-tests-enterprise-ci.outputs.mlrunBranch }}"" \\\n  make test-system-dockerized\n', '""${GITHUB_WORKSPACE}/automation/scripts/github_workflow_free_space.sh""\n', 'pip install -r automation/requirements.txt -r dockerfiles/test-system/requirements.txt \\\n  -r dockerfiles/mlrun-api/requirements.txt -r dev-requirements.txt \\\n  -r extras-requirements.txt && pip install -e .\n', 'sudo apt-get install curl jq', 'echo ""branch=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_OUTPUT\n', 'echo ""mlrun_hash=$(git rev-parse --short=8 $GITHUB_SHA)"" >> $GITHUB_OUTPUT\n', 'echo ""ui_hash=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch ${{ steps.git_info.outputs.branch }} https://github.com/mlrun/ui.git mlrun-ui 2> /dev/null && \\\n  cd mlrun-ui && \\\n  git rev-parse --short=8 HEAD && \\\n  cd .. && \\\n  rm -rf mlrun-ui)"" >> $GITHUB_OUTPUT\n', 'echo ""mlrun_hash=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch development https://github.com/mlrun/mlrun.git mlrun-upstream 2> /dev/null && \\\n  cd mlrun-upstream && \\\n  git rev-parse --short=8 HEAD && \\\n  cd .. && \\\n  rm -rf mlrun-upstream)"" >> $GITHUB_OUTPUT\necho ""ui_hash=$( \\\n  cd /tmp && \\\n  git clone --single-branch --branch development https://github.com/mlrun/ui.git mlrun-ui 2> /dev/null && \\\n  cd mlrun-ui && \\\n  git rev-parse --short=8 HEAD && \\\n  cd .. && \\\n  rm -rf mlrun-ui)"" >> $GITHUB_OUTPUT\necho ""unstable_version_prefix=$(cat automation/version/unstable_version_prefix)"" >> $GITHUB_OUTPUT\n', 'action_mlrun_hash=${{ steps.git_action_info.outputs.mlrun_hash }} && \\\nupstream_mlrun_hash=${{ steps.git_upstream_info.outputs.mlrun_hash }} && \\\nexport mlrun_hash=${action_mlrun_hash:-`echo $upstream_mlrun_hash`}\necho ""mlrun_hash=$(echo $mlrun_hash)"" >> $GITHUB_OUTPUT\naction_mlrun_ui_hash=${{ steps.git_action_ui_info.outputs.ui_hash }} && \\\nupstream_mlrun_ui_hash=${{ steps.git_upstream_info.outputs.ui_hash }} && \\\nexport ui_hash=${action_mlrun_ui_hash:-`echo $upstream_mlrun_ui_hash`}\necho ""ui_hash=$(echo $ui_hash)"" >> $GITHUB_OUTPUT\necho ""mlrun_version=$(echo ${{ steps.git_upstream_info.outputs.unstable_version_prefix }}+$mlrun_hash)"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_tag=$(echo ${{ steps.git_upstream_info.outputs.unstable_version_prefix }}-$mlrun_hash)"" >> $GITHUB_OUTPUT\necho ""mlrun_ui_version=${{ steps.git_upstream_info.outputs.unstable_version_prefix }}-$ui_hash"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_repo=$( \\\n  input_docker_repo=$INPUT_DOCKER_REPO && \\\n  echo ${input_docker_repo:-mlrun})"" >> $GITHUB_OUTPUT\necho ""mlrun_docker_registry=$( \\\n  input_docker_registry=$INPUT_DOCKER_REGISTRY && \\\n  echo ${input_docker_registry:-ghcr.io/})"" >> $GITHUB_OUTPUT\necho ""mlrun_system_tests_clean_resources=$( \\\n  input_system_tests_clean_resources=$INPUT_CLEAN_RESOURCES_IN_TEARDOWN && \\\n  echo ${input_system_tests_clean_resources:-true})"" >> $GITHUB_OUTPUT\n', '# TODO: There are a couple of modifications to the helm chart that we are doing right now:\n#       1. The grafana prometheus stack is disabled as there are currently no system tests checking its\n#          functionality. Once the model monitoring feature is complete and we have system tests for it, we\n#          can enable it.\n#       2. The mlrun DB is set as the old SQLite db. There is a bug in github workers when trying to run a mysql\n#          server pod in minikube installed on the worker, the mysql pod crashes. There isn\'t much information\n#          about this issue online as this isn\'t how github expect you to use mysql in workflows - the worker\n#          has a mysql server installed directly on it and should be enabled and used as the DB. So we might\n#          want in the future to use that instead, unless the mysql will be able to come up without crashing.\npython automation/deployment/ce.py deploy \\\n    --verbose \\\n    --minikube \\\n    --namespace=${NAMESPACE} \\\n    --registry-secret-name="""" \\\n    --disable-prometheus-stack \\\n    --sqlite /mlrun/db/mlrun.db \\\n    --override-mlrun-api-image=""${{ steps.computed_params.outputs.mlrun_docker_registry }}${{ steps.computed_params.outputs.mlrun_docker_repo }}/mlrun-api:${{ steps.computed_params.outputs.mlrun_docker_tag }}"" \\\n    --override-mlrun-ui-image=""ghcr.io/mlrun/mlrun-ui:${{ steps.computed_params.outputs.mlrun_ui_version }}"" \\\n    --set \'mlrun.api.extraEnvKeyValue.MLRUN_HTTPDB__BUILDER__MLRUN_VERSION_SPECIFIER=""mlrun[complete] @ git+https://github.com/mlrun/mlrun@${{ steps.computed_params.outputs.mlrun_hash }}""\' \\\n    --set mlrun.api.extraEnvKeyValue.MLRUN_IMAGES_REGISTRY=""${{ steps.computed_params.outputs.mlrun_docker_registry }}"" \\\n    --set mlrun.api.extraEnvKeyValue.MLRUN_LOG_LEVEL=""DEBUG""\n', 'python automation/system_test/prepare.py env \\\n  --mlrun-dbpath ""http://$(minikube ip):${MLRUN_API_NODE_PORT}"" \\\n  --github-access-token ""${{ secrets.SYSTEM_TEST_GITHUB_ACCESS_TOKEN }}""\n', 'MLRUN_SYSTEM_TESTS_CLEAN_RESOURCES=""${{ steps.computed_params.outputs.mlrun_system_tests_clean_resources }}"" \\\nMLRUN_VERSION=""${{ steps.computed_params.outputs.mlrun_version }}"" \\\n  make test-system-open-source\n', 'set -x\nminikube ip\nminikube logs\nminikube kubectl -- --namespace ${NAMESPACE} logs -l app.kubernetes.io/component=api,app.kubernetes.io/name=mlrun --tail=-1\nminikube kubectl -- --namespace ${NAMESPACE} get all\nminikube kubectl -- --namespace ${NAMESPACE} get all -o yaml\nminikube kubectl -- --namespace ${NAMESPACE} describe pods\nminikube kubectl -- --namespace ${NAMESPACE} get cm\nminikube kubectl -- --namespace ${NAMESPACE} get cm -o yaml\nminikube kubectl -- --namespace ${NAMESPACE} get secrets\nminikube kubectl -- --namespace ${NAMESPACE} get secrets -o yaml\nminikube kubectl -- --namespace ${NAMESPACE} get pvc\nminikube kubectl -- --namespace ${NAMESPACE} get pv\nset +x\n']"
"['poetry install --no-interaction --no-root --no-ansi', 'poetry install --no-interaction --no-ansi', 'sudo apt-get update\nsudo apt-get install -y libegl1 libgl1 libxkbcommon0\n', './scripts/lint.sh', 'bash -c ""poetry run pytest tests/ -s --cov=streamdeck_ui/ --cov-report=term-missing ${@-}""', 'poetry run coverage xml']"
"['docker pull rlworkgroup/metaworld-ci:latest\ndocker build \\\n    --cache-from rlworkgroup/metaworld-ci:latest \\\n    -f docker/Dockerfile \\\n    -t ""rlworkgroup/metaworld-ci:$GITHUB_RUN_ID"" \\\n    .\n', 'docker run \\\n    -t \\\n    --rm \\\n    -e ""MJKEY=$MJKEY"" \\\n    --memory 7000m \\\n    --memory-swap 7000m \\\n    --name metaworld-ci rlworkgroup/metaworld-ci:$GITHUB_RUN_ID pytest -n 0 -v -m \'not large and not skip_on_ci\'\n']"
"[""python -m pip install --upgrade pip\npython -m pip install 'torch>=1.8,<2' -f https://download.pytorch.org/whl/cpu/torch/\n"", 'python -m pip install --upgrade pip\npython -m pip install invoke .[test]\n', 'invoke integration', 'python -m pip install --upgrade pip\npython -m pip install invoke .[dev]\n', 'invoke lint', ""python -m pip install --upgrade pip\npython -m pip install 'torch==1.8' -f https://download.pytorch.org/whl/cpu/torch/\n"", ""python -m pip install --upgrade pip\npython -m pip install 'torch==1.11.0' -f https://download.pytorch.org/whl/cpu/torch/\n"", 'python -m pip install --upgrade pip\npython -m pip install invoke .[test]\n', 'invoke minimum', 'python -m pip install --upgrade pip\npython -m pip install invoke rundoc .\n', 'invoke readme', ""python -m pip install --upgrade pip\npython -m pip install 'torch>=1.8,<2' -f https://download.pytorch.org/whl/cpu/torch/\n"", 'python -m pip install --upgrade pip\npython -m pip install invoke .[test]\n', 'invoke unit']"
"['sudo apt-get install libsndfile1-dev\npython -m pip install --upgrade pip\npip install pytest\npip install librosa\npip install torch==${{matrix.torch-version}}\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'cd Installation/ && pytest\n']"
['nix run --accept-flake-config .#test-runner-${{ matrix.nix }}']
"['echo ""RESCUEZILLA_RELEASE_NAME=\\""$(git grep -h -E -o \'[a-zA-Z].*)\' CHANGELOG | head -1)\\"""" >> $GITHUB_ENV', 'echo ""DEB_VERSION=$(git describe --tags  --match=\'[0-9].[0-9]*\' --abbrev=0)"" >> $GITHUB_ENV', 'make docker-build', 'make docker-run', 'make docker-add-safe-directory', 'make docker-status', 'make docker-deb', 'make docker-test', 'make docker-status', 'make docker-kinetic', 'sudo mv build/rescuezilla.amd64.kinetic.iso build/rescuezilla-${{github.ref_name}}-64bit.kinetic.iso', 'make docker-status', 'make docker-jammy', 'sudo mv build/rescuezilla.amd64.jammy.iso build/rescuezilla-${{github.ref_name}}-64bit.jammy.iso', 'make docker-status', 'make docker-focal', 'sudo mv build/rescuezilla.amd64.focal.iso build/rescuezilla-${{github.ref_name}}-64bit.focal.iso', 'make docker-status', 'make docker-bionic-i386', 'sudo mv build/rescuezilla.i386.bionic.iso build/rescuezilla-${{github.ref_name}}-32bit.bionic.iso', 'cd build && sudo ../src/scripts/create_iso_checksums.sh rescuezilla_${{env.DEB_VERSION}}-1_all.deb  rescuezilla-${{github.ref_name}}-*.iso']"
"['pipx install poetry', 'poetry env use ""3.7""\npoetry install\n', 'poetry version ${{ env.version }}', 'poetry build', 'pipx install poetry', 'poetry env use ""3.7""\npoetry install\n', 'poetry run portray on_github_pages -f', 'pipx install poetry', 'poetry env use ""3.7""\npoetry install\n', 'poetry version ${{ github.event.release.tag_name }}', 'poetry build', 'poetry publish -u __token__ -p ${{ secrets.PYPI_PASSWORD }}', 'pipx install poetry', 'poetry env use python\npoetry install\n', 'bash scripts/test.sh --ci\n', 'pipx install poetry', 'poetry env use ""3.7""\npoetry install\n', 'poetry version ${{ github.event.release.tag_name }}', 'poetry build', 'poetry publish -r testpypi -u __token__ -p ${{ secrets.TEST_PYPI_PASSWORD }}']"
"['sudo apt-get update -qq\nsudo apt-get install -y xvfb qtbase5-dev qtdeclarative5-dev libqt5webkit5-dev libsqlite3-dev qt5-default qttools5-dev-tools\n# start xvfb in the background\nsudo /usr/bin/Xvfb $DISPLAY -screen 0 1280x1024x24 &\ncur=`pwd`\nwget http://www.coppeliarobotics.com/files/CoppeliaSim_Edu_V4_1_0_Ubuntu18_04.tar.xz\ntar -xf CoppeliaSim_Edu_V4_1_0_Ubuntu18_04.tar.xz\nexport COPPELIASIM_ROOT=""$cur/CoppeliaSim_Edu_V4_1_0_Ubuntu18_04""\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT:$COPPELIASIM_ROOT/platforms\nexport QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT\n\n# Install PyRep\ngit clone https://github.com/stepjam/PyRep.git\ncd PyRep\npip3 install -r requirements.txt\npip3 install setuptools\npip3 install .\n\n# Gym needed for some of unit tests\npip3 install gym\n\ncd ../\npip3 install -r requirements.txt\npython3 -m unittest discover tests/demos', 'sudo apt-get update -qq\nsudo apt-get install -y xvfb qtbase5-dev qtdeclarative5-dev libqt5webkit5-dev libsqlite3-dev qt5-default qttools5-dev-tools\n# start xvfb in the background\nsudo /usr/bin/Xvfb $DISPLAY -screen 0 1280x1024x24 &\ncur=`pwd`\nwget http://www.coppeliarobotics.com/files/CoppeliaSim_Edu_V4_1_0_Ubuntu18_04.tar.xz\ntar -xf CoppeliaSim_Edu_V4_1_0_Ubuntu18_04.tar.xz\nexport COPPELIASIM_ROOT=""$cur/CoppeliaSim_Edu_V4_1_0_Ubuntu18_04""\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT:$COPPELIASIM_ROOT/platforms\nexport QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT\n\n# Install PyRep\ngit clone https://github.com/stepjam/PyRep.git\ncd PyRep\npip3 install -r requirements.txt\npip3 install setuptools\npip3 install .\n\n# Gym needed for some of unit tests\npip3 install gym\n\ncd ../\npip3 install -r requirements.txt\npython3 -m unittest discover tests/unit']"
"['poetry install --no-interaction --no-ansi --remove-untracked -E nlp -E vision\n', 'make test bandit\n', 'poetry install --no-interaction --no-ansi --no-dev\n', 'poetry run python -c ""import baal; import baal.active.dataset; \\\n    import baal.active.heuristics; import baal.active.active_loop; \\\n    import baal.bayesian; import baal.calibration; import baal.modelwrapper""\n']"
"['python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r build_tools/requirements.txt\n', 'pip install --verbose --editable .', 'pytest --cov-config=.coveragerc --cov-report=xml --cov=torchensemble torchensemble\n', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install -r build_tools/requirements.txt\n', 'black --skip-string-normalization --check --config pyproject.toml ./\nchmod +x ""${GITHUB_WORKSPACE}/build_tools/linting.sh""\n./build_tools/linting.sh\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
""
[]
"['pip install -r requirements.txt', 'yamllint --format github --strict \\\n  device-types/ module-types/\n', 'pre-commit run --config .pre-commit-hooks-config.yaml --all-files\n', 'pre-commit run --config .pre-commit-yamlfmt-config.yaml --all-files\n', 'pytest --tb=short -v']"
""
"['pip install flake8', 'chmod a+x scripts/run_ci.sh', 'bash scripts/run_ci.sh']"
"['python -m pip install --upgrade pip\npython -m pip install -q -U setuptools numpy\npython -m pip install flake8 pytest\nif [[ ${{matrix.tf-version}} == ""tf-nightly"" ]]; then python -m pip install tf-nightly; else python -m pip install -q ""tensorflow==""${{matrix.tf-version}}; fi\npip install -e .[tests,t5]\n', 'TRAX_TEST="" ${{matrix.trax-test}}"" ./oss_scripts/oss_tests.sh\n', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m unittest discover tests\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pytest\npytest', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"[""python3 -m venv env\n. env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install '.[dev]'\n"", '. env/bin/activate\nmake linter\n', 'sudo apt-get update\nsudo apt-get install -y ffmpeg\npython3 -m venv env\n. env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n', '. env/bin/activate\nmake test_eval\n']"
"['pip install -e .[cache-tasks,test]', 'pytest', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
"['blossom-ci', 'echo detect.excluded.detector.types=PIP >> application.properties\n', 'blossom-ci', 'blossom-ci', 'python -m pip install -U pip wheel\npython -m pip install -r requirements-dev.txt\nBUILD_MONAI=1 ./runtests.sh --build\n', 'conda info\nconda list\n', 'conda activate monai\n# this `cpuonly` and -c conda-forge is needed to reduce the paging file size on a github instance\n# force to install `cpuonly==2.0.0` is to fix the same issue as:\n# https://github.com/pytorch/vision/issues/4240\nconda install pytorch torchvision torchaudio cpuonly==2.0.0 -c pytorch -c conda-forge\nconda deactivate\n', 'conda activate monai\n$(pwd)/runtests.sh --build --unittests\nconda deactivate\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT', 'rm -rf /github/home/.cache/torch/hub/bundle/\npython -m pip install --upgrade pip wheel\npython -m pip install -r requirements-dev.txt\n', '# clean up temporary files\n$(pwd)/runtests.sh --build --clean\n# run tests\npython -m tests.ngc_bundle_download\n', 'apt-get update\napt-get install -y wget\n', 'which python\npython -m pip install --upgrade pip wheel\npython -m pip uninstall -y torch torchvision\npython -m pip install ${{ matrix.pytorch }}\npython -m pip install -r requirements-dev.txt\npython -m pip list\n', 'export LAUNCH_DELAY=$[ $RANDOM % 16 * 60 ]\necho ""Sleep $LAUNCH_DELAY""\nsleep $LAUNCH_DELAY\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5, 3, device=torch.device(""cuda:0"")))\'\nBUILD_MONAI=1 ./runtests.sh --build --coverage --unittests --disttests  # unit tests with coverage report\nBUILD_MONAI=1 ./runtests.sh --build --coverage --net  # integration tests with coverage report\ncoverage xml --ignore-errors\nif pgrep python; then pkill python; fi\n', 'apt-get update\nDEBIAN_FRONTEND=""noninteractive"" apt-get install -y libopenslide0\n', 'which python\npython -m pip install --upgrade pip wheel\npython -m pip install -r requirements-dev.txt\npython -m pip list\n', 'export LAUNCH_DELAY=$[ $RANDOM % 16 * 60 ]\necho ""Sleep $LAUNCH_DELAY""\nsleep $LAUNCH_DELAY\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5, 3, device=torch.device(""cuda:0"")))\'\nBUILD_MONAI=1 ./runtests.sh --build --coverage --unittests --disttests  # unit tests with coverage report\nBUILD_MONAI=1 ./runtests.sh --build --coverage --net  # integration tests with coverage report\ncoverage xml --ignore-errors\nif pgrep python; then pkill python; fi\n', 'which python\npython -m pip install --upgrade pip wheel twine\npython -m pip list\n', 'pip uninstall monai\npip list | grep -iv monai\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\nroot_dir=$PWD\necho ""$root_dir""\nset -e\n\n# build tar.gz and wheel\nbash runtests.sh --clean  # clear any existing dev temp files\npython -m pip uninstall -y torch torchvision\npython setup.py check -m -s\npython setup.py sdist bdist_wheel\npython -m twine check dist/*\n\n# move packages to a temp dir\ntmp_dir=$(mktemp -d)\ncp dist/monai* ""$tmp_dir""\nrm -r build dist monai.egg-info\ncd ""$tmp_dir""\nls -al\n\n# install from tar.gz\nname=$(ls *.tar.gz | head -n1)\necho $name\npython -m pip install $name[all]\npython -c \'import monai; monai.config.print_config()\' 2>&1 | grep -iv ""unknown""\npython -c \'import monai; print(monai.__file__)\'\n\n# run tests\ncp $root_dir/requirements*.txt ""$tmp_dir""\ncp -r $root_dir/tests ""$tmp_dir""\npwd\nls -al\n\nexport LAUNCH_DELAY=$[ $RANDOM % 16 * 60 ]\necho ""Sleep $LAUNCH_DELAY""\nsleep $LAUNCH_DELAY\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\n\npython -m pip install -r requirements-dev.txt\nPYTHONPATH=""$tmp_dir"":$PYTHONPATH BUILD_MONAI=1 python ./tests/runner.py -p \'test_((?!integration).)\'  # unit tests\nif pgrep python; then pkill python; fi\n', 'cd /opt/monai\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5,3, device=torch.device(""cuda:0"")))\'\nngc --version\nBUILD_MONAI=1 ./runtests.sh --build --coverage --pytype --unittests --disttests  # unit tests with pytype checks, coverage report\nBUILD_MONAI=1 ./runtests.sh --build --coverage --net  # integration tests with coverage report\ncoverage xml --ignore-errors\nif pgrep python; then pkill python; fi\n', 'which python\npython -m pip install --upgrade pip wheel\npython -m pip install -r requirements-dev.txt\nBUILD_MONAI=1 python setup.py develop  # install monai\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\necho ""devices=$CUDA_VISIBLE_DEVICES"" >> $GITHUB_OUTPUT\n', 'cd /opt\ngit clone --depth 1 --branch main --single-branch https://github.com/Project-MONAI/tutorials.git  # latest commit of main branch\ncd tutorials\npython -m pip install -r requirements.txt\n', 'export CUDA_VISIBLE_DEVICES=${{ steps.monai-install.outputs.devices }}\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\ncd /opt/tutorials\npython -c \'import monai; monai.config.print_debug_info()\'\n$(pwd)/runner.sh\npython -c \'import monai; monai.config.print_debug_info()\'\nif pgrep python; then pkill python; fi\n', 'git describe\npython -m pip install -U pip wheel setuptools\npython setup.py build\ncat build/lib/monai/_version.py\n', 'ls -al\nrm -rf {*,.[^.]*}\n', 'docker --version\n# get tag info for versioning\ncat _version.py\nmv _version.py monai/\n\n# build ""latest"": remove flake package as it is not needed on hub.docker.com\nsed -i \'/flake/d\' requirements-dev.txt\ndocker build -t projectmonai/monai:latest -f Dockerfile .\n\n# distribute as always w/ tag ""latest"" to hub.docker.com\necho ""${{ secrets.DOCKER_PW }}"" | docker login -u projectmonai --password-stdin\n\ndocker push projectmonai/monai:latest\ndocker logout\ndocker image prune -f\n', ""export CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\npython -c 'import monai; monai.config.print_debug_info()'\ncd /opt/monai\nls -al\nngc --version\n./runtests.sh --min\n"", 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT', 'pwd && git log -1 && which python\npython -m pip install --upgrade pip wheel\npip uninstall -y monai\npip uninstall -y monai\npip uninstall -y monai-weekly\npip uninstall -y monai-weekly\npython -m pip install --upgrade torch torchvision torchaudio torchtext\npython -m pip install -r requirements-dev.txt\nrm -rf /github/home/.cache/torch/hub/mmars/\n', 'python -m pip list\ngit config --global --add safe.directory /__w/MONAI/MONAI\ngit clean -ffdx && git reset --hard HEAD\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils -c 1 | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5,3, device=torch.device(""cuda:0"")))\'\n', 'pwd && git log -1 && which python\n./runtests.sh -b\npython -m tests.test_auto3dseg_bundlegen\npython -m tests.test_auto3dseg_ensemble\npython -m tests.test_auto3dseg_hpo\npython -m tests.test_integration_autorunner\npython -m tests.test_integration_gpu_customization\n', './runtests.sh --build --net', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT', 'pwd && git log -1 && which python\npython -m pip install --upgrade pip wheel\npip uninstall -y monai\npip uninstall -y monai\npip uninstall -y monai-weekly\npip uninstall -y monai-weekly\npython -m pip install --upgrade torch torchvision torchaudio torchtext\npython -m pip install -r requirements-dev.txt\nrm -rf /github/home/.cache/torch/hub/mmars/\n', 'python -m pip list\ngit config --global --add safe.directory /__w/MONAI/MONAI\ngit clean -ffdx && git reset --hard HEAD\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils -c 1 | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5,3, device=torch.device(""cuda:0"")))\'\n', 'pwd\ncd ../\nrm -rf research-contributions\nrm -rf algorithm_templates\ngit clone --depth 1 --branch main --single-branch https://github.com/Project-MONAI/research-contributions.git\nls research-contributions/\ncp -r research-contributions/auto3dseg/algorithm_templates MONAI/\ncd research-contributions && git log -1 && cd ../MONAI\npwd\nls -ll\nexport OMP_NUM_THREADS=4\nexport MKL_NUM_THREADS=4\nexport MONAI_TESTING_ALGO_TEMPLATE=algorithm_templates\npwd && git log -1 && which python\n./runtests.sh -b\npython -m tests.test_auto3dseg_ensemble\npython -m tests.test_auto3dseg_hpo\npython -m tests.test_integration_autorunner\npython -m tests.test_integration_gpu_customization\n', './runtests.sh --build --unittests', 'apt-get update\napt-get install -y wget\n\nif [ ${{ matrix.environment }} = ""PT110+CUDA111"" ] || \\\n  [ ${{ matrix.environment }} = ""PT113+CUDA116"" ]\nthen\nPYVER=3.8 PYSFX=3 DISTUTILS=python3-distutils && \\\napt-get update && apt-get install -y --no-install-recommends \\\n  curl \\\n  pkg-config \\\n  python$PYVER \\\n  python$PYVER-dev \\\n  python$PYSFX-pip \\\n  $DISTUTILS \\\n  rsync \\\n  swig \\\n  unzip \\\n  zip \\\n  zlib1g-dev \\\n  libboost-locale-dev \\\n  libboost-program-options-dev \\\n  libboost-system-dev \\\n  libboost-thread-dev \\\n  libboost-test-dev \\\n  libgoogle-glog-dev \\\n  libjsoncpp-dev \\\n  cmake \\\n  git && \\\nrm -rf /var/lib/apt/lists/* && \\\nexport PYTHONIOENCODING=utf-8 LC_ALL=C.UTF-8 && \\\nrm -f /usr/bin/python && \\\nrm -f /usr/bin/python`echo $PYVER | cut -c1-1` && \\\nln -s /usr/bin/python$PYVER /usr/bin/python && \\\nln -s /usr/bin/python$PYVER /usr/bin/python`echo $PYVER | cut -c1-1` &&\ncurl -O https://bootstrap.pypa.io/get-pip.py && \\\npython get-pip.py && \\\nrm get-pip.py;\nfi\n', 'echo ""cupy-cuda114"" >> requirements-dev.txt', 'which python\npython -m pip install --upgrade pip wheel\n# fixes preinstalled ruamel_yaml error from the docker image\nrm -rf $(python -c ""from distutils.sysconfig import get_python_lib; print(get_python_lib())"")/ruamel*\nrm -rf $(python -c ""from distutils.sysconfig import get_python_lib; print(get_python_lib())"")/llvmlite*  #6377\npython -m pip install ${{ matrix.pytorch }}\npython -m pip install -r requirements-dev.txt\npython -m pip list\n', 'git clone --depth 1 \\\n  https://github.com/Project-MONAI/MONAI-extra-test-data.git /MONAI-extra-test-data\nexport MONAI_EXTRA_TEST_DATA=""/MONAI-extra-test-data""\nnvidia-smi\nexport LAUNCH_DELAY=$(python -c ""import numpy; print(numpy.random.randint(30) * 10)"")\necho ""Sleep $LAUNCH_DELAY""\nsleep $LAUNCH_DELAY\nexport CUDA_VISIBLE_DEVICES=$(coverage run -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5, 3, device=torch.device(""cuda:0"")))\'\npython -c ""import monai; monai.config.print_config()""\n# build for the current self-hosted CI Tesla V100\nBUILD_MONAI=1 TORCH_CUDA_ARCH_LIST=""7.0"" ./runtests.sh --build --disttests\n./runtests.sh --quick --unittests\nif [ ${{ matrix.environment }} = ""PT113+CUDA116"" ]; then\n  # test the clang-format tool downloading once\n  coverage run -m tests.clang_format_utils\nfi\ncoverage xml --ignore-errors\nif pgrep python; then pkill python; fi\n', 'which python\npython -m pip install --upgrade pip wheel\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', '# min. requirements\npython -m pip install torch --index-url https://download.pytorch.org/whl/cpu\npython -m pip install -r requirements-min.txt\npython -m pip list\nBUILD_MONAI=0 python setup.py develop  # no compile of extensions\n', 'python -c \'import torch; print(torch.__version__); print(torch.rand(5,3))\'\npython -c ""import monai; monai.config.print_config()""\n./runtests.sh --min\n', 'which python\npython -m pip install --user --upgrade pip setuptools wheel\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', '# min. requirements\npython -m pip install torch --extra-index-url https://download.pytorch.org/whl/cpu\npython -m pip install -r requirements-min.txt\npython -m pip list\nBUILD_MONAI=0 python setup.py develop  # no compile of extensions\n', 'python -c \'import torch; print(torch.__version__); print(torch.rand(5,3))\'\npython -c ""import monai; monai.config.print_config()""\n./runtests.sh --min\n', 'which python\npython -m pip install --user --upgrade pip setuptools wheel\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', '# min. requirements\nif [ ${{ matrix.pytorch-version }} == ""latest"" ]; then\n  python -m pip install torch\nelse\n  python -m pip install torch==${{ matrix.pytorch-version }}\nfi\npython -m pip install -r requirements-min.txt\npython -m pip list\nBUILD_MONAI=0 python setup.py develop  # no compile of extensions\n', 'python -c \'import torch; print(torch.__version__); print(torch.rand(5,3))\'\npython -c ""import monai; monai.config.print_config()""\n./runtests.sh --min\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip wheel\npython -m pip install -r requirements-dev.txt\n', '# clean up temporary files\n$(pwd)/runtests.sh --build --clean\n# Github actions have 2 cores, so parallelize pytype\n$(pwd)/runtests.sh --build --${{ matrix.opt }} -j 2\n', 'which python\npython -m pip install --upgrade pip wheel\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\necho ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install torch==1.13.1+cpu torchvision==0.14.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -m pip install --pre -U itk\n', 'python -m pip install torch==1.13.1 torchvision==0.14.1\ncat ""requirements-dev.txt""\npython -m pip install -r requirements-dev.txt\npython -m pip list\npython setup.py develop  # test no compile installation\n', 'python setup.py develop --uninstall\nBUILD_MONAI=1 python setup.py develop  # compile the cpp extensions\n', 'python -c \'import torch; print(torch.__version__); print(torch.rand(5,3))\'\npython -c ""import monai; monai.config.print_config()""\npython -m unittest -v\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', 'python -m pip install --user --upgrade pip setuptools wheel twine\n# install the latest pytorch for testing\n# however, ""pip install monai*.tar.gz"" will build cpp/cuda with an isolated\n# fresh torch installation according to pyproject.toml\npython -m pip install torch>=1.9 torchvision\n', 'pip uninstall monai\npip list | grep -iv monai\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\nset -e\n\n# build tar.gz and wheel\npython setup.py check -m -s\npython setup.py sdist bdist_wheel\npython -m twine check dist/*\n', 'echo ""pwd=$PWD"" >> $GITHUB_OUTPUT', 'echo ""tmp_dir=$(mktemp -d)"" >> $GITHUB_OUTPUT', 'printf ${{ steps.root.outputs.pwd }}\nprintf ${{ steps.mktemp.outputs.tmp_dir }}\n# move packages to a temp dir\ncp dist/monai* ""${{ steps.mktemp.outputs.tmp_dir }}""\nrm -r build dist monai.egg-info\ncd ""${{ steps.mktemp.outputs.tmp_dir }}""\nls -al\n', '# install from wheel\npython -m pip install monai*.whl\npython -c \'import monai; monai.config.print_config()\' 2>&1 | grep -iv ""unknown""\npython -c \'import monai; print(monai.__file__)\'\npython -m pip uninstall -y monai\nrm monai*.whl\n', '# install from tar.gz\nname=$(ls *.tar.gz | head -n1)\necho $name\npython -m pip install $name[all]\npython -c \'import monai; monai.config.print_config()\' 2>&1 | grep -iv ""unknown""\npython -c \'import monai; print(monai.__file__)\'\n', '# run min tests\ncp ${{ steps.root.outputs.pwd }}/requirements*.txt .\ncp -r ${{ steps.root.outputs.pwd }}/tests .\nls -al\npython -m pip install -r requirements-dev.txt\npython -m unittest -v\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip wheel\npython -m pip install -r docs/requirements.txt\n', 'cd docs/\nmake clean\nmake html 2>&1 | tee tmp_log\nif [[ $(grep -c ""ERROR:"" tmp_log) != 0 ]]; then echo ""found errors""; grep ""ERROR:"" tmp_log; exit 1; fi\nif [[ $(grep -c ""WARNING:"" tmp_log) != 0 ]]; then echo ""found warnings""; grep ""WARNING:"" tmp_log; exit 1; fi\n', 'python -m pip install --user --upgrade setuptools wheel\n', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*\nroot_dir=$PWD\necho ""$root_dir""\nset -e\n\n# build tar.gz and wheel\npython setup.py sdist bdist_wheel --build-number $(date +\'%Y%m%d%H%M\')\ntmp_dir=$(mktemp -d)\ncp dist/monai* ""$tmp_dir""\ncd ""$tmp_dir""\nls -al\n\n# install from tar.gz\npython -m pip install monai*.tar.gz\npython -c \'import monai; monai.config.print_config()\' 2>&1 | grep -iv ""unknown""\npython -c \'import monai; print(monai.__file__)\'\npython -m pip uninstall -y monai\nrm monai*.tar.gz\n\n# install from wheel\npython -m pip install monai*.whl\npython -c \'import monai; monai.config.print_config()\' 2>&1 | grep -iv ""unknown""\npython -c \'import monai; print(monai.__file__)\'\n\n# clean up\ncd ""$root_dir""\nrm -r ""$tmp_dir""\nrm -rf monai/\nls -al .\n', 'python -m pip install -r requirements-min.txt\npython -m tests.min_tests\n', 'ls -al dist/\nrm dist/monai*.tar.gz\nls -al dist/\n', 'git describe\npython -m pip install --user --upgrade setuptools wheel\npython setup.py build\ncat build/lib/monai/_version.py\n', 'ls -al\nrm -rf {*,.[^.]*}\n', 'echo ""tag=${GITHUB_REF#refs/*/}"" >> $GITHUB_OUTPUT', 'echo ""$RELEASE_VERSION""\ncat _version.py\n', '# get tag info for versioning\nmv _version.py monai/\n# version checks\ntarget="" \\""version\\"": \\""$RELEASE_VERSION\\""""\nlocal=`grep ""\\""version\\"""" monai/_version.py`\necho ""$target""\necho ""$local""\nif [[ ""$local"" == ""$target"" ]]; then\n  echo ""matched version string""\nelse\n  echo ""unmatched version string, please check the tagging branch.""\n  exit 1\nfi\n# remove flake package as it is not needed on hub.docker.com\nsed -i \'/flake/d\' requirements-dev.txt\ndocker build -t projectmonai/monai:""$RELEASE_VERSION"" -f Dockerfile .\n# distribute with a tag to hub.docker.com\necho ""${{ secrets.DOCKER_PW }}"" | docker login -u projectmonai --password-stdin\ndocker push projectmonai/monai:""$RELEASE_VERSION""\ndocker logout\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', 'which python\npython -m pip install --upgrade pip wheel\npython -m pip install --upgrade torch torchvision\npython -m pip install -r requirements-dev.txt\n', 'python -m pip list\ngit config --global --add safe.directory /__w/MONAI/MONAI\ngit clean -ffdx\ndf -h\n# python -m pip cache info\nnvidia-smi\nexport CUDA_VISIBLE_DEVICES=$(python -m tests.utils | tail -n 1)\necho $CUDA_VISIBLE_DEVICES\ntrap \'if pgrep python; then pkill python; fi;\' ERR\npython -c $\'import torch\\na,b=torch.zeros(1,device=""cuda:0""),torch.zeros(1,device=""cuda:1"");\\nwhile True:print(a,b)\' > /dev/null &\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5, 3, device=torch.device(""cuda:0"")))\'\nBUILD_MONAI=1 ./runtests.sh --build --coverage --unittests --disttests  # unit tests with coverage report\nBUILD_MONAI=1 ./runtests.sh --build --coverage --net  # integration tests with coverage report\ncoverage xml --ignore-errors\nif pgrep python; then pkill python; fi\n', 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip wheel\npython -m pip install -r requirements-dev.txt\n', ""python -m pip list\npython -c 'import torch; print(torch.__version__); print(torch.rand(5,3))'\nBUILD_MONAI=0 ./runtests.sh --build --quick --unittests --disttests\nBUILD_MONAI=1 ./runtests.sh --build --quick --min\ncoverage xml --ignore-errors\n"", 'echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', ""BUILD_MONAI=0 pip install git+https://github.com/Project-MONAI/MONAI#egg=MONAI\npython -c 'import monai; monai.config.print_config()'\ncd $(python -c 'import monai; import os; print(os.path.dirname(monai.__file__))')\nls .\npip uninstall -y monai\n"", ""BUILD_MONAI=1 pip install git+https://github.com/Project-MONAI/MONAI#egg=MONAI\npython -c 'import monai; monai.config.print_config()'\n"", 'cd $GITHUB_WORKSPACE\nrm -rf monai/\nls -al .\npython -m pip install -r requirements-min.txt\npython -m tests.min_tests\n', 'python -m pip install --user --upgrade setuptools wheel\n', 'export HEAD_COMMIT_ID=$(git rev-parse HEAD)\nsed -i \'s/name\\ =\\ monai$/name\\ =\\ monai-weekly/g\' setup.cfg\necho ""__commit_id__ = \\""$HEAD_COMMIT_ID\\"""" >> monai/__init__.py\ngit diff setup.cfg monai/__init__.py\ngit config user.name ""CI Builder""\ngit config user.email ""monai.contact@gmail.com""\ngit add setup.cfg monai/__init__.py\ngit commit -m ""Weekly build at $HEAD_COMMIT_ID""\nexport YEAR_WEEK=$(date +\'%y%U\')\necho ""Year week for tag is ${YEAR_WEEK}""\nif ! [[ $YEAR_WEEK =~ ^[0-9]{4}$ ]] ; then echo ""Wrong \'year week\' format.  Should be 4 digits.""; exit 1 ; fi\ngit tag ""1.3.dev${YEAR_WEEK}""\ngit log -1\ngit tag --list\npython setup.py sdist bdist_wheel\n']"
['python -m pip install --upgrade pip\npip install setuptools wheel twine\npython setup.py sdist bdist_wheel\n']
""
"['python -m pip install --upgrade pip\npip install hatch\nhatch env create\n', 'hatch run lint-check\n', 'hatch run test-cov-xml\n', 'hatch build\npip install dist/fastapi_users-*.whl\npython test_build.py\n', 'python -m pip install --upgrade pip\npip install hatch\n', 'hatch build\nhatch publish\n', 'python -m pip install --upgrade pip\npip install hatch\n', 'hatch run mkdocs build', 'git config user.name fastapi-users-ci\ngit config user.email fastapi-users-ci@francoisvoron.com\ngit fetch origin gh-pages --depth=1\nhatch run mike deploy --push --update-aliases ${{ env.DOC_TAG }}\n']"
"['python -m pip install --upgrade pip setuptools wheel\npython setup.py install_egg_info # Workaround https://github.com/pypa/pip/issues/4537\npip install -e .[dev]\npip install black flake8 isort --upgrade # Testing packages\n', 'make lint\n', ""sudo sed -i 's/azure\\.//' /etc/apt/sources.list # workaround for flaky pandoc install\nsudo apt-get update # from here https://github.com/actions/virtual-environments/issues/675\nsudo apt-get install pandoc -o Acquire::Retries=3 # install pandoc\npython -m pip install --upgrade pip setuptools wheel # update python\npip install ipython --upgrade # needed for Github for whatever reason\npython setup.py install_egg_info # Workaround https://github.com/pypa/pip/issues/4537\npip install -e .[dev]\npip install jupyter 'ipykernel<5.0.0' 'ipython<7.0.0' # ipykernel workaround: github.com/jupyter/notebook/issues/4050\n"", 'sphinx-build -b html docs docs/_build/html\n', 'python -m pip install --upgrade pip setuptools wheel\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', ""python -m pip install --upgrade pip setuptools wheel\npip install pytest pytest-xdist # Testing packages\npip uninstall textattack --yes # Remove TA if it's already installed \npython setup.py install_egg_info # Workaround https://github.com/pypa/pip/issues/4537\npip install -e .[dev]\npip freeze\n"", 'sudo apt-get remove mysql-client libmysqlclient-dev -y >/dev/null 2>&1\nsudo apt-get remove php* -y >/dev/null 2>&1\nsudo apt-get autoremove -y >/dev/null 2>&1\nsudo apt-get autoclean -y >/dev/null 2>&1\nsudo rm -rf /usr/local/lib/android >/dev/null 2>&1\ndocker rmi $(docker image ls -aq) >/dev/null 2>&1\ndf -h\n', 'pytest tests -v\n']"
""
['python /home/$USER/workspace/BlenderProcHelper/github_action.py']
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt\npython -m pip install --upgrade --upgrade-strategy eager .[prophet,tensorflow,torch]\nif [ ""$RUNNER_OS"" == ""Linux"" ]; then  # Currently, we only support KeOps on Linux.\n  python -m pip install --upgrade --upgrade-strategy eager .[prophet,tensorflow,torch,keops]\nfi\npython -m pip freeze\n', 'if [ ""$RUNNER_OS"" == ""macOS"" ]; then  # Avoid numba/OpenMP segfault in CVMDrift (https://github.com/SeldonIO/alibi-detect/issues/648)\n  export NUMBA_THREADING_LAYER=""workqueue""\nfi\npytest --randomly-seed=0 alibi_detect\n# Note: The pytest-randomly seed is fixed at 0 for now. Once the legacy np.random.seed(0)\'s \n# are removed from tests, this can be removed, allowing all tests to use random seeds.\n', 'make build_pypi\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt\npython -m pip install --upgrade --upgrade-strategy eager .[all]\n', 'flake8 alibi_detect\n', 'mypy alibi_detect\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements/docs.txt\npython -m pip freeze\n', 'make build_docs\n', 'make build_latex\n', 'pip install ""tox>=3.21.0,<4.0.0""\nmake licenses\nmake check_licenses\n', 'pip install ""tox>=3.21.0,<4.0.0""\ntox\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt -r testing/requirements.txt\npython -m pip install --upgrade --upgrade-strategy eager .[prophet,torch,tensorflow]\npython -m pip freeze\n', 'pytest --suppress-no-test-exit-code --no-cov -rA --durations=0 -vv -p no:randomly testing/test_notebooks.py\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade --upgrade-strategy eager -r requirements/dev.txt -r testing/requirements.txt\npython -m pip install --upgrade --upgrade-strategy eager .[prophet,torch,tensorflow]\npython -m pip freeze\n', 'tests=""test_notebook_execution[$(echo ${FILES} | sed \'s|doc/source/examples/||g\' | sed \'s| | or |g\')]"" &&\npytest --suppress-no-test-exit-code --no-cov -rA --durations=0 -vv -p no:randomly testing/test_notebooks.py -k ""$tests""\n']"
"['git pull', 'pip install black==22.3.0 flake8', 'python -m black --config=pyproject.toml asteroid tests egs', 'sudo apt update\nsudo apt install libsndfile1-dev libsndfile1\n', 'python -m pip install --upgrade --user pip --quiet\npython -m pip install numpy Cython --upgrade-strategy only-if-needed --quiet\nif [ $TORCH_INSTALL == ""1.8.0"" ]; then\n  INSTALL=""torch==1.8.0+cpu torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html""\nelse\n  INSTALL=""--pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html""\nfi\npython -m pip install $INSTALL\npython -m pip install -r requirements/dev.txt --upgrade-strategy only-if-needed --quiet\npython --version\npip --version\npython -m pip list\n', 'coverage run -a -m py.test tests  --ignore tests/models/publish_test.py\nchmod +x ./tests/cli_test.sh\n./tests/cli_test.sh\n', 'coverage run -a -m py.test tests/models/publish_test.py\n', 'chmod +x ./tests/cli_test.sh\n./tests/cli_test.sh\n', 'coverage report -m\ncoverage xml -o coverage.xml\n', 'sudo apt update\nsudo apt install git-core\nsudo apt install libsndfile1-dev libsndfile1  # Needed for sndfile import\n', 'python -m pip install --upgrade --user pip --quiet\npython -m pip install numpy Cython --upgrade-strategy only-if-needed --quiet\npython -m pip install -r requirements/docs.txt\npython --version\npip --version\npython -m pip list\n', 'pip install -e .  # Install asteroid-* scripts\ncd docs\nmake html\ntouch build/html/.nojekyll  # prevents use jekyll to build doc\ngrep -qHrn ""System Message"" build/html && exit 1 || true # Check for ""System Message"" errors\n', 'git clone https://github.com/ammaraskar/sphinx-action-test.git --branch gh-pages --single-branch gh-pages\ncp -r docs/build/html/* gh-pages/\ncd gh-pages\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""Update documentation"" -a || true\n# The above command will fail if no changes were present, so we ignore that.\n', 'pip install black==22.3.0 flake8', 'python -m black --config=pyproject.toml --check asteroid tests egs', 'python -m flake8  asteroid tests  --show-source --statistics  --select=F6,F7,F82,F52\npython -m flake8 --config .flake8 --exit-zero asteroid tests  --statistics\n', 'sudo apt update\nsudo apt install libsndfile1-dev libsndfile1\n', 'python -m pip install --upgrade --user pip --quiet\npython -m pip install numpy Cython --upgrade-strategy only-if-needed --quiet\nif [ $TORCH_INSTALL == ""1.8.0"" ]; then\n  INSTALL=""torch==1.8.0+cpu torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html""\nelse\n  INSTALL=""--pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html""\nfi\npython -m pip install $INSTALL\npython -m pip install -r requirements/dev.txt --quiet\npython --version\npip --version\npython -m pip list\n', 'RUN_SLOW=1 py.test tests/version_consistency\necho ""Just echo""\n', 'sudo apt update\nsudo apt install libsndfile1-dev libsndfile1\n', 'python -m pip install --upgrade pip --quiet\npython -m pip install numpy Cython --upgrade-strategy only-if-needed --quiet\npython -m pip install -r requirements/torchhub.txt --quiet\npython --version\npip --version\npython -m pip list\n', 'python -c ""import torch; print(torch.hub.list(\'mpariente/asteroid:$BRANCH\'))""\n', 'python -c ""import torch; print(torch.hub.help(\'mpariente/asteroid:$BRANCH\', \'conv_tasnet\'))""\n']"
"['python -m pip install --upgrade pip setuptools wheel\npip install pytest pytest-cov\npip install -r requirements_testing.txt\npip install -e .\npytest --cov=./ --cov-report=xml\n', 'python -m pip install --upgrade pip setuptools wheel\npip install flake8 pytest\npip install --upgrade --upgrade-strategy eager -r requirements_testing.txt\npip install --upgrade --upgrade-strategy eager -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install -r scripts/requirements.txt', 'python scripts/add_link.py ""${{ github.event.inputs.testflight_link }}"" ""${{ github.event.inputs.table }}"" ""${{ github.event.inputs.app_name }}""', 'git_diff=`git diff`\nif [ -z ""$git_diff"" ]; then echo ""Nothing Changed"";exit; fi\ngit config --global user.email ""github_bot@noreply.github.com""\ngit config --global user.name ""github_bot""\ngit add .\ngit commit -m ""Add A New Link""\ngit push\n', 'python scripts/del_link.py ""${{ github.event.inputs.testflight_link }}"" ""${{ github.event.inputs.table }}""', 'git_diff=`git diff`\nif [ -z ""$git_diff"" ]; then echo ""Nothing Changed"";exit; fi\ngit config --global user.email ""github_bot@noreply.github.com""\ngit config --global user.name ""github_bot""\ngit add .\ngit commit -m ""Del A TestFilght Link""\ngit push\n', 'if [ -z ""$python_version"" ]; then python_version=\'3.11.1\'; fi', 'git config --global user.email ""github_bot@noreply.github.com""\ngit config --global user.name ""github_bot""\ngit pull --unshallow\ngit checkout status_ordered\ngit checkout origin/main db/sqlite3.db\n', 'python scripts/order_status.py', 'git_diff=`git diff`\nif [ -z ""$git_diff"" ]; then echo ""Nothing Changed"";exit; fi\ngit add .\ngit commit -m ""Github Action Auto Updated - `date`""\ngit push origin status_ordered\n', 'if [ -z ""$python_version"" ]; then python_version=\'3.11.1\'; fi', 'pip install -r scripts/requirements.txt', 'python scripts/update_status.py', 'git_diff=`git diff`\nif [ -z ""$git_diff"" ]; then echo ""Nothing Changed"";exit; fi\ngit config --global user.email ""github_bot@noreply.github.com""\ngit config --global user.name ""github_bot""\ngit add .\ngit commit -m ""Github Action Auto Updated - `date`""\ngit push\n', 'git config --global user.email ""github_bot@noreply.github.com""\ngit config --global user.name ""github_bot""\ngit remote add upstream https://github.com/pluwen/awesome-testflight-link.git \ngit fetch upstream\ngit checkout main\ngit merge upstream/main --allow-unrelated-histories\ngit push origin main\n']"
"['yarn install --frozen-lockfile && tsc -v', 'make lint-server', 'make yarn-install', 'make eslint-check-ui', 'make prettier-check-ui', 'make install-server\nmake test-dependencies\n', 'pip freeze', 'make pytest', 'python3 ./.github/workflows/scripts/fetch_packages_metadata.py $days\n', 'make build-dependencies\nmake yarn-install\nmake build-ui\n', 'make install-server', 'make test-ui-unit', 'make build-dependencies\nmake yarn-install\nmake build-ui\n', 'make install-server\nmake install-examples\n', 'make test-integration', 'make docs', 'make PYTHON_VERSION=${{ matrix.python-version }} elyra-image-env', 'make REMOVE_RUNTIME_IMAGE=1 validate-runtime-images']"
"['sudo apt update\nsudo apt install -y redis\nredis-server --version\n', 'python --version\nSETTINGS_FILE=""tacticalrmm/settings.py""\nSETUPTOOLS_VER=$(grep ""^SETUPTOOLS_VER"" ""$SETTINGS_FILE"" | awk -F\'[= ""]\' \'{print $5}\')\nWHEEL_VER=$(grep ""^WHEEL_VER"" ""$SETTINGS_FILE"" | awk -F\'[= ""]\' \'{print $5}\')\npip install --upgrade pip\npip install setuptools==${SETUPTOOLS_VER} wheel==${WHEEL_VER}\npip install -r requirements.txt -r requirements-test.txt\n', 'black --exclude migrations/ --check tacticalrmm\nif [ $? -ne 0 ]; then\n    exit 1\nfi\n', 'flake8 --config .flake8 .\nif [ $? -ne 0 ]; then\n    exit 1\nfi\n', 'pytest\nif [ $? -ne 0 ]; then\n    exit 1\nfi\n', 'echo ::set-output name=version::${GITHUB_REF#refs/tags/v}\n']"
"['docker build . --file Dockerfile --tag my-image-name:$(date +%s)', 'if [ -f docker-compose.test.yml ]; then\n  docker-compose --file docker-compose.test.yml build\n  docker-compose --file docker-compose.test.yml run sut\nelse\n  docker build . --file Dockerfile\nfi\n', 'docker build . --file Dockerfile --tag $IMAGE_NAME', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'python -m pip install --upgrade pip\npython -m pip install pytest rzpipe meson==0.62.0 ninja coverage ciphey frida objection\n\n# Install graphviz & ninja\nsudo apt-get -y install graphviz ninja-build\n\n# Install Rizin\nsudo git clone --branch v0.3.4 https://github.com/rizinorg/rizin /opt/rizin/\ncd /opt/rizin/\nmeson build\nninja -C build\nsudo ninja -C build install\nsudo ldconfig -v\ncd -\n\n# Install click >= 8.0.0 for CLI supports\npython -m pip install click==8.0.3\n', 'pip install .', 'python -m pip install black pytest sphinx sphinx-rtd-theme\npython -m pip install coveralls pytest-cov\npytest --cov=./\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install ciphey frida objection\npython -m pip install black pytest sphinx sphinx-rtd-theme\n\n# Install click >= 8.0.0 for CLI supports\npython -m pip install click==8.0.3\n', 'sudo apt-get -y install graphviz', 'brew install graphviz', 'choco install graphviz', 'python -m pip install cython numpy versioneer pybind11 matplotlib lxml', 'pip install .', 'freshquark', 'quark --help\ngit clone https://github.com/quark-engine/apk-samples\nquark -a apk-samples/malware-samples/14d9f1a92dd984d6040cc41ed06e273e.apk -s\nquark -a apk-samples/malware-samples/14d9f1a92dd984d6040cc41ed06e273e.apk -d\nquark -a apk-samples/malware-samples/14d9f1a92dd984d6040cc41ed06e273e.apk -s -g\nquark -a apk-samples/malware-samples/14d9f1a92dd984d6040cc41ed06e273e.apk -d -g\nquark -a apk-samples/malware-samples/14d9f1a92dd984d6040cc41ed06e273e.apk -s -c\n', 'echo ""Ahmyth_RESULT=$(quark -a apk-samples/malware-samples/Ahmyth.apk -s -t 100 | grep 100% | wc -l | awk \'{print $1}\')"" >> $GITHUB_ENV\necho ""a4db_RESULT=$(quark -a apk-samples/malware-samples/13667fe3b0ad496a0cd157f34b7e0c991d72a4db.apk -s -t 100 | grep 100% | wc -l | awk \'{print $1}\')"" >> $GITHUB_ENV\necho ""e273e_RESULT=$(quark -a apk-samples/malware-samples/14d9f1a92dd984d6040cc41ed06e273e.apk -s -t 100 | grep 100% | wc -l | awk \'{print $1}\')"" >> $GITHUB_ENV\n', 'if [ ""${{ env.Ahmyth_RESULT }}"" == ""35"" ]; then\n  exit 0\nelse\n  exit 1\nfi\n', 'if [ ""${{ env.a4db_RESULT }}"" == ""19"" ]; then\n  exit 0\nelse\n  exit 1\nfi\n', 'if [ ""${{ env.e273e_RESULT }}"" == ""36"" ]; then\n  exit 0\nelse\n  exit 1\nfi\n']"
"['if [[ ""${GITHUB_REF#refs/heads/}"" = ""${GITHUB_REF}"" ]]; then\n  APP_VERSION=${GITHUB_REF#refs/tags/}\nelse\n  git fetch --tags --unshallow\n  APP_VERSION=$(git describe --tags)_${GITHUB_REF#refs/heads/}\nfi\necho ""version=$APP_VERSION"" >> $GITHUB_OUTPUT\n', 'docker run --rm ${{ env.TEST_IMAGE }} info\n', 'if [[ ""${GITHUB_REF#refs/heads/}"" = ""${GITHUB_REF}"" ]]; then\n  APP_VERSION=${GITHUB_REF#refs/tags/}\nelse\n  git fetch --tags --unshallow\n  version=$(git describe --tags --abbrev=0)\n  subver=${{ github.run_number }}\n  APP_VERSION=$version.post$subver\nfi\necho ""version=$APP_VERSION"" >> $GITHUB_OUTPUT\n', 'set -x\necho ""__version__ = \'$APP_VERSION\'"" > plextraktsync/__init__.py\ncat plextraktsync/__init__.py\npython -c ""from plextraktsync import __version__; print(__version__)""\npython -m pip install --upgrade build\n#\n# Patch requirements.txt fpor setuptools-declarative-requirements:\n# https://github.com/s0undt3ch/setuptools-declarative-requirements/issues/6\nsed -i -e \'/^-i/d\' requirements.txt\npython -m build\n', 'python -m pip install -r requirements.txt -r requirements.pipenv.txt', 'pipenv install --deploy', './plextraktsync.sh info']"
"['echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")', 'python -m pip install poetry\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry config virtualenvs.in-project true', 'poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'poetry install -E compiler', 'poetry run python -m tests.generate -v', 'poetry run python -m pytest tests/', 'python -m pip install poetry', 'poetry build', 'poetry publish -n']"
"['ls -la ./\nsudo rm -rf ./* || true\nsudo rm -rf ./.??* || true\nls -la ./\n', '${{ matrix.python-version }} -m pip install --upgrade pip\n${{ matrix.python-version }} -m pip install -r dev_requirements.txt\n', '${{ matrix.python-version }} -m pip install .\n', '${{ matrix.python-version }} -m pip install coverage\n${{ matrix.python-version }} -m coverage run tests.py\n${{ matrix.python-version }} -m coverage combine\n', 'ls -la ./\nrm -rf ./* || true\nrm -rf ./.??* || true\nls -la ./\n', '${{ matrix.python-version }} -m pip install --upgrade pip\n${{ matrix.python-version }} -m pip install -r dev_requirements.txt\n', '${{ matrix.python-version }} -m pip install .\n', '${{ matrix.python-version }} -m pip install coverage\n${{ matrix.python-version }} -m coverage run tests.py\n${{ matrix.python-version }} -m coverage combine\n', 'id=$(echo ${{ env.pythonLocation }} | sed \'s/\\/bin//g\')\necho ""::set-output name=id::$id""\n', 'sudo apt-get update\nsudo apt-get install autoconf automake libtool pkg-config gettext libjson-c-dev\nsudo apt-get install libusb-1.0-0-dev libdbus-glib-1-dev libbluetooth-dev libnl-genl-3-dev flex bison\n', 'python -m pip install --upgrade pip\npython -m pip install -r dev_requirements.txt\n', 'python -m pip install .\n', 'python tests.py\n', 'python -m pip install coverage\npython -m coverage run tests.py\npython -m coverage combine\n', 'id=$(echo ${{ env.pythonLocation }} | sed \'s/\\/bin//g\')\necho ""::set-output name=id::$id""\n', 'brew install autoconf automake libtool pkg-config gettext json-c gcc\n', 'python -m pip install --upgrade pip\npython -m pip install -r dev_requirements.txt\n', 'python -m pip install .\n', 'python tests.py\n', 'python -m pip install coverage\npython -m coverage run tests.py\npython -m coverage combine\n', 'python -m pip install --upgrade pip\npython -m pip install -r dev_requirements.txt\n', 'choco install wget --no-progress\nwget --user ${{ secrets.NPCAP_OEM_USERNAME }} --password ${{ secrets.NPCAP_OEM_PASSWORD }} https://npcap.org/oem/dist/npcap-1.60-oem.exe\nStart-Process npcap-1.60-oem.exe -ArgumentList ""/loopback_support=yes /winpcap_mode=yes /dot11_support=yes /S"" -wait\n', 'python -m pip install .\n', 'python tests.py\n', 'python -m pip install coverage\npython -m coverage run tests.py\npython -m coverage combine\n', 'choco install wget --no-progress\nwget --user ${{ secrets.NPCAP_OEM_USERNAME }} --password ${{ secrets.NPCAP_OEM_PASSWORD }} https://npcap.org/oem/dist/npcap-1.60-oem.exe\nStart-Process npcap-1.60-oem.exe -ArgumentList ""/loopback_support=yes /winpcap_mode=yes /dot11_support=yes /S"" -wait\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install twine\npython3 -m twine upload --skip-existing ./wheelhouse/nfstream-*.whl', 'sudo apt-get update\nsudo apt-get install autoconf automake libtool pkg-config gettext libjson-c-dev\nsudo apt-get install libusb-1.0-0-dev libdbus-glib-1-dev libbluetooth-dev libnl-genl-3-dev flex bison\npython3 -m pip install --upgrade pip\npython3 -m pip install -r dev_requirements.txt\npython3 -m pip install .\n']"
"['python -m pip install --upgrade pip\ncd $GITHUB_WORKSPACE && pip install .[test]\n', 'cd $GITHUB_WORKSPACE/docs/readme\npython README_examples.py\n', 'python -m pip install --upgrade setuptools pip wheel twine\npython setup.py sdist bdist_wheel\npython -m twine check dist/*\n', 'python -m pip install --upgrade pip\npip install sphinx-book-theme\npip install sphinxemoji\npip install sphinx-copybutton\npip install ipython\npip install myst-parser\npip install myst-nb\npip install numpy\npip install pandas\npip install scipy\npip install scikit-learn\npip install matplotlib\npip install mne\npip install PyWavelets\npip install EMD-signal\npip install astropy\npip install seaborn\npip install EMD-signal\npip install cvxopt\npip install ts2vg\npip install https://github.com/neuropsychology/neurokit/zipball/dev\n', 'cd docs\nsphinx-build -b html . _build\n', 'python -m pip install --upgrade pip\npip install sphinx\npip install sphinx-book-theme\npip install sphinxemoji\npip install sphinx-copybutton\npip install ipython\npip install myst-parser\npip install myst-nb\npip install numpy\npip install pandas\npip install scipy\npip install scikit-learn\npip install matplotlib\npip install mne\npip install PyWavelets\npip install astropy\npip install seaborn\npip install EMD-signal\npip install cvxopt\npip install ts2vg\npip install https://github.com/neuropsychology/neurokit/zipball/dev\n', 'cd docs\nsphinx-build -b html . _build\n', 'python -m pip install --upgrade pip\npip install https://github.com/DominiqueMakowski/popularipy/zipball/master\ncd $GITHUB_WORKSPACE && pip install .[test]\n', 'cd $GITHUB_WORKSPACE/docs/readme\npython README_examples.py\n', 'python -m pip install pep517 --user', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade setuptools pip wheel\n', 'python -m pip install isort\nisort neurokit2 -l 120 --balanced --multi-line 3 --lines-between-types 1 --lines-after-imports 2 --trailing-comma --skip neurokit2/complexity/__init__.py --diff\n', 'pip install black\n# The GitHub editor is 127 chars wide. See https://black.readthedocs.io/en/stable/installation_and_usage.html\nblack neurokit2 --line-length 120 .\n', 'pip install flake8\n# The GitHub editor is 127 chars wide. See https://flake8.pycqa.org/en/latest/user/configuration.html\nflake8 neurokit2 --exclude neurokit2/__init__.py --max-line-length=127 --max-complexity=10 --ignore E303,C901,E203,W503,E712\n', 'pip install pylint\n# See http://pylint.pycqa.org/en/latest/index.html\npylint neurokit2 --max-line-length=127 --load-plugins=pylint.extensions.docparams --load-plugins=pylint.extensions.docstyle --variable-naming-style=any --argument-naming-style=any --reports=n --suggestion-mode=y --disable=E303 --disable=R0913 --disable=R0801 --disable=C0114 --disable=E203 --disable=E0401 --disable=W9006 --disable=C0330 --disable=R0914 --disable=R0912 --disable=R0915 --disable=W0102 --disable=W0511 --disable=C1801 --disable=C0111 --disable=R1705 --disable=R1720 --disable=C0301 --disable=C0415 --disable=C0103 --disable=C0302 --disable=R1716 --disable=W0632 --disable=E1136 --extension-pkg-whitelist=numpy --exit-zero --fail-under 9.80\n', 'python -m pip install --upgrade setuptools pip wheel\npython -m pip install tox\n', 'cd $GITHUB_WORKSPACE && python -m tox -e py\n']"
"['python setup.py sdist', 'ls -lh dist/dm-tree*.tar.gz', 'set -xe\npwd\npython --version\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install cibuildwheel\n', 'set -xe && python -m cibuildwheel --output-dir wheelhouse', 'ls -lh wheelhouse/dm_tree*.whl', 'set -xe\npwd\npython --version\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install cibuildwheel\n', 'set -xe && python -m cibuildwheel --output-dir wheelhouse', 'ls -lh wheelhouse/dm_tree*.whl', 'set -xe\npwd\npython --version\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install cibuildwheel\n', 'set -xe && python -m cibuildwheel --output-dir wheelhouse', 'ls -lh wheelhouse/dm_tree*.whl']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
""
"['pip install wheel\npip install pyinstaller\npip install -r requirements.txt\n', 'pyinstaller FGO-py/fgoBuild.spec', 'cd dist\n7z a ../FGO-py.7z -mx9\n']"
""
"['echo \'#define MyAppVersion ""${{ steps.tag.outputs.version-without-v }}""\' > launcher/windows/version.txt', 'echo \'#define MyAppVersion ""1.0-${{ github.sha }}""\' > launcher/windows/version.txt', '.\\launcher\\windows\\build.ps1', ""New-Item -ItemType directory -Path certificate\nSet-Content -Path certificate\\certificate.txt -Value '${{ secrets.WINDOWS_CERTIFICATE }}'\ncertutil -decode certificate\\certificate.txt certificate\\certificate.pfx\n& 'C:/Program Files (x86)/Windows Kits/10/bin/10.0.17763.0/x86/signtool.exe' sign /td sha256 /fd sha256 /f certificate\\certificate.pfx /p '${{ secrets.WINDOWS_CERTIFICATE_PASS }}' /tr http://timestamp.digicert.com installers/DepthAI_setup.exe\n"", 'docker buildx build \\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 \\\n  --tag luxonis/depthai:${{ github.sha }} \\\n  --output ""type=image,push=true"" .\n', 'echo ::set-output name=short_ref::${GITHUB_REF#refs/*/}', 'docker buildx build \\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 \\\n  --tag luxonis/depthai:${{ steps.vars.outputs.short_ref }} \\\n  --tag luxonis/depthai:latest \\\n  --output ""type=image,push=true"" .\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'yum update -y && yum install -y sudo curl', ""ln -snf /usr/share/zoneinfo/UTC /etc/localtime && echo UTC > /etc/timezone\ncurl -fL https://docs.luxonis.com/install_dependencies.sh > install_dependencies.sh\nsed '/udevadm control --reload-rules && sudo udevadm trigger/d' install_dependencies.sh > tmp_script.sh\nbash tmp_script.sh\n"", 'python3 install_requirements.py\n', 'sudo apt-get update  -qq && sudo apt-get -qq install curl', ""ln -snf /usr/share/zoneinfo/UTC /etc/localtime && echo UTC > /etc/timezone\ncurl -fL https://docs.luxonis.com/install_dependencies.sh > install_dependencies.sh\nsed '/udevadm control --reload-rules && sudo udevadm trigger/d' install_dependencies.sh > tmp_script.sh\nbash tmp_script.sh\n"", 'python3 install_requirements.py\n', ""curl -fL https://docs.luxonis.com/install_dependencies.sh > install_dependencies.sh\nsed '/udevadm control --reload-rules && sudo udevadm trigger/d' install_dependencies.sh > tmp_script.sh\nbash tmp_script.sh\n"", 'python3 install_requirements.py\n', ""Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))"", 'choco install cmake git python --version 3.10 pycharm-community -y', 'python install_requirements.py\n']"
"['sudo apt update\nsudo apt install yarn\nsudo yarn global add vuepress\n', 'sudo vuepress build\n', ""sudo git init\nsudo git add -A\nsudo git commit -m 'deploy'\nsudo git push -f https://x-access-token:${GITHUB_TOKEN}@github.com/jonasrauber/eagerpy.git master:gh-pages\n"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip setuptools\npip install -r requirements-dev.txt\n', 'flake8 . --count --show-source --statistics\n', 'black --check --verbose .\n', 'pip install -e .\n', 'function retry-with-backoff() {\n  for BACKOFF in 0 1 2 4 8 16 32 64; do\n    sleep $BACKOFF\n    if ""$@""; then\n      return 0\n    fi\n  done\n  return 1\n}\nretry-with-backoff pip install -r requirements.txt\n', 'mypy -p eagerpy\n', 'mypy tests/\n', 'pytest --cov-report term-missing --cov=eagerpy --verbose\n', 'pytest --cov-report term-missing --cov=eagerpy --cov-append --verbose --backend numpy\n', 'pytest --cov-report term-missing --cov=eagerpy --cov-append --verbose --backend pytorch\n', 'pytest --cov-report term-missing --cov=eagerpy --cov-append --verbose --backend jax\n', 'pytest --cov-report term-missing --cov=eagerpy --cov-append --verbose --backend tensorflow\n', 'codecov\n', 'coveralls\n']"
"['git diff origin/${{ github.base_ref }} --name-only | grep CHANGELOG.md', 'sudo apt-get update\nsudo apt-get install -y gdal-bin\npsql template1 -c ""CREATE EXTENSION citext;"" -U postgres -h localhost -p 5432\npsql template1 -c ""CREATE EXTENSION hstore;"" -U postgres -h localhost -p 5432\npsql template1 -c ""CREATE EXTENSION postgis;"" -U postgres -h localhost -p 5432\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade tox\n', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'pip install tox', 'tox -e flake8,isort,pydocstyle,black,mypy', 'python -m pip install -U build twine wheel\npython -m build\ntwine check --strict dist/*\n']"
"['set -e\nsed -i ""s/\'giotto-tda\'/\'giotto-tda-nightly\'/1"" setup.py\nsed -i \'s/""giotto-tda""/""giotto-tda-nightly""/1\' setup.py\nsed -i ""s/__version__.*/__version__ = \'$(Build.BuildNumber)\'/1"" gtda/_version.py\n', 'cd ${{ matrix.boost_install_dir }}\nmkdir -p boost/boost\ncd -\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*']"
"['mkdir -p ./tmp\nbash build/gen_pkgbuild.sh > ./tmp/PKGBUILD\ncat ./tmp/PKGBUILD\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload --skip-existing dist/*\n', 'python -m pip install --upgrade pip\npip install windows-curses pyinstaller\n', 'pyinstaller --onefile --name epy-win epy.py\n', 'poetry install', 'poetry run coverage run -m pytest -vv tests\npoetry run coverage report\n']"
"['cd .\\src\\client\n\necho ::group::Install Dependencies\npip install wheel\npip install pyinstaller -r requirements.txt\necho ::endgroup::\n\necho ::group::Run Test\nsed -e ""s/\\""\\[v.*semver$/\\""[test-${{ github.sha }}]\\""/"" -i.bak yobot.py\npython main_test.py\necho ::endgroup::\n\necho ::group::Build Binary\npyinstaller main.spec\necho ::endgroup::\n', 'cd .\\src\\client\n\necho ::group::Install Dependencies\npip install wheel\npip install pyinstaller -r requirements.txt\necho ::endgroup::\n\necho ::group::Build Binary\nSET PYTHONOPTIMIZE=1\npyinstaller main.spec\necho ::endgroup::\n\necho ::group::Archive Binary\ncd .\\dist\n7z a yobot-windows64.zip .\\yobot.exe\necho ::endgroup::\n', 'echo ""RELEASE_VER=${GITHUB_REF#refs/tags/}"" >> $GITHUB_ENV\n']"
[]
"['curl -sSL \\\n  ""https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py"" | python\n\n# Adding `poetry` to `$PATH`:\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry config virtualenvs.in-project true\npoetry run pip install -U pip\npoetry install\n', 'poetry run flake8 .\npoetry run mypy classes ./tests/**/*.py\npoetry run codespell classes tests docs typesafety README.md CONTRIBUTING.md CHANGELOG.md\npoetry run pytest classes tests docs/pages README.md\npoetry run doc8 -q docs\npoetry run poetry check\npoetry run pip check\npoetry run safety check --full-report\n# We do this to speed up the build:\npoetry run pytest typesafety -p no:cov -o addopts="""" --mypy-ini-file=setup.cfg\n']"
"['pip install nox==2019.11.9', 'pip install poetry==1.0.5', 'nox --sessions tests-3.8 coverage', 'pip install nox==2019.11.9', 'pip install poetry==1.0.5', 'nox', 'poetry build', 'poetry publish --username=__token__ --password=${{ secrets.PYPI_TOKEN }}', 'pip install poetry==1.0.5', ""poetry version patch && version=$(poetry version | awk '{print $2}') && poetry version $version.dev.$(date +%s)"", 'poetry build', 'pip install nox==2019.11.9', 'pip install poetry==1.0.5', 'nox']"
""
"['python3 -m pip install codespell', 'codespell --ignore-words-list=""ba,fo,hel,revered,womens"" --skip=""./README.*.md,*.svg,*.ai,./benchmarks/snippets.py,./tests,./tools""', 'pip install FAQtory', 'faqtory suggest ""$TITLE"" > suggest.md', 'poetry install', 'source $VENV\nmake format-check\n', 'source $VENV\nmake typecheck\n', 'source $VENV\npytest tests -v --cov=./rich --cov-report=xml:./coverage.xml --cov-report term-missing\n', 'COMMIT=$(git rev-parse --short ""$GIT_SHA"")\nAUTHORS=\'@willmcgugan @oleksis @Adilius\'\nBODY=""ðŸ¤“ $AUTHORS README.md changed ðŸ“. Check the [commit $COMMIT](https://github.com/willmcgugan/rich/commit/$GIT_SHA) ðŸ‘€""\nDISCUSSIONID=\'MDEwOkRpc2N1c3Npb24zMzI2NzM0\'\ngh api graphql -H \'GraphQL-Features: discussions_api\' -f body=""$BODY"" -F discussionId=""$DISCUSSIONID"" -f query=\'mutation($body: String!, $discussionId: ID!){addDiscussionComment(input:{body: $body , discussionId: $discussionId}){comment{id}}}\'\n']"
""
"['echo ""Deploying...""\nRUNNER_LABELS=""cml,aws""\nRUNNER_REPO=""https://github.com/${GITHUB_REPOSITORY}""\nMACHINE=""cml$(date +%s)""\ndocker-machine create \\\n  --driver amazonec2 \\\n  --amazonec2-instance-type p3.2xlarge \\\n  --amazonec2-vpc-id ""$VPC"" \\\n  --amazonec2-region us-east-1 \\\n  --amazonec2-zone c \\\n  --amazonec2-ssh-user ubuntu \\\n  --amazonec2-ami ami-06a25ee8966373068 \\\n  --amazonec2-root-size 150 \\\n  ""$MACHINE""\neval ""$(docker-machine env --shell sh ""$MACHINE"")""\n\n(\ndocker-machine ssh ""$MACHINE"" ""sudo mkdir -p \\\n  /docker_machine && \\\nsudo chmod 777 /docker_machine"" && \\\ndocker-machine scp -r -q ~/.docker/machine/ \\\n  ""$MACHINE:/docker_machine"" && \\\ndocker run --name elasticsearch -d \\\n           -p 9200:9200 \\\n           -e ""discovery.type=single-node"" \\\n          elasticsearch:7.9.2 && \\\ndocker run --name postgres -d \\\n            -p 5432:5432 \\\n            --net host \\\n            -e POSTGRES_PASSWORD=password \\\n            -v /docker_machine/machine:/root/.docker/machine \\\n            -e ""DOCKER_MACHINE=$MACHINE"" \\\n            postgres && \\\nsleep 4 && \\\ndocker exec -i postgres psql -U postgres -c ""CREATE DATABASE haystack;"" && \\\ndocker run --name runner -d \\\n  --gpus all \\\n  -v /docker_machine/machine:/root/.docker/machine \\\n  --net host \\\n  -e ""DOCKER_MACHINE=$MACHINE"" \\\n  -e ""repo_token=$repo_token"" \\\n  -e ""RUNNER_LABELS=$RUNNER_LABELS"" \\\n  -e ""RUNNER_REPO=$RUNNER_REPO"" \\\n  -e RUNNER_IDLE_TIMEOUT=120 \\\n  dvcorg/cml-py3:latest && \\\nsleep 20 && echo ""Deployed $MACHINE""\n) || (echo ""Shut down machine"" && docker-machine rm -y -f ""$MACHINE"" && exit 1)\n', 'apt-get update -y\napt-get install python3-dev -y\npip install .[elasticsearch,faiss,weaviate,ray,rest,ui,dev]\ncd test/benchmarks && python run.py --retriever_index --retriever_query --reader --ci --save_markdown\n{\n  echo -en ""## Benchmarks: Retriever Indexing\\n""\n  cat retriever_index_results.md\n  echo -en ""\\n\\n## Benchmarks: Retriever Querying\\n""\n  cat retriever_query_results.md\n  echo -en ""\\n\\n## Benchmarks: Reader\\n""\n  cat reader_results.md\n} >> report.md\ncml-send-comment report.md\n', 'EXPECTED_VERSION=$(cat VERSION.txt)\nif [[ $EXPECTED_VERSION == *""-""* ]]; then\n  EXPECTED_VERSION=$(cut -d \'-\' -f 1 < VERSION.txt)$(cut -d \'-\' -f 2 < VERSION.txt)\nfi\nTAG=""base-${{ matrix.target }}-${{ steps.meta.outputs.version }}""\n\nPLATFORM=""linux/amd64""\nVERSION=$(docker run --platform ""$PLATFORM"" --rm ""deepset/haystack:$TAG"" python -c""import haystack; print(haystack.__version__)"")\n[[ ""$VERSION"" = ""$EXPECTED_VERSION"" ]] || echo ""::error \'Haystack version in deepset/haystack:$TAG image for $PLATFORM is different from expected\'""\n\nPLATFORM=""linux/arm64""\nVERSION=$(docker run --platform ""$PLATFORM"" --rm ""deepset/haystack:$TAG"" python -c""import haystack; print(haystack.__version__)"")\n[[ ""$VERSION"" = ""$EXPECTED_VERSION"" ]] || echo ""::error \'Haystack version in deepset/haystack:$TAG image for $PLATFORM is different from expected\'""\n\n# Remove image after test to avoid filling the GitHub runner and prevent its failure\ndocker rmi ""deepset/haystack:$TAG""\n', 'VERSION=$(gh api repos/${{ github.repository }}/releases/latest --jq "".tag_name"")\necho ""release=$VERSION"" >> ""$GITHUB_OUTPUT""\n', 'cp .github/utils/docstrings_checksum.py ""${{ runner.temp }}/docstrings_checksum.py""', 'CHECKSUM=$(python ""${{ runner.temp }}/docstrings_checksum.py"" --root ""${{ github.workspace }}"")\necho ""checksum=$CHECKSUM"" >> ""$GITHUB_OUTPUT""\n', 'CHECKSUM=$(python ""${{ runner.temp }}/docstrings_checksum.py"" --root ""${{ github.workspace }}"")\necho ""checksum=$CHECKSUM"" >> ""$GITHUB_OUTPUT""\n', 'echo ""should_run=${{ steps.base-docstrings.outputs.checksum != steps.head-docstrings.outputs.checksum }}"" >> ""$GITHUB_OUTPUT""', 'gh pr edit ${{ github.event.pull_request.html_url }} --add-label ""type:documentation""', 'python -c ""from transformers import AutoModel;[AutoModel.from_pretrained(model_name) for model_name in [\'vblagoje/bart_lfqa\',\'yjernite/bart_eli5\', \'vblagoje/dpr-ctx_encoder-single-lfqa-wiki\', \'vblagoje/dpr-question_encoder-single-lfqa-wiki\', \'facebook/dpr-question_encoder-single-nq-base\', \'facebook/dpr-ctx_encoder-single-nq-base\', \'elastic/distilbert-base-cased-finetuned-conll03-english\', \'deepset/bert-medium-squad2-distilled\']]""\n', 'docker run -d -p 9200:9200 -e ""discovery.type=single-node"" -e ""ES_JAVA_OPTS=-Xms128m -Xmx256m"" elasticsearch:7.9.2\n', 'docker run -d -p 9201:9200 -p 9600:9600 -e ""discovery.type=single-node"" opensearchproject/opensearch:1.3.5\n', ""docker run -d -p 8080:8080 --name haystack_test_weaviate --env AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED='true' --env PERSISTENCE_DATA_PATH='/var/lib/weaviate' --env ENABLE_EXPERIMENTAL_BM25='true' --env DISK_USE_READONLY_PERCENTAGE='95' semitechnologies/weaviate:1.17.2"", 'pip install .', 'pytest e2e/${{ matrix.folder }}\n', 'pip install .[all,dev]', 'pytest examples/', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .', ""python -c 'import haystack'"", 'pip install toml\npython .github/utils/pyproject_to_requirements.py pyproject.toml --extra all > ${{ env.REQUIREMENTS_FILE }}\n', 'echo ""${{ steps.license_check_report.outputs.report }}""', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .\npip freeze > ${{ env.REQUIREMENTS_FILE }}\n', 'echo ""${{ steps.license_check_report.outputs.report }}""', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install -U pip\npip install .[all]\npip freeze > ${{ env.REQUIREMENTS_FILE }}\n', 'echo ""${{ steps.license_check_report.outputs.report }}""', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install -U pip\npip install .[all-gpu]\npip freeze > ${{ env.REQUIREMENTS_FILE }}\n', 'echo ""${{ steps.license_check_report.outputs.report }}""', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install "".[all,dev]""', 'mkdir .mypy_cache/\nmypy --install-types --non-interactive ${{ steps.files.outputs.all_changed_files }} --exclude=rest_api/build/ --exclude=rest_api/test/\n', 'pip install "".[all,dev]""\npip install ./haystack-linter\n', 'pylint -ry -j 0 ${{ steps.files.outputs.all_changed_files }}\n', 'echo ""Skipped mypy""', 'echo ""Skipped pylint""', 'echo ""current_release_minor=$(cut -d ""."" -f 1,2 < VERSION.txt)"" >> ""$GITHUB_OUTPUT""\n', 'git config --global user.name ""github-actions[bot]""\ngit config --global user.email ""github-actions[bot]@users.noreply.github.com""\ngit checkout -b v${{ steps.versions.outputs.current_release_minor }}.x\ngit push -u origin v${{ steps.versions.outputs.current_release_minor }}.x\n', 'git checkout main\nNEW_VERSION=$(awk -F. \'/[0-9]+\\./{$2++;print}\' OFS=. < VERSION.txt)\necho ""$NEW_VERSION"" > VERSION.txt\ncat VERSION.txt\ngit checkout -b bump-version\ngit add .\ngit commit -m ""Update unstable version""\ngit push -u origin bump-version\ngh pr create -B main -H bump-version --title \'Bump unstable version\' --body \'Part of the release process\' --label \'ignore-for-release-notes\'\n', 'pip install requests', 'git checkout main\npython ./.github/utils/release_docs.py --new-version ${{ steps.versions.outputs.current_release_minor }}\n', 'sudo apt update && sudo apt-get install libsndfile1 ffmpeg', 'pip install --upgrade pip\npip install -U -e .[all,dev]\npip install -e ./rest_api\n', 'python .github/utils/generate_openapi_specs.py', 'VERSION=""$(cut -d ""."" -f 1,2 < VERSION.txt)""\nif [[ ""$(cat VERSION.txt)"" == *""rc0"" ]]; then\n  VERSION=""$VERSION-unstable""\nfi\nSPECS_ID=$(curl https://dash.readme.com/api/v1/api-specification \\\n  -u ""$README_API_KEY:"" \\\n  --header ""x-readme-version: $VERSION"" \\\n  | jq -r "".[0].id"")\necho ""id=$SPECS_ID"" >> ""$GITHUB_OUTPUT""\n', 'curl \\\n-X POST \\\n-H ""Accept: application/vnd.github+json"" \\\n-H ""Authorization: Bearer ${{ secrets.HAYSTACK_BOT_TOKEN }}"" \\\nhttps://api.github.com/repos/deepset-ai/haystack-json-schema/dispatches \\\n-d \'{""event_type"":""generate-pipeline-schemas"",""client_payload"":{""ref"":""${{ env.HAYSTACK_REF }}""}}\'\n', 'pip install hatch', 'hatch build', 'hatch publish -y', 'python -m pip install --upgrade pip\npip install -r docs/pydoc/requirements.txt\n', './.github/utils/pydoc-markdown.sh', 'echo ""minor=$(cut -d ""."" -f 1,2 < VERSION.txt)"" >> ""$GITHUB_OUTPUT""', 'pip install --upgrade pip\npip install .[formatting]\n', 'if ! black . --check; then\n  git status\n  echo ""###################################################################################################""\n  echo ""# ""\n  echo ""# CHECK FAILED! Black found issues with your code formatting.""\n  echo ""# ""\n  echo ""# Either:""\n  echo ""# 1. Run Black locally before committing:""\n  echo ""# ""\n  echo ""#     pip install .[formatting]""\n  echo ""#     black .""\n  echo ""# ""\n  echo ""# 2. Install the pre-commit hook:""\n  echo ""# ""\n  echo ""#     pre-commit install --hook-type pre-push""\n  echo ""# ""\n  echo ""# 3. See https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md for help.""\n  echo ""# ""\n  echo ""# If you have further problems, please open an issue: https://github.com/deepset-ai/haystack/issues""\n  echo ""# ""\n  echo ""##################################################################################################""\n  exit 1\nfi\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install -U ""./rest_api[dev]""\npip install "".[dev]""\npip install .\n', 'pytest ${{ env.PYTEST_PARAMS }} rest_api/\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install --upgrade pip\npip install .[formatting]\n', 'if ! black . --check; then\n  git status\n  echo ""###################################################################################################""\n  echo ""# ""\n  echo ""# CHECK FAILED! Black found issues with your code formatting.""\n  echo ""# ""\n  echo ""# Either:""\n  echo ""# 1. Run Black locally before committing:""\n  echo ""# ""\n  echo ""#     pip install .[formatting]""\n  echo ""#     black .""\n  echo ""# ""\n  echo ""# 2. Install the pre-commit hook:""\n  echo ""# ""\n  echo ""#     pre-commit install --hook-type pre-push""\n  echo ""# ""\n  echo ""# 3. See https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md for help.""\n  echo ""# ""\n  echo ""# If you have further problems, please open an issue: https://github.com/deepset-ai/haystack/issues""\n  echo ""# ""\n  echo ""##################################################################################################""\n  exit 1\nfi\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[all,dev]', 'pytest --cov-report xml:coverage.xml --cov=""haystack"" -m ""unit"" test/${{ matrix.topic }}', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[elasticsearch,dev,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_elasticsearch.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,sql,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_sql.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,opensearch,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_opensearch.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_deepsetcloud.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[faiss,dev,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_faiss.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,weaviate,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_weaviate.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,pinecone,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_pinecone.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,preprocessing]', 'pytest --maxfail=5 -m ""document_store and integration"" test/document_stores/test_memory.py\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,preprocessing]', 'pytest --maxfail=5 -m ""integration"" test/prompt\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'pip install .[dev,preprocessing]', 'pytest --maxfail=5 -m ""integration"" test/agents\n', 'if [ ""${{ job.status }}"" = ""success"" ]; then\n  echo ""alert_type=success"" >> ""$GITHUB_OUTPUT"";\nelse\n  echo ""alert_type=error"" >> ""$GITHUB_OUTPUT"";\nfi\n', 'echo ""Finish him!""', 'echo ""Skipped!""', 'go install github.com/rhysd/actionlint/cmd/actionlint@latest', 'actionlint']"
""
"['cd xde && pip install -r requirements.txt', 'cd xde && ./xde build', 'cd xde && ./xde up', 'cd xde && ./xde test --not-interactive', 'pip install .', 'xxh --version', 'pip install xonsh', 'xonsh xxh-appimage-build.xsh', 'ls -lah build/xxh-x86_64.AppImage', 'pip install xonsh', 'xonsh xxh-portable-musl-alpine-build.xsh', 'ls -lah build/xxh-portable-musl-alpine-Linux-x86_64.tar.gz', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['curl -sL https://deb.nodesource.com/setup_14.x | sudo bash\ncurl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -\necho ""deb https://dl.yarnpkg.com/debian/ stable main"" | sudo tee /etc/apt/sources.list.d/yarn.list\nsudo apt-get update\nsudo apt-get install \\\n  libssl-dev \\\n  libcurl4-openssl-dev \\\n  python3-dev \\\n  build-essential \\\n  libxml2-dev \\\n  libxmlsec1-dev \\\n  libxmlsec1-openssl \\\n  musl-dev \\\n  yarn \\\n  nodejs\npip install \\\n  -r requirements.txt \\\n  -r requirements-test.txt \\\n  -r requirements-docs.txt\npip install .\nyarn --cwd ui install\nyarn --cwd ui build:prod\nterraform -chdir=terraform/central-account/ init\nterraform -chdir=terraform/spoke-accounts/ init\npre-commit install\n', 'pre-commit run -a\n', 'git fetch --prune --unshallow --tags', 'curl -sL https://deb.nodesource.com/setup_14.x | sudo bash\ncurl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -\necho ""deb https://dl.yarnpkg.com/debian/ stable main"" | sudo tee /etc/apt/sources.list.d/yarn.list\nsudo apt-get update\nsudo apt-get install \\\n  yarn \\\n  nodejs\nyarn --cwd ui install\nyarn --cwd ui build:prod\n', 'pip install build wheel setupmeta', 'python -m build --sdist --wheel --outdir dist/ .', 'docker build -t consoleme .\n', 'docker tag consoleme consoleme/consoleme:unstable\ndocker push --all-tags consoleme/consoleme\n', 'export CONSOLEME_VERSION=$(python3 setup.py -q --version | sed -r \'s/\\+/\\./g\')\nexport CONSOLEME_MAJOR=$(cut -d \'.\' -f 1 <<< ""$CONSOLEME_VERSION"")\nexport CONSOLEME_MINOR=$(cut -d \'.\' -f 1,2 <<< ""$CONSOLEME_VERSION"")\nexport CONSOLEME_PATCH=$(cut -d \'.\' -f 1,2,3 <<< ""$CONSOLEME_VERSION"")\ndocker tag consoleme consoleme/consoleme:$CONSOLEME_VERSION\ndocker tag consoleme consoleme/consoleme:$CONSOLEME_MAJOR\ndocker tag consoleme consoleme/consoleme:$CONSOLEME_MINOR\ndocker tag consoleme consoleme/consoleme:$CONSOLEME_PATCH\ndocker tag consoleme consoleme/consoleme:latest\ndocker push --all-tags consoleme/consoleme\n', 'pip install awscli', 'mkdir -p ""$HOME/tools/ecs-cli""\ncurl -Lo ""$HOME/tools/ecs-cli/ecs-cli"" https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest\nchmod +x ""$HOME/tools/ecs-cli/ecs-cli""\necho ""$HOME/tools/ecs-cli"" >> $GITHUB_PATH', 'aws s3 cp s3://consolemeoss/consolemeoss_deploy/docker-compose-ecs.yaml .\naws s3 cp s3://consolemeoss/consolemeoss_deploy/docker-compose-ecs-celery.yaml .\naws s3 cp s3://consolemeoss/consolemeoss_deploy/ecs-params.yml .\naws s3 cp s3://consolemeoss/consolemeoss_deploy/deploy.sh .', 'chmod +x deploy.sh\n./deploy.sh', 'pip install awscli', 'aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n', 'docker build -t consoleme .\n', 'docker tag consoleme public.ecr.aws/consoleme/consoleme:unstable\ndocker push --all-tags public.ecr.aws/consoleme/consoleme\n', '# Extract tags from version\nexport CONSOLEME_VERSION=$(python3 setup.py -q --version | sed -r \'s/\\+/\\./g\')\nexport CONSOLEME_MAJOR=$(cut -d \'.\' -f 1 <<< ""$CONSOLEME_VERSION"")\nexport CONSOLEME_MINOR=$(cut -d \'.\' -f 1,2 <<< ""$CONSOLEME_VERSION"")\nexport CONSOLEME_PATCH=$(cut -d \'.\' -f 1,2,3 <<< ""$CONSOLEME_VERSION"")\n# Tag image\ndocker tag consoleme public.ecr.aws/consoleme/consoleme:$CONSOLEME_VERSION\ndocker tag consoleme public.ecr.aws/consoleme/consoleme:$CONSOLEME_MAJOR\ndocker tag consoleme public.ecr.aws/consoleme/consoleme:$CONSOLEME_MINOR\ndocker tag consoleme public.ecr.aws/consoleme/consoleme:$CONSOLEME_PATCH\ndocker tag consoleme public.ecr.aws/consoleme/consoleme:latest\ndocker push --all-tags public.ecr.aws/consoleme/consoleme\n']"
"['python -m pip install --upgrade pip\npip install black==22.3.0\npip install flake8\n', 'black --line-length=100 --target-version=py39 --check --diff .\n', '# The code-base needs to be cleaned up. There are too many Flake8\n# related warnings now. Ignore known problems to catch new ones.\nflake8 --ignore=C901,E203,E262,E265,E266,E402,E501,E712,E713,E722,E731,E741,F401,F403,F405,F811,F821,F841,W503\n# Run full scan for visibility purposes.\nflake8 --exit-zero\n', ""# Read each line of requirement.txt and flag if any line doesn't contain ==, @, newline, or #\nsed '/^$/d' < requirements.txt | while read i; do if [[ ! $i =~ [==|@|^#] ]]; then echo $i is not pinned; fi; done\n"", 'sudo apt-get update\nsudo apt-get install libgpgme-dev libldap2-dev libsasl2-dev swig\ncat requirements-dev.txt | grep tox | xargs pip install\n', 'tox -e py39-unit', 'sudo apt-get update\nsudo apt-get install libgpgme-dev libldap2-dev libsasl2-dev swig\n', ""pip install wheel  # allow pip to use wheel instead of legacy 'setup.py install'\n./hack/verify-requirements.sh\n"", 'pip install -r ./requirements-dev.txt\n', 'make types-test', 'sudo apt-get update\nsudo apt-get install libgpgme-dev libldap2-dev libsasl2-dev swig\npython -m pip install --upgrade pip\ncat requirements-dev.txt | grep tox | xargs pip install\n', 'tox -e py39-e2e', 'sudo apt-get update\nsudo apt-get install libgpgme-dev libldap2-dev libsasl2-dev swig\npython -m pip install --upgrade pip\ncat requirements-dev.txt | grep tox | xargs pip install\n', 'tox -e py39-registry', 'docker build -t localhost/quay-local:latest .', 'docker-compose up -d redis quay-db\ndocker exec -t quay-db bash -c \'while ! pg_isready; do echo ""waiting for postgres""; sleep 2; done\'\nDOCKER_USER=""1001:0"" docker-compose up -d --no-build quay\n', 'cd web && npm run quay:seed', 'python -m pip install --upgrade pip\npip install pytest requests\n', 'docker restart quay-quay\nsleep 30\nmake integration-test\n', 'sudo apt-get update\nsudo apt-get install libgpgme-dev libldap2-dev libsasl2-dev swig\nsudo systemctl unmask docker\nsudo systemctl start docker\ndocker version\npython -m pip install --upgrade pip\ncat requirements-dev.txt | grep tox | xargs pip install\n', 'tox -e py39-mysql', 'sudo apt-get update\nsudo apt-get install libgpgme-dev libldap2-dev libsasl2-dev swig\nsudo systemctl unmask docker\nsudo systemctl start docker\ndocker version\npython -m pip install --upgrade pip\ncat requirements-dev.txt | grep tox | xargs pip install\n', 'tox -e py39-psql', 'BRANCH_NAME=${GITHUB_REF#refs/heads/}\necho ""version=${BRANCH_NAME/${{ env.BRANCH_PREFIX }}/}"" >> $GITHUB_OUTPUT\n', 'curl -fsSL https://clis.cloud.ibm.com/install/linux | sh', 'ibmcloud login -q --apikey ${{ secrets.IBMCLOUD_API_KEY }} -r eu-gb', 'ibmcloud plugin install vpc-infrastructure', '#creation of zvsi\nibmcloud is instance-create $ZVSI_INSTANCE_NAME $ZVSI_VPC $ZVSI_ZONE_NAME $ZVSI_PROFILE_NAME $ZVSI_SUBNET --image $ZVSI_IMAGE --keys $ZVSI_KEY --resource-group-id $ZVSI_RG_ID\n#Reserving a floating ip to the ZVSI\nibmcloud is floating-ip-reserve $ZVSI_FP_NAME --zone $ZVSI_ZONE_NAME --resource-group-id $ZVSI_RG_ID --in $ZVSI_INSTANCE_NAME\n#Bouding the Floating ip to the ZVSI\nibmcloud is floating-ip-update $ZVSI_FP_NAME --nic primary --in $ZVSI_INSTANCE_NAME\nsleep 20\n  \n#Saving the Floating IP to login ZVSI\nZVSI_HOST=$(ibmcloud is floating-ip $ZVSI_FP_NAME | awk \'/Address/{print $2}\')\necho $ZVSI_HOST\necho ""IP=${ZVSI_HOST}"" >> $GITHUB_OUTPUT\n', 'mkdir ~/.ssh\nchmod 700 ~/.ssh\n\ntouch ~/.ssh/id_builder_aarch64\nchmod 600 ~/.ssh/id_builder_aarch64\necho ""$BUILDER_AARCH64_SSH_KEY"" >~/.ssh/id_builder_aarch64\n\ntouch ~/.ssh/id_builder_ppc64le\nchmod 600 ~/.ssh/id_builder_ppc64le\necho ""$BUILDER_PPC64LE_SSH_KEY"" >~/.ssh/id_builder_ppc64le\n\ntouch ~/.ssh/id_builder_s390x\nchmod 600 ~/.ssh/id_builder_s390x\necho ""$BUILDER_S390X_SSH_KEY"" > ~/.ssh/id_builder_s390x\n\ntouch ~/.ssh/known_hosts\nchmod 600 ~/.ssh/known_hosts\ncat >~/.ssh/known_hosts <<END\n$BUILDER_AARCH64_SSH_KNOWN_HOSTS\n$BUILDER_PPC64LE_SSH_KNOWN_HOSTS\nEND\n\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\ncat >~/.ssh/config <<END\nHost builder-aarch64\n  IdentityFile ""~/.ssh/id_builder_aarch64""\n$BUILDER_AARCH64_SSH_CONFIG\n\nHost builder-ppc64le\n  IdentityFile ""~/.ssh/id_builder_ppc64le""\n$BUILDER_PPC64LE_SSH_CONFIG\n\nHost builder-s390x\n  StrictHostKeyChecking no\n  HostName $BUILDER_S390X_SSH_HOST\n  User root\n  IdentityFile ""~/.ssh/id_builder_s390x""\nEND\n', '#Delete the created ZVSI\nibmcloud is instance-delete $ZVSI_INSTANCE_NAME --force\nsleep 20\n\n#Release the created FP\nibmcloud is floating-ip-release $ZVSI_FP_NAME --force\n', 'tag=`basename ${{ github.ref }}`\necho ""PREFIX=quay-${tag}/"" >> $GITHUB_ENV\necho ""TAG=${tag}"" >> $GITHUB_ENV\n', 'git archive --prefix ""${PREFIX}"" -o release.tar ""${GITHUB_REF}""\ngzip release.tar\n', 'curl -o /tmp/git-chglog.tar.gz -fsSL\\\n  https://github.com/git-chglog/git-chglog/releases/download/v0.14.0/git-chglog_0.14.0_linux_amd64.tar.gz\ntar xvf /tmp/git-chglog.tar.gz --directory /tmp\nchmod u+x /tmp/git-chglog\necho ""creating change log for tag: $TAG""\n/tmp/git-chglog --tag-filter-pattern ""v3.*"" -o changelog v3.6.0-alpha.4..\n', 'tag=`basename ${{ github.ref }}`\necho ""VERSION=${tag}"" >> $GITHUB_ENV\n', 'tag=`basename ${{ github.ref }}`\necho ""QUAY_VERSION=${tag}"" >> $GITHUB_ENV\necho ""TAG=quay.io/projectquay/quay:${tag}"" >> $GITHUB_ENV\necho ""QUAY_USER=projectquay+quay_github"" >> $GITHUB_ENV\necho ""::add-mask::${{ secrets.QUAY_TOKEN }}""\n', 'd=$(mktemp -d)\ntrap \'rm -rf ""$d""\' EXIT\ntar -xz -f ${{steps.download.outputs.download-path}}/release.tar.gz --strip-components=1 -C ""$d""\ndocker build --build-arg QUAY_VERSION --tag ""${TAG}"" ""$d""\n', 'docker login -u ""${QUAY_USER}"" -p \'${{ secrets.QUAY_TOKEN }}\' quay.io\ndocker push ""${TAG}""\n', 'psql ""postgresql://postgres_user:postgres_password@localhost:5432/postgres_db"" \\\n  --command=""create extension if not exists pg_trgm;""\n', 'cd dist-spec/conformance\nCGO_ENABLED=0 go test -c -o conformance.test\n', 'IP=$(ip addr show dev eth0 | sed -n \'s: *inet \\([0-9.]*\\)/.*:\\1:p\')\n\nmkdir -p /tmp/config\ncat >/tmp/config/config.yaml <<EOF\nBUILDLOGS_REDIS: {""host"": ""$IP"", ""port"": 6379}\nDATABASE_SECRET_KEY: anothercrazykey!\nDB_URI: postgresql://postgres_user:postgres_password@$IP:5432/postgres_db\nDISTRIBUTED_STORAGE_CONFIG: {""default"": [""LocalStorage"", {""storage_path"": ""/datastorage/registry""}]}\nDISTRIBUTED_STORAGE_PREFERENCE: [""default""]\nSERVER_HOSTNAME: localhost\nSETUP_COMPLETE: true\nUSER_EVENTS_REDIS: {""host"": ""$IP"", ""port"": 6379}\n\nDATA_MODEL_CACHE_CONFIG:\n  engine: memcached\n  endpoint: [127.0.0.1, 18080]\n  repository_blob_cache_ttl: 60s\n  catalog_page_cache_ttl: 60s\n  namespace_geo_restrictions_cache_ttl: 240s\n  # OCI Conformance tests don\'t expect tags to be cached.\n  # If we implement cache invalidation, we can enable it back.\n  active_repo_tags_cache_ttl: 0s\nEOF\n\n# Run the Quay container. See also:\n# https://access.redhat.com/documentation/en-us/red_hat_quay/3.8/html/deploy_red_hat_quay_-_high_availability/deploying_red_hat_quay\ndocker run \\\n  --detach \\\n  --health-cmd=""curl -fsS http://localhost:8080/health/instance"" \\\n  --health-interval=10s \\\n  --health-retries=5 \\\n  --health-timeout=5s \\\n  --name=quay \\\n  --publish=127.0.0.1:443:8443/tcp \\\n  --publish=127.0.0.1:80:8080/tcp \\\n  --volume=/tmp/config:/conf/stack:Z \\\n  localhost/quay\n\nfor i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20; do\n  status=$(docker inspect -f \'{{.State.Health.Status}}\' quay)\n  printf ""[%s] status: %s\\n"" ""$(date -u +\'%F %T\')"" ""$status""\n  if [ ""$status"" == ""healthy"" ]; then\n    break\n  fi\n  sleep 10\ndone\ndocker inspect --format=\'{{json .State.Health}}\' quay\n\ndocker exec -i quay python <<EOF\nfrom app import app\nfrom data import model\nfrom data.database import configure\nconfigure(app.config)\nmyuser = model.user.create_user(""myuser"", ""p@ssw0rd"", ""admin@localhost.local"", auto_verify=True)\nmyorg = model.organization.create_organization(""myorg"", ""myorg@localhost.local"", myuser)\nEOF\n', '# Registry details\nexport OCI_ROOT_URL=""http://localhost""\nexport OCI_NAMESPACE=""myuser/myrepo""\nexport OCI_CROSSMOUNT_NAMESPACE=""myorg/other""\nexport OCI_USERNAME=""myuser""\nexport OCI_PASSWORD=""p@ssw0rd""\n\n# Which workflows to run\nexport OCI_TEST_PULL=1\nexport OCI_TEST_PUSH=1\nexport OCI_TEST_CONTENT_DISCOVERY=1\nexport OCI_TEST_CONTENT_MANAGEMENT=0\n\n# Extra settings\nexport OCI_HIDE_SKIPPED_WORKFLOWS=0\nexport OCI_DEBUG=1\nexport OCI_DELETE_MANIFEST_BEFORE_BLOBS=0\n\n./dist-spec/conformance/conformance.test\n', 'mkdir -p .oci-test-results/ .logs/\nmv report.html junit.xml .oci-test-results/ || true\ndocker logs quay >.logs/quay.log 2>&1 || true\n', 'docker rm --force --volumes quay\n', 'curl -o /tmp/git-chglog.tar.gz -fsSL\\\n  https://github.com/git-chglog/git-chglog/releases/download/v0.14.0/git-chglog_0.14.0_linux_amd64.tar.gz\ntar xvf /tmp/git-chglog.tar.gz --directory /tmp\nchmod u+x /tmp/git-chglog\necho ""creating change log for tag: ${{ github.event.inputs.tag }}""\n/tmp/git-chglog --next-tag ""${{ github.event.inputs.tag }}"" --tag-filter-pattern ""v3.*"" -o CHANGELOG.md v3.6.0-alpha.4..\n']"
[]
""
"['echo ${{ github.event.pull_request.user.login }}', 'pip install tox', 'tox -e mypy', 'pip install pre-commit', 'pre-commit run --all-files flake8', 'pip install black', 'black --check --diff .', 'pip install pre-commit', 'pre-commit run --all-files doc8', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'pip install tox', 'tox -e py', ""OS_LOWERCASE=$(echo $RUNNER_OS | tr '[:upper:]' '[:lower:]')\ncurl -Os https://uploader.codecov.io/latest/${OS_LOWERCASE}/codecov\nchmod +x codecov\n./codecov\n"", 'pip install .', 'tiotr --help', 'tiohd --help']"
"['python3 -m pip install -r requirements_test.txt', 'pytest\n']"
""
""
"['create_build=false\n# get the data for the last commit and store it to a file\ncurl -sL https://api.github.com/repos/$GITHUB_REPOSITORY/commits | jq -r \'.[0]\' > $HOME/commit.json\n# get the date field from the file\ndate=""$(jq -r \'.commit.author.date\' $HOME/commit.json)""\n# parse the date format into seconds\ntimestamp=$(date --utc -d ""$date"" +%s)\n# work out how many days ago that was\ndays=$(( ( $(date --utc +%s) - $timestamp ) / 86400 ))\n# remove the file\nrm -f $HOME/commit.json\n# TODO: also check these\n# https://pypi.org/pypi/amulet-core/json\n# https://pypi.org/pypi/amulet-nbt/json\n# https://pypi.org/pypi/PyMCTranslate/json\n# https://pypi.org/pypi/minecraft-resource-pack/json\n# if it was less than one day ago\nif [ $days -lt 1 ]; then\n  # trigger a build\n  create_build=true\n  # get the most recent build number and create a new one based off it\n  regex_version=\'^[0-9]+\\.[0-9]+(\\.[0-9]+)?(\\.[0-9]+)?(b[0-9]+)?$\'\n  curl -sL https://api.github.com/repos/$GITHUB_REPOSITORY/releases | jq --arg reg $regex_version -r \'[.[] | select(.tag_name | test($reg))][0]\' > $HOME/release.json\n  tag_name=""$(jq -r \'.tag_name\' $HOME/release.json)""\n  # get the time of the commit that the release was built off\n  commit_date=""$(jq -r \'.created_at\' $HOME/release.json)""\n  rm -f $HOME/release.json\n  # convert time format into seconds\n  commit_timestamp=$(date --utc -d ""$commit_date"" +%s)\n  if $commit_timestamp >= $timestamp; then\n    # if the release is built off a newer or equal commit don\'t build\n    create_build=false\n  fi\n  beta_regex_version=\'^([0-9]+\\.[0-9]+(\\.[0-9]+)?(\\.[0-9]+)?b)([0-9]+)$\'\n  release_regex_version=\'^([0-9]+\\.[0-9]+(\\.[0-9]+)?\\.)([0-9]+)$\'\n  if [[ $tag_name =~ $beta_regex_version ]]; then\n    # if it is a beta release increment the beta number and add .dev\n    version_number=${BASH_REMATCH[1]}$((${BASH_REMATCH[-1]} + 1)).dev\n  elif [[ $tag_name =~ $release_regex_version ]]; then\n    # if it is a full release increment the number and add b0.dev\n    version_number=${BASH_REMATCH[1]}$((${BASH_REMATCH[-1]} + 1))b0.dev\n  else\n    create_build=false\n  fi\n  if $create_build; then\n    clean_date=$(date --utc +%Y%m%d%H%M)\n    echo ""TAG_NAME=$version_number$clean_date"" >> $GITHUB_ENV\n  fi\necho ""CREATE_BUILD=$create_build"" >> $GITHUB_ENV\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pyinstaller==4.9 build\n', 'python -m build -C--global-option=--find-libs=True -C--global-option=--find-libs=True\npython -m pip install dist/amulet_map_editor-*.whl --upgrade\n', 'built_path=Amulet-v${{ github.event.release.tag_name }}-$RUNNER_OS-${{ matrix.cfg.architecture }}.zip\necho ""BUILT_PATH=$built_path"" >> $GITHUB_ENV\npython -m PyInstaller -y Amulet.spec\ncd dist\nif [ ""$RUNNER_OS"" == ""Windows"" ]; then\n  7z a $built_path Amulet\nelif [ ""$RUNNER_OS"" == ""macOS"" ]; then\n  zip -r $built_path Amulet\nelse\n  echo ""$RUNNER_OS not supported""\n  exit 1\nfi\ncd ..\n', 'twine upload dist/amulet[_-]map[_-]editor* --skip-existing\n', 'python -m pip install --upgrade pip\npip install black\n', 'python -m black --check --diff .\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m unittest discover -v -s tests']"
['git checkout HEAD^2']
""
""
"['python -m pip install --upgrade pip\npython -m pip install --upgrade pre-commit\n', 'pre-commit run --all-files --show-diff-on-failure', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['# Download dasel to modify pyproject.toml\ncurl -sSLf https://github.com/TomWright/dasel/releases/download/v2.0.2/dasel_linux_amd64 \\\n  -L -o /tmp/dasel && chmod +x /tmp/dasel\n\n# Modify pyproject.toml to set the nightly version in the form of\n# x.y.z.postN, where N is the number of commits since the last release\n/tmp/dasel put -f pyproject.toml project.name -v aesara-nightly\n/tmp/dasel put -f pyproject.toml tool.hatch.version.raw-options.version_scheme -v post-release\n/tmp/dasel put -f pyproject.toml tool.hatch.version.raw-options.local_scheme -v no-local-version\n\n# Install build prerequisites\npython -m pip install -U pip build\n', 'python -m build --sdist .', 'python -m pip install -U pip\npython -m pip install build\npython -m build\n', 'mkdir -p test-sdist\ncd test-sdist\npython -m venv venv-sdist\nvenv-sdist/bin/python -m pip install ../dist/aesara-*.tar.gz\nvenv-sdist/bin/python -c ""import aesara;print(aesara.__version__)""\necho ""Checking for lazylinker_c.c...""\ntest -n ""$(find . -name lazylinker_c.c)"" && echo ""Found lazylinker_c.c""\necho ""Checking for d3viz template.html...""\ntest -n ""$(find . -name template.html | grep d3viz)"" && echo ""Found template.html""\n', 'mkdir -p test-wheel\ncd test-wheel\npython -m venv venv-wheel\nvenv-wheel/bin/python -m pip install ../dist/aesara-*.whl\nvenv-wheel/bin/python -c ""import aesara;print(aesara.__version__)""\necho ""Checking for lazylinker_c.c...""\ntest -n ""$(find . -name lazylinker_c.c)"" && echo ""Found lazylinker_c.c""\necho ""Checking for d3viz template.html...""\ntest -n ""$(find . -name template.html | grep d3viz)"" && echo ""Found template.html""\n', 'echo $MATRIX_CONTEXT\nexport MATRIX_ID=`echo $MATRIX_CONTEXT | md5sum | cut -c 1-32`\necho $MATRIX_ID\necho ""::set-output name=id::$MATRIX_ID""\n', 'mamba install --yes -q -c conda-forge ""python~=${PYTHON_VERSION}=*_cpython"" mkl ""numpy>=1.23.3"" scipy pip mkl-service graphviz cython pytest coverage pytest-cov pytest-benchmark sympy filelock etuples logical-unification miniKanren cons typing_extensions ""setuptools>=48.0.0""\nif [[ $INSTALL_NUMBA == ""1"" ]]; then mamba install --yes -q -c conda-forge -c numba ""python~=${PYTHON_VERSION}=*_cpython"" ""numba>=0.57.0""; fi\nmamba install --yes -q -c conda-forge ""python~=${PYTHON_VERSION}=*_cpython"" ""numpy>=1.23.3"" jax jaxlib\npip install --no-deps -e ./\nmamba list && pip freeze\npython -c \'import aesara; print(aesara.config.__str__(print_doc=False))\'\npython -c \'import aesara; assert(aesara.config.blas__ldflags != """")\'\n', 'if [[ $FAST_COMPILE == ""1"" ]]; then export AESARA_FLAGS=$AESARA_FLAGS,mode=FAST_COMPILE; fi\nif [[ $FLOAT32 == ""1"" ]]; then export AESARA_FLAGS=$AESARA_FLAGS,floatX=float32; fi\nexport AESARA_FLAGS=$AESARA_FLAGS,warn__ignore_bug_before=all,on_opt_error=raise,on_shape_error=raise,gcc__cxxflags=-pipe\npython -m pytest -x -r A --verbose --runslow --cov=aesara/ --cov-report=xml:coverage/coverage-${MATRIX_ID}.xml --no-cov-on-fail $PART --benchmark-skip\n', 'mamba install --yes -q -c conda-forge -c numba ""python~=${PYTHON_VERSION}=*_cpython"" mkl numpy scipy pip mkl-service cython pytest jax jaxlib pytest-benchmark ""numba>=0.57.0""\npip install -e ./\nmamba list && pip freeze\npython -c \'import aesara; print(aesara.config.__str__(print_doc=False))\'\npython -c \'import aesara; assert(aesara.config.blas__ldflags != """")\'\n', 'export AESARA_FLAGS=mode=FAST_COMPILE,warn__ignore_bug_before=all,on_opt_error=raise,on_shape_error=raise,gcc__cxxflags=-pipe\npython -m pytest --benchmark-only --benchmark-json output.json\n', 'exit 1', 'python -m pip install -U coverage>=5.1 coveralls\n']"
"['python -m pip install --upgrade pip poetry\npoetry install\n', 'poetry run pre-commit run pyupgrade --all-files\n', 'poetry run pre-commit run black --all-files\n', 'poetry run pre-commit run flake8 --all-files\n', 'poetry run pre-commit run isort --all-files\n', 'poetry run pre-commit run mypy --all-files\n', 'poetry run pre-commit run trailing-whitespace --all-files\n', 'poetry run pre-commit run end-of-file-fixer --all-files\n', 'poetry run pre-commit run check-docstring-first --all-files\n', 'poetry run pre-commit run debug-statements --all-files\n', 'poetry run pre-commit run check-ast --all-files\n', 'python -m pip install --upgrade pip poetry\npoetry install\n', 'poetry run pytest --cov kasa --cov-report xml\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip\npython -m pip install setuptools wheel\npython -m pip install .\npython -m pip install -r requirements_dev.txt\n', 'pytest --cov=. -s tests', 'python -m pip install --upgrade pip\npython -m pip install setuptools wheel\npython -m pip install .\npython -m pip install -r requirements_dev.txt\n', 'pytest --cov=. -s tests', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry build', 'poetry run pip install types-setuptools\npoetry run mypy\n', 'poetry run pytest --cov=.', 'python -m pip install --upgrade pip\npython -m pip install setuptools twine\n', 'python setup.py sdist\ntwine upload dist/*\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry build', 'poetry run pip install types-setuptools\npoetry run mypy\n', 'poetry run pytest --cov=.']"
"['python -m pip install --upgrade pip\npip install Cython\npip install torch\n', 'pip install -U text2vec\npython -c ""import text2vec; print(text2vec.__version__)""\npip uninstall -y text2vec\n', 'pip install -r requirements.txt\npip install .', 'python -m pip install --upgrade pip\npip install Cython\npip install torch\n', 'pip install -U text2vec\npython -c ""import text2vec; print(text2vec.__version__)""\npip uninstall -y text2vec\n', 'pip install -r requirements.txt\npip install .\npip install pytest\npip install sentence-transformers\n', 'python -m pytest\n', 'python -m pip install --upgrade pip\npip install torch\n', 'python -c ""import sys; print(sys.version)""\npython -c ""import torch; print(torch.__version__)""\n', 'pip install -U text2vec\npython -c ""import text2vec; print(text2vec.__version__)""\npip uninstall -y text2vec\n', 'python -m pip install --upgrade pip\npip install Cython\npip install -r requirements.txt\npip install .']"
[]
"['python -m pip install --upgrade pip\npip install .\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
['python -m pip install --upgrade pip\nsudo apt install make python3-dev python3-tk python3-setuptools python3-wheel python3-wxgtk4.0 -y\npip install pyautogui pynput pyinstaller \npyinstaller --icon=atbswp/img/icon.png --clean --windowed --onefile --add-data atbswp/img:img --add-data atbswp/lang:lang atbswp/atbswp.py\n']
"['python -m pip install --upgrade pip\npython -m pip install poetry==1.3.2\npython -m poetry install\n', 'python -m poetry run flake8 docat tests\n', 'python -m poetry run mypy .\n', 'python -m poetry run pytest\n', 'yarn install', 'yarn build', 'yarn lint', 'yarn test', 'docker build . --tag ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:${{ github.sha }}\ndocker tag ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:${{ github.sha }} ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:unstable\n', 'docker tag ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:${{ github.sha }} ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:$(git describe --tags)\ndocker tag ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:${{ github.sha }} ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat:latest\n', 'docker push --all-tags ${{ matrix.registry.name }}/${{ matrix.registry.org }}/docat\n']"
"['pipx install poetry', 'poetry install -E all', 'poetry run pytest tests/test_benchmarks.py --benchmark-enable --benchmark-json output.json\n', 'pipx install poetry!=1.4.1', 'poetry install -E all', 'poetry run isort --check --diff vpype vpype_cli vpype_viewer tests\npoetry run black --check --diff vpype vpype_cli vpype_viewer tests\npoetry run mypy\n', 'sudo apt-get update -y -qq\nsudo apt-get install -y -qq libegl1-mesa libegl1-mesa-dev\n', 'poetry run pytest --cov=./ --cov-report=xml\n', 'poetry run pytest\n', 'poetry run pytest --skip-image-similarity\n', 'pipx install poetry!=1.4.1', 'poetry install -E all', 'VERSION=""`poetry version --short`-${GITHUB_SHA}""\necho ""version=${VERSION}"" >> $GITHUB_ENV\npoetry run ${GITHUB_WORKSPACE}\\\\scripts\\\\build.bat\n', 'pipx install poetry!=1.4.1', 'poetry install -E all', 'VERSION=""`poetry version --short`""\necho ""version=${VERSION}"" >> $GITHUB_ENV\npoetry run ${GITHUB_WORKSPACE}\\\\scripts\\\\build.bat\n', 'pipx install poetry!=1.4.1', 'poetry build']"
"['pipx install poetry', 'chmod +x ./scripts/duckdb-less_packages.sh\n./scripts/duckdb-less_packages.sh\n', 'poetry install --no-interaction --no-root --only linting', 'source .venv/bin/activate\npython3 -m black .\n', 'git config user.name ""$(git log -n 1 --pretty=format:%an)""\ngit config user.email ""$(git log -n 1 --pretty=format:%ae)""\n# short-circuit if we have no changes, otherwise attempt to commit and push\n# should only fail on forks, in which case contributors will need to manually run black, commit, and push\ngit diff-index --quiet HEAD || (echo ""Attempting to commit changes"" && git commit -am \'lint with black\' && git push -f)\n', 'source .venv/bin/activate\npython3 -m black --check .\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'source .venv/bin/activate\nmv scripts/generate_dialect_comparison_docs.py generate_dialect_comparison_docs.py\npython generate_dialect_comparison_docs.py\n', 'python3 scripts/preprocess_markdown_includes.py', 'tree docs/', 'pip install --upgrade pip', 'pip install -r scripts/docs-requirements.txt && mkdocs gh-deploy --force', 'pip freeze', 'mkdocs --version', 'tree docs/', 'poetry install --no-interaction --no-root --only linting', 'source .venv/bin/activate\nruff --show-source .\n', 'pipx install poetry', 'poetry publish --build -u __token__ -p $TOKEN', 'poetry install --no-interaction --no-root --with benchmarking', 'poetry install --no-interaction', ""source .venv/bin/activate\npytest benchmarking/test_performance.py  --benchmark-json benchmarking/output.json -k 'test_2_rounds_1k_sqlite or test_2_rounds_1k_duckdb'\npython benchmarking/combine_benchmarks_timeseries.py\n"", 'poetry install --no-interaction --no-root --with benchmarking', 'poetry install --no-interaction', 'source .venv/bin/activate\npytest benchmarking/test_performance.py  --benchmark-json benchmarking/output.json -k \'test_2_rounds_1k_sqlite or test_2_rounds_1k_duckdb\'\npython benchmarking/combine_benchmarks_timeseries.py\ngit checkout master\ngit config --global user.email ""actions@users.noreply.github.com""\ngit config --global user.name ""robin""\ngit add -A -f benchmarking/time_series.json\ngit commit -m ""Updated benchmarks timeseries"" || exit 0\ngit push --force\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'source .venv/bin/activate\npytest tests/\n', 'cd splink_demos\ncp ../benchmarking/conftest.py conftest.py\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install ..\ngrep -v ""^splink=="" requirements.txt > temp && mv temp requirements.txt\npip install -r requirements.txt\npython ../scripts/make_test_datasets_smaller.py\nfind . -type f -name \'example_*ipynb\' -print0 | while IFS= read -r -d $\'\\0\' file; do\n  if sed -i -E \'s/max_pairs\\s*=\\s*[0-9.eE+-]+/max_pairs = 10000/g\' ""$file""; then\n    echo ""Modified: $file""\n  fi\ndone\nfind . -type f -name \'example_*ipynb\' -exec grep -H \'max_pairs\' {} \\;\nsed -i \'s/display(ui,out)/pass/\' example_real_time_record_linkage.ipynb\nsed -i \'s/display(linker.waterfall_chart(recs, filter_nulls=False))/pass/\' example_real_time_record_linkage.ipynb\npython -m pytest --nbmake --nbmake-kernel=python3 example_*ipynb\n', 'cd splink_demos\ncp ../benchmarking/conftest.py conftest.py\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install ..\ngrep -v ""splink=="" requirements.txt > temp && mv temp requirements.txt\npip install -r requirements.txt\npython -m pytest --nbmake --nbmake-kernel=python3 0[0-6]_*ipynb\n', 'chmod +x ./scripts/pseudo_symlink_cls.sh\nsource ./scripts/pseudo_symlink_cls.sh\n']"
"['echo ""$GITHUB_CONTEXT""', 'python3.7 -m pip install flit', 'python3.7 -m flit install --extras doc', 'python3.7 -m pip install git+https://${{ secrets.ACTIONS_TOKEN }}@github.com/squidfunk/mkdocs-material-insiders.git', 'python3.7 -m mkdocs build', 'python3.7 -m mkdocs build --config-file mkdocs.insiders.yml', 'bash ./scripts/zip-docs.sh', 'rm -rf ./site\nmkdir ./site\n', 'cd ./site\nunzip docs.zip\nrm -f docs.zip\n', 'pip install flit', 'flit install --symlink', 'bash scripts/publish.sh', 'pip install smokeshow', 'smokeshow upload coverage-html', 'pip install flit', 'python -m flit install --symlink', 'python -m pip install "".[test]""', 'pip install ""click<8.0.0""', 'bash scripts/lint.sh', 'mkdir coverage', 'bash scripts/test.sh', 'pip install coverage[toml]', 'ls -la coverage', 'coverage combine coverage', 'coverage report', 'coverage html --show-contexts --title ""Coverage for ${{ github.sha }}""']"
"['echo ""PY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_OUTPUT\necho ""PIP_CACHE=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install -U .', 'python -m pip install click\npython install-pdm.py --version head\necho ""$HOME/.local/bin"" >> $GITHUB_PATH\n', 'python -m pip install -U setuptools\npdm install -v -dGtest\npdm run pip install -U setuptools\npdm info\n', 'pdm run pytest -n auto --cov=pdm --cov-config=setup.cfg --cov-report=xml tests', 'python -m pip install .\npdm self add pdm-packer\n', 'pdm pack', 'python pdm.pyz --version', 'pip install -U .\npdm install -G doc\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch origin gh-pages:gh-pages\ncd docs && pdm run mike deploy --update-aliases dev\ngit push origin gh-pages\n', 'python -m pip install build\npython -m build\n', 'python -m pip install dist/*.whl\npdm --help\n', 'pdm install -G doc\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch origin gh-pages:gh-pages\ntag=""${{ github.ref_name }}""\nDOC_VERSION=${tag%.*}\ncd docs\npdm run mike deploy --no-redirect --update-aliases ""$DOC_VERSION"" latest\ngit push origin gh-pages\n', 'pdm publish --no-build', ""awk '/-{3,}/{flag=1;next}/Release/{if (flag==1)exit}flag' CHANGELOG.md > .changelog.md\n""]"
"['pipx install poetry', 'poetry install --no-interaction', 'poetry run pre-commit run --all-files\n', 'pipx install poetry', 'sudo apt update\nsudo apt install -y libre2-dev libpq-dev\n', 'poetry install --no-interaction', 'CONFIG=tests/test.env poetry run alembic upgrade head\n', 'scripts/generate-build-info.sh ${{ github.sha }}\ncat app/build_info.py\n', 'poetry run pytest\n', 'scripts/generate-build-info.sh ${{ github.sha }}\ncat app/build_info.py\n', 'changelog=$(cat << EOH\n${{ steps.build_changelog.outputs.changelog }}\nEOH\n)\nmessageWithoutNewlines=$(echo ""${changelog}"" | awk \'{printf ""%s\\\\n"", $0}\')\nmessageWithoutDoubleQuotes=$(echo ""${messageWithoutNewlines}"" | sed ""s/\\""/\'/g"")\necho ""${messageWithoutDoubleQuotes}""\n\necho ""SLACK_CHANGELOG=${messageWithoutDoubleQuotes}"" >> $GITHUB_ENV\n']"
"['poetry run make ci-${{ matrix.job }}', 'poetry run python -m lifecycle.migrate', '# Copy current, latest config to local\ncp authentik/lib/default.yml local.env.yml\ncp -R .github ..\ncp -R scripts ..\ngit checkout $(git describe --tags $(git rev-list --tags --max-count=1))\nrm -rf .github/ scripts/\nmv ../.github ../scripts .\n', 'poetry run python -m lifecycle.migrate', 'set -x\ngit fetch\ngit reset --hard HEAD\ngit clean -d -fx .\ngit checkout $GITHUB_SHA\npoetry install\n', 'poetry run python -m lifecycle.migrate', 'poetry run make test\npoetry run coverage xml\n', 'poetry run coverage run manage.py test tests/integration\npoetry run coverage xml\n', 'docker-compose -f tests/e2e/docker-compose.yml up -d\n', 'npm ci\nmake -C .. gen-client-ts\nnpm run build\n', 'poetry run coverage run manage.py test ${{ matrix.job.glob }}\npoetry run coverage xml\n', 'echo mark', '# Create folder structure for go embeds\nmkdir -p web/dist\nmkdir -p website/help\ntouch web/dist/test website/help/test\n', 'make gen-client-go', 'make gen-client-go', 'go test -timeout 0 -v -race -coverprofile=coverage.out -covermode=atomic -cover ./...\n', 'echo mark', 'make gen-client-go', 'make gen-client-go', 'npm ci\nnpm run build-proxy\n', 'set -x\nexport GOOS=${{ matrix.goos }}\nexport GOARCH=${{ matrix.goarch }}\nexport CGO_ENABLED=0\ngo build -tags=outpost_static_embed -v -o ./authentik-outpost-${{ matrix.type }}_${{ matrix.goos }}_${{ matrix.goarch }} ./cmd/${{ matrix.type }}\n', 'npm ci', 'make gen-client-ts', 'npm run lint', 'npm ci', 'make gen-client-ts', 'npm run tsc', 'npm ci', 'make gen-client-ts', 'npm run prettier-check', ""npm ci\n# lit-analyse doesn't understand path rewrites, so make it\n# belive it's an actual module\ncd node_modules/@goauthentik\nln -s ../../src/ web\n"", 'make gen-client-ts', 'npm run lit-analyse', 'echo mark', 'npm ci', 'make gen-client-ts', 'npm run build', 'npm ci', 'npm run prettier-check', 'npm ci', 'npm test', 'npm ci', 'npm run ${{ matrix.job }}', 'echo mark', 'npm ci\nnpm run build-proxy\n', 'set -x\nexport GOOS=${{ matrix.goos }}\nexport GOARCH=${{ matrix.goarch }}\nexport CGO_ENABLED=0\ngo build -tags=outpost_static_embed -v -o ./authentik-outpost-${{ matrix.type }}_${{ matrix.goos }}_${{ matrix.goarch }} ./cmd/${{ matrix.type }}\n', 'echo ""PG_PASS=$(openssl rand -base64 32)"" >> .env\necho ""AUTHENTIK_SECRET_KEY=$(openssl rand -base64 32)"" >> .env\ndocker-compose pull -q\ndocker-compose up --no-start\ndocker-compose start postgresql redis\ndocker-compose run -u root server test-all\n', 'docker pull ghcr.io/goauthentik/server:latest\ncontainer=$(docker container create ghcr.io/goauthentik/server:latest)\ndocker cp ${container}:web/ .\n', 'echo ""PG_PASS=$(openssl rand -base64 32)"" >> .env\necho ""AUTHENTIK_SECRET_KEY=$(openssl rand -base64 32)"" >> .env\ndocker buildx install\ndocker build -t testing:latest .\necho ""AUTHENTIK_IMAGE=testing"" >> .env\necho ""AUTHENTIK_TAG=latest"" >> .env\ndocker-compose up --no-start\ndocker-compose start postgresql redis\ndocker-compose run -u root server test-all\n', 'poetry run ak compilemessages', 'make gen-client-ts', 'npm ci\nnpm publish\n', 'export VERSION=`node -e \'console.log(require(""../gen-ts-api/package.json"").version)\'`\nnpm i @goauthentik/api@$VERSION\n']"
"['sudo apt-get update\nsudo apt-get install -y --no-install-recommends apt-utils libsasl2-dev libssl-dev libldap2-dev python-dev-is-python3 libfuzzy-dev net-tools git libcurl4-openssl-dev\npython -m pip install --upgrade pip\nif [ -f requirements/project-requirements.txt ];\nthen pip install -r requirements/project-requirements.txt;\nfi\nif [ -f requirements/certego-requirements.txt ];\nthen pip install -r requirements/certego-requirements.txt;\nfi\n# Set the `CODEQL-PYTHON` environment variable to the Python executable\n# that includes the dependencies\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'git branch -a --list | cat\nFRONTEND_CHANGES=$(git diff --compact-summary origin/${{ github.base_ref }} -- frontend/* | wc -l)\necho ""::set-output name=frontend::$FRONTEND_CHANGES""\n', 'pip3 install --upgrade pip\npip3 install -r requirements/test-requirements.txt\n', 'black . --check --diff --exclude ""migrations|venv""\n', 'flake8 . --config=.flake8 --show-source\n', 'isort . --profile black --filter-files --check-only --diff --skip configuration/ldap_config.py\n', 'unzip -P infected tests/test_files.zip -d test_files\ncp docker/env_file_app_template docker/env_file_app\ncp docker/env_file_postgres_template docker/env_file_postgres\n', 'pip3 install -r requirements/pre-requirements.txt\n', 'cp docker/env_file_integrations_template docker/env_file_integrations\npython3 start.py --all_analyzers ci up --build -d\n', 'python3 start.py ci up --build -d\n', 'docker ps -a\n', 'docker exec intelowl_uwsgi pip3 install coverage\n', 'docker exec intelowl_uwsgi coverage run manage.py test --keepdb tests\n', 'docker exec intelowl_uwsgi coverage combine\ndocker exec intelowl_uwsgi coverage xml\ndocker cp intelowl_uwsgi:/opt/deploy/intel_owl/coverage.xml coverage.xml\n', 'npm i --no-optional --no-audit --no-fund\n', 'npm run lint\n', 'npm run prettier:check\n', 'npm run prettier:stylelint-check\n', 'npm run test -- --silent --coverage\n', 'pip install -r requirements/docs-requirements.txt\n', 'make -C docs html\n']"
"['echo ""The job was automatically triggered by a ${{ github.event_name }} event.""\necho ""This job is now running on a ${{ runner.os }} server hosted by GitHub!""\necho ""The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.""\necho ""  ""\necho ""github.ref = ${{ github.ref }}""\necho ""github.sha = ${{ github.sha }}""\necho ""github.event.pull_request.head.ref = ${{ github.event.pull_request.head.ref }}""\necho ""github.event.pull_request.head.sha = ${{ github.event.pull_request.head.sha }}""\necho ""github.event.pull_request.base.ref = ${{ github.event.pull_request.base.ref }}""\necho ""github.event.pull_request.base.sha = ${{ github.event.pull_request.base.sha }}""\necho ""  ""\n', 'echo ""The ${{ github.repository }} repository has been cloned to the runner.""', 'python -m pip install --upgrade pip\npip install pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pip install .', 'python -m pytest', 'echo ""This job\'s status is ${{ job.status }}.""', 'python -m pip install --upgrade pip\npip install packaging\n', 'git fetch origin master ${{ github.event.pull_request.base.sha }}\ngit fetch origin master ${{ github.event.pull_request.head.sha }}\n', 'git show ${{ github.event.pull_request.base.sha }}:src/mplfinance/_version.py > scripts/tv0.py\ngit show ${{ github.sha }}:src/mplfinance/_version.py > scripts/tv1.py\npython scripts/version_update_check.py tv0 tv1\n', ""egrep 'version_info .*=' src/mplfinance/_version.py"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\nls -l dist/*\n', 'twine upload dist/*', ""egrep 'version_info .*=' src/mplfinance/_version.py"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\nls -l dist/*\n', 'twine upload --repository-url https://test.pypi.org/legacy/ dist/*']"
"['DOCKER_IMAGE=principialabs/torch-points3d\nVERSION=noop\nif [ ""${{ github.event_name }}"" = ""schedule"" ]; then\n  VERSION=nightly\nelif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/}\nelif [[ $GITHUB_REF == refs/heads/* ]]; then\n  VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r \'s#/+#-#g\')\n  if [ ""${{ github.event.repository.default_branch }}"" = ""$VERSION"" ]; then\n    VERSION=edge\n  fi\nelif [[ $GITHUB_REF == refs/pull/* ]]; then\n  VERSION=pr-${{ github.event.number }}\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION}-${{ matrix.image-version }}""\nTAGS=""$TAGS,${DOCKER_IMAGE}:sha-${GITHUB_SHA::8}-${{ matrix.image-version }}""\nTAGS=""$TAGS,${DOCKER_IMAGE}:latest-${{ matrix.image-version }}""\n\necho ::set-output name=tags::${TAGS}\n\necho ""$TAGS""\n', 'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python\n', 'version=""$(basename -- ${{ github.ref }})""\nsed -i ""/^version/s/\\(.*\\)/version = \\""$version\\""/"" pyproject.toml\ncat pyproject.toml\n', '$HOME/.poetry/bin/poetry build\n', '$HOME/.poetry/bin/poetry publish\n', 'sudo apt-get install libopenblas-dev libsparsehash-dev', 'python -m pip install -U pip\npip install virtualenv\npython -m venv ${pythonEnv}\nsource ${pythonEnv}/bin/activate\nbash docker/install_python.sh cpu\npip install .\n', 'source ${pythonEnv}/bin/activate\npip install flake8 mypy==0.910  # Dev tools\nmake staticchecks\n', 'source ${pythonEnv}/bin/activate\npip install coverage\ncoverage run -m unittest -v\ncoverage xml\n']"
"['pip install black', 'black --check --diff .', 'pip install -r requirements.lock\n', '# Launch cartpole experiment and store the last reward of the training\npython muzero.py cartpole \'{""training_steps"": 7500}\' 2>&1 | tee log.txt\n', '# Retrieve last reward\nBEST_REWARD=$(cat log.txt | sed -n -E \'s/^.*reward: ([0-9]+).*$/\\1/p\' | sort -n | tail -1)\n\n# Display best reward\necho ""Best reward of cartpole training: "" $BEST_REWARD\n\n# Validate reward value\nif ((BEST_REWARD < 250)); then\n  exit 1\nfi\n']"
"['conda --version', 'which python', 'conda install -y scipy\npip install codecov\npip install pytest\npython setup.py install\n', 'pip install -e .[test]\n', 'python -m pytest\n', 'pip install coverage\ncoverage run -m pytest\ncoverage xml\n']"
['pip install pyinstaller -r requirements.txt\npyinstaller ./lsassy/console.py --onefile --clean -n lsassy --additional-hooks-dir=hooks\n']
"['sudo apt-get install shellcheck\n', 'shellcheck --version\n', './scripts/checkShellScripts.sh\n', 'git config --global user.name ""$(git --no-pager log --format=format:\'%an\' -n 1)""\ngit config --global user.email ""$(git --no-pager log --format=format:\'%ae\' -n 1)""\n', 'poetry install', 'poetry install --no-interaction\ncd apps/librelingo_yaml_loader\npoetry install --no-interaction\ncd ../librelingo_json_export\npoetry install --no-interaction\ncd ../..\n./scripts/verifyTestCourseJsonFiles.sh\n', 'yarn install', 'NODE_ENV=production yarn export:production', 'poetry install', 'poetry run pytest --doctest-modules', 'poetry run mypy apps', 'poetry run black apps --check', ""poetry run pylint $(git ls-files '*.py')"", 'poetry install --no-interaction\ncd apps/librelingo_yaml_loader\npoetry install --no-interaction\ncd ../librelingo_json_export\npoetry install --no-interaction\ncd ../..\n./scripts/verifyTestCourseJsonFiles.sh\n']"
""
"['pdm lock -v --no-cross-platform -G ci-quality', 'pdm install -G ci-quality', 'pdm run duty check-docs', 'pdm run duty check-quality', 'pdm run duty check-types', 'pdm run duty check-dependencies', 'pdm run duty check-api', 'if ${{ github.repository_owner == \'pawamoy-insiders\' }}; then\n  echo \'jobs=[\n    {""os"": ""macos-latest""},\n    {""os"": ""windows-latest""},\n    {""python-version"": ""3.8""},\n    {""python-version"": ""3.9""},\n    {""python-version"": ""3.10""},\n    {""python-version"": ""3.11""}\n  ]\' | tr -d \'[:space:]\' >> $GITHUB_OUTPUT\nelse\n  echo \'jobs=[]\' >> $GITHUB_OUTPUT\nfi\n', 'pdm lock -v --no-cross-platform -G ci-tests', 'pdm install --no-editable -G ci-tests', 'pdm run duty test', 'python -m pip install build', 'python -m build']"
"['export OPENSSL_ROOT_DIR=$(brew --prefix openssl@1.1)\nexport OPENSSL_LIBRARIES=""${OPENSSL_ROOT_DIR}/lib""\necho $OPENSSL_ROOT_DIR\nls $OPENSSL_ROOT_DIR\necho $OPENSSL_LIBRARIES\nls $OPENSSL_LIBRARIES\nrm katrain/KataGo/katago\nrm katrain/KataGo/*.dll\nrm katrain/KataGo/katago.exe\ncd ..\ngit clone --branch stable https://github.com/lightvector/KataGo.git\npushd KataGo/cpp\ncmake . -DUSE_BACKEND=OPENCL -DBUILD_DISTRIBUTED=1\nmake\ncp katago ../../katrain/katrain/KataGo/katago-osx\n', 'curl -L --output ""platypus.zip"" https://github.com/sveinbjornt/Platypus/releases/download/5.3/platypus5.3.zip\nunzip ""platypus.zip""\ngunzip Platypus.app/Contents/Resources/platypus_clt.gz\ngunzip Platypus.app/Contents/Resources/ScriptExec.gz\nmkdir -p /usr/local/bin\nmkdir -p /usr/local/share/platypus\ncp Platypus.app/Contents/Resources/platypus_clt /usr/local/bin/platypus\ncp Platypus.app/Contents/Resources/ScriptExec /usr/local/share/platypus/ScriptExec\ncp -a Platypus.app/Contents/Resources/MainMenu.nib /usr/local/share/platypus/MainMenu.nib\nchmod -R 755 /usr/local/share/platypus\n', 'cd ..\ngit clone https://github.com/kivy/kivy-sdk-packager.git\ncd kivy-sdk-packager/osx\n./create-osx-bundle.sh -n ""KaTrain"" -a ""Sander Land"" -o ""org.katrain.KaTrain"" -i ""../../katrain/katrain/img/icon.ico""\n', 'pushd ../kivy-sdk-packager/osx/build/KaTrain.app/Contents/Resources/venv/bin\nsource activate\npopd\npython -m pip install .\n', 'export KATRAIN_VERSION=`python -c \'from katrain.core.constants import VERSION;print(VERSION)\' `\necho ""Setting version to ${KATRAIN_VERSION}""\ncd ../kivy-sdk-packager/osx/build\npushd KaTrain.app/Contents/Resources/\nln -s ./venv/bin/KaTrain yourapp\npopd\n../fix-bundle-metadata.sh KaTrain.app -n KaTrain -v ""${KATRAIN_VERSION}"" -a ""Sander Land"" -o ""org.katrain.KaTrain"" -i ""../../katrain/katrain/img/icon.ico""\n../cleanup-app.sh KaTrain.app\n../relocate.sh KaTrain.app\n', 'pushd ../kivy-sdk-packager/osx\n./create-osx-dmg.sh build/KaTrain.app KaTrain\npopd\nmkdir osx_app\ncp ../kivy-sdk-packager/osx/KaTrain.dmg osx_app/\n', 'pip3 install -e .\npip3 install pytest wheel twine polib\n', 'pytest -v -s tests', 'python i18n.py', 'python3 setup.py sdist\npython3 setup.py bdist_wheel\n', ""twine upload --verbose dist/* || echo 'File exists'"", 'pip3 install -e .\npip3 install pytest wheel polib\n', 'pytest tests', 'python i18n.py -todo', 'python3 setup.py sdist\npython3 setup.py bdist_wheel\n']"
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'flake8 andriller/\n', 'pytest --cov=andriller tests/\ncoverage html\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\npip install build\n', 'python -m build']"
"['python3 -m pip install --upgrade pip\npip3 install -r requirements.txt\n', 'npm ci\n', 'python ./content/yaml_to_lark_utils.py\n', 'pybabel compile -f -d translations\n', './feed_dev_database.sh', 'build-tools/github/start-debug-server --daemon', 'echo ""You must be Felienne to run this workflow :D"" >&2\nexit 1\n', 'python3 -m pip install --upgrade pip\npip3 install -r requirements.txt\n', 'npm ci\n', 'python ./content/yaml_to_lark_utils.py\n', 'pybabel compile -f -d translations\n', 'build-tools/github/validate --all\n', 'python3 -m pip install --upgrade pip\npip3 install -r requirements.txt\n', 'npm ci\n', 'python ./content/yaml_to_lark_utils.py\n', 'pybabel compile -f -d translations\n', 'build-tools/github/validate --weblate\n', 'build-tools/github/validate --all\n', 'npm ci\n', 'python3 -m pip install --upgrade pip\npip3 install -r requirements.txt\n', 'pybabel compile -f -d translations\n', 'build-tools/heroku/generate-grammars-and-js']"
"['sudo apt-get update\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;\nbash miniconda.sh -b -p $HOME/miniconda\nexport PATH=""$HOME/miniconda/bin:$PATH""\nconda update -q conda\nconda config --add channels conda-forge\nconda config --add channels pytorch\nconda install -y anaconda-client conda-build conda-verify\nconda config --set anaconda_upload no\n', 'export PATH=""$HOME/miniconda/bin:$PATH""\nconda build --quiet --no-test --output-folder conda_build conda.recipe\nls conda_build/noarch/*.tar.bz2 | xargs -I {} anaconda -v -t $CONDA_TOKEN upload -u $CONDA_USERNAME {}\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 .\n', 'python3 -m pip install mypy types-setuptools\n# stop the build if there are Python syntax errors or undefined names\npython3 -m mypy piq/ --allow-redefinition\n', 'python -m pip install --upgrade pip setuptools wheel\npip install torchvision==${{ matrix.torchvision-version }}\npip install -r requirements.txt\npip install pytest \\\n            pytest-cov \\\n            tensorflow \\\n            libsvm \\\n            pybrisque \\\n            ""scikit-image<=0.20.0"" \\\n            pandas \\\n            tqdm\npip install --upgrade scipy\n', 'pytest -x --cov=./piq --cov-report=xml\n']"
"['sudo apt-get install -y jq\npython -m pip install --upgrade pip\nmake requirements npm-install\nmkdir ~/.aws\ntouch ~/.aws/credentials ~/.aws/config\nls ~/.aws/\n', 'make ci\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install --upgrade pip\npip install --quiet tensorflow tensorflow-probability torch jax jaxlib scikit-learn plotly nbformat ipython pylint coverage pytest\npip install .\n', 'coverage run -m pytest tests/commit\n', 'bash <(curl -s https://codecov.io/bash)', 'pylint --rcfile=./demos/.pylintrc demos\npylint --rcfile=./tests/.pylintrc tests\npylint --rcfile=./phi/.pylintrc phi\n', 'python --version\npython -m pip install --upgrade pip\npip install --quiet pdoc3 tensorflow torch jax jaxlib plotly ""nbformat==5.4.0"" ipython notebook ipywidgets\npip install .\npip list\n', 'pdoc --html --output-dir docs --force phi', 'jupyter nbconvert --to html --execute --allow-errors docs/*.ipynb\njupyter nbconvert --to html --output-dir docs/ docs/prerendered/*.ipynb\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'python -m pip install --upgrade pip\npip install flake8 pytest torch==1.8.1 torchvision==0.9.1\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest --ignore=tests/test_unwrapped_parameters.py --ignore=tests/test_backward.py --ignore=tests/test_concat_split.py --ignore=tests/test_serialization.py\n', 'python -m pip install --upgrade pip\npip install flake8 pytest torch==1.12.1 torchvision==0.13.1\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest --ignore=tests/test_unwrapped_parameters.py --ignore=tests/test_backward.py \n', 'python -m pip install --upgrade pip\npip install flake8 pytest torch torchvision\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest --ignore=tests/test_concat_split.py\n']"
"['python -m pip install --upgrade pip\npip install wheel twine\n', 'pip install -r maro/requirements.build.txt\n', 'cython ./maro/backends/backend.pyx ./maro/backends/np_backend.pyx ./maro/backends/raw_backend.pyx ./maro/backends/frame.pyx --cplus -3 -E NODES_MEMORY_LAYOUT=ONE_BLOCK -X embedsignature=True\n', 'python setup.py bdist_wheel\n', 'python setup.py sdist\n', 'mkdir -p dist\ncp wheelhouse/pymaro-*-manylinux*.whl dist\n', 'twine upload --verbose dist/*.whl\n', 'pip install -r ./maro/requirements.build.txt\ncython ./maro/backends/backend.pyx ./maro/backends/np_backend.pyx ./maro/backends/raw_backend.pyx ./maro/backends/frame.pyx --cplus -3 -E NODES_MEMORY_LAYOUT=ONE_BLOCK -X embedsignature=True\ncat ./maro/__misc__.py | grep __version__ | egrep -o [0-9].[0-9].[0-9,a-z]+ | { read version; docker build -f ./docker_files/cpu.playground.df . -t ${{ secrets.DOCKER_HUB_USERNAME }}/maro:cpu -t ${{ secrets.DOCKER_HUB_USERNAME }}/maro:latest -t ${{ secrets.DOCKER_HUB_USERNAME }}/maro:cpu-$version; }\n', 'docker login --username ${{ secrets.DOCKER_HUB_USERNAME }} --password ${{ secrets.DOCKER_HUB_PASSWORD }}\n', 'docker push ${{ secrets.DOCKER_HUB_USERNAME }}/maro\n', 'python -m pip install --upgrade pip\npip install -U -r ./docs/requirements.docs.txt\n', 'pip install -r maro/requirements.build.txt\n', 'cython ./maro/backends/backend.pyx ./maro/backends/np_backend.pyx ./maro/backends/raw_backend.pyx ./maro/backends/frame.pyx --cplus -3 -E NODES_MEMORY_LAYOUT=ONE_BLOCK -X embedsignature=True\n', 'python setup.py build_ext -i\n', 'cd ./docs\nsphinx-apidoc -f -o ./source/apidoc ../maro/\nmake html\n', 'pip install -r maro/requirements.build.txt\n', 'cython ./maro/backends/backend.pyx ./maro/backends/np_backend.pyx ./maro/backends/raw_backend.pyx ./maro/backends/frame.pyx --cplus -3 -E NODES_MEMORY_LAYOUT=ONE_BLOCK -X embedsignature=True\n', 'python setup.py build_ext -i\n', 'pip install torch===1.7.1 torchvision===0.8.2 -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -m pip install --upgrade pip\npip install -r tests/requirements.test.txt\npip install flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=10 --statistics\n', 'coverage run --rcfile=./tests/.coveragerc\ncoverage report --rcfile=./tests/.coveragerc\n', 'coverage run --rcfile=./tests/.coveragerc\ncoverage xml --rcfile=./tests/.coveragerc\n', 'pip install -r maro/requirements.build.txt\npip install -r tests/requirements.test.txt\n', 'cython ./maro/backends/backend.pyx ./maro/backends/np_backend.pyx ./maro/backends/raw_backend.pyx ./maro/backends/frame.pyx -3 -E FRAME_BACKEND=NUMPY,NODES_MEMORY_LAYOUT=ONE_BLOCK -X embedsignature=True\n', 'python setup.py install\nmaro meta deploy\n', 'sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable""\nsudo apt-get update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\n', ""ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa <<< y >/dev/null\n"", 'python -m tests.cli.scripts.generate_config\n', 'python -m unittest -f tests/cli/grass/test_grass_azure.py\n', 'git checkout HEAD^2']"
"['pip install black', 'black --check .', 'black .\ngit config --global user.name \'Khaled Nassar\'\ngit config --global user.email \'knassar702@gmail.com\'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit checkout $GITHUB_HEAD_REF\ngit commit -am ""fixup: Format Python code with Black""\ngit push\n']"
"['git checkout HEAD^2', 'poetry install\n', 'make flake8', 'poetry install\nnpm install prettier\n', 'make checkblack', 'make checkisort', 'make checkprettier', 'poetry install\nsudo apt install unzip\n', 'make test-migration\n', 'poetry install\n', 'make mypy', 'echo ""${{ secrets.DOCKER_PASSWORD }}"" | docker login -u ""${{ secrets.DOCKER_USERNAME }}"" --password-stdin', 'echo ""TAG=${GITHUB_REF/refs\\/tags\\//}"" >> $GITHUB_ENV', 'printf ""    TAG: %s\\n""  ""$TAG""\n', 'echo ${{ steps.buildx.outputs.platforms }}', 'docker buildx build \\\n--cache-from ""type=local,src=/tmp/.buildx-cache"" \\\n--cache-to ""type=local,dest=/tmp/.buildx-cache"" \\\n--platform linux/amd64,linux/arm64 \\\n--tag ${{ secrets.DOCKER_USERNAME }}/lnbits-legend:${TAG} \\\n--output ""type=registry"" ./\n', 'docker buildx build \\\n--cache-from ""type=local,src=/tmp/.buildx-cache"" \\\n--cache-to ""type=local,dest=/tmp/.buildx-cache"" \\\n--platform linux/amd64,linux/arm64 \\\n--tag ${{ secrets.DOCKER_USERNAME }}/lnbits-legend:latest \\\n--output ""type=registry"" ./\n', 'poetry install\n', 'make pylint', 'poetry install\nnpm install\n', 'make pyright', 'docker build -t lnbitsdocker/lnbits-legend .\ngit clone https://github.com/lnbits/legend-regtest-enviroment.git docker\ncd docker\nchmod +x ./tests\n./tests\nsudo chmod -R a+rwx .\n', 'poetry install\n', 'sudo chmod -R a+rwx . && rm -rf ./data && mkdir -p ./data\nmake test-real-wallet\n', 'docker build -t lnbitsdocker/lnbits-legend .\ngit clone https://github.com/lnbits/legend-regtest-enviroment.git docker\ncd docker\nchmod +x ./tests\n./tests\nsudo chmod -R a+rwx .\n', 'poetry install\n', 'sudo chmod -R a+rwx . && rm -rf ./data && mkdir -p ./data\nmake test-real-wallet\n', 'docker build -t lnbitsdocker/lnbits-legend .\ngit clone https://github.com/lnbits/legend-regtest-enviroment.git docker\ncd docker\nchmod +x ./tests\n./tests\nsudo chmod -R a+rwx .\n', 'poetry install\n', 'sudo chmod -R a+rwx . && rm -rf ./data && mkdir -p ./data\nmake test-real-wallet\n', 'docker build -t lnbitsdocker/lnbits-legend .\ngit clone https://github.com/lnbits/legend-regtest-enviroment.git docker\ncd docker\nchmod +x ./tests\n./tests\nsudo chmod -R a+rwx .\ndocker exec lnbits-legend-lnbits-1 /bin/bash -c ""poetry run python tools/create_fake_admin.py""\n', 'poetry install\n', 'sudo chmod -R a+rwx . && rm -rf ./data && mkdir -p ./data\nmake test-real-wallet\n', 'docker build -t lnbitsdocker/lnbits-legend .\ngit clone https://github.com/lnbits/legend-regtest-enviroment.git docker\ncd docker\nchmod +x ./tests\n./tests\nsudo chmod -R a+rwx .\n', 'poetry install\n', 'sudo chmod -R a+rwx . && rm -rf ./data && mkdir -p ./data\nmake test-real-wallet\n', 'poetry install\n', 'make test', 'poetry install\n', 'make test']"
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install . -r test_requirements.txt -r requirements.txt\n', 'pytest | tee pytest-coverage.txt\n', 'pytest | tee pytest-coverage.txt;\nexit_code=${PIPESTATUS[0]};\necho ""::set-output name=exit_code::$exit_code""\n', 'python -m pip install --upgrade pip\npip install -r docs/requirements.txt -r requirements.txt\n', 'sphinx-build -W -b html docs /tmp/_docs_build']"
"['git clone --depth=1 -b dev https://github.com/home-assistant/core.git hass\nrm -rf ./hass/homeassistant/components/unifiprotect\nln -s $GITHUB_WORKSPACE/custom_components/unifiprotect ./hass/homeassistant/components/unifiprotect\n\npython -m pip install --upgrade pip\npip install pyunifiprotect mypy black isort pyupgrade pylint pylint_strict_informational\npip install ./hass\n', 'isort --check-only --quiet custom_components/unifiprotect', 'black --check custom_components/unifiprotect', 'cd ./hass && mypy homeassistant/components/unifiprotect', 'find . ! -path ""./hass/*"" -name ""*.py"" | xargs pyupgrade', 'pylint --rcfile pyproject.toml custom_components/unifiprotect']"
"['python -m pip install --upgrade pip\npip install pipenv\npipenv install\n', 'pipenv install pytest\npipenv run pytest -v']"
"['pip install reuse', 'reuse lint']"
"['python3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel twine\n', 'python3 setup.py sdist bdist_wheel\npython3 -m twine upload dist/*\n', 'python -m pip install --upgrade pip\npip install flake8 pytest torch\npip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-line-length=96 --statistics\n', 'pytest\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel twine\n', 'python3 setup.py sdist bdist_wheel\npython3 -m twine upload dist/*\n']"
""
"['exit 1', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\ndocker buildx ls\ndocker buildx build --push \\\n  --tag benbusby/whoogle-search:latest \\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 .\ndocker buildx build --push \\\n  --tag ghcr.io/benbusby/whoogle-search:latest \\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 .\n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\ndocker buildx ls\ndocker buildx build --push \\\n  --tag benbusby/whoogle-search:${GITHUB_REF#refs/*/v}\\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 .\ndocker buildx build --push \\\n  --tag ghcr.io/benbusby/whoogle-search:${GITHUB_REF#refs/*/v}\\\n  --platform linux/amd64,linux/arm/v7,linux/arm64 .\n', 'docker build --tag whoogle-search:test .\ndocker run --publish 5000:5000 --detach --name whoogle-search-nocompose whoogle-search:test\nsleep 15\ndocker exec whoogle-search-nocompose curl -f http://localhost:5000/healthz || exit 1\n', 'docker rm -f whoogle-search-nocompose\nWHOOGLE_IMAGE=""whoogle-search:test"" docker-compose up --detach\nsleep 15\ndocker exec whoogle-search curl -f http://localhost:5000/healthz || exit 1\n', 'docker build --tag whoogle-search:test .\ndocker run --publish 5000:5000 --detach --name whoogle-search-nocompose whoogle-search:test\nsleep 15\ndocker exec whoogle-search-nocompose curl -f http://localhost:5000/healthz || exit 1\n', 'docker rm -f whoogle-search-nocompose\nWHOOGLE_IMAGE=""whoogle-search:test"" docker-compose up --detach\nsleep 15\ndocker exec whoogle-search curl -f http://localhost:5000/healthz || exit 1\n', 'python -m pip install build setuptools --user', 'echo ""DEV_BUILD=$(date +%s)"" >> $GITHUB_ENV', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'docker build --tag whoogle-search:test .\n', 'curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b .\nchmod +x ./grype\n./grype whoogle-search:test --only-fixed\n', 'pip install --upgrade pip && pip install -r requirements.txt', './run test']"
"['npm install --save-dev @commitlint/{config-conventional,cli} commitlint-plugin-jira-rules commitlint-config-jira', 'echo ""NODE_PATH=$GITHUB_WORKSPACE/node_modules"" >> $GITHUB_ENV', 'git remote add upstream https://github.com/lux-org/lux', 'git fetch upstream', 'npx commitlint --from upstream/master --to $(git log upstream/master..HEAD --pretty=format:""%h"" | tail -1) --verbose', 'python -m pip install --upgrade pip\npip install jupyter-client==6.1.6\npip install wheel\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install sqlalchemy\n\n# pip uninstall -y  lux-widget\n# pip install git+git://github.com/lux-org/lux-widget.git\n# # Temporary Fix (#372)\n# cd /opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/luxwidget/\n# mkdir labextension\n# cd labextension\n# wget https://raw.githubusercontent.com/lux-org/lux-widget/master/luxwidget/nbextension/package.json\n', 'python lux/data/upload_car_data.py\npython lux/data/upload_aug_test_data.py\npython lux/data/upload_airbnb_nyc_data.py\n', 'pytest --cov-report term --cov=lux tests/ tests_sql/\nbash <(curl -s https://codecov.io/bash)\n']"
"['set -x\n# $GITHUB_REF is in format `refs/heads/<branch_name>`. We fetch it under\n# the name `commit-count` so we can refer to it below.\n# Do an unshallow fetch so we retrieve all commits (this is necessary\n# because ations/checkout@v2 fetches a shallow copy).\ngit fetch origin --unshallow $GITHUB_REF:commit-count\ngit fetch origin main\ndiff=$(git rev-list --count origin/main...commit-count)\n# $GITHUB_REF adds an additional commit to the commit tree, so $diff is\n# one too high when executing this as a Github Action.\nif (( $diff > 6)); then\n  echo ""ERROR! More than 5 commits in PR -- please squash your commits.""\n  url=https://flax.readthedocs.io/en/latest/contributing.html#too-many-commits-in-a-pull-request\n  echo ""See $url for help on how to resolve this.""\n  exit 1\nfi\n', 'pip install -e .[all]\n', 'python -c ""import flax""\n', 'echo ""DATE=$(date +%j)"" >> $GITHUB_OUTPUT', 'if [ -d ""venv"" ]; then rm -rf venv; fi\npython3 -m venv venv\nvenv/bin/python3 -m pip install .[all]\nvenv/bin/python3 -m pip install .[testing]\nvenv/bin/python3 -m pip install tensorflow_datasets[dev]\nvenv/bin/python3 -m pip install -r docs/requirements.txt\n', 'venv/bin/python3 -m pip install -e .[all]\n', 'if [[ ""${{ matrix.test-type }}"" == ""doctest"" ]]; then\n  tests/run_all_tests.sh --no-pytest --no-pytype --no-mypy --use-venv\nelif [[ ""${{ matrix.test-type }}"" == ""pytest"" ]]; then\n  tests/run_all_tests.sh --no-doctest --no-pytype --no-mypy --with-cov --use-venv\nelif [[ ""${{ matrix.test-type }}"" == ""pytype"" ]]; then\n  tests/run_all_tests.sh --no-doctest --no-pytest --no-mypy --use-venv\nelif [[ ""${{ matrix.test-type }}"" == ""mypy"" ]]; then\n  tests/run_all_tests.sh --no-doctest --no-pytest --no-pytype --use-venv\nelse\n  echo ""Unknown test type: ${{ matrix.test-type }}""\n  exit 1\nfi\n', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n   ""state"": ""\'$lowercase_status\'"",\n   ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n   ""description"": ""\'$status\'"",\n   ""context"": ""github-actions/Build""\n   }\'\n', 'python -m pip install --upgrade pip\npip install setuptools build wheel twine\n', 'python -m build\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install pkgmt\n', 'pkgmt check-links --only-404', 'conda create -q -n conda-env python=${{ matrix.python-version }}\n\n# for the tests that depend on git repos\ngit config --global user.email ""someone@example.com""\ngit config --global user.name ""Someone""\n', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# for some reason, conda does not resolve the env correctly and\n# installs and old version of pygraphviz (1.3) which breaks in\n# Python 3.7\nconda install -y ""pygraphviz>=1.6"" -c conda-forge\n\n# Test vanilla installation, make sure core modules are importable\npip install .\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\npython -c \'import ploomber.io\'\n\n# install dev requirements\npip install .[dev]\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# doc tests\nexport PY_IGNORE_IMPORTMISMATCH=1\npytest src/ploomber --doctest-modules --ignore src/ploomber/resources\nunset PY_IGNORE_IMPORTMISMATCH\n\npytest tests/cli/test_cloud_execution.py --cov=ploomber\npytest tests/cli/test_cloud.py --cov=ploomber\npytest tests/cloud --cov=ploomber\npytest tests/telemetry --cov=ploomber\n', 'conda create -q -n conda-env python=${{ matrix.python-version }}\n\n# for the tests that depend on git repos\ngit config --global user.email ""someone@example.com""\ngit config --global user.name ""Someone""\n', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# for some reason, conda does not resolve the env correctly and\n# installs and old version of pygraphviz (1.3) which breaks in\n# Python 3.7\n# To support python 3.10, installing only for other versions.\nVERSION=$(python -c \'import sys; print(""."".join(map(str, sys.version_info[1:2])))\')\nif [ $VERSION!=""10"" ]; then\n  echo ""installing pygraphviz""\n  conda install -y ""pygraphviz>=1.6"" -c conda-forge\nelse\n  echo ""skipping pygraphviz installation""\nfi\n\n# Test vanilla installation, make sure core modules are importable\npip install .\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\npython -c \'import ploomber.io\'\n\n# install dev requirements\npip install .[dev]\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# doc tests\nexport PY_IGNORE_IMPORTMISMATCH=1\npytest src/ploomber --doctest-modules --ignore src/ploomber/resources\nunset PY_IGNORE_IMPORTMISMATCH\n\npytest tests/cli/test_cloud_execution.py --cov=ploomber \npytest tests/cli/test_cloud.py --cov=ploomber\npytest tests/cloud --cov=ploomber\npytest tests/telemetry --cov=ploomber\n', 'echo ""Integration tests... success! ;-)""\n', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\nconda install -y pygraphviz -c conda-forge\n\n# Test vanilla installation, make sure core modules are importable\n# note: I needed to add ""--ignore-installed certifi"" because the\n# installation started failing suddenly\npip install .  --ignore-installed certifi\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\n\n# install dev requirements\npip install .[dev]\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\npytest tests/cli/test_cloud_execution.py\npytest tests/cli/test_cloud.py\npytest tests/cloud\npytest tests/telemetry\n', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# To support python 3.10, installing only for other versions.\nVERSION=$(python -c \'import sys; print(""."".join(map(str, sys.version_info[1:2])))\')\nif [ $VERSION != ""10"" ]; then\n  echo ""installing pygraphviz""\n  conda install -y pygraphviz -c conda-forge\nelse\n  echo ""skipping pygraphviz installation""\nfi\n\n# Test vanilla installation, make sure core modules are importable\n# note: I needed to add ""--ignore-installed certifi"" because the\n# installation started failing suddenly\npip install .  --ignore-installed certifi\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\n\n# install dev requirements\npip install .[dev]\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\npytest tests/cli/test_cloud_execution.py\npytest tests/cli/test_cloud.py\npytest tests/cloud\npytest tests/telemetry\n', 'echo ""Integration tests... success! ;-)""\n', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# there\'s a bug in jupyter, we need to install this first\nconda install pywin32\n\n# install pygraphviz\nconda install -y pygraphviz -c conda-forge\n\n# seems like pygraphviz just doesnt work on windows\n# https://github.com/pygraphviz/pygraphviz/issues/40\n# pip install pygraphviz\n\n# Test vanilla installation, make sure core modules are importable\npip install .\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\n\n# install dev requirements\npip install .[dev]\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\npytest tests/cli/test_cloud_execution.py\npytest tests/cli/test_cloud.py\npytest tests/cloud\npytest tests/telemetry\n', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# there\'s a bug in jupyter, we need to install this first\nconda install pywin32\n\n# install graphviz (pygraphviz not available in conda for win-64,\n# it will be installed from pip)\n# conda install -y graphviz -c conda-forge\n# note: this installation still gives trouble (error when saving png\n# files but it makes pygraphviz importable), we can get rid of this\n# once we update the tests to mock the @requires decorator to make\n# it think that pygraphviz is installed when it\'s not. the tests\n# currently do not call pygraphviz (it\'s mocked), but @requires imports\n# To support python 3.10, installing only for other versions.\nVERSION=$(python -c \'import sys; print(""."".join(map(str, sys.version_info[1:2])))\')\nif [ $VERSION -ne 10 ]; then\n  echo ""installing pygraphviz""\n  conda install -y -c alubbock pygraphviz\n  dot -c\nelse\n  echo ""skipping pygraphviz installation""\nfi\n# seems like pygraphviz just doesnt work on windows\n# https://github.com/pygraphviz/pygraphviz/issues/40\n# pip install pygraphviz\n\n# Test vanilla installation, make sure core modules are importable\npip install .\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\n\n# install dev requirements\npip install .[dev]\n\n# install R and IR kernel\nconda install -y r-base r-irkernel -c conda-forge\necho ""IRkernel::installspec()"" | Rscript -\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\npytest tests/cli/test_cloud_execution.py\npytest tests/cli/test_cloud.py\npytest tests/cloud\npytest tests/telemetry\n', 'echo ""Integration tests... success! ;-)""\n', 'conda create -q -n conda-env python=${{ matrix.python-version }} -c conda-forge\n# for the tests that depend on git repos\ngit config --global user.email ""someone@example.com""\ngit config --global user.name ""Someone""\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# upgrade pip to use the most recent resolver\npip install pip --upgrade\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# for some reason, conda does not resolve the env correctly and\n# installs and old version of pygraphviz (1.3) which breaks in\n# Python 3.7\nconda install -y ""pygraphviz>=1.6"" -c conda-forge\n\n# Test vanilla installation, make sure core modules are importable\npip install .\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\npython -c \'import ploomber.io\'\n\n# install dev requirements\npip install .[dev]\n\n# install IR kernel\necho ""install.packages(\'IRkernel\', repos=\'http://cran.us.r-project.org\'); IRkernel::installspec()"" | Rscript -\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# doc tests\nexport PY_IGNORE_IMPORTMISMATCH=1\npytest src/ploomber --doctest-modules --ignore src/ploomber/resources\nunset PY_IGNORE_IMPORTMISMATCH\n\n# run tests\npip install pytest-timeout\npytest tests --cov=ploomber --ignore=tests/cli/test_cloud_execution.py --ignore=tests/cli/test_cloud.py --ignore=tests/cloud --ignore=tests/telemetry --timeout=120\n', ""python -m pip install --upgrade pip\npip install 'pkgmt[check]'\n"", 'pkgmt check\n', 'brew services start postgresql\necho ""Check PostgreSQL service is running""\ni=10\nCOMMAND=\'pg_isready\'\nwhile [ $i -gt 0 ]; do\n    echo ""Check PostgreSQL service status""\n    eval $COMMAND && break\n    ((i--))\n    if [ $i == 0 ]; then\n        echo ""PostgreSQL service not ready, all attempts exhausted""\n        exit 1\n    fi\n    echo ""PostgreSQL service not ready, wait 10 more sec, attempts left: $i""\n    sleep 10\ndone\n', 'psql --command=""CREATE USER postgres PASSWORD \'postgres\'"" postgres', 'psql --command=""ALTER DATABASE postgres OWNER TO postgres"" postgres', 'PGPASSWORD=postgres pg_isready --dbname postgres --username postgres', '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# upgrade pip to use the most recent resolver\npip install pip --upgrade\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\nconda install -y pygraphviz -c conda-forge\n\n# Test vanilla installation, make sure core modules are importable\n# note: I needed to add ""--ignore-installed certifi"" because the\n# installation started failing suddenly\npip install .  --ignore-installed certifi\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\n\n# install dev requirements\npip install .[dev]\n\n# install IR kernel\necho ""install.packages(\'IRkernel\', repos=\'http://cran.us.r-project.org\'); IRkernel::installspec()"" | Rscript -\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\npip install pytest-timeout\npytest tests --ignore=tests/cli/test_cloud_execution.py --ignore=tests/cli/test_cloud.py --ignore=tests/cloud --ignore=tests/telemetry --timeout=120\n', '$pgService = Get-Service -Name postgresql*\nSet-Service -InputObject $pgService -Status running -StartupType automatic\nStart-Process -FilePath ""$env:PGBIN\\pg_isready"" -Wait -PassThru\n', '& $env:PGBIN\\psql --command  ""ALTER USER postgres PASSWORD \'postgres\'""\n', ""$env:PGPASSWORD = 'postgres'\n& $env:PGBIN\\pg_isready --dbname postgres --username postgres\n"", '\neval ""$(conda shell.bash hook)""\nconda activate conda-env\n\n# make the build fail quickly on linting errors\npip install pkgmt\npkgmt lint\n\n# there\'s a bug in jupyter, we need to install this first\nconda install pywin32\n\n# install pygraphviz\nconda install -y pygraphviz -c conda-forge\n\n# seems like pygraphviz just doesnt work on windows\n# https://github.com/pygraphviz/pygraphviz/issues/40\n# pip install pygraphviz\n\n# Test vanilla installation, make sure core modules are importable\npip install .\npython -c \'import ploomber\'\npython -c \'import ploomber.tasks\'\npython -c \'import ploomber.products\'\npython -c \'import ploomber.clients\'\n\n# install dev requirements\npip install .[dev]\n\n# install R and IR kernel\nconda install -y r-base r-irkernel -c conda-forge\necho ""IRkernel::installspec()"" | Rscript -\n\n# install sample package for tests\npip install --editable tests/assets/test_pkg\n', 'eval ""$(conda shell.bash hook)""\nconda activate conda-env\npip install pytest-timeout\npytest tests --ignore=tests/cli/test_cloud_execution.py --ignore=tests/cli/test_cloud.py --ignore=tests/cloud --ignore=tests/telemetry --timeout=120\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\nsudo apt-get update && DEBIAN_FRONTEND=noninteractive sudo apt-get install -y make\n', 'make\ntwine upload dist/*']"
"['python -m pip install --upgrade pip\npython -m pip install codecov\npip install -r requirements-dev.txt\n', 'make lint\nmake checkbuild\n', 'make cov\ncodecov\n', 'python -m pip install -U pip wheel twine', 'python setup.py sdist bdist_wheel', 'twine upload dist/*\n']"
"['sudo apt update\nsudo apt-get -y install ffmpeg\npip install -U pip\npip install -e .[data_preparation,test] --use-deprecated=legacy-resolver\n', 'pytest', 'pylint ddsp', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
"['pip install poetry', 'poetry install', 'poetry run make test', 'echo ""C:\\Program Files (x86)\\WiX Toolset v3.11\\bin"" >> $GITHUB_PATH', 'poetry run .\\install\\windows\\build-app.bat', 'pip install poetry', 'poetry install', 'poetry run make test', './dev_scripts/env.py --distro ${{ env.distro }} \\\n    --version ${{ env.version }} \\\n    build-dev\n', './install/linux/build-image.sh', './dev_scripts/env.py --distro ${{ env.distro }} \\\n    --version ${{ env.version }} \\\n    run --dev ./dangerzone/install/linux/build-deb.py\n', './dev_scripts/env.py --distro ${{ matrix.distro }} \\\n    --version ${{ matrix.version }} \\\n    build\n', '# Create a Podman config specifically for Bullseye (see #388).\nmkdir bullseye_fix\ncd bullseye_fix\ncat > containers.conf <<EOF\n[engine]\ncgroup_manager=""cgroupfs""\nevents_logger=""file""\nEOF\n\n# Copy the Podman config into the container image we created for the\n# Dangerzone environment.\ncat > Dockerfile.bullseye <<EOF\nFROM dangerzone.rocks/debian:bullseye-backports\nRUN mkdir -p /home/user/.config/containers\nCOPY containers.conf /home/user/.config/containers/\nEOF\n\n# Create a new image from the Dangerzone environment and re-tag it.\npodman build -t dangerzone.rocks/debian:bullseye-backports \\\n    -f Dockerfile.bullseye .\n', './dev_scripts/env.py --distro ${{ matrix.distro }} \\\n    --version ${{ matrix.version }} \\\n    run dangerzone-cli dangerzone/tests/test_docs/sample-pdf.pdf\n', 'docker build container --tag dangerzone.rocks/dangerzone:latest', 'cat ${{ steps.scan_container.outputs.sarif }}', 'cat ${{ steps.scan_app.outputs.sarif }}', ""VERSION=$(curl https://api.github.com/repos/freedomofpress/dangerzone/releases/latest | jq -r '.tag_name')\nwget https://github.com/freedomofpress/dangerzone/releases/download/${VERSION}/container.tar.gz\n"", 'docker load -i container.tar.gz', 'cat ${{ steps.scan_container.outputs.sarif }}', ""VERSION=$(curl https://api.github.com/repos/freedomofpress/dangerzone/releases/latest | jq -r '.tag_name')\ngit checkout $VERSION\n"", 'cat ${{ steps.scan_app.outputs.sarif }}']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/']"
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m tests.main -a']"
"['python -m pip install --upgrade pip\npip install "".[dev]"" wheel\n', 'inv assets', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install "".[test]""\n', 'inv cover qa', 'python -m pip install --upgrade pip\npip install -e ""base[dev]""\n', 'pip install -e ""base[ci]""', 'cd base\ninv benchmark --max-time 4 --save\nmv .benchmarks ../ref/\n', 'pip install -e ""ref[ci]""', 'cd ref\ninv benchmark --max-time 4 --compare\n']"
"['pip install -r docs/requirements.txt', 'pip install .[all]', 'python docs/_scripts/gen_envs_mds.py', 'python docs/_scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/_scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'python -m pip install --upgrade setuptools wheel', 'python setup.py sdist bdist_wheel', 'pip install -r docs/requirements.txt', 'pip install .[all]', 'python docs/_scripts/gen_envs_mds.py', 'python docs/_scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/_scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'pip install -r docs/requirements.txt', 'pip install .[all]', 'python docs/_scripts/gen_envs_mds.py', 'python docs/_scripts/gen_envs_display.py', 'sphinx-build -b dirhtml -v docs _build', 'mv _build/404/index.html _build/404.html', 'python docs/_scripts/move_404.py _build/404.html', 'rm -r _build/.doctrees', 'sudo apt-get install python3-opengl xvfb\npip install -e .[all]\npip install -e .[testing]\nAutoROM -v\n', 'python setup.py sdist\npip install dist/*.tar.gz\n', 'xvfb-run -s ""-screen 0 1024x768x24"" pytest -v --cov=pettingzoo --cov-report term\n', 'sudo apt-get install python3-opengl xvfb\ncd tutorials/${{ matrix.tutorial }}\npip install -r requirements.txt\npip uninstall -y pettingzoo\npip install -e ../..\nfor f in *.py; do xvfb-run -a -s ""-screen 0 1024x768x24"" python ""$f""; done\n', 'pip install -e .[all]\npip install -e .[testing]\nAutoROM -v\n', 'pytest ./test/pytest_runner_test.py\npytest ./test/all_parameter_combs_test.py\n', 'pip install pre-commit', 'pre-commit --version', 'pre-commit install', 'pre-commit run --all-files']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install twine setuptools wheel --upgrade\n', 'python setup.py bdist_wheel\ntwine check dist/*\ntwine upload dist/*\n']"
""
"['pip install curlylint', ""curlylint --rule 'aria_role: true' \\\n  --rule 'django_forms_rendering: true' \\\n  --rule 'html_has_lang: true' \\\n  --rule 'image_alt: true' \\\n  --rule 'meta_viewport: true' \\\n  --rule 'no_autofocus: true' \\\n  --rule 'tabindex_no_positive: true' \\\n  --exclude '_modal.html|create_status/layout.html|reading_modals/layout.html' \\\n  bookwyrm/templates\n"", 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pytest -n 3\n', 'npm install stylelint stylelint-config-recommended stylelint-config-standard stylelint-order eslint', 'npx eslint bookwyrm/static \\\n  --ext .js,.jsx,.ts,.tsx\n', 'npm install prettier', 'npx prettier --check bookwyrm/static/js/*.js', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pylint bookwyrm/\n']"
"['python -m pip install --upgrade pip\npip install flake8\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt -r requirements_tests.txt\npip install .\n', 'python tests/download_images.py\n', 'python -c ""import pymatting""\npytest\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'make docs-build-ci\n', 'make ci\n', 'codecov --token=${{ secrets.CODECOV_TOKEN }}\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'make ci\n', 'make docs-build-ci\n', 'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python\n', 'source $HOME/.poetry/env && poetry build\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'make docs-build-ci\n', 'make ci\n', 'codecov --token=${{ secrets.CODECOV_TOKEN }}\n']"
"['python -m pip install --upgrade pip\npip install --upgrade setuptools\npip install .\n', 'pip install -U flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --show-source --statistics\n', 'pip install pytest\npytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/* --verbose\n']"
[]
""
"['curl -L https://github.com/iwalton3/mpv-winbuild-cmake/releases/download/v20230304-git-362256e/mpv-dev-x86_64-20230304-git-362256e.7z > mpv.7z\n7z x mpv.7z\nmv libmpv-2.dll mpv-2.dll\npip install wheel\npip install git+https://github.com/pythonnet/pythonnet@438bcdd3f0b2e2ee358b407cfc96e4385e5783fc\npip install .[all] pywebview==3.4 pywin32==304\n./gen_pkg.sh --skip-build\npython ./get_pywebview_natives.py\n', './gen_pkg.sh --get-pyinstaller; cd pyinstaller/bootloader; python ./waf distclean all; cd ..; pip install .\n', '.\\build-win.bat\n', './artifacts.sh standard\n', 'curl -L https://github.com/iwalton3/mpv-winbuild-cmake/releases/download/v20230304-git-362256e/mpv-dev-i686-20230304-git-362256e.7z > mpv.7z\n7z x mpv.7z\nmv libmpv-2.dll mpv-2.dll\npip install wheel\npip install git+https://github.com/pythonnet/pythonnet@438bcdd3f0b2e2ee358b407cfc96e4385e5783fc\npip install .[all] pywebview==3.4 pywin32==304\n./gen_pkg.sh --skip-build\npython ./get_pywebview_natives.py\n', './gen_pkg.sh --get-pyinstaller; cd pyinstaller/bootloader; python ./waf distclean all; cd ..; pip install .\n', '.\\build-win-32.bat\n', './artifacts.sh legacy\n']"
"[""sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys FA8E1301F4D3932C\nsudo add-apt-repository 'deb http://ppa.launchpad.net/sri-csl/formal-methods/ubuntu bionic main'\nsudo apt-get update\nsudo apt-get install yices2\npip install coverage codecov\npip install yowasp-yosys==0.20.* # last version compatible with Python 3.7\npip install -e .[builtin-yosys]\npip install -r docs/requirements.txt\n"", 'export AMARANTH_USE_YOSYS=builtin YOSYS=yowasp-yosys SBY=yowasp-sby SMTBMC=yowasp-yosys-smtbmc\nexport PYTHONWARNINGS=error\npython -m coverage run -m unittest discover -t . -s tests\nunset PYTHONWARNINGS\nsphinx-build -b doctest docs/ docs/_build\ncodecov\n', 'git fetch --tags https://github.com/amaranth-lang/amaranth.git\n', 'pip install -r docs/requirements.txt\npip install .\n', 'sphinx-build docs docs/_build\n', 'touch pages/.nojekyll\n']"
"['./scripts/install_deps.sh', './scripts/build.sh', './scripts/ci_test.sh']"
"['npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'pip install -r requirements.txt', 'black --check scripts tests', 'flake8 scripts tests', 'isort --check-only --diff --recursive scripts tests', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/pools/common/unitary --pool ${{ env.pool }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'pytest tests/pools/common/integration --pool ${{ env.pool }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/zaps/common --pool ${{ env.pool }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/pools/common/unitary --pool ${{ env.pool }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'pytest tests/pools/common/integration --pool ${{ env.pool }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/zaps/common --pool ${{ env.pool }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/token', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}', 'npm install', 'pip install wheel\npip install -r requirements.txt\n', 'brownie test tests/${{ matrix.target }} --pool ${{ env.pool }} --${{ matrix.type }}']"
[]
"['git config --local user.email ""agvrpw-builder[bot]@gh.xdavidhu.me""\ngit config --local user.name ""agvrpw-builder[bot]""\ngit commit -m ""Build writeups"" -a\n']"
"['cd ./deploy/kubernetes_operator/\ngo build ./cmd/apiserver/main.go\n', 'cd ./deploy/kubernetes_operator/\ngo build ./cmd/operator/main.go\n', 'sudo apt-get -y update\nsudo apt-get -y install libgmp-dev libmpc-dev libmpfr-dev\npip install --upgrade pip\npip install -r requirements.txt\n', 'bash ./ci/ci_test.sh', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""', 'echo ""::set-output name=sha_short::$(git rev-parse --short HEAD)""']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\n', 'inv fmt test']"
"['python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --upgrade pip\npip install poetry\n', 'poetry install\n# Set the `CODEQL-PYTHON` environment variable to the Python executable\n# that includes the dependencies\necho ""CODEQL_PYTHON=$(poetry run which python)"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke docs', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'DEBIAN_FRONTEND=noninteractive sudo apt-get update -y\nDEBIAN_FRONTEND=noninteractive sudo apt-get install xclip -y\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'export XDG_SESSION_TYPE=x11\nxvfb-run --server-args=""-screen 0 1280x720x24"" invoke code.test\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test -a', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'python -m pip install --force-reinstall pip==${{matrix.pip-version}} wheel', 'python -m pip install rpaframework rpaframework-recognition robotframework-browser rpaframework-aws rpaframework-google rpaframework-assistant rpaframework-openai rpaframework-recognition rpaframework-windows\n', 'python -m pip check\npython -c ""import RPA""\n# TODO: run whole test suite here\n', '# show freeze file in CI log\npython -m pip freeze\npython -m pip freeze > requirements.txt\n', 'python -m pip install pip-audit==2.4.14\npython -m pip_audit --no-deps -r requirements.txt\n', 'python -m pip install --upgrade pip', 'pip install poetry', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info[:3]))"")\n', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n', 'echo  The package is ${{ github.event.inputs.package }} and version is ${{ github.event.inputs.version }}', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke code.lint -e', 'invoke code.test-python', 'python -m pip install --upgrade pip', 'pip install poetry', 'pip install -U ""invoke>=1.6.0,<2.0.0""', 'invoke install-invocations', 'invoke install', 'invoke install.setup-poetry --devpi-url ${{ secrets.devpiUrl }} --username ci --password ${{ secrets.devpiPassword }}\ninvoke build.publish --ci --version=prerelease --yes-to-all\n']"
"['/runner/change_schema.sh wizard', 'dbt deps', 'dbt compile --target-path .', 'aws s3 cp s3://manifest-spellbook/manifest.json previous_manifest.json', 'aws s3 cp previous_manifest.json s3://manifest-spellbook/previous_manifest.json', 'echo ""GIT_SHA=$(echo ${{ github.sha }} | tr - _ | cut -c1-7)"" >> $GITHUB_ENV\n', 'aws s3 cp previous_manifest.json s3://manifest-spellbook/manifest_$GIT_SHA.json', 'aws s3 cp manifest.json s3://manifest-spellbook/manifest.json', 'echo ""FILES=$(echo ${{ steps.abc.outputs.added_modified }})"" >> $GITHUB_ENV\n', 'dbt deps', 'dbt compile', 'test=$(dbt --quiet --no-print ls --select config.schema:no_schema --output path)\n[[ -z ""$test"" ]] && { echo ""Success: All models have a custom schema""; exit 0; } || { echo ""Found models without custom schema:""; echo ""$test""; exit 1; }\n', 'echo ""GIT_SHA=$(echo ${{ github.sha }} | tr - _ | cut -c1-8)"" >> $GITHUB_ENV\n', '/runner/change_schema.sh git_$GIT_SHA', 'aws s3 cp s3://manifest-spellbook/manifest.json manifest.json', 'dbt deps', 'dbt compile', 'dbt seed --select state:modified --state .', 'dbt -x run --select state:modified --exclude tag:prod_exclude --defer --state .', 'dbt test --select state:new state:modified --exclude tag:prod_exclude --defer --state .', 'echo ""INC_MODEL_COUNT=$(echo dbt ls --select state:modified,config.materialized:incremental --state . --resource-type model  | wc -l)"" >> $GITHUB_ENV\n', 'dbt run --select state:modified,config.materialized:incremental --exclude tag:prod_exclude --defer --state .', 'dbt test --select state:modified,config.materialized:incremental --exclude tag:prod_exclude --defer --state .', '/runner/dunesql_check.py --schema test_schema --pr_schema git_$GIT_SHA', 'git fetch origin ${{ github.event.pull_request.base.sha }}', 'git fetch origin ${{ github.event.pull_request.head.sha }}', ""git diff ${{ github.event.pull_request.base.sha}}..${{ github.event.pull_request.head.sha }} models/prices/**/*.sql | grep '^\\+ ' > scripts/new_lines.txt\n"", 'python scripts/check_tokens.py --file_name scripts/new_lines.txt', 'pip install pipenv\n', 'pipenv install\n', 'pipenv run pytest test_token_checker.py', 'gh api repos/duneanalytics/spellbook-deploy/dispatches \\\n    --raw-field event_type=spellbook-updated\n']"
""
"['git checkout HEAD^2', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'cp build/suzieq-cfg-travis.yml suzieq-cfg.yml', 'poetry run pip install pytest-split', 'mkdir test_durations', 'poetry run pytest --splits 4 --group ${{ matrix.group }} --store-durations --durations-path test_durations/.test_durations_${{ matrix.python-version }}', 'python3 tests/utilities/merge_test_durations_ci.py', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run ${{ matrix.linter }} suzieq/', 'pip install pre-commit', 'pre-commit run --all-files --show-diff-on-failure']"
""
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install flake8\n', '# Stop the build if there are style issues, Python syntax errors, or undefined names\nflake8 . --count --show-source --statistics\n', 'python -m pip install --upgrade pip\npip install wheel\npip install torch==${TORCH} torchvision --index-url https://download.pytorch.org/whl/cpu\npip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\npip install .\n\npip install plotly\n', 'pip install pytest pytest-cov\npip install coveralls\n', 'coverage run --source=e3nn -m pytest --doctest-modules --ignore=docs/ .\n', 'COVERALLS_REPO_TOKEN=${{ secrets.COVERALLS_TOKEN }} coveralls\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\nflake8 .\n', 'echo ""::set-output name=date::$(/bin/date -u ""+%Y%m%d%H%M%S"")""\n', 'ls -la ${{ github.workspace }}/hub-mirror-cache\n']"
""
"['python -m pip install --upgrade pip\npip install -e "".[test]""\n', 'pytest', 'pip install build', 'python -m build']"
"['git checkout HEAD^2', 'pip install poetry', 'poetry install\npoetry run python -m unittest -v tests/*.py\n']"
""
"['python -m pip install --upgrade pip\npip install pytest\npip install .\n', 'cd tests\npytest unit_tests.py\n', 'python -m pip install --upgrade pip\npip install pylint\npip install black\npip install .\n', 'pylint --fail-under=10 deepface/\n']"
"['pwd && npm install && npm run build', 'pwd && npm install && npm run build', 'npm install\n', 'npm run lint\n', 'export DISPATCH_LIGHT_BUILD=1\npython -m pip install --upgrade pip\npip install psycopg[binary]\npip install -e "".[dev]""\n', 'npm ci -D --prefix src/dispatch/static/dispatch\nnpm install -D @playwright/test\n', 'npx playwright install --with-deps', 'dispatch database restore --dump-file data/dispatch-sample-data.dump && dispatch database upgrade', 'npx playwright test', 'export DISPATCH_LIGHT_BUILD=1\npython -m pip install --upgrade pip\npip install -e "".[dev]""\n', 'ruff check src tests\n', 'pip install pytest-cov\npytest --junitxml=junit/test-results.xml --cov=com --cov-report=xml --cov-report=html\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 pywebio --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 pywebio --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics', 'npm install\ngulp\ncp dist/pywebio.min.js ../pywebio/html/js\n', 'pip3 install twine\npython3 setup.py sdist\ntwine upload --username ""__token__"" --disable-progress-bar --verbose dist/*\n', 'echo ""RELEASE_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', 'git config --global user.email ""wang0.618@qq.com""\ngit config --global user.name ""${{ github.actor }}""\ngit config --global credential.helper \'!f() { sleep 1; echo ""username=${{ github.actor }}""; echo ""password=${GH_TOKEN}""; }; f\'\n\ngit clone https://github.com/wang0618/pywebio-assets.git\nrm -rf pywebio-assets/*\ncp -r pywebio/html/* pywebio-assets\n\ncd pywebio-assets\ngit add .\ngit commit -m ""Build from https://github.com/wang0618/PyWebIO/commit/$GITHUB_SHA""\ngit tag $RELEASE_VERSION\n\ngit push -u origin --tags\ngit push -u origin\n', 'npm install\nDEV=1 gulp\ncp dist/pywebio.min.* ../pywebio/html/js\n', 'pip3 install -e "".[all]""\ncd docs && CODE_EXPORT_PATH=../demos/doc_demos make clean text\n', 'python3 tools/build_dev_version.py', 'git fetch --unshallow origin\ngit remote add aliyun ""https://code.aliyun.com/wang0618/pywebio.git""\ngit config credential.helper \'!f() { sleep 1; echo ""username=${ALIYUN_GIT_USER}""; echo ""password=${ALIYUN_GIT_PASSWORD}""; }; f\'\nrm .gitignore\ngit add pywebio/__version__.py\ngit add pywebio/html/js\ngit add demos/doc_demos\ngit config user.email ""${ALIYUN_GIT_USER}""\ngit config user.name ""${ALIYUN_GIT_USER}""\ngit commit --amend --no-edit\ngit push -f -u aliyun --tags || exit 0\ngit push -f -u aliyun || exit 0\n', 'npm install\ngulp\ncp dist/pywebio.min.* ../pywebio/html/js\n', 'npm install -D @percy/agent', 'pip3 install "".[all]""', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'echo ""PERCY_TOKEN=${{ secrets.PERCY_TOKEN }}"" >> $GITHUB_ENV', 'bash <(curl -s https://codecov.io/bash)']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-jax.txt\npip install -r requirements-test.txt\npip install .\npip install pytest pytest-xdist\n', 'pip freeze\n', 'pytest -n auto haiku --ignore=haiku/_src/integration/\n', 'sudo apt install -y pandoc\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-jax.txt\npip install -r requirements-test.txt\npip install -r docs/requirements.txt\npip install .\n', 'pip freeze\n', 'cd docs\nmake coverage_check\n', 'cd docs\nmake doctest\n', 'cd docs\nmake html\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-jax.txt\npip install -r requirements-test.txt\npip install .\npip install pytest pytest-xdist\n', 'pip freeze\n', '# Isolate the float64 test because it needs to set a flag at start-up,\n# so no other tests can be run before it.\npytest haiku/_src/integration/float64_test.py\npytest -n auto haiku/_src/integration/ --ignore=haiku/_src/integration/float64_test.py\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pacman -Syyu --noconfirm\n', 'pacman -S --noconfirm base git tar which zip libffi\n', 'pacman -S --noconfirm python python-pipenv python-setuptools\n', 'pacman -S --noconfirm fontforge', 'pipenv --python /bin/python install; pipenv run python build.py fonts\n', 'mv build CozetteFonts; cp ./LICENSE CozetteFonts/LICENSE; zip -r CozetteFonts.zip CozetteFonts\n']"
"['python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install -U poetry==1.3.2\npoetry config virtualenvs.create false\n', 'python -m poetry install\n', './dev/test.sh', 'sudo apt install pandoc\npython -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install -U poetry==1.3.2\npoetry config virtualenvs.create false\n', 'python -m poetry install --extras ""simulation""', './dev/build-docs.sh', 'sudo apt install pandoc\npython -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install -U poetry==1.3.2\npoetry config virtualenvs.create false\n', 'python -m poetry install --extras ""simulation""', './dev/deploy-docs.sh', 'python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install poetry==1.3.2\npoetry config virtualenvs.create false\n', 'cd examples/quickstart_pytorch\npython -m poetry install\n', 'cd examples/quickstart_pytorch\npython -c ""from torchvision.datasets import CIFAR10; CIFAR10(\\""./dataset\\"", download=True)""\n', 'cd examples/quickstart_pytorch\n\npython server.py &\nsleep 3\n\npython client.py 1 &\nsleep 3\n\nif [[ $(ps aux | grep ""[p]ython client.py 1"" | awk \'{ print $2 }\') ]];\n  then echo ""Client process 1 started correctly"";\n  else echo ""Client process 1 crashed"" && exit 1;\nfi\n\npython client.py 2 &\nsleep 3\n\nif [[ $(ps aux | grep ""[p]ython client.py 2"" | awk \'{ print $2 }\') ]];\n  then echo ""Client process 2 started correctly"";\n  else echo ""Client process 2 crashed"" && exit 1;\nfi\n\nif [[ $(ps aux | grep ""[p]ython server.py"" | awk \'{ print $2 }\') ]];\n  then echo ""Server process started correctly"";\n  else echo ""Server process crashed"" && exit 1;\nfi\n\nkillall python\n', 'python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install poetry==1.3.2\npoetry config virtualenvs.create false\n', 'cd examples/quickstart_tensorflow\npython -m poetry install\n', 'python -c ""import tensorflow as tf; tf.keras.datasets.cifar10.load_data()""\n', 'cd examples/quickstart_tensorflow\n\npython server.py &\nsleep 3\n\npython client.py 1 &\nsleep 3\n\nif [[ $(ps aux | grep ""[p]ython client.py 1"" | awk \'{ print $2 }\') ]];\n  then echo ""Client process 1 started correctly"";\n  else echo ""Client process 1 crashed"" && exit 1;\nfi\n\npython client.py 2 &\nsleep 3\n\nif [[ $(ps aux | grep ""[p]ython client.py 2"" | awk \'{ print $2 }\') ]];\n  then echo ""Client process 2 started correctly"";\n  else echo ""Client process 2 crashed"" && exit 1;\nfi\n\nif [[ $(ps aux | grep ""[p]ython server.py"" | awk \'{ print $2 }\') ]];\n  then echo ""Server process started correctly"";\n  else echo ""Server process crashed"" && exit 1;\nfi\n\nkillall python\n', 'python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install poetry==1.3.2\npoetry config virtualenvs.create false\n', 'cd examples/pytorch_from_centralized_to_federated\npython -m poetry install\n', 'cd examples/pytorch_from_centralized_to_federated\npython -c ""from torchvision.datasets import CIFAR10; CIFAR10(\\""./dataset\\"", download=True)""\n', 'python3 -c ""import flwr""\n', 'cd examples/pytorch_from_centralized_to_federated\n\npython3 server.py &\nsleep 3\n\nif [[ $(ps aux | grep ""python server.py"" | awk \'{ print $2 }\') ]];\n  then echo ""Server process started correctly"";\n  else echo ""Server process crashed"" && exit 1;\nfi\n\npython3 client.py 1 &\nsleep 3\n\nif [[ $(ps aux | grep ""python client.py 1"" | awk \'{ print $2 }\') ]];\n  then echo ""Client process 1 started correctly"";\n  else echo ""Client process 1 crashed"" && exit 1;\nfi\n\npython3 client.py 2 &\nsleep 3\n\nif [[ $(ps aux | grep ""python client.py 2"" | awk \'{ print $2 }\') ]];\n  then echo ""Client process 2 started correctly"";\n  else echo ""Client process 2 crashed"" && exit 1;\nfi\n', 'python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install -U poetry==1.3.2\npoetry config virtualenvs.create false\n', 'python -m poetry install --extras ""simulation""', './dev/check-protos.sh', './dev/test.sh', 'python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install -U poetry==1.3.2\n', './dev/publish-nightly.sh\n', 'python -m pip install -U pip==22.3.1\npython -m pip install -U setuptools==65.6.3\npython -m pip install -U poetry==1.3.2\npoetry config virtualenvs.create false\n', 'python -m poetry install\n', './dev/test-tool.sh', ""arch -x86_64 xcodebuild test -scheme flwr -destination 'platform=iOS Simulator,name=iPhone 14 Pro Max,OS=16.2'""]"
"['pip install --upgrade pip setuptools; pip install -r requirements.txt;', './scripts/lint_all.sh', 'pip install --upgrade pip setuptools; pip install -r requirements.txt; sudo apt-get install -y clang-format-6.0', './scripts/format_check.sh', './scripts/ci_install.sh', 'echo ""Y\\n"" | ./configure.sh', './scripts/build_pip_package_test.sh', './scripts/run_example.sh', './scripts/ci_install.sh', 'echo ""Y\\n"" | ./configure.sh', './scripts/test_all.sh', './scripts/ci_install.sh', 'echo ""Y\\n"" | ./configure.sh', './scripts/msan_test.sh', 'pip install --upgrade pip seaborn==0.10.0', './scripts/ci_install.sh', 'echo ""Y\\n"" | ./configure.sh', './scripts/build_pip_package_test.sh', './scripts/ci_validate_tutorials.sh', './scripts/ci_install.sh', 'echo ""Y\\n"" | ./configure.sh', 'pip install -U cirq --pre', './scripts/test_all.sh']"
"['echo ${{ steps.docker_build.outputs.digest }}', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'pip install bitsandbytes==0.37.0\n', 'pip install .\n', 'cd benchmarks\npython benchmark_throughput.py --preset minimalistic\npython benchmark_tensor_compression.py\npython benchmark_dht.py\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'pip install bitsandbytes==0.37.0\n', 'pip install .\n', 'cd tests\nexport HIVEMIND_MEMORY_SHARING_STRATEGY=file_descriptor\npytest --durations=0 --durations-min=1.0 -v\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'pip install . --global-option=build_py --global-option=""--buildgo"" --no-use-pep517\n', 'cd tests\nexport HIVEMIND_MEMORY_SHARING_STRATEGY=file_descriptor\npytest -k ""p2p"" -v\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'pip install bitsandbytes==0.37.0\n', 'pip install -e . --no-use-pep517\n', 'export HIVEMIND_MEMORY_SHARING_STRATEGY=file_descriptor\npytest --cov hivemind -v tests\n']"
"['pip install --constraint=.github/workflows/constraints.txt pip\npip install --constraint=.github/workflows/constraints.txt nox\n', 'nox --force-color --session=docs', 'nox --force-color --session=linkcheck', 'pip install --constraint=.github/workflows/constraints.txt pip\npip install --constraint=.github/workflows/constraints.txt pre-commit\n', 'import hashlib\nimport sys\n\npython = ""py{}.{}"".format(*sys.version_info[:2])\npayload = sys.version.encode() + sys.executable.encode()\ndigest = hashlib.sha256(payload).hexdigest()\nresult = ""${{ runner.os }}-{}-{}-pre-commit"".format(python, digest)\n\nprint(""::set-output name=result::{}"".format(result))\n', 'pre-commit run --all-files --show-diff-on-failure --color=always', 'pip install --constraint=.github/workflows/constraints.txt pip\npipx install --pip-args=--constraint=.github/workflows/constraints.txt cookiecutter\npipx install --pip-args=--constraint=.github/workflows/constraints.txt nox\npipx inject --pip-args=--constraint=.github/workflows/constraints.txt nox nox-poetry\npipx install --pip-args=--constraint=.github/workflows/constraints.txt poetry\n', 'cookiecutter --no-input cookiecutter-hypermodern-python', 'git init\ngit config --local user.name ""GitHub Action""\ngit config --local user.email ""action@github.com""\ngit add .\ngit commit --message=""Initial import""\n', 'git init\ngit config --local user.name ""GitHub Action""\ngit config --local user.email ""action@github.com""\n# https://github.com/cookiecutter/cookiecutter/issues/405\n$ErrorActionPreference = ""Continue""\ngit add .\n$ErrorActionPreference = ""Stop""\ngit add --renormalize .\ngit commit --message=""Initial import""\n', 'import hashlib\nimport sys\n\npython = ""py{}.{}"".format(*sys.version_info[:2])\npayload = sys.version.encode() + sys.executable.encode()\ndigest = hashlib.sha256(payload).hexdigest()\nresult = ""${{ runner.os }}-{}-{}-pre-commit"".format(python, digest)\n\nprint(""::set-output name=result::{}"".format(result))\n', 'nox --force-color --python=${{ matrix.python-version }}', 'poetry install --ansi', 'pip install --constraint=.github/workflows/constraints.txt cutty\ncutty --version\n', 'cutty import --non-interactive --cwd=${PROJECT} --revision=${GITHUB_SHA}\n', 'if ! git -C ${TEMPLATE} show --no-patch --format=%B ${GITHUB_SHA} | grep -q ^Retrocookie-Original-Commit:\nthen\n  git -C $PROJECT push https://$GITHUB_ACTOR:$GITHUB_TOKEN@github.com/cjolowicz/$PROJECT.git HEAD:main\nfi\n']"
"['gh pr edit $PRNUM --add-label ""PR:Ready-to-Review""\ngh pr edit $PRNUM --remove-label ""PR:No-Issue""\n', 'gh pr comment $PRNUM --body ""PR is not linked to any issue, please make the corresponding changes in the body. The issue should look like [this](https://ibb.co/Bg9x53w). For help follow this [link](https://github.com/HarshCasper/Rotten-Scripts/blob/master/CONTRIBUTING.md)""\ngh pr edit $PRNUM --add-label ""PR:No-Issue""\n', 'echo ${{ github.event.label.name }}\nSCORE=${{ github.event.label.name }}\necho $SCORE\ncurl --data ""score=$SCORE"" --request POST https://woc-backend.gigalixirapp.com/updateLeaderBoard/\n']"
""
"['pip install tox', 'tox -e doc8', 'tox -e readme', 'sudo apt-get install graphviz\ntox -e docs-test\n', 'pip install tox', 'tox -e py', 'tox -e integration', 'tox -e doctests', 'pip install tox', 'tox -e pyroma', 'tox -e docstr-coverage', 'tox -e mypy', 'tox -e manifest', 'tox -e flake8']"
"['bash test.sh', 'RELEASE_VER=${GITHUB_REF#refs/*/}\nPACKAGE_VER=""v`python setup.py --version`""\nif [ $RELEASE_VER != $PACKAGE_VER ]\nthen\n  echo ""package ver. ($PACKAGE_VER) != release ver. ($RELEASE_VER)""; exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install -e .\npip install coverage\ncurl -o tests/test.mp3 https://www.kozco.com/tech/piano2-CoolEdit.mp3\ncat <<< ""$HEADERS_AUTH"" > tests/headers_auth.json\ncat <<< ""$TEST_CFG"" > tests/test.cfg\ncoverage run\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install . sphinx sphinx-rtd-theme\n', 'cd docs\nmake html', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['git fetch origin master --unshallow\ngit symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master\n', 'sudo swapoff -a\nsudo rm -f /swapfile\nsudo apt clean\ndocker rmi $(docker images -a -q)\ndf -h\n', 'make install-dependencies\n', 'PATH=.venv/bin/:$PATH PYTHONPATH=. python3 .github/workflows/build_and_test_run_fuzzer_benchmarks.py ${{ matrix.benchmark }}\n', 'git fetch origin master --unshallow\ngit symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master\n', 'make install-dependencies\n', 'FUZZBENCH_TEST_INTEGRATION=1 make presubmit\n']"
"['brew install libomp', 'pip3 install tox==3.24.4\npip3 install tox-gh-actions==2.8.1\n', 'tox\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip3 install setuptools wheel twine\nchmod +x ./publish.sh ./build.sh\n./publish.sh\n', 'sudo python3 -m pip install --upgrade pip\nsudo pip3 install -r requirements.txt -r requirements.dev.txt\n\nsudo apt-get update && sudo apt-get install -y python-apt\n', 'sudo -E python3 -m pytest']"
""
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', ""python -m unittest discover -p '*_test.py' -s '.'\n"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['mkdir ./.pkg/code\nmkdir ./.pkg/public\ncp -r ./altfe/ ./.pkg/code/altfe/\ncp -r ./app/ ./.pkg/code/app/\ncp ./main.py ./.pkg/code/\ncp -r ./usr/ ./.pkg/public/usr/\ncp ./app/config/biu_default.yml ./.pkg/public/config.yml\ncp ./LICENSE ./.pkg/public/\ncp ./README.md ./.pkg/public/\n', 'pip install -r ./requirements.txt\npip install pyinstaller  \n', 'cd ./.pkg/\npython ./py-pkger.py auto\n', 'cd ./.pkg/dist/\nmv ./main.exe ./PixivBiu.exe\nCompress-Archive * ../../${{ matrix.version }}.zip\n', 'cd ./.pkg/dist/\nmv ./main ./PixivBiu\nzip -r ../../${{ matrix.version }}.zip *\n', 'mv ./win_x64.zip ./PixivBiu_${{ github.event.inputs.releaseTag }}_win_x64.zip\nmv ./win_x86.zip ./PixivBiu_${{ github.event.inputs.releaseTag }}_win_x86.zip\nmv ./mac.zip ./PixivBiu_${{ github.event.inputs.releaseTag }}_mac_intel.zip\nmv ./ubuntu_x64.zip ./PixivBiu_${{ github.event.inputs.releaseTag }}_ubuntu_x64.zip\n']"
""
"['python -m pip install --user build', 'python -m build --sdist --wheel --outdir dist/']"
"['docker login -u $DOCKER_USER -p $DOCKER_PASSWORD', 'docker build -t electriceye/electriceye:$IMAGE_TAG .\ndocker tag electriceye/electriceye:$IMAGE_TAG electriceye/electriceye:latest\ndocker push electriceye/electriceye:$IMAGE_TAG\ndocker push electriceye/electriceye:latest\n', 'echo ""electriceye/electriceye:$IMAGE_TAG""', 'docker build -t $REGISTRY/$REGISTRY_ALIAS/$REPOSITORY:$IMAGE_TAG .\ndocker tag $REGISTRY/$REGISTRY_ALIAS/$REPOSITORY:$IMAGE_TAG $REGISTRY/$REGISTRY_ALIAS/$REPOSITORY:latest\ndocker push $REGISTRY/$REGISTRY_ALIAS/$REPOSITORY:$IMAGE_TAG\ndocker push $REGISTRY/$REGISTRY_ALIAS/$REPOSITORY:latest\n', 'echo ""$REGISTRY/$REGISTRY_ALIAS/$REPOSITORY:$IMAGE_TAG""', 'docker build -t ${{ steps.get-ocir-repository.outputs.repo_path }}:$IMAGE_TAG .\ndocker tag ${{ steps.get-ocir-repository.outputs.repo_path }}:$IMAGE_TAG ${{ steps.get-ocir-repository.outputs.repo_path }}:latest\ndocker push ${{ steps.get-ocir-repository.outputs.repo_path }}:$IMAGE_TAG\ndocker push ${{ steps.get-ocir-repository.outputs.repo_path }}:latest\n', 'echo ""${{ steps.get-ocir-repository.outputs.repo_path }}:$IMAGE_TAG""', 'docker build . --file Dockerfile --tag localbuild/electriceye:latest', 'cat ""syft-cdx-sbom.json""', 'cat ""${{ steps.scan.outputs.sarif }}""']"
"['pipenv install --dev\n', 'pipenv run python -m coverage run -m pytest\npipenv run python -m coverage report\npipenv run python -m coverage html\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\npipenv run python -m coverage_badge -o coverage.svg -f\ngit add coverage.svg\ngit commit -m ""Update coverage"" coverage.svg || echo ""No changes to commit""\n', 'pipenv install --dev\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch --prune --unshallow\ngit fetch --tags\nlatest_tag=$(git describe --tags `git rev-list --tags --max-count=1`)\necho ""latest tag: $latest_tag""\nnew_tag=$(echo $latest_tag | awk -F. -v a=""$1"" -v b=""$2"" -v c=""$3"" \'{printf(""%d.%d.%d"", $1+a, $2+b , $3+1)}\')\necho ""new tag: $new_tag""\n\ngit pull\n## update python version\necho ""version = \'$new_tag\'"" > \'airiam/version.py\'\ngit commit -m ""bump version"" airiam/version.py || echo ""No changes to commit""\ngit push origin\ngit tag $new_tag\ngit push origin $new_tag\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit fetch --tags\ngit pull\npipenv run python setup.py sdist bdist_wheel\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\npipenv run pip install homebrew-pypi-poet\npipenv run pip install airiam -U\ngit pull\npipenv run poet -f airiam > HomebrewFormula/airiam.rb\ngit commit -m ""update brew formula"" airiam/version.py HomebrewFormula/airiam.rb || echo ""No brew changes to commit""\ngit push origin\n', 'python -m pip install --upgrade pip pipenv\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'pipenv lock -r > requirements.txt\npip install -r requirements.txt\n', 'python -m pip install --upgrade pipenv wheel\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\npipenv  --python 3.7 update\ngit add -u\ngit commit -m ""update pipenv packages""\n', 'pipenv install --dev\n', 'pipenv run python -m coverage run -m pytest\npipenv run python -m coverage report\npipenv run python -m coverage html']"
"['python -m pip install --upgrade pip\npip install -e .[tests]\n', 'NUMBA_DISABLE_JIT=1 pytest --cov=./ --cov-report=xml --ignore ./xrspatial/tests/test_polygonize.py\n', 'sudo apt-get -y install pandoc\npython -m pip install --upgrade pip\n', 'pip install .[doc,tests]\npip list\n', 'make -C docs html\n', 'echo ""CHANGELOG_VERSION=$(cat CHANGELOG.md | grep -oP \'(?<=###\\s)(.*)(?=\\s\\-)\' | head -n 1 | sed \'s/Version\\s/v/\')"" >> $GITHUB_ENV\necho ""TAG_VERSION=`echo $(git describe --tags --abbrev=0)`"" >> $GITHUB_ENV\n', 'echo ""CHANGELOG_VERSION($CHANGELOG_VERSION) is different from TAG_VERSION($TAG_VERSION)""\nexit 1\n', 'python -m pip install --upgrade pip\npip install build\n', 'git fetch --tags -f', 'python -m build --sdist --wheel\n', 'echo ""PKG_SIZE=$(find dist -maxdepth 1 -regex \'.*gz\' | xargs stat --format=\'%s\')"" >> $GITHUB_ENV', 'echo ""PKG_SIZE($PKG_SIZE bytes) is greater than 100MB""\nexit 1\n', 'python -m pip install --upgrade pip\npip install -e .[tests]\n', 'pytest']"
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['pip freeze\n', ""pip install 'black==22.6.0' 'flake8==5.0.3' 'pydocstyle==6.1.1'\n\nblack -l 79 --check deepdow/ tests\nflake8 deepdow tests\npydocstyle deepdow\n"", 'pip install .[dev,docs,examples]\n', 'pip freeze\n', 'pytest\n', 'pip install sphinx\nsphinx-build -b doctest docs/ temp_doctest\n']"
"['python -m pip install --upgrade pip\npip install pillow tqdm numpy requests\n', 'PRNUM=${{ github.event.pull_request.number }} ONLYCHECK=1 python tools/canonicialize_masks.py\n', 'PRNUM=${{ github.event.pull_request.number }} python tools/check_img_size.py', 'python -m pip install --upgrade pip\npip install pillow tqdm numpy\n', 'python tools/check_file_location.py\n', 'python -m pip install --upgrade pip\npip install pillow tqdm numpy\n', 'python tools/check_filename.py\n', 'python runstat.py\n', 'test $(git diff --name-only HEAD 0266ab9dbef438f36171de9fe3cb7938bb2bd0b2 imgs/0* | wc -l) -eq 0', 'test $(git diff --name-only HEAD 4cee1fb54a016a72af204dd1b8effe4cb007df54 imgs/1* | wc -l) -eq 0']"
"['echo ""python_version=$(python --version)"" >> $GITHUB_OUTPUT', 'pip install poetry', 'python -m venv .venv', 'poetry run python -m pip install --upgrade pip', 'poetry install', 'poetry run black . --check', 'poetry run isort . --check', 'poetry export -f requirements.txt | poetry run safety check --bare --stdin', 'poetry run mypy --show-error-codes openapi_python_client', 'poetry run pylint openapi_python_client', 'poetry run pytest --cov=openapi_python_client --cov-report=term-missing tests end_to_end_tests/test_end_to_end.py --basetemp=tests/tmp', 'poetry run coverage xml', 'echo ""python_version=$(python --version)"" >> $GITHUB_OUTPUT', 'pip install poetry\npython -m venv .venv\npoetry run python -m pip install --upgrade pip\npoetry install\n', 'poetry run openapi-python-client update --url http://localhost:3000/openapi.json --config integration-tests-config.yaml\n', 'python .github/check_for_changes.py', 'cd integration-tests\npython -m venv .venv\npoetry run python -m pip install --upgrade pip\npoetry install\n', 'cd integration-tests\npoetry run pytest\n', 'git checkout HEAD^2', 'knope release --dry-run', 'knope release', 'pip install --upgrade poetry', 'poetry publish --build -u __token__ -p ${{ secrets.PYPI_TOKEN }}']"
"['sudo apt-get install libxml2-dev libxslt-dev', 'poetry install\n', 'poetry run black --check .\n', 'poetry run mypy .\n', 'poetry run flake8 --select F401\n', 'poetry run coverage run --omit=""tests*"" -m pytest\n', 'poetry run coverage report\n']"
"['pip install reuse\n', 'reuse lint\n']"
"['python --version\npython -m pip install -r requirements.txt\npython -m pip install mypy\n', './tests.py']"
"['sudo apt-get install -y patchelf qtchooser qt5-qmake qtbase5-dev-tools ninja-build cmake g++', 'sudo apt-get install -y libusb-1.0-0-dev libpcap-dev qtbase5-dev python3-dev', 'cmake -G Ninja -B build -S ${{ github.workspace }}', 'cmake --build build', 'npm ci\ncd tests\n./prepare_tests.js\nexport QT_DEBUG_PLUGINS=1\n./test_hobbits.js ${{ github.workspace }}/build/bin/hobbits-runner -p ${{ github.workspace }}/build/plugins\ncd ${{ github.workspace }}\n', 'ninja package -C build', 'cp ci/justversion.releaserc.json .releaserc.json\nnpm ci\nnpx semantic-release\nrm .releaserc.json\n', 'sudo apt-get install -y patchelf qtchooser qt5-qmake qtbase5-dev-tools ninja-build cmake g++', 'sudo apt-get install -y libusb-1.0-0-dev libpcap-dev qtbase5-dev python3-dev', 'cmake -G Ninja -B build -S ${{ github.workspace }}', 'cmake --build build', './prepare_tests.js\nexport QT_DEBUG_PLUGINS=1\n./test_hobbits.js ${{ github.workspace }}/build/bin/hobbits-runner -p ${{ github.workspace }}/build/plugins\n', 'ninja package -C build', 'cp ci/full.releaserc.json .releaserc.json\nnpx semantic-release\n']"
"['python -m pip install --upgrade pip\npip install -e .[linkify,rtd]\n', 'sphinx-build -nW --keep-going -b ${{ matrix.format }} docs/ docs/_build/${{ matrix.format }}\n', 'python -m pip install --upgrade pip\npip install -e .[linkify,rtd]\n', 'sphinx-build -nW --keep-going -b ${{ matrix.format }} docs/ docs/_build/${{ matrix.format }}\n', 'python -m pip install --upgrade pip\npip install -e "".[linkify,testing]"" ""sphinx${{ matrix.sphinx }}""\n', 'pytest --cov=myst_parser --cov-report=xml --cov-report=term-missing\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install tomlkit\n', 'python .github/workflows/docutils_setup.py pyproject.toml README.md', 'pip install .[linkify,testing-docutils] docutils==${{ matrix.docutils-version }}\n', 'python -c ""\\\ntry:\n    import sphinx\nexcept ImportError:\n    pass\nelse:\n    raise AssertionError()""\n', 'pytest tests/test_docutils.py tests/test_renderers/test_fixtures_docutils.py tests/test_renderers/test_include_directive.py tests/test_renderers/test_myst_config.py', 'echo ""test"" | myst-docutils-html', 'pip install flit~=3.4\n', 'flit publish\n', 'pip install flit~=3.4 tomlkit\n', 'python .github/workflows/docutils_setup.py pyproject.toml README.md', 'flit publish\n']"
"[""pip install '.[flynt,isort]'"", 'import sys\nfrom pathlib import Path\nfrom pprint import pprint\nfrom subprocess import check_output, STDOUT\ncmd = [""darker"", ""--options-for-readme""]\nusage = check_output(cmd, stderr=STDOUT, encoding=""utf-8"")\nreadme = Path(""README.rst"").read_text()\nif usage in readme:\n    sys.exit(0)\npprint(str.splitlines(usage, keepends=True), width=94)\nsys.exit(1)\n', ""pip install 'isort>=5.0.1'"", 'pip install -U \\\n  black \\\n  flynt \\\n  isort \\\n  mypy>=0.990 \\\n  pytest \\\n  types-requests \\\n  types-toml\n', 'pip install -U \\\n  airium \\\n  black \\\n  defusedxml \\\n  pip-requirements-parser \\\n  pygments \\\n  pylint \\\n  pytest>=6.2.0 \\\n  regex \\\n  requests \\\n  requests-cache \\\n  ruamel.yaml \\\n  toml\npip list\n', 'python -m pip install wheel', 'python setup.py bdist_wheel', 'from os import environ\nfrom pathlib import Path\nfrom runpy import run_path\nversion = run_path(""src/darker/version.py"")[""__version__""]\nPath(environ[""GITHUB_OUTPUT""]).open(""a"").write(\n    f""wheel-path=dist/darker-{version}-py3-none-any.whl\\n""\n)\n', 'nix-shell \\\n  --pure \\\n  --argstr pythonVersion ${{ matrix.python-version }} \\\n  --run \'\n    python -m venv venv\n    source venv/bin/activate\n    pip install ""${{needs.build-wheel.outputs.wheel-path}}[test]""\n    pytest\n  \' \\\n  ./default.nix\n', 'pip install ""${{needs.build-wheel.outputs.wheel-path}}[test]"" ${{ matrix.upgrade }} ${{ matrix.constraints }}', ""sed -i 's/py311/py39/' pyproject.toml\n"", 'pytest --darker\n', 'pytest\n', 'python -m pip install twine', 'python setup.py sdist', 'twine check dist/*', 'pip install pyupgrade', 'python -c ""\nimport sys\nfrom pyupgrade._main import main\nfrom glob import glob\nfiles = glob(\'**/*.py\', recursive=True)\nsys.exit(main(files + [\'--py37-plus\']))\n"" || ( git diff ; false )\n', 'pip install -U pip-tools', 'pip-compile setup.cfg', 'pip install -U safety', 'safety check --file requirements.txt', 'pip install click packaging requests\npython release_tools/bump_version.py \\\n  --minor \\\n  --dry-run \\\n  --token=${{ secrets.GITHUB_TOKEN }}\n', ""# strict dependency resolution added in pip 20.3\n# CVE-2021-3572 fixed in pip 21.1\npython -m pip install --upgrade 'pip>=21.1'\npip install \\\n  --constraint=constraints-future.txt \\\n  --upgrade \\\n  --upgrade-strategy=eager \\\n  -e '.[test]'\n"", 'pytest\n', 'import json\nimport os\nimport urllib.request\nfrom distutils.version import LooseVersion\nfrom importlib.metadata import version\nfrom textwrap import dedent\n\nfor linenum, line in enumerate(open(""setup.cfg""), 1):\n    constraint = line.strip()\n    if constraint.startswith(""black>=""):\n        column = line.index(""black>="") + 1\n        end_column = len(line)\n        break\nelse:\n    raise RuntimeError(""black>= line not found in setup.cfg"")\n\nresponse = urllib.request.urlopen(\n    \'https://pypi.org/pypi/black/json\'\n).read().decode()\nlatest_version = max(\n    LooseVersion(s)\n    for s in json.loads(response)[\'releases\'].keys()\n)\n\nprint(\n    dedent(\n        f""""""\n        ### :x: Future Black incompatibility? :x:\n\n        You could add a maximum version constraint for Black on\n        `setup.cfg` line {linenum}, e.g.\n        `{constraint},<={latest_version}`\n\n        See [#382](/akaihola/darker/issues/382)\n        for more information\n        """"""\n    ),\n    file=open(os.getenv(""GITHUB_STEP_SUMMARY""), ""a""),\n)\n\nprint(\n    ""::notice ""\n    ""file=setup.cfg,""\n    f""line={linenum},""\n    f""col={column},""\n    f""endColumn={end_column},""\n    ""title=Future Black incompatibility?::""\n    ""You could add a maximum version constraint for Black here, ""\n    f""e.g. {constraint},<={latest_version}""\n)\n']"
"['echo ""VERSION = \\""${GITHUB_REF##refs/tags/v}\\"""" > OpenAttack/version.py\npip install twine --user\npip install wheel\npython setup.py sdist bdist_wheel\n', 'python setup.py install\npip install -r requirements-dev.txt\ncd test\npython run_test.py\n']"
"['python -m pip install --upgrade pip\npip install .[dev]\n', 'python -m pip install --upgrade pip\npip install torch===1.10.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\npip install .[dev]\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=12 --max-line-length=127 --statistics\n', 'black --check --diff .\n', 'pytest test\n', 'python -m pip install --upgrade pip\npip install codecov pytest-cov\npip install .[dev]\n', 'pytest --cov=geotorch test/ --cov-report term-missing', 'codecov', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['pip3 install --upgrade setuptools pyinstaller requests', 'pip3 install --upgrade pywebview', 'echo ::set-output name=option::--strip', 'pyinstaller --onefile --name legendary ${{ steps.strip.outputs.option }} -i ../assets/windows_icon.ico cli.py', 'sudo apt install ruby\nsudo gem install fpm\n', 'fpm --input-type python --output-type deb --python-package-name-prefix python3 --deb-suggests python3-webview --maintainer ""Rodney <rodney@rodney.io>"" --category python --depends ""python3 >= 3.9"" setup.py', 'source /etc/os-release\necho ::set-output name=version::$NAME-$VERSION_ID\n']"
""
[]
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/* --verbose\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pypandoc\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/* --verbose\n', 'python -m pip install --upgrade pip\npip install numpy -I\npip install pytest torch mxnet\npip install nose\npip install -e .\n  \n', 'for f in tests/*.py; do python ""$f""; done\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# The GitHub editor is 127 chars wide\nflake8 . --count  --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install torch==2.0.0 --extra-index-url https://download.pytorch.org/whl/cpu', 'python -m pip install torch==2.0.0', 'python -m pip install -e .[tests] -f https://download.pytorch.org/whl/torch_stable.html\n', '# We\'re mocking out the torch. So now we also need to sub this on out\nfind ../. -type f -exec sed -i \'s/{torch.__version__}/1.12.0/g\' {} +\nfind ../. -type f -exec sed -i \'s/torch.__version__/""1.12.0""/g\' {} +\nmake sphinx-site -C website/\n# now refresh the code\ngit checkout . && python -m pip install -e .[tests] -f https://download.pytorch.org/whl/torch_stable.html\n', 'pytest', 'coverage run -a -m pytest tests/test_no_extra_install.py', 'wget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew unlink libomp\nbrew install ./libomp.rb\nexport CC=/usr/bin/clang\nexport CXX=/usr/bin/clang++\nexport CPPFLAGS=""$CPPFLAGS -Xpreprocessor -fopenmp""\nexport CFLAGS=""$CFLAGS -I/usr/local/opt/libomp/include""\nexport CXXFLAGS=""$CXXFLAGS -I/usr/local/opt/libomp/include""\nexport LDFLAGS=""$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib -L/usr/local/opt/libomp/lib -lomp""\n', 'python -m pip install .[extra,onnx,sparkml]\npython -m pip install pandas\n', 'python -m pip install apache-tvm==0.10.0', 'brew install llvm@14\n', 'cd ~/work/hummingbird\ngit clone https://github.com/apache/tvm.git\ncd tvm\ngit checkout tags/v0.10.0\ngit submodule update --recursive --init\ncmake -E make_directory build\n', 'MACOSX_DEPLOYMENT_TARGET=10.13 cmake ""-DUSE_RPC=ON"" ""-DUSE_GRAPH_RUNTIME=ON"" ""-DUSE_LLVM=$(brew --prefix llvm@14)/bin/llvm-config --link-static"" ..', 'make -j3\n', 'python -m pip install -U wheel packaging\npython setup.py bdist_wheel\npython -m pip install dist/tvm-*.whl\n', 'pytest', 'coverage run -a -m pytest tests\ncoverage xml\n']"
"['# install python modules\npython -m pip install --upgrade pip\npip install -q -U numpy\npip install git+https://github.com/repodiac/german_transliterate.git#egg=german_transliterate\npip install -q tensorflow-gpu==${{ matrix.tensorflow-version }}\npip install -q -e .\npip install -q -e .[test]\npip install typing_extensions\nsudo apt-get install libsndfile1-dev\npython -m pip install black\n', 'python -m black . \n', 'pytest test']"
""
"['python -m pip install black==22.3.0\nblack --check cyberbrain test\n', 'npm install', 'npm run lint', 'npm run unittest', 'pdm info -v', 'pdm install -v', 'pdm run tox -v', 'pdm info -v', 'pdm install -v', 'npm install', 'xvfb-run -a npm test', 'npm test']"
"['python -m pip install --upgrade pip\npython -m pip install mypy pytest pytest-cov\npip install torch==${{ matrix.pytorch-version }} torchvision transformers\npip install compressai\n', 'mypy --install-types --non-interactive .\n', 'pytest --cov=torchinfo --cov-report= --durations=0\n', 'pytest --no-output -k ""not test_eval_order_doesnt_matter and not test_google and not test_uninitialized_tensor and not test_input_size_half_precision and not test_recursive_with_missing_layers""\n']"
"['sudo python -m pip install --upgrade pip\nsudo pip install cookiecutter\n', 'sudo ./scripts/test.sh\n']"
"['pip install tox', 'python helper/github-ci-vars.py', 'sudo apt-get update\nsudo apt-get install -y gdal-bin libsqlite3-mod-spatialite\n', 'pip install tox', 'tox --skip-missing-interpreters=false -e ${{ matrix.setup.toxenv }}', 'echo ""All Done""', 'python -m pip install --upgrade pip\npip install -r requirements/packaging.txt\n', 'python setup.py publish']"
""
"['python -m pip install --upgrade pip\nmake install\n', 'make test', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nmake install\n', 'make dist\necho ""To be add: pack testing""\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nmake install\n', 'make deploy-version\npython setup.py sdist bdist_wheel\ntwine upload --skip-existing dist/*\n', ""echo 'Not A Publish Branch'""]"
""
"['let DIFF=(`date +%s -d ""1 day ago""`-`git log -1 --pretty=format:%ct`) &&\necho ""date_diff=$DIFF"" >> $GITHUB_ENV\n', 'pip install interrogate &&\ninterrogate avalanche -vv --generate-badge interrogate-badge.svg -o documentation-coverage.txt\n', 'mv interrogate-badge.svg avalanche-reports/badge/ &&\nmv documentation-coverage.txt avalanche-reports/docstring_coverage/ &&\ncd avalanche-reports &&\ngit config --local user.email ${{ secrets.EMAIL }} &&\ngit config --local user.name ${{ secrets.NAME }} &&\ngit pull &&\ngit add . &&\ngit commit -m ""Update coverage badge"" -a || true\n', 'apt-get update &&\napt-get install ffmpeg libsm6 libxext6  -y\n', 'mamba create -n avalanche-env -y -v python=${{ matrix.python-version }} -c conda-forge &&\nconda run -n avalanche-env --no-capture-output mamba install -y -v pytorch==1.12.1 torchvision cpuonly -c pytorch &&\nconda run -n avalanche-env --no-capture-output mamba env update --file environment.yml -v\n', 'python -m unittest discover tests &&\necho ""Running checkpointing tests..."" &&\nbash ./tests/checkpointing/test_checkpointing.sh &&\necho ""Running distributed training tests..."" &&\ncd tests &&\nPYTHONPATH=.. python run_dist_tests.py &&\ncd ..\n', 'conda env export > failed-environment.yml &&\ndiff avalanche-docker/${{ matrix.python-version }}/environment-${{ matrix.python-version }}.yml failed-environment.yml > diff_fail.txt\n', 'rm -f avalanche-docker/${{ matrix.python-version }}/environment-${{ matrix.python-version }}.yml &&\nconda env export > $""avalanche-docker/""${{ matrix.python-version }}""/environment-""${{ matrix.python-version }}"".yml"" &&\ncd avalanche-docker &&\ngit config --local user.email ${{ secrets.BOT_EMAIL }} &&\ngit config --local user.name ${{ secrets.BOT_NAME }} &&\ngit pull &&\ngit add . &&\ngit commit -m $""Update environment with python ""${{ matrix.python-version }} -a || true &&\ngit pull\n', 'pip install nbconvert', 'cd notebooks\nfind . -type f -name ""*ipynb"" -exec sh -c \'\n   for file do\n     dir=${file%/*}\n     jupyter nbconvert --output-dir ""../docs/gitbook/${dir}"" --to markdown --ClearOutputPreprocessor.enabled=True ${file}\n   done\' sh {} +\n', 'echo $GITHUB_WORKSPACE &&\ngit config --system --add safe.directory $GITHUB_WORKSPACE\ngit config --local user.email ${{ secrets.BOT_EMAIL }} &&\ngit config --local user.name ${{ secrets.BOT_NAME }} &&\ngit add . &&\ngit commit -m ""Update gitbook documentation"" -a || true\n# The above command will fail if no changes were present, so we ignore that.\n', 'pip install pycodestyle', 'pycodestyle avalanche tests examples --exclude ""examples/tvdetection/**"" > pep8_report.txt\ncat pep8_report.txt\n', 'pip install coverage coveralls\n', ""mkdir coverage &&\ncoverage run -m unittest discover tests &&\ncoverage lcov -o coverage/lcov.info --omit '**/config-3.py,**/config.py'\n"", 'python -m unittest discover tests &&\necho ""Checking that optional dependencies are not needed"" &&\npip uninstall -y higher ctrl-benchmark torchaudio gym pycocotools lvis &&\nPYTHONPATH=. python examples/eval_plugin.py &&\necho ""Running checkpointing tests..."" &&\nbash ./tests/checkpointing/test_checkpointing.sh &&\necho ""Running distributed training tests..."" &&\ncd tests &&\nPYTHONPATH=.. python run_dist_tests.py &&\ncd .. &&\necho ""While running unit tests, the following datasets were downloaded:"" &&\nls ~/.avalanche/data\n']"
"['python setup.py install\npip install -r tests/test-requirements.txt\npytest tests/tests.py\n', 'pip install wheel && python setup.py sdist bdist_wheel']"
"['python -m pip install --upgrade pip\npip install -e . --upgrade\n', 'python -c ""import torchcam; print(torchcam.__version__)""', 'python -m pip install --upgrade pip\npip install -e "".[demo]"" --upgrade\n', 'screen -dm streamlit run demo/app.py --server.port 8080\nsleep 10 && nc -vz localhost 8080\n', 'import os\nstatus, errormsg = os.getenv(\'STATUS\'), os.getenv(\'ERROR\')\nif status != \'built\': raise AssertionError(f""There was an error building the page on GitHub pages.\\n\\nStatus: {status}\\n\\nError messsage: {errormsg}"")\n', 'python -m pip install --upgrade pip\npip install -e "".[docs]""\n', 'cd docs && bash build.sh', 'test -e docs/build/index.html || exit', 'pip install requests', 'echo ""::set-output name=merger::$(python .github/verify_labels.py ${{ github.event.pull_request.number }})""', 'python -m pip install --upgrade pip\npip install -e "".[docs]""\n', 'cd docs && bash build.sh', 'test -e docs/build/index.html || exit', 'python -m pip install --upgrade pip\npip install setuptools wheel twine --upgrade\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}\n', 'BUILD_VERSION=${VERSION:1} python setup.py sdist bdist_wheel\ntwine check dist/*\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install torchcam\npython -c ""import torchcam; print(torchcam.__version__)""\n', 'conda install -y conda-build conda-verify anaconda-client\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}\n', 'BUILD_VERSION=${VERSION:1} python setup.py sdist\nmkdir conda-dist\nconda-build .conda/ -c pytorch --output-folder conda-dist\nls -l conda-dist/noarch/*tar.bz2\nanaconda upload conda-dist/noarch/*tar.bz2\n', 'conda install -c frgfm torchcam\npython -c ""import torchcam; print(torchcam.__version__)""\n', 'python -m pip install --upgrade pip\npip install -e . --upgrade\npip install -r scripts/requirements.txt\n', 'python scripts/cam_example.py --arch resnet18 --class-idx 232 --noblock --method LayerCAM', 'python -m pip install --upgrade pip\npip install -e . --upgrade\npip install -r scripts/requirements.txt\n', 'python scripts/eval_latency.py --arch resnet18 LayerCAM', 'pip install ruff\nruff --version\nruff check --diff .\n', 'pip install isort\nisort --version\nisort .\nif [ -n ""$(git status --porcelain --untracked-files=no)"" ]; then exit 1; else echo ""All clear""; fi\n', 'python -m pip install --upgrade pip\npip install -e . --upgrade\npip install mypy\n', 'mypy --version\nmypy\n', 'pip install pydocstyle[toml]\npydocstyle --version\npydocstyle\n', 'pip install ""black==22.3.0""\nblack --version\nblack --check --diff .\n', 'pip install bandit[toml]\nbandit --version\nbandit -r . -c pyproject.toml\n', 'python -m pip install --upgrade pip\npip install -e "".[test]"" --upgrade\n', 'coverage run -m pytest tests/\ncoverage xml\n']"
"['make devenv', 'make test', 'make package\ntwine upload dist/*\n', 'make devenv', 'make lint', 'make test', 'make devenv', 'make testnospark', 'pip install ""setuptools_scm<7""', 'pip install ""pandas<1.3.0""', 'make devenv', 'pip install ""pandas>=2""', 'make testcore', 'pip install ""setuptools_scm<7""', 'pip install ""pandas<1.3.0""', 'make devenv', 'pip install ""pandas>=2""', 'pip uninstall -y qpd fugue-sql-antlr sqlglot', 'make testnosql', 'make devenv', 'make testnotebook', 'make devenv', 'pip install ""pyspark==${{ matrix.spark-version }}""', 'pip install ""pandas==${{ matrix.pandas-version }}""', 'pip install ""ibis-framework<5""', 'make testspark', 'make devenv', 'make sparkconnect', 'make testsparkconnect', 'pip install -r requirements.txt', 'pip install pyarrow==7.0.0', ""python -m pytest --reruns 2 --only-rerun 'Overflow in cast' tests/fugue tests/fugue_dask tests/fugue_ibis tests/fugue_duckdb""]"
"['python -m pip install --upgrade pip\nif [ -f scripts/requirements.txt ]; then pip install -r scripts/requirements.txt; fi\n', 'python scripts/process_worldwide.py\npython scripts/process_us.py\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit commit --allow-empty -m ""Auto-update of the data packages"" -a\n', 'npm install -g data-cli\ndata --version\ndata init data\n', 'python scripts/update_datapackage.py\ndata push\n']"
[]
"['psql -AXqte ""host=127.0.0.1 dbname=test user=postgres"" << HERE\nSELECT version();\nCREATE SCHEMA test_schema;\nCREATE SCHEMA test_schema_2;\nCREATE EXTENSION hstore;\nHERE\n', 'echo ""DEPS=$DEPS ./psycopg_c"" >> $GITHUB_ENV\n', 'pip install $DEPS', 'pip download --no-deps --no-binary :all: ${{ matrix.pip_sqlalchemy }}\nmkdir sa_home\ncase $(file --brief --mime-type SQLAlchemy*) in\n  application/gzip)\n    tar -C sa_home -xzf SQLAlchemy*\n    ;;\n  application/zip)\n    unzip -d sa_home -q SQLAlchemy*\n    ;;\n  *)\n    echo ""Unexpected format for $(file --mime-type SQLAlchemy*)"" >&2\n    exit 1\n    ;;\nesac\nmv sa_home/$( ls sa_home ) sa_home/sa\ncd sa_home/sa\npip install .\n', 'pytest -n 2 -q --dburi $URL --backend-only --dropfirst --color=yes', 'psql -AXqte ""host=127.0.0.1 dbname=postgres user=postgres"" << HERE\nSELECT version();\nHERE\n', 'echo ""DEPS=$DEPS ./psycopg_c"" >> $GITHUB_ENV\n', 'pip install $DEPS', 'pip download --no-deps --no-binary :all: ${{ matrix.pip_django }}\nmkdir django_home\ncase $(file --brief --mime-type Django*) in\n  application/gzip)\n    tar -C django_home -xzf Django*\n    ;;\n  application/zip)\n    unzip -d django_home -q Django*\n    ;;\n  *)\n    echo ""Unexpected format for $(file --mime-type Django*)"" >&2\n    exit 1\n    ;;\nesac\nmv django_home/$( ls django_home ) django_home/django\ncat << HERE > django_home/django/tests/test_postgresql.py\nDATABASES = {\n    ""default"": {\n        ""ENGINE"": ""django.db.backends.postgresql"",\n        ""HOST"": ""127.0.0.1"",\n        ""USER"": ""postgres"",\n        ""PASSWORD"": ""password"",\n    },\n    ""other"": {\n        ""ENGINE"": ""django.db.backends.postgresql"",\n        ""HOST"": ""127.0.0.1"",\n        ""USER"": ""postgres"",\n        ""PASSWORD"": ""password"",\n    },\n}\n\nSECRET_KEY = ""django_tests_secret_key""\n\n# Use a fast hasher to speed up tests.\nPASSWORD_HASHERS = [\n    ""django.contrib.auth.hashers.MD5PasswordHasher"",\n]\n\nDEFAULT_AUTO_FIELD = ""django.db.models.AutoField""\n\nUSE_TZ = False\nHERE\n', 'pip install .\npip install -r tests/requirements/py3.txt\n', './runtests.py --settings=test_postgresql postgres_tests backends queries', 'pip install ./psycopg[dev,test] codespell', 'black --check --diff .', 'flake8', 'mypy', 'codespell', 'sudo apt-get install -y libgeos-dev', 'pip install ./psycopg[docs] ./psycopg_pool', 'sphinx-build -W -T -b html docs docs/_build/html', 'python3 ./tools/build/copy_to_binary.py', 'python3 ./tools/build/copy_to_binary.py', '$PgSvc = Get-Service ""postgresql*""\nSet-Service $PgSvc.Name -StartupType manual\n$PgSvc.Start()\n', 'python3 ./tools/build/copy_to_binary.py', 'pip install build', 'python -m build -o dist --${{ matrix.format }} ${{ matrix.package }}', 'pip install ./psycopg[test] dist/*', ""pytest -m 'pool and not slow and not flakey' --color yes"", 'pip install build', 'python -m build -o dist --${{ matrix.format }} ${{ matrix.package }}', 'pip install `ls dist/*`[test] ./psycopg_pool', 'pip install dist/* ./psycopg[test] ./psycopg_pool', ""pytest -m 'not slow and not flakey' --color yes"", 'docker pull ${{ matrix.postgres }}\ndocker run --rm -d --name postgres -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=password ${{ matrix.postgres }} \\\n  -c max_prepared_transactions=10\n', 'sudo ./tools/build/ci_install_libpq.sh ${{ matrix.libpq }}', 'echo ""DEPS=$DEPS ./psycopg_c"" >> $GITHUB_ENV\n', 'echo ""DEPS=$DEPS dnspython"" >> $GITHUB_ENV\necho ""MARKERS=$MARKERS dns"" >> $GITHUB_ENV\n', 'echo ""DEPS=$DEPS shapely"" >> $GITHUB_ENV\necho ""MARKERS=$MARKERS postgis"" >> $GITHUB_ENV\n', 'echo ""DEPS=$DEPS dnspython shapely"" >> $GITHUB_ENV\necho ""PIP_CONSTRAINT=${{ github.workspace }}/tests/constraints.txt"" \\\n  >> $GITHUB_ENV\n', 'pip install $DEPS', './tools/build/ci_test.sh', 'brew install postgresql@14', 'brew services start postgresql', 'echo ""DEPS=$DEPS ./psycopg_c"" >> $GITHUB_ENV\n', 'pip install $DEPS', './tools/build/ci_test.sh', '$PgSvc = Get-Service ""postgresql*""\nSet-Service $PgSvc.Name -StartupType manual\n$PgSvc.Start()\n', 'pip install delvewheel wheel\n\n# The windows runner is a total mess, with random copies of the libpq\n# scattered all over the places. Give precedence to the one under our\n# control (or the illusion of it).\nexport PATH=""/c/Program Files/PostgreSQL/14/bin/:$PATH""\n\n# If the wheel is not delocated, import fails with some dll not found\n# (but it won\'t tell which one).\npip wheel -v -w ./psycopg_c/dist/ ./psycopg_c/\ndelvewheel repair --no-mangle ""libiconv-2.dll;libwinpthread-1.dll"" \\\n  -w ./wheelhouse/ psycopg_c/dist/*.whl\necho ""DEPS=$DEPS $(ls ./wheelhouse/*.whl)"" >> $GITHUB_ENV\n', 'pip install $DEPS', './tools/build/ci_test.sh', 'docker pull cockroachdb/cockroach:${{ matrix.crdb }}\ndocker run --rm -d --name crdb -p 26257:26257 \\\n  cockroachdb/cockroach:${{ matrix.crdb }} start-single-node --insecure\n', 'sudo ./tools/build/ci_install_libpq.sh ${{ matrix.libpq }}', 'echo ""DEPS=$DEPS ./psycopg_c"" >> $GITHUB_ENV\n', 'pip install $DEPS', './tools/build/ci_test.sh']"
""
"['python -m pip install --upgrade pip setuptools twine --user', 'python setup.py sdist']"
""
"['echo ""${{ steps.create_release.outputs.upload_url }}"" > release-url.txt\n', 'SYSTEM_ID=$(echo ${{ matrix.container }} | cut -f1 \'-d:\')\nDISTRIBUTION=$(echo ${{ matrix.container }} | cut -f2 \'-d:\')\nARTIFACT_PREFIX=""${SYSTEM_ID}_${DISTRIBUTION}""\necho ""Artifact prefix: $ARTIFACT_PREFIX""\nRELEASE_URL=$(cat release-url/release-url.txt)\necho ""Release URL: $RELEASE_URL""\necho ""::set-output name=artifact_prefix::${ARTIFACT_PREFIX}""\necho ""::set-output name=release_url::${RELEASE_URL}""\n', 'DRAKVUF_REF=$(sh -c \'cd sandbox && git ls-tree HEAD drakvuf\' | cut -f3 \'-d \' | cut -f1 -d$\'\\t\')\necho ""::set-output name=drakvuf_ref::${DRAKVUF_REF}""\n', 'sh -c \'cd drakvuf && sh package/build.sh ${{ matrix.container }}\'\nDRAKVUF_DEB_PATH=$(find drakvuf/package/out/*.deb | head -n1)\nDRAKVUF_DEB_NAME=$(basename ""$DRAKVUF_DEB_PATH"")\necho ""::set-output name=drakvuf_deb_path::${DRAKVUF_DEB_PATH}""\necho ""::set-output name=drakvuf_deb_name::${{ steps.gen_vars.outputs.artifact_prefix }}_${DRAKVUF_DEB_NAME}""\n', 'sh -c \'cd sandbox && BASEIMAGE=${{ matrix.container }} sh drakcore/package/build.sh\'\nDRAKCORE_DEB_PATH=$(find sandbox/out/drakcore*.deb | grep -v dbgsym | head -n1)\nDRAKCORE_DEB_NAME=$(basename ""$DRAKCORE_DEB_PATH"")\necho ""::set-output name=drakcore_deb_path::${DRAKCORE_DEB_PATH}""\necho ""::set-output name=drakcore_deb_name::${{ steps.gen_vars.outputs.artifact_prefix }}_${DRAKCORE_DEB_NAME}""\n', 'sh -c \'cd sandbox && BASEIMAGE=${{ matrix.container }} sh drakrun/package/build.sh\'\nDRAKRUN_DEB_PATH=$(find sandbox/out/drakrun*.deb | grep -v dbgsym | head -n1)\nDRAKRUN_DEB_NAME=$(basename ""$DRAKRUN_DEB_PATH"")\necho ""::set-output name=drakrun_deb_path::${DRAKRUN_DEB_PATH}""\necho ""::set-output name=drakrun_deb_name::${{ steps.gen_vars.outputs.artifact_prefix }}_${DRAKRUN_DEB_NAME}""\n']"
"['pip3 install twine\ncd gcastle\npip3 install -r ./requirements.txt\npython3 setup.py sdist bdist_wheel\ntwine upload -u ${{ secrets.PYPI_USERNAME }} -p ${{ secrets.PYPI_PASSWORD }} dist/*\nls dist/*\n', 'cd gcastle\nLABELS=$(python -c \'import web; print(web.__version__)\')\necho ""::set-output name=TAGS::gcastle/castleboard:$LABELS""\ncd ..\n']"
"['pip install -r requirements.txt\n', 'flake8 .', 'isort --diff --check .', 'black --line-length 88 --check .', 'python -m pip install --upgrade pip\npip install -r requirements_test.txt\n', 'pytest \\\n  -qq \\\n  --timeout=9 \\\n  --durations=10 \\\n  -n auto \\\n  -o console_output_style=count \\\n  -p no:sugar \\\n  tests\n']"
""
"['sudo apt-get update -y\nsudo apt-get install -y python3.8 python3.8-dev python3-pip python3-certifi python3-setuptools patchelf desktop-file-utils libgdk-pixbuf2.0-dev php php-curl php-zip php-bcmath php-json php-pear php-mbstring php-dev\nsudo apt-get remove -y ansible\n# Install appimagetool AppImage\nsudo wget https://github.com/AppImage/AppImageKit/releases/download/12/appimagetool-x86_64.AppImage -O /usr/local/bin/appimagetool\nsudo chmod +x /usr/local/bin/appimagetool\nsudo pip3 install appimage-builder==0.8.1\nmkdir -p appimage-builder-cache\nwget https://github.com/AppImage/AppImageKit/releases/download/12/runtime-x86_64 -O appimage-builder-cache/runtime-x86_64\n', 'appimage-builder --recipe appimage-builder.yml --skip-test\n', 'sudo apt-get update -y\nsudo apt-get install -y python3.8 python3.8-dev python3-pip python3-certifi python3-setuptools patchelf desktop-file-utils libgdk-pixbuf2.0-dev php php-curl php-zip php-bcmath php-json php-pear php-mbstring php-dev\nsudo apt-get remove -y ansible\n# Install appimagetool AppImage\nsudo wget https://github.com/AppImage/AppImageKit/releases/download/12/appimagetool-x86_64.AppImage -O /usr/local/bin/appimagetool\nsudo chmod +x /usr/local/bin/appimagetool\nsudo pip3 install appimage-builder==0.8.1\nmkdir -p appimage-builder-cache\nwget https://github.com/AppImage/AppImageKit/releases/download/12/runtime-x86_64 -O appimage-builder-cache/runtime-x86_64\n', 'appimage-builder --recipe appimage-builder.yml --skip-test\n', 'docker pull shiftleft/scan-slim:latest\ndocker pull shiftleft/scan:latest\ndocker save -o scanslim.tar shiftleft/scan-slim:latest\ndocker save -o scan.tar shiftleft/scan:latest\ndocker run --rm -e ""WORKSPACE=${PWD}"" -v $PWD:/app shiftleft/scan:docker scan --src /app/scanslim.tar -o /app/reports --type docker\ndocker run --rm -e ""WORKSPACE=${PWD}"" -e ""FETCH_LICENSE=true"" -e ""ENABLE_OSS_RISK=true"" -v $PWD:/app shiftleft/scan:docker scan --src /app/scan.tar -o /app/reports --type docker\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --statistics\n', 'pytest --cov=. test\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest-cov ipython\n', 'flake8 traceback_with_variables --count --show-source --statistics --max-line-length=127\n', 'python -m pytest -vv --cov=./traceback_with_variables --cov-report=xml\n', 'python -m pip install --upgrade pip\npip install flake8 pytest-cov ipython\n', 'flake8 traceback_with_variables --count --show-source --statistics --max-line-length=127\n', 'python -m pytest -s --cov=./traceback_with_variables --cov-report=xml\n']"
"['git config --local user.email ""szuyanglb@outlook.com""\ngit config --local user.name ""yanglbme""\ngit commit -m ""chore: auto compress images"" -a\n']"
"['make docker-build-dev', 'make docker-test', 'make docker-flake8']"
"['uname -a\nfree -g\npwd\n', 'python --version\npython -m pip install --upgrade pip\npip install tensorflow==${{ matrix.tf-version }} ""numpy<1.24.0""\npip install git+https://github.com/DataCanvasIO/Hypernets\npip install -r requirements.txt ""protobuf<4.0"" ""numpy<1.24.0""\npip install pytest-cov==2.4.0  python-coveralls  codacy-coverage\npip list\n', 'pytest --cov=deeptables --durations=30 -k ""not dask and not batch_trainer""\n', 'uname -a\nfree -g\npwd\n', 'python --version\npython -m pip install --upgrade pip\npip install tensorflow==${{ matrix.tf-version }}\npip install git+https://github.com/DataCanvasIO/Hypernets\npip install -r requirements.txt ""protobuf<4.0"" ""numpy==1.19.5"" ""featuretools<=0.27"" ""h5py==2.10.0""\npip install pytest-cov==2.4.0  python-coveralls  codacy-coverage\npip list\n', 'pytest --cov=deeptables --durations=30 -k ""not dask and not batch_trainer""\n']"
"['python -m pip install --upgrade pip setuptools virtualenv wheel', 'python -m unittest']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
"['pip install --upgrade pip ruff setuptools wheel', 'ruff . --select=E9,F63,F7,F82,PLE,YTT --show-source .', 'ruff --exit-zero --select=ALL --statistics --target-version=py37 .', 'ruff --exit-zero --select=ALL --ignore=ANN204,COM812,ERA001,RSE102 --statistics --target-version=py37 . | grep ""\\[\\*\\]""', 'pip install black codespell mypy pytest safety', 'black --check . || true', 'codespell', 'pip install -r requirements.txt || pip install --editable . || pip install . || true', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'safety check']"
"['sudo apt-get install -y libsndfile1\n', 'python -c ""import sys; print(sys.version)""', 'pip install -r requirements.txt\npip install --editable .\npip install ctc-segmentation\n', 'pytest tests/consistency\n', 'pytest tests/unittests\n', 'pytest --doctest-modules speechbrain\n', 'pytest tests/integration\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/', 'pip install -r requirements.txt\npip install --editable .\npip install -r docs/docs-requirements.txt\n', 'cd docs\nmake html\n']"
""
""
"['python -m pip install --upgrade pip\npip install setuptools pyinstaller\npip install .\n', 'make -C bindist\ncd bindist && echo ""DIST_FILE=`make dist-name | tr -d \'\\n\'`"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip\npip install setuptools\npip install .\n', 'graphtage -dumpversion | grep -qv git', 'python -m venv /tmp/pip-audit-env\nsource /tmp/pip-audit-env/bin/activate\n\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install .\n', 'echo ""::set-env name=VERSION::${GITHUB_REF#refs/*/}""', 'cd graphtage\npython -m pip install --upgrade pip\npip install setuptools\npip install .[dev]\n', 'cd graphtage/docs\nmake html\n', 'cd gh-pages\ngit pull\nrm -rf ${VERSION}\nmkdir ${VERSION}\ncp -r ../graphtage/docs/_build/html/* ${VERSION}/\ncd ${VERSION}\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\nif [ ""$GITHUB_REF"" == ""refs/heads/master"" ]; then\n  cd ..\n  # This is not tag, so it is the latest:\n  rm -f latest\n  ln -s ${VERSION} latest\n  git add latest\nfi\ngit commit -m ""Update documentation for ${GITHUB_REF}"" -a || true\n# The above command will fail if no changes were present, so we ignore\n# the return code.\n', 'python -m pip install --upgrade pip\npip install setuptools\npip install .[dev]\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 graphtage test --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 graphtage test --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'cd docs\nmake html\n', 'pip install pytest\npytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['DOCKER_IMAGE=milesmcc/shynet\nTAGS=""${DOCKER_IMAGE}:edge""\necho ::set-output name=tags::${TAGS}\n', 'DOCKER_IMAGE=milesmcc/shynet\nTAGS=""${DOCKER_IMAGE}:${{ github.event.inputs.tag }}""\necho ::set-output name=tags::${TAGS}\n', 'DOCKER_IMAGE=milesmcc/shynet\nVERSION=${GITHUB_REF#refs/tags/}\nTAGS=""${DOCKER_IMAGE}:${VERSION},${DOCKER_IMAGE}:latest""\necho ::set-output name=tags::${TAGS}\n', 'poetry install', 'cp TEMPLATE.env .env\npoetry run ./shynet/manage.py test']"
"['if [[ ""${{ github.event_name }}"" == ""push"" ]]\nthen\n  REF=""${{ github.ref }}""\nelse\n  REF=""refs/pull/${{ github.event.number }}/merge""\nfi\nprintf ""REF=%s\\n"" ""$REF"" >> ${{ github.env }}\n', '# ssh checkout of additional remotes fails :/\ngit remote add OSS https://github.com/${{ env.OSS_REPO }}\ngit fetch OSS ""$REF"":proposed_base\ngit config --global user.email ""autorebase@address.invalid""\ngit config --global user.name  ""Automatic rebase check""\n', 'git rebase proposed_base || (cat <<""EOF"" 1>&2\n\n********\n\nThis branch failed to rebase onto the ""determined-ee"" repo!\n\nThis may or may not be an issue with your branch that\nyou need to resolve.\n\nFor more information, please consult this confluence page:\n\nhttps://hpe-aiatscale.atlassian.net/wiki/spaces/ENGINEERIN/pages/1295450385/GitHub+Check+Rebaseable+Test\n\n********\n\nEOF\nexit 1\n)\n', 'echo ""branch: $BRANCH""\nSUM=""$(echo -n ""$BRANCH"" | md5sum | awk \'{print $1}\')"" && echo $SUM\necho ""SUM=${SUM}"" >> $GITHUB_OUTPUT\n', 'make -C proto get-deps\nmake -C master get-deps\nmake -C agent get-deps\n', 'make -C proto build', 'make -C bindings force-gen\nmake -C bindings check/python\n', 'make -C bindings get-deps\nmake -C bindings check/typescript-fetch\n', 'python --version\npython -m pip install --upgrade pip\npip --version\npip install -r docs/requirements.txt\n', 'make -C docs check', 'make -C proto get-deps\nsudo apt-get update && sudo apt-get install -y clang-format\nmake -C ${{ matrix.service }} get-deps\n', 'make -C proto build\nmake -C ${{ matrix.service }} check\n', './.github/scripts/lint-migrations.sh', 'echo ""VERSION=$(< ./VERSION )"" >> $GITHUB_ENV', 'python --version\npython -m pip install --upgrade pip wheel setuptools\npip --version\npip install ""torch==1.9.0""\npip install -r requirements.txt\n', 'make build\npip install --find-links dist determined==${{ env.VERSION }}\npip install --no-deps --force-reinstall --find-links dist determined==${{ env.VERSION }}\n', 'sudo apt-get update && sudo apt-get install -y ffmpeg libsm6 libxext6\nmake build\npip install --find-links dist model-hub==${{ env.VERSION }}\npip install --no-deps --force-reinstall --find-links dist model-hub==${{ env.VERSION }}\n', 'make -C ${{ matrix.component }} check', 'make -C webui/react get-deps-npm', 'make -C webui/react check', 'sudo make -C .tmp/git-secrets-repo install\n# needed to avoid having the secret scan include the git-secrets repo\nrm -rf .tmp\n', '# workaround git-secrets requiring the say command\nln -s ""$(which echo)"" /usr/local/bin/say\ngit secrets --install\ngit secrets --register-aws\ngit secrets --add \'""private_key"":\\s""-----BEGIN\\sPRIVATE\\sKEY-----\'\n', 'git secrets --scan-history', '# does github.event have the id in it already?  Hmm...\n#API=repos/{{ github.repository }}/releases/latest\n#gh api $API -q \'""rel_id="" + (.id|tostring)\' >> $GITHUB_ENV\nprintf \'rel_id=${{ github.event.release.id }}\\n\' >> $GITHUB_ENV\n', 'API=repos/${{ github.repository }}/releases/tags/${{ inputs.tag }}\ngh api $API -q \'""rel_id="" + (.id|tostring)\' >> $GITHUB_ENV\n', 'NEW_CHART_DIR=$( mktemp -d )\npushd ""$NEW_CHART_DIR""\nASSET_URL=$( \\\n  gh api ""repos/${{ github.repository }}/releases/${rel_id}/assets"" \\\n  -q \'.[] | select(.name|test(""determined-helm-chart.*"")) | .url\' \\\n  )\n#CHART_NAME=$( gh api $ASSET_URL -q \'.name|sub(""-helm-chart"";"""")\' )\nCHART_NAME=$( gh api $ASSET_URL -q .name )\ngh api -H ""Accept: application/octet-stream"" ""$ASSET_URL"" \\\n  > ""$CHART_NAME""\npopd\n{\n  printf \'NEW_CHART_DIR=%s\\n\' ""$NEW_CHART_DIR""\n  printf \'CHART_NAME=%s\\n\' ""$CHART_NAME""\n  printf \'CHART_URL=%s\\n\' \\\n    ""$( gh api ""$ASSET_URL"" -q \'.browser_download_url\')""\n} >> $GITHUB_ENV\n', 'helm repo index ""$NEW_CHART_DIR"" --merge index.yaml --url=replaceme\n# diff returns 1 on difference, 2 on error, or 0 on no difference.\n# We want difference.\nif diff {""$NEW_CHART_DIR"",.}/index.yaml\nthen\n  echo ""No difference; chart probably already present""\n  exit -1\nelse\n  (( $? == 1 )) || exit $?\nfi\nsed ""s|replaceme.*|${CHART_URL}|"" \\\n  < ""$NEW_CHART_DIR""/index.yaml \\\n  > index.yaml\n', ""from os import environ\nfrom os.path import basename\nfrom string import Template\n\nchart_basename = basename( environ.get( 'CHART_NAME' ) )\nenviron['CHART_PERMALINK'] = '/' + chart_basename\nwith open('redirect_template.yaml', 'r') as src:\n  t = Template(src.read())\n\nwith open(chart_basename, 'w') as dst:\n  dst.write( t.safe_substitute(environ) )\n\nwith open( environ.get('GITHUB_ENV'), 'w+' ) as envfile:\n  envfile.write( f'CHART_REDIR_FILE={chart_basename}' )\n"", '# commit as github-actions[bot] the magic email address is necessary\n# to get the right icon to show up by the user on github. :)\ngit config user.name  github-actions\ngit config user.email \\\n  41898282+github-actions[bot]@users.noreply.github.com\ngit add ""$CHART_REDIR_FILE""\n# extract version from Chart.yaml\nv=$( tar xOzf $NEW_CHART_DIR/$CHART_NAME --wildcards \'*/Chart.yaml\' \\\n  | sed -n \'s/^version: *//ip\' )\nmsg=""Add chart \'$v\' due to ${{ github.event_name }}""\ngit commit index.yaml $CHART_REDIR_FILE -m ""$msg""\ngit push\n', 'cat ${{ steps.scan.outputs.sarif }}', 'exit 1', 'tools/scripts/track-pr pr-merged ""$PR_ID""', 'tools/scripts/track-pr pr-${{ github.event.action }} ""$PR_ID"" ""$PR_LABEL""']"
""
"['sudo apt update && sudo apt install -y pkg-config libhdf5-dev\npip3 install -q tensorflow==${{ matrix.tf-version }}\npip install -q protobuf==3.19.0\npip install -q requests\npip install -e .\n', 'pip install -q pytest\npip install -q pytest-cov\npip install -q python-coveralls\npytest --cov=deepmatch --cov-report=xml\n']"
['make build']
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest -v\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install flake8\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python main.py -i 23\n']"
"['sudo apt-get update', 'sudo apt-get install -y --no-install-recommends gobject-introspection gir1.2-glib-2.0 gir1.2-keybinder-3.0 gir1.2-gtk-3.0 gir1.2-vte-2.91 gir1.2-notify-0.7 gettext intltool libdbus-glib-1-dev libgirepository1.0-dev libcairo-dev xvfb\n', 'python -m pip install --upgrade pip\npip install -e .\npython setup.py develop\n', 'python -m compileall -f terminatorlib/ tests/ remotinator terminator', ""pip install -e '.[test]'\nxvfb-run -a python setup.py test\n""]"
"['set -xe\npython -VV\npython -m site\npython -m pip --quiet install --upgrade pip setuptools wheel\n', 'python -m pip --quiet install --upgrade twine\n', 'set -xe\npython setup.py sdist bdist_wheel\n', 'python3 -VV\npython3 -m site\n', 'python3 -m pip --quiet install --upgrade pip setuptools wheel\n', 'python3 -m pip --quiet install --upgrade ${{ env._PIP_PACKAGE_NAMES }}\n', '# Manually install ""mypy"" in the standard way.\npython3 -m pip --quiet install mypy\n# Log this ""mypy"" version for debuggability.\nmypy --version\n# Run this ""mypy"" instance against our main package.\nmypy ./beartype/\n', '# If the current platform is macOS, export a ""tox""-specific\n# environment variable circumventing ""pip"" installation issues by\n# instructing ""tox"" to reinstall already installed Python packages.\n# By default, ""tox"" avoids doing so for efficiency. This is required\n# to specifically circumvent installation of NumPy under macOS. As\n# discussed at numpy/numpy#15947, macOS bundles a patently broken\n# BLAS replacement called ""Accelerate"" causing NumPy to raise\n# exceptions on importation resembling:\n#     RuntimeError: Polyfit sanity test emitted a warning, most\n#     likely due to using a buggy Accelerate backend. If you compiled\n#     yourself, more information is available at\n#     https://numpy.org/doc/stable/user/building.html#accelerated-blas-lapack-libraries\n#     Otherwise report this to the vendor that provided NumPy.\n#     RankWarning: Polyfit may be poorly conditioned\n#\n# The kludge leveraged here is the canonical solution. See also:\n#     https://github.com/numpy/numpy/issues/15947#issuecomment-745428684\n#\n# Ideally, we would instead isolate setting this environment variable\n# in a prior step with sane GitHub Actions syntax: e.g.,\n#     if: ${{ matrix.platform }} == \'macos-latest\'\n#     env:\n#       _TOX_PIP_INSTALL_OPTIONS: \'--force-reinstall\'\n#\n# Sadly, the ""env:"" map only locally exports the environment\n# variables it declares to the current step. Thanks, GitHub Actions.\nif [[ ${{ matrix.platform }} == \'macos-latest\' ]]; then\n    export _TOX_PIP_INSTALL_OPTIONS=\'--force-reinstall\'\n    echo ""Massaging macOS dependencies with \\""pip install ${_TOX_PIP_INSTALL_OPTIONS}\\""...""\nfi\n# Dismantled, this is:\n# * ""--skip-missing-interpreters=false"" disables the corresponding\n#   ""skip_missing_interpreters = true"" setting globally enabled by\n#   our top-level ""tox.ini"" configuration, forcing CI failures for\n#   unavailable Python environments. See also:\n#       https://github.com/tox-dev/tox/issues/903\npython3 -m tox --skip-missing-interpreters=false\n']"
"['pip install autoflake isort black\n', 'autoflake --in-place --recursive --remove-all-unused-imports --remove-unused-variables --ignore-init-module-imports .\n', 'isort .\n', 'black --exclude ""exampleconfig\\.py"" .\n', 'sudo apt-get install libpq-dev libxml2-dev libxslt-dev python3\npython -m pip install --upgrade pip\npip install wheel flake8==5.0.4 flake8-print flake8-quotes\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors\nflake8 . --count --select=E999 --show-source --statistics --exclude=""exampleconfig.py""\n']"
"['sudo apt update\nsudo apt install ffmpeg\n', 'pip install -e .', 'auto-editor --debug\nauto-editor test all\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'cd ae-ffmpeg\nmv ae_ffmpeg/Windows ./\nmv ae_ffmpeg/Darwin-arm64 ./\n\npython setup.py bdist_wheel --plat-name=macosx_10_9_x86_64\ntwine upload dist/*\nrm -rf dist build\n\nmv Darwin-arm64 ae_ffmpeg\nmv ae_ffmpeg/Darwin-x86_64 ./\n\npython setup.py bdist_wheel --plat-name=macosx_11_0_arm64\ntwine upload dist/*\nrm -rf dist build\n\nmv Windows ae_ffmpeg\nmv ae_ffmpeg/Darwin-arm64 ./\n\npython setup.py bdist_wheel --plat-name=win_amd64\ntwine upload dist/*\nrm -rf dist build\n\nmv ae_ffmpeg/Windows ./\n\npython setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
""
"['python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'pip install --upgrade --editable .\npip install --upgrade --editable ./tools/awspolicygen\nexport GITHUB_REF=""${{ github.ref }}""\nexport GITHUB_REF_TYPE=""${{ github.ref_type }}""\nexport GITHUB_EVENT_NAME=""${{ github.event_name }}""\nexport API_TOKEN=""${{ secrets.API_TOKEN }}""\nexport SPACES_KEY=""${{ secrets.SPACES_KEY }}""\nexport SPACES_SECRET=""${{ secrets.SPACES_SECRET }}""\nexport AWS_ACCESS_KEY_ID=""${{ secrets.S3_RESOTOPUBLIC_AWS_ACCESS_KEY_ID }}""\nexport AWS_SECRET_ACCESS_KEY=""${{ secrets.S3_RESOTOPUBLIC_AWS_SECRET_ACCESS_KEY }}""\nawspolicygen --verbose --spaces-name somecdn --spaces-region ams3 --spaces-path resoto/aws/ --aws-s3-bucket resotopublic --aws-s3-bucket-path cf/\n', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n\npip install --upgrade --editable plugins/aws/\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n\npip install --upgrade --editable plugins/aws/\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n\npip install --upgrade --editable plugins/aws/\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install --upgrade --editable resotolib/\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install tox wheel flake8 build\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install tox wheel flake8 build\npip install --upgrade --editable resotolib/\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install tox wheel flake8 build\npip install --upgrade --editable resotolib/\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install tox wheel flake8 build\npip install --upgrade --editable resotolib/\n', 'tox', 'python -m build --sdist --wheel --outdir dist/', 'echo ""short=${GITHUB_SHA::7}"" >> $GITHUB_OUTPUT', 'echo ""Creating 20GB swap files for ARM builds""\nsudo swapoff -a\nsudo rm -f /mnt/swapfile\ntime sudo dd if=/dev/zero of=/mnt/swapfile bs=10M count=1000\nsudo chmod 600 /mnt/swapfile\nsudo mkswap /mnt/swapfile\nsudo swapon /mnt/swapfile\ntime sudo dd if=/dev/zero of=/swapfile bs=10M count=1000\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\nfree -m\n', 'GITHUB_REF=""${{ github.ref }}""\nGITHUB_TAG=${GITHUB_REF##*/}\nif [ ""${{ github.ref_type }}"" = tag ]; then\n    echo ""targets=linux/amd64,linux/arm64"" >> $GITHUB_OUTPUT\n    echo ""uitag=latest"" >> $GITHUB_OUTPUT\n    if [[ ""$GITHUB_TAG"" =~ [0-9]([ab]|rc)[0-9]* ]]; then\n      echo ""latest=false"" >> $GITHUB_OUTPUT\n    else\n      echo ""latest=true"" >> $GITHUB_OUTPUT\n    fi\nelse\n    echo ""targets=linux/amd64"" >> $GITHUB_OUTPUT\n    echo ""uitag=edge"" >> $GITHUB_OUTPUT\n    echo ""latest=false"" >> $GITHUB_OUTPUT\nfi\n', 'echo ${{ steps.sha.outputs.short }}\necho ${{ steps.platform.outputs.targets }}\necho ${{ steps.platform.outputs.uitag }}\necho ${{ steps.platform.outputs.latest }}\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements-test.txt\npip install resotolib/ plugins/aws/ plugins/digitalocean/ plugins/dockerhub/ plugins/example_collector/ plugins/gcp/ plugins/github/ plugins/k8s/ plugins/onelogin/ plugins/onprem/ plugins/posthog/ plugins/random/ plugins/scarf/ plugins/slack/ plugins/vsphere/\n', 'pytest test/core/model_check_test.py', 'yarn install --frozen-lockfile\n', 'cp resotocore/resotocore/static/api-doc.yaml resoto.com/openapi/resotocore-edge.yml\n', 'yarn gen-api-docs\n', 'wget -qO ResotoOrgList.json https://cdn.some.engineering/resoto/aws/edge/ResotoOrgList.json\nwget -qO ResotoCollect.json https://cdn.some.engineering/resoto/aws/edge/ResotoCollect.json\nwget -qO ResotoMutate.json https://cdn.some.engineering/resoto/aws/edge/ResotoMutate.json\n', 'yq -i \'(.services.resotoworker.environment += ""RESOTOWORKER_OVERRIDE=resotoworker.collector=example"")\' docker-compose.yaml\n', 'cd ${{ github.workspace }}/resoto.com/static/img/kroki\nfind . -type f -name ""*.svg"" -prune -exec rm {} \\+\n', 'PSK= RESOTOCORE_ANALYTICS_OPT_OUT=true docker-compose up -d\ncd ${{ github.workspace }}/resoto.com/docs/reference/unified-data-model\npython3 ${{ github.workspace }}/resoto.com/tools/export_models.py\n', 'yarn build\n', 'yarn optimize\nyarn format\n', 'GITHUB_REF=""${{ github.ref }}""\ntag=${GITHUB_REF##*/}\necho ""tag=${tag}"" >> $GITHUB_OUTPUT\n\nif [[ ${{ github.ref }} =~ ^refs/tags/[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n  echo ""prerelease=false"" >> $GITHUB_OUTPUT\n  echo ""docsVersion=$(cut -d \'.\' -f 1 <<< ${GITHUB_REF##*/}).X"" >> $GITHUB_OUTPUT\nelse\n  echo ""prerelease=true"" >> $GITHUB_OUTPUT\nfi\n', 'yarn install --frozen-lockfile\n', 'yarn run docusaurus docs:version ""${{ steps.release.outputs.docsVersion }}""\n', 'GITHUB_REF=""${{ github.ref }}""\ntag=${GITHUB_REF##*/}\nversion_major=$(cut -d \'.\' -f 1 <<< ${GITHUB_REF##*/})\nprev_release=$(echo $(gh api repos/someengineering/resoto/releases) | jq -r ""map(select(.tag_name | startswith(\\""${version_major}.\\"")) | select(.prerelease == false))[0].tag_name"")\n[ -n ""$prev_release"" ] && prev_release=$(echo $(gh api repos/someengineering/resoto/releases/latest) | jq -r \'.tag_name\')\ndir=""resoto.com/releases/${{ steps.release.outputs.tag }}""\nmkdir -p $dir\nfile=""${dir}/index.mdx""\npython3 tools/release_notes.py ${prev_release} ${{ steps.release.outputs.tag }} > $file\nlink=""/releases/${{ steps.release.outputs.tag }}""\necho $(jq \'.[""${{ steps.release.outputs.docsVersion }}""]=""${{ steps.release.outputs.tag }}""\' resoto.com/latestRelease.json) > resoto.com/latestRelease.json\necho ""tag=${{ steps.release.outputs.tag }}"" >> $GITHUB_OUTPUT\necho ""file=$file"" >> $GITHUB_OUTPUT\necho ""link=$link"" >> $GITHUB_OUTPUT\n', 'cp resotocore/resotocore/static/api-doc.yaml resoto.com/openapi/resotocore-${{ steps.release.outputs.docsVersion }}.yml\n', 'yarn gen-api-docs\n', 'wget -qO ResotoOrgList.json https://cdn.some.engineering/resoto/aws/${{ steps.release.outputs.tag }}/ResotoOrgList.json\nwget -qO ResotoCollect.json https://cdn.some.engineering/resoto/aws/${{ steps.release.outputs.tag }}/ResotoCollect.json\nwget -qO ResotoMutate.json https://cdn.some.engineering/resoto/aws/${{ steps.release.outputs.tag }}/ResotoMutate.json\n', 'yq -i \'(.services.[].image |= sub("":edge"", "":${{ steps.release.outputs.tag }}"")) | (.services.resotoworker.environment += ""RESOTOWORKER_OVERRIDE=resotoworker.collector=example"")\' docker-compose.yaml\n', 'cd ${{ github.workspace }}/resoto.com/static/img/kroki\nfind . -type f -name ""*.svg"" -prune -exec rm {} \\+\n', 'PSK= RESOTOCORE_ANALYTICS_OPT_OUT=true docker-compose up -d\ncd ${{ github.workspace }}/resoto.com/versioned_docs/version-${{ steps.release.outputs.docsVersion }}/reference/unified-data-model\npython3 ${{ github.workspace }}/resoto.com/tools/export_models.py\n', 'yarn build\n', 'yarn optimize\nyarn format\n', '[ ${{ steps.release.outputs.prerelease }} == \'false\' ] && echo -e ""### Release Notes\\n\\nhttps://resoto.com${{ steps.release_notes.outputs.link }}\\n"" > release_body.txt\necho -e ""### Docker Images\\n"" >> release_body.txt\necho -e ""- \\`somecr.io/someengineering/resotocore:${{ steps.release.outputs.tag }}\\`"" >> release_body.txt\necho -e ""- \\`somecr.io/someengineering/resotoworker:${{ steps.release.outputs.tag }}\\`"" >> release_body.txt\necho -e ""- \\`somecr.io/someengineering/resotoshell:${{ steps.release.outputs.tag }}\\`"" >> release_body.txt\necho -e ""- \\`somecr.io/someengineering/resotometrics:${{ steps.release.outputs.tag }}\\`"" >> release_body.txt\n', '(cd helm-charts && helm-docs)', 'python -m pip install --upgrade pip\npip install poetry\n', 'poetry build\n', 'python -m pip install --upgrade pip\npip install --upgrade --editable ../resotolib\npip install -r ../requirements-test.txt\npip install -r ../requirements.txt\n', 'coverage run --source resotocore -m pytest\ncoverage combine\ncoverage xml\n', './setup_venv.sh --dev --no-venv --no-plugins', 'black --line-length 120 --check resotocore tests', 'flake8 resotocore\npylint resotocore\n', 'mypy --install-types --non-interactive --strict resotocore tests', 'python -m pip install --upgrade pip\npip install --upgrade --editable ../resotolib\npip install -r ../requirements-test.txt\npip install -r ../requirements-dev.txt\npip install -r ../requirements.txt\n', 'pytest', 'python -m pip install --upgrade pip\npip install -r requirements-jupyterlite.txt\njupyter lite build --config jupyter_lite_config.json\n', 'python -m build --sdist --wheel --outdir dist/']"
"['pip install tox', 'tox -e lint', 'pip install tox', 'tox -e mypy', 'pip install tox', 'tox -e ${{ matrix.tox }}', 'pip install tox coveralls', 'tox -e coverage', 'coveralls --service=github']"
"['pip install shiv\n', 'python build_collector.py\n', 'python build_backend.py\n', 'pip install shiv\n', 'python build_collector.py\n', 'python build_backend.py\n', '7z a -tzip ""stormspotter-${{ matrix.os }}.zip"" ""backend/ssbackend.pyz"" ""stormcollector/sscollector.pyz"" ""stormcollector/cloud.cfg"" ""frontend/dist/spa/""\n']"
"['pip install setuptools==57.4.0\npip install torch\npip install recommonmark==0.7.1\npip install -e .[docs]\npip install Jinja2==2.11.3\npip install markupsafe==2.0.1\n', 'cd adapter_docs && make html-multi-version && cd ..\n', 'pip install torch==1.12.1\npip install .[quality]\n', 'make quality\nmake repo-consistency\n', 'pip install torch==1.12.1\npip install .[sklearn,testing,sentencepiece,vision]\npip install datasets\n', 'make test-adapter-methods\n', 'pip install torch==1.12.1\npip install .[sklearn,testing,sentencepiece,vision]\npip install datasets\n', 'make test-adapter-models\n']"
"['python -m pip install --upgrade pip\npython -m pip install pipenv==2022.10.4 pre-commit\npipenv install --dev --skip-lock\n', 'pre-commit install --install-hooks', 'GITGUARDIAN_API_KEY=${{ secrets.GITGUARDIAN_API_KEY }} pre-commit run --show-diff-on-failure --all-files', 'echo ""PIPENV_SKIP_LOCK=0"" >> $GITHUB_ENV\n', ""python -m pip install --upgrade pip\npython -m pip install --upgrade pipenv==2022.10.4\npipenv install --system --dev\n# Hack: workaround urllib3 still being installed on windows-2022 builder\npip install --force-reinstall 'urllib3<2'\n"", '# Those are win32-only dependencies from pytest\npython -m pip install atomicwrites colorama\n', 'echo ""GGTEST_DOCKER_IMAGE=mcr.microsoft.com/windows/nanoserver:ltsc2022"" >> $GITHUB_ENV\n', 'pip install wheel check-wheel-contents\npython setup.py clean --all bdist_wheel\n# The created wheel (.whl) file will be found and analyzed within the `dist/` folder\ncheck-wheel-contents dist/\n', 'coverage run --source ggshield -m pytest --disable-pytest-warnings --disable-socket tests/unit\n', 'coverage report --fail-under=80\ncoverage xml\n', 'make functest\n', 'curl -L https://github.com/goreleaser/nfpm/releases/download/v2.15.0/nfpm_amd64.deb -o nfpm_amd64.deb\nsudo dpkg -i nfpm_amd64.deb\n\npip install shiv==1.0.1\n', 'version=$(git describe --tags | sed -e \'s/^v//\' -e \'s/-[0-9]*-g/+/\')\necho ""Set version number to \'$version\'""\nsed -i ""s/__version__ = .*/__version__ = \\""$version\\""/"" ggshield/__init__.py\n', 'scripts/build-packages/build-packages', 'python --version\npython -m pip install --upgrade pip\npython -m pip install --upgrade pipenv\npipenv install --system --skip-lock\n', 'scripts/perfbench/perfbench setup\n', 'scripts/perfbench/perfbench run --repeats $REPEATS ${{ inputs.run_options }}\n', 'scripts/perfbench/perfbench report --max-delta $MAX_DELTA >> $GITHUB_STEP_SUMMARY\n', 'curl -L https://github.com/goreleaser/nfpm/releases/download/v2.15.0/nfpm_amd64.deb -o nfpm_amd64.deb\nsudo dpkg -i nfpm_amd64.deb\n\npip install shiv==1.0.1\n', 'scripts/build-packages/build-packages', 'echo ""tag=${GITHUB_REF/refs\\/tags\\//}"" >> $GITHUB_OUTPUT\n', 'gh release create --draft ${{ steps.tags.outputs.tag }}\n', 'gh release upload \\\n  ${{ steps.tags.outputs.tag }} \\\n  packages/ggshield-*.pyz \\\n  packages/ggshield_*.deb \\\n  packages/ggshield-*.rpm\n', 'version=${GITHUB_REF/refs\\/tags\\/v/}\n\ngit config user.name github-actions\ngit config user.email github-actions@github.com\n\nscripts/update-ggshield --commit ""$version""\n\ngit push\n', 'pip install cloudsmith-cli', 'scripts/push-to-cloudsmith\n']"
"['python -m pip install --upgrade pip\npip install tox\npip install "".[dev]""\n', 'tox', 'coverage combine\ncoverage report\ncoverage xml\n', 'python -m pip install --upgrade pip\npip install "".[dev]""\n', 'make -f Makefile.sphinx html\n']"
"['pip3 install mkdocs-material  mike\n', 'git config --global user.name Documentation Bot\ngit config --global user.email bot@github.com\n', 'mike deploy --push --update-aliases upstream', ""python3 -c 'import pcbnew; print(pcbnew.GetBuildVersion())'"", 'sudo apt-get -qq install --yes --no-install-recommends \\\n  zip inkscape make git libmagickwand-dev \\\n  libgraphicsmagick1-dev libmagickcore-dev \\\n  openscad bats\nsudo python -m pip install PcbDraw\nsudo env PYTHONPATH=$PYTHONPATH LD_LIBRARY_PATH=$LD_LIBRARY_PATH \\\n  python -m pip install -e .\\[dev\\]\n', 'make test', 'sudo env PYTHONPATH=$PYTHONPATH LD_LIBRARY_PATH=$LD_LIBRARY_PATH make package', 'make pcm', 'echo ${{ steps.docker_build.outputs.digest }}']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
"['python -m pip install --upgrade pip\npip install -r requirements/requirements-test.txt\n', '# Checks if need to reformat files\nblack --check --diff .\n', 'flake8 . --statistics\n', 'pytest tests/\n', 'python -m pip install sphinx sphinx-rtd-theme\ncd docs && sphinx-build -W source build\n', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'python setup.py clean --all\nrm dist/*']"
"['python -m pip install --upgrade pip\npython -m pip install surfaces flake8 pytest\npython -m pip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pytest tests -p no:warnings\n']"
"['make poetry-download', 'poetry config virtualenvs.in-project true\npoetry install\n', 'make check-codestyle\n', 'make test\n', 'make check-safety\n', 'git checkout HEAD^2']"
""
"['sudo apt-get update\nsudo apt-get install ffmpeg\nsudo apt install ffmpeg\n', 'brew install ffmpeg', 'choco install ffmpeg --no-progress', 'python -m pip install --upgrade pip\npip install pytest matplotlib==3.2 pandas plotly\n', 'pytest\n']"
"['git config --global core.symlinks true', 'pipx install -f \'poetry~=1.2.0\'\npoetry config virtualenvs.in-project true\n# use /home/runner to workaround \'act\' running as root for local development (HOME not available)\nsh -c ""$(curl --location https://taskfile.dev/install.sh)"" -- -d -b /home/runner/.local/bin\n', 'echo ""${RUNNER_CONTEXT}""\nwhich python\n', 'task web-components:install:ci web-components:build:local-report web-components:build:report link-local-report\n', 'pushd docs; poetry env use 3.10; popd\ntask docs:install\n', ""# We're only pushing to the server as a side-effect, we never actually use what we push.\n# TODO: Implement environment variables to avoid writing the token to disk.\npoetry run datapane login --token '${{ secrets.DOCS_BUILD_DATAPANE_API_KEY }}'\n# disable analytics within the docs, we know what's in here\nmkdir -p ~/.config/datapane && touch ~/.config/datapane/no_analytics\n"", 'task docs:build\n', ""if ( ! test -f site/index.html ); then\n  echo '::error title=Docs Artifacts not found::Missing the built docs site'\n  exit 1\nfi\n"", 'if ( test ""${HEAD_REPO}"" != ""${BASE_REPO}"" ); then\n  # Looks like a fork, don\'t let them touch the normal branches\n  _branch=""${HEAD_REPO}/${DOCS_BRANCH}""\nelse\n  _branch=""${DOCS_BRANCH}""\nfi\nnpx wrangler pages publish \\\n  --project-name datapane-docs \\\n  --branch ""$_branch"" \\\n  --commit-hash ""$DOCS_COMMIT"" \\\n  site\n', 'git config --global core.symlinks true', 'echo ""${RUNNER_CONTEXT}""\nwhich python\n', 'python -c \'from pip._internal.locations import USER_CACHE_DIR; print(f""PIP_DIR={USER_CACHE_DIR}"")\' >> ${GITHUB_ENV}', 'python -c \'import sys; print(f""PYTHON_VERSION={sys.hexversion}"")\' >> ${GITHUB_ENV}', ""pip install 'poetry~=1.2.0'\npoetry config virtualenvs.in-project true\n# rmdir --ignore-fail-on-non-empty -v .venv\npoetry install -E plotting -E cloud\n# Run the type checker\npoetry run python -m mypy src/\n# run CLI tests (junit tests not captured yet)\npoetry run python -m pytest -v --ignore=tests/client/e2e/ tests --junitxml=${{github.workspace}}/test-reports/datapane-local-${{ matrix.os }}-${{ matrix.python-version }}.xml\n"", 'python -m pip install --upgrade pip\npip install pre-commit\npip freeze --local\n', 'SKIP=no-commit-to-branch pre-commit run --show-diff-on-failure --color=always --all-files', 'pre-commit run --show-diff-on-failure --color=always --all-files', 'python -c \'from pip._internal.locations import USER_CACHE_DIR; print(f""PIP_DIR={USER_CACHE_DIR}"")\' >> ${GITHUB_ENV}', 'python -c \'import sys; print(f""PYTHON_VERSION={sys.hexversion}"")\' >> ${GITHUB_ENV}', 'pip install \'poetry~=1.2.0\'\npoetry config virtualenvs.in-project true\npoetry install\n# --config \'lunr_search={""fuzziness"": 1, ""index_docstrings"": True}\'\npoetry run pdoc --html --config show_source_code=False --config list_class_variables_in_index=False --config sort_identifiers=False --config \'google_analytics=""UA-167952366-2""\' --config show_inherited_members=True -f --output-dir ./docs1 datapane.client.api\n# HACK for lunr js sarch not working fully - see https://github.com/pdoc3/pdoc/issues/250\n# mv -v docs1/datapane/client/api/* docs\nmkdir ./docs\nmv -t ./docs ./docs1/datapane/client/api/*\n']"
[]
"['nix-env -if . -A mach-nix', 'export MACHNIX_VERSION=$GITHUB_SHA\nmach-nix --version\necho ""tensorflow"" > reqs.txt\nmach-nix env ./env -r reqs.txt\n', 'WORKERS=5 nix run .#tests-unit\n', 'WORKERS=5 nix run .#tests-eval\n']"
""
"['python -m pip install httpx yt-dlp .', 'python tests/sync/extras.py', 'python tests/async/extras.py', 'python -m pip install httpx .', 'python tests/sync/playlists.py', 'python tests/async/playlists.py', 'python -m pip install httpx .', 'python tests/sync/search.py', 'python tests/async/search.py', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['docker build . --file Dockerfile --tag xueyikang/stealer', 'docker login -u xueyikang -p ${{ secrets.CREDENTIAL }}', 'docker push  xueyikang/stealer']"
"['case ""${{ runner.os }}"" in\nLinux)\n  sudo apt-get install -yy  libsamplerate0 libsndfile1\n  ;;\nmacOS)\n  brew install libsamplerate\n  ;;\nesac\n', 'python -m pip install --upgrade pip\npip install -U numpy\npip install -r requirements.txt\npip install -e "".[dev,lint,test]""\n\n# Install NSF implementations for PWG\npip install git+https://github.com/nnsvs/ParallelWaveGAN@nnsvs\n# Install hn-uSFGAN\npip install git+https://github.com/nnsvs/HN-UnifiedSourceFilterGAN@nnsvs\n# Install SiFi-GAN\npip install git+https://github.com/nnsvs/SiFiGAN@nnsvs\n', 'pysen run lint\n', 'pip install pytest pytest-cov\npytest --cov=./ --cov-report=xml\n', 'case ""${{ runner.os }}"" in\nLinux)\n  ./tests/test_recipes.sh\n  ;;\nesac\n']"
[]
"['python -m pip install --upgrade pip wheel\npip install -r requirements.txt coremltools openvino-dev tensorflow-cpu --extra-index-url https://download.pytorch.org/whl/cpu\npython --version\npip --version\npip list\n', 'python benchmarks.py --data coco128.yaml --weights ${{ matrix.model }}.pt --img 320 --hard-fail 0.29\n', 'python benchmarks.py --data coco128-seg.yaml --weights ${{ matrix.model }}-seg.pt --img 320 --hard-fail 0.22\n', 'python export.py --weights ${{ matrix.model }}-cls.pt --include onnx --img 224\npython detect.py --weights ${{ matrix.model }}.onnx --img 320\npython segment/predict.py --weights ${{ matrix.model }}-seg.onnx --img 320\npython classify/predict.py --weights ${{ matrix.model }}-cls.onnx --img 224\n', 'python -m pip install --upgrade pip wheel\nif [ ""${{ matrix.torch }}"" == ""1.7.0"" ]; then\n    pip install -r requirements.txt torch==1.7.0 torchvision==0.8.1 --extra-index-url https://download.pytorch.org/whl/cpu\nelse\n    pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu\nfi\n', 'python -c ""import utils; utils.notebook_init()""\necho ""RUNNER_OS is ${{ runner.os }}""\necho ""GITHUB_EVENT_NAME is ${{ github.event_name }}""\necho ""GITHUB_WORKFLOW is ${{ github.workflow }}""\necho ""GITHUB_ACTOR is ${{ github.actor }}""\necho ""GITHUB_REPOSITORY is ${{ github.repository }}""\necho ""GITHUB_REPOSITORY_OWNER is ${{ github.repository_owner }}""\npython --version\npip --version\npip list\n', '# export PYTHONPATH=""$PWD""  # to run \'$ python *.py\' files in subdirectories\nm=${{ matrix.model }}  # official weights\nb=runs/train/exp/weights/best  # best.pt checkpoint\npython train.py --imgsz 64 --batch 32 --weights $m.pt --cfg $m.yaml --epochs 1 --device cpu  # train\nfor d in cpu; do  # devices\n  for w in $m $b; do  # weights\n    python val.py --imgsz 64 --batch 32 --weights $w.pt --device $d  # val\n    python detect.py --imgsz 64 --weights $w.pt --device $d  # detect\n  done\ndone\npython hubconf.py --model $m  # hub\n# python models/tf.py --weights $m.pt  # build TF model\npython models/yolo.py --cfg $m.yaml  # build PyTorch model\npython export.py --weights $m.pt --img 64 --include torchscript  # export\npython - <<EOF\nimport torch\nim = torch.zeros([1, 3, 64, 64])\nfor path in \'$m\', \'$b\':\n    model = torch.hub.load(\'.\', \'custom\', path=path, source=\'local\')\n    print(model(\'data/images/bus.jpg\'))\n    model(im)  # warmup, build grids for trace\n    torch.jit.trace(model, [im])\nEOF\n', ""m=${{ matrix.model }}-seg  # official weights\nb=runs/train-seg/exp/weights/best  # best.pt checkpoint\npython segment/train.py --imgsz 64 --batch 32 --weights $m.pt --cfg $m.yaml --epochs 1 --device cpu  # train\npython segment/train.py --imgsz 64 --batch 32 --weights '' --cfg $m.yaml --epochs 1 --device cpu  # train\nfor d in cpu; do  # devices\n  for w in $m $b; do  # weights\n    python segment/val.py --imgsz 64 --batch 32 --weights $w.pt --device $d  # val\n    python segment/predict.py --imgsz 64 --weights $w.pt --device $d  # predict\n    python export.py --weights $w.pt --img 64 --include torchscript --device $d  # export\n  done\ndone\n"", ""m=${{ matrix.model }}-cls.pt  # official weights\nb=runs/train-cls/exp/weights/best.pt  # best.pt checkpoint\npython classify/train.py --imgsz 32 --model $m --data mnist160 --epochs 1  # train\npython classify/val.py --imgsz 32 --weights $b --data ../datasets/mnist160  # val\npython classify/predict.py --imgsz 32 --weights $b --source ../datasets/mnist160/test/7/60.png  # predict\npython classify/predict.py --imgsz 32 --weights $m --source data/images/bus.jpg  # predict\npython export.py --weights $b --img 64 --include torchscript  # export\npython - <<EOF\nimport torch\nfor path in '$m', '$b':\n    model = torch.hub.load('.', 'custom', path=path, source='local')\nEOF\n"", 'LYCHEE_URL=$(curl -s https://api.github.com/repos/lycheeverse/lychee/releases/latest | grep ""browser_download_url"" | grep ""x86_64-unknown-linux-gnu.tar.gz"" | cut -d \'""\' -f 4)\ncurl -L $LYCHEE_URL -o lychee.tar.gz\ntar xzf lychee.tar.gz\nsudo mv lychee /usr/local/bin\n']"
""
"['pipx install poetry\npoetry config virtualenvs.prefer-active-python true\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH\n', 'echo ""date=$(/bin/date -u ""+%m%w%Y"")"" >> $GITHUB_OUTPUT\n', '# start xvfb in background\nsudo /usr/bin/Xvfb $DISPLAY -screen 0 1280x1024x24 &\n', 'tinyTexPackages=$(python -c ""import json;print(\' \'.join(json.load(open(\'.github/manimdependency.json\'))[\'macos\'][\'tinytex\']))"")\nIFS=\' \'\nread -a ttp <<< ""$tinyTexPackages""\noriPath=$PATH\nsudo mkdir -p $PWD/macos-cache\necho ""Install TinyTeX""\nsudo curl -L -o ""/tmp/TinyTeX.tgz"" ""https://github.com/yihui/tinytex-releases/releases/download/daily/TinyTeX-1.tgz""\nsudo tar zxf ""/tmp/TinyTeX.tgz"" -C ""$PWD/macos-cache""\nexport PATH=""$PWD/macos-cache/TinyTeX/bin/universal-darwin:$PATH""\nsudo tlmgr update --self\nfor i in ""${ttp[@]}""; do\n  sudo tlmgr install ""$i""\ndone\nexport PATH=""$oriPath""\necho ""Completed TinyTeX""\n', 'brew install cairo', 'echo ""/Library/TeX/texbin"" >> $GITHUB_PATH\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\necho ""$PWD/macos-cache/TinyTeX/bin/universal-darwin"" >> $GITHUB_PATH\n', '$tinyTexPackages = $(python -c ""import json;print(\' \'.join(json.load(open(\'.github/manimdependency.json\'))[\'windows\'][\'tinytex\']))"") -Split \' \'\n$OriPath = $env:PATH\necho ""Install Tinytex""\nInvoke-WebRequest ""https://github.com/yihui/tinytex-releases/releases/download/daily/TinyTeX-1.zip"" -O ""$($env:TMP)\\TinyTex.zip""\nExpand-Archive -LiteralPath ""$($env:TMP)\\TinyTex.zip"" -DestinationPath ""$($PWD)\\ManimCache\\LatexWindows""\n$env:Path = ""$($PWD)\\ManimCache\\LatexWindows\\TinyTeX\\bin\\windows;$($env:PATH)""\ntlmgr update --self\nforeach ($c in $tinyTexPackages){\n  $c=$c.Trim()\n  tlmgr install $c\n}\n$env:PATH=$OriPath\necho ""Completed Latex""\n', '$env:Path += "";"" + ""$($PWD)\\ManimCache\\LatexWindows\\TinyTeX\\bin\\windows""\n$env:Path = ""$env:USERPROFILE\\.poetry\\bin;$($env:PATH)""\necho ""$env:Path"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\n', 'poetry config experimental.new-installer false\npoetry install\n', 'poetry run python -m pytest\n', 'poetry run python -m pytest -v --cov-append --ignore-glob=""*opengl*"" --doctest-modules manim\n', 'cd docs && poetry run make doctest O=-tskip-manim\n', 'import os\nref_tag = os.getenv(\'tag_act\').split(\'/\')[-1]\nwith open(os.getenv(\'GITHUB_OUTPUT\'), \'w\') as f:\n  print(f""tag_name={ref_tag}"", file=f)\n', 'python -m pip install --upgrade poetry', 'poetry config http-basic.pypi __token__ ""$PYPI_API_TOKEN""', 'poetry publish --build\npoetry build\n', 'pip install requests', 'import requests\nimport os\nref_tag = os.getenv(\'tag_act\').split(\'/\')[-1]\naccess_token = os.getenv(\'access_token\')\nheaders = {\n    ""Accept"":""application/vnd.github.v3+json"",\n    ""Authorization"": f""token {access_token}""\n}\nurl = f""https://api.github.com/repos/ManimCommunity/manim/releases/tags/{ref_tag}""\nc = requests.get(url,headers=headers)\nupload_url=c.json()[\'upload_url\']\nwith open(os.getenv(\'GITHUB_OUTPUT\'), \'w\') as f:\n  print(f""upload_url={upload_url}"", file=f)\n  print(f""tag_name={ref_tag[1:]}"", file=f)\n']"
""
"['echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y git make gcc\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make test_aux', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends git make gcc\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make data_tests', 'set -ex\nbase=""ghcr.io/coqui-ai/tts""\ntags="""" # PR build\n\nif [[ ${{ matrix.base }} = ""python:3.10.8-slim"" ]]; then\n  base=""ghcr.io/coqui-ai/tts-cpu""\nfi\n\nif [[ ""${{ startsWith(github.ref, \'refs/heads/\') }}"" = ""true"" ]]; then\n  # Push to branch\n  github_ref=""${{ github.ref }}""\n  branch=${github_ref#*refs/heads/} # strip prefix to get branch name\n  tags=""${base}:${branch},${base}:${{ github.sha }},""\nelif [[ ""${{ startsWith(github.ref, \'refs/tags/\') }}"" = ""true"" ]]; then\n  VERSION=""v$(cat TTS/VERSION)""\n  if [[ ""${{ github.ref }}"" != ""refs/tags/${VERSION}"" ]]; then\n    echo ""Pushed tag does not match VERSION file. Aborting push.""\n    exit 1\n  fi\n  tags=""${base}:${VERSION},${base}:latest,${base}:${{ github.sha }}""\nfi\necho ""::set-output name=tags::${tags}""\n', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0\n', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends git make gcc\nsudo apt-get install espeak-ng\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make inference_tests', 'set -ex\nversion=$(cat TTS/VERSION)\ntag=""${GITHUB_REF/refs\\/tags\\/}""\nif [[ ""v$version"" != ""$tag"" ]]; then\n  exit 1\nfi\n', 'python -m pip install -U pip setuptools wheel build\n', 'python -m build\n', 'pip install dist/*.tar.gz\n', 'python -m pip install -U pip setuptools wheel build\npython -m pip install -r requirements.txt\n', 'python setup.py bdist_wheel --plat-name=manylinux1_x86_64\npython -m pip install dist/*-manylinux*.whl\n', 'mkdir dist\n', 'ls -lh dist/\n', 'cat << EOF > ~/.pypirc\n[pypi]\nusername=__token__\npassword=${{ secrets.PYPI_TOKEN }}\nEOF\n', 'python -m pip install twine\n', 'twine upload --repository pypi dist/*\n', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'sudo apt-get update\nsudo apt-get install -y git make gcc\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make lint', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends git make gcc\nsudo apt-get install espeak\nsudo apt-get install espeak-ng\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make test_text', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends git make gcc\nsudo apt-get install espeak\nsudo apt-get install espeak-ng\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make test_tts', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y git make gcc\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'make test_vocoder', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y git make gcc\nsudo apt-get install espeak espeak-ng\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'nose2 -F -v -B TTS tests.zoo_tests.test_models.test_models_offset_0_step_3\nnose2 -F -v -B TTS tests.zoo_tests.test_models.test_voice_conversion\n', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y git make gcc\nsudo apt-get install espeak espeak-ng\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'nose2 -F -v -B --with-coverage --coverage TTS tests.zoo_tests.test_models.test_models_offset_1_step_3', 'echo ""${{ github.event.head_commit.message }}""', 'cat /etc/os-release', 'export TRAINER_TELEMETRY=0', 'sudo apt-get update\nsudo apt-get install -y git make gcc\nsudo apt-get install espeak espeak-ng\nmake system-deps\n', 'python3 -m pip install --upgrade pip setuptools wheel', ""sed -i 's/https:\\/\\/coqui.gateway.scarf.sh\\//https:\\/\\/github.com\\/coqui-ai\\/TTS\\/releases\\/download\\//g' TTS/.models.json\n"", 'python3 -m pip install .[all]\npython3 setup.py egg_info\n', 'nose2 -F -v -B --with-coverage --coverage TTS tests.zoo_tests.test_models.test_models_offset_2_step_3']"
"['sudo apt update\nsudo apt install \\\n  fontconfig    \\\n  imagemagick   \\\n  libgeos++-dev \\\n  libproj-dev   \\\n  poppler-utils\npython -m pip install --upgrade pip\npip install -r requirements/requirements.txt\n', 'sudo apt update\nsudo apt install \\\n  texlive-base              \\\n  texlive-extra-utils       \\\n  texlive-fonts-extra       \\\n  texlive-fonts-recommended \\\n  texlive-latex-base        \\\n  texlive-latex-extra       \\\n  texlive-latex-recommended \\\n  texlive-xetex\n', ""# adjust the ImageMagick policies to convert PDF to PNG\n# remove all policies restricting Ghostscript ability to process files\n# https://stackoverflow.com/q/52998331\n# https://stackoverflow.com/a/59193253\nsudo sed -i '/disable ghostscript format types/,+6d' /etc/ImageMagick-6/policy.xml\n#\nmake -C fonts/\ncp -r fonts/ /usr/share/fonts/\nfc-cache\nmake all\n"", 'make check\n']"
"['python -m pip install --upgrade pip\n# cpu version of pytorch\npip install torch==1.11+cpu -f https://download.pytorch.org/whl/torch_stable.html\n\n# Install Atari Roms\npip install autorom\nwget https://gist.githubusercontent.com/jjshoots/61b22aefce4456920ba99f2c36906eda/raw/00046ac3403768bfe45857610a3d333b8e35e026/Roms.tar.gz.b64\nbase64 Roms.tar.gz.b64 --decode &> Roms.tar.gz\nAutoROM --accept-license --source-file Roms.tar.gz\n\npip install .[extra_no_roms,tests,docs]\n# Use headless version\npip install opencv-python-headless\n', 'make lint\n', 'make doc\n', 'make check-codestyle\n', 'make type\n', 'make pytest\n']"
"['pip install flit', 'flit install --symlink', 'flit publish', 'git branch -a', 'pip install flit', 'flit install --symlink', 'pytest --cov=ninja --cov-report=xml tests', 'pip install ""Django${{ matrix.django-version }}"" pydantic', 'pip install pytest pytest-asyncio pytest-django psycopg2-binary', 'pytest', 'pip install flit', 'flit install --symlink', 'pytest --cov=ninja', 'pip install flit', 'flit install --symlink', 'black --check ninja tests', 'isort --check ninja tests', 'flake8 ninja tests', 'mypy ninja']"
""
"['echo ::set-output name=tag::${GITHUB_REF#refs/*/}', 'echo ${{ steps.docker_build.outputs.digest }}']"
"['make all', 'make website\nmake publish-website\naws cloudfront create-invalidation --distribution-id $AWS_CLOUDFRONT_DISTRIBUTION_ID --paths ""/*""\n', 'sleep 50s\ncurl -X POST -H ""Content-Type: application/json"" --user $ALGOLIA_CRAWLER_USER_ID:$ALGOLIA_CRAWLER_API_KEY  https://crawler.algolia.com/api/1/crawlers/$ALGOLIA_CRAWLER_ID/reindex\n', 'make publish-lightwave', 'make publish-university', 'make publish-pycharm', 'cd tools/vscode-extension && make setup', 'make publish-vsc-extension', 'make setup-ui build-ui\nmake setup-py-tests\nmake setup-vsc\nmake setup-e2e\nmake build-server\n', 'make test-e2e-ci\nmake test-py-ci\nmake test-vsc-ci\n', '.\\setup.ps1', '.\\test.ps1', 'brew install docker\ncolima start\n', 'make setup-ui build-ui\nmake setup-py-tests\nmake setup-vsc\nmake setup-e2e\nmake build-server\n', 'make test-e2e-macos-ci\nmake test-py-ci\nmake test-vsc-ci\n', 'echo ""VERSION=${{ github.event.inputs.version || env.VERSION }}"" >> $GITHUB_ENV\necho ""TAG=${{ github.event.inputs.tag || env.TAG }}"" >> $GITHUB_ENV\n', 'make setup', 'make test-ui-ci\nmake test-intellij-ci\n', 'make release-nightly', 'git tag -f ${{ env.TAG }} && git push -f --tags', 'cd ts && npm ci && npm publish', 'make setup', 'make release', 'make test-ui-ci\nmake test-py-ci\n', 'make publish-website\naws cloudfront create-invalidation --distribution-id $AWS_CLOUDFRONT_DISTRIBUTION_ID --paths ""/*""\n', 'make publish-release-s3', 'make build-apps publish-apps-s3-hac', 'make publish-apps-s3-mc', 'curl -X POST -H ""Content-Type: application/json"" --user $ALGOLIA_CRAWLER_USER_ID:$ALGOLIA_CRAWLER_API_KEY  https://crawler.algolia.com/api/1/crawlers/$ALGOLIA_CRAWLER_ID/reindex\n', 'make publish-university', 'make release-db-windows', 'make release-db-linux', 'make release-db-darwin', 'GOFLAGS=""-e"" snyk test --file=go.mod -d --fail-on=all\n', 'echo ""any_changed=true"" >> $GITHUB_OUTPUT', 'pip install setuptools wheel httpx uvicorn starlette pdoc3 pytest flake8\npip install -r py/examples/requirements.txt\npip freeze > py/requirements.txt\ncat py/requirements.txt\n', 'snyk test --org=wave --remote-repo-url=wave/${{ steps.extract_ref.outputs.ref }} --file=py/requirements.txt --project-name=WAVE/wave/${{ steps.extract_ref.outputs.ref }}/py/requirements.txt -d --fail-on=all --policy-path=.snyk', 'echo ""ref=$(echo ${GITHUB_REF##*/})"" >> $GITHUB_OUTPUT', 'GOFLAGS=""-e"" snyk monitor --org=wave --remote-repo-url=wave/${{ steps.extract_ref.outputs.ref }} --file=go.mod --project-name=WAVE/wave/${{ steps.extract_ref.outputs.ref }}/go.mod -d --fail-on=all\n', 'pip install setuptools wheel httpx uvicorn starlette pdoc3 pytest flake8\npip install -r py/examples/requirements.txt\npip freeze > py/requirements.txt\ncat py/requirements.txt\n', 'snyk monitor --org=wave --remote-repo-url=wave/${{ steps.extract_ref.outputs.ref }} --file=py/requirements.txt --project-name=WAVE/wave/${{ steps.extract_ref.outputs.ref }}/py/requirements.txt -d --fail-on=all']"
"['docker run --mount ""type=bind,src=$(pwd),dst=/tmp/acme"" \\\n    -w ""/tmp/acme"" --rm ${{ matrix.docker-image }} /bin/bash test.sh\n', 'pip install --upgrade pip setuptools twine\n', 'python setup.py sdist\ntwine upload dist/*\n']"
[]
"['poetry install --with=tests', 'poetry run pytest tests -v --cov=./ --cov-report=xml', 'poetry install --with=docs', 'poetry run sphinx-build docs/source _site', 'python -m pip install flake8', 'python -m pip install isort', 'poetry install --with=docs,pyright', 'python3 -m pip install --upgrade pip\npip3 install -e .\npip3 install pytest\npip3 install tabulate # required to md export\npip3 install kaleido # required for plotly export\n', 'pytest tests/test_model_performance.py\n', 'echo ""## Model Benchmark"" >> report.md\npython tests/metrics/compareMetrics.py >> report.md\n', 'echo ""<details>\\n<summary>Model training plots</summary>\\n"" >> report.md\necho ""## Model Training"" >> report.md\necho ""### PeytonManning"" >> report.md\ncml asset publish tests/metrics/PeytonManning.svg --md >> report.md\necho ""### YosemiteTemps"" >> report.md\ncml asset publish tests/metrics/YosemiteTemps.svg --md >> report.md\necho ""### AirPassengers"" >> report.md\ncml asset publish tests/metrics/AirPassengers.svg --md >> report.md\necho ""\\n</details>"" >> report.md\n# Post reports as comments in GitHub PRs\ncml comment update --pr report.md # post to PR\ncml check create --title=ModelReport report.md # update status of check in PR\n', 'poetry install --with=docs', 'poetry run sphinx-build docs/source _site']"
""
[]
"['python3 -m pip install --upgrade pip\npython3 -m pip install -r requirements/dev.txt\npython3 -m pip install numpy\n', 'curl -LO ""https://github.com/bazelbuild/bazelisk/releases/download/v1.15.0/bazelisk-linux-amd64""\necho ""19fd84262d5ef0cb958bcf01ad79b528566d8fef07ca56906c5c516630a0220b  bazelisk-linux-amd64"" | sha256sum --check\nmkdir -p ""${GITHUB_WORKSPACE}/bin/""\nmv bazelisk-linux-amd64 ""${GITHUB_WORKSPACE}/bin/bazel""\nchmod +x ""${GITHUB_WORKSPACE}/bin/bazel""\n', '""${GITHUB_WORKSPACE}/bin/bazel"" --bazelrc=.bazelrc.travis build -- //... -//experiments/v3/protos:*\n', '""${GITHUB_WORKSPACE}/bin/bazel"" --bazelrc=.bazelrc.travis test --test_output=errors -- //... -//experiments/v3/protos:*\n', 'python setup.py install', 'bean-check --help\nbean-doctor --help\nbean-example --help\nbean-format --help\n', 'python setup.py sdist --format zip', 'pip install dist/beancount-*.zip', 'bean-check --help\nbean-doctor --help\nbean-example --help\nbean-format --help\n', 'pip install -r requirements/dev.txt', 'make lint', 'pip install -r requirements/dev.txt', 'python setup.py build_ext -i', 'make ctest', 'make test', 'git describe --tags', ""# Check that release tag and package version agree.\nimport os, sys\ntag = os.getenv('TAG')\nprint(f'git tag: {tag}')\nversion = open('beancount/VERSION').read().strip()\nprint(f'package version: {version}')\nif tag != version:\n    sys.exit(1)\n"", 'python -m pip install wheel twine', 'python setup.py bdist_wheel', 'twine upload dist/beancount-$TAG-*.whl', 'python setup.py build_ext']"
""
""
"['sudo apt-get install -y xvfb libdbus-1-3 libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xinput0 libxcb-xfixes0\n', 'pip install black codespell flake8 isort pytest', 'black --check . || true', 'codespell --quiet-level=2 -x ./guietta/examples/progress_bar.py', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'isort --profile black . || true', 'sudo apt-get install -y xvfb libdbus-1-3 libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xinput0 libxcb-xfixes0\n', 'pip install black codespell flake8 isort pytest', 'pip install -r requirements_github.txt', 'xvfb-run pytest']"
"['pip install poetry', 'poetry install\npoetry build\n', '# Test that package publishing is going to work with the testpypi\n# instance first before trying to package plugins.\npoetry config pypi-token.testpypi ${{ secrets.TEST_PYPI_TOKEN }}\npoetry config repositories.testpypi https://test.pypi.org/legacy/\npoetry publish --repository testpypi\n', ""# Have pwncat download all plugins needed\npoetry run pwncat-cs --download-plugins\n\n# They are stored in ~/.local/share/pwncat by default\ntar czvf pwncat-plugins.tar.gz --transform='s|.*pwncat/||' ~/.local/share/pwncat/*\n"", '# Everything looks good, publish to pypi\npoetry config pypi-token.pypi ${{ secrets.PYPI_TOKEN }}\npoetry publish\n', '# Grab tag name without the `v`\nversion=$(git describe --tags --abbrev=0 | sed \'s/v//\')\n\n# build a changelog with just logs from this release\necho ""## Changelog"" >> this_version_changelog.md\ncat CHANGELOG.md | sed -n \'/^## \\[\'""$version""\'\\]/,/^## /p\' | head -n -1 | tail -n +2 >> this_version_changelog.md\necho ""[Full Changelog](https://github.com/calebstewart/pwncat/blob/v$version/CHANGELOG.md)"" >> this_version_changelog.md\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npython setup.py install --user\n', 'pytest\n']"
"['pip install black', 'black --check .', 'black .\ngit config --global user.name github-actions\ngit config --global user.email \'${GITHUB_ACTOR}@users.noreply.github.com\'          \ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit commit -am ""fixup! Format Python code with psf/black push""\ngit push --force origin HEAD:$GITHUB_REF\n', 'python -m pip install --upgrade pip\npip install flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
""
"['python -m pip install --upgrade pip\n\n# Install Atari Roms\npip install autorom\nwget https://gist.githubusercontent.com/jjshoots/61b22aefce4456920ba99f2c36906eda/raw/00046ac3403768bfe45857610a3d333b8e35e026/Roms.tar.gz.b64\nbase64 Roms.tar.gz.b64 --decode &> Roms.tar.gz\nAutoROM --accept-license --source-file Roms.tar.gz\n\n# cpu version of pytorch - faster to download\npip install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\npip install pybullet==3.2.5\n# for v4 MuJoCo envs:\npip install mujoco\npip install -r requirements.txt\n# Use headless version\npip install opencv-python-headless\n# install parking-env to test HER\npip install highway-env==1.8.1\npip install -e .\n', 'make lint\n', 'make check-codestyle\n', 'make doc\n', 'make type\n', 'make pytest\n', 'python -m pip install --upgrade pip\n\n# Install Atari Roms\npip install autorom\nwget https://gist.githubusercontent.com/jjshoots/61b22aefce4456920ba99f2c36906eda/raw/00046ac3403768bfe45857610a3d333b8e35e026/Roms.tar.gz.b64\nbase64 Roms.tar.gz.b64 --decode &> Roms.tar.gz\nAutoROM --accept-license --source-file Roms.tar.gz\n\n# cpu version of pytorch - faster to download\npip install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\npip install pybullet==3.2.5\npip install -r requirements.txt\n# Use headless version\npip install opencv-python-headless\npip install highway-env==1.8.1\n# Add support for pickle5 protocol\n# TODO: remove me when dropping python 3.7\npip install pickle5\npip install -e .\n', 'make check-trained-agents\n']"
"['python -m pip install --upgrade pip\npython -m pip install ""poetry>=1.2.0a1""\npython -m poetry plugin add poetry-version-plugin \n', 'python -m poetry config virtualenvs.in-project true', 'python -m poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'python -m poetry install', 'python -m poetry config pypi-token.pypi $PYPI_TOKEN\nbash scripts/publish.sh\n', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")', 'python -m pip install --upgrade pip\npython -m pip install ""poetry>=1.2.0a1""\npython -m poetry plugin add poetry-version-plugin \n', 'python -m poetry config virtualenvs.in-project true', 'python -m poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'python -m poetry install', 'python -m poetry run bash scripts/test.sh']"
"['python -m pip install --upgrade pip\npip install Cython numpy==1.19.5 wheel setuptools\n', 'python setup.py sdist bdist_wheel -p manylinux1_x86_64\n', 'python setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install Cython black==22.3.0 mypy==0.942 pylint==2.13.5 yapf isort\n', 'pip install -e .\n', './scripts/format\n', './scripts/lint\n', 'pip install torch==1.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'brew install libomp\n', 'python -m pip install --upgrade pip\npip install numpy Cython pytest-cov gym==0.17.2 flask onnxruntime stable-baselines3\npip install git+https://github.com/takuseno/d4rl-atari\npip install git+https://github.com/takuseno/d4rl-pybullet\n', 'pip install -U protobuf==3.20.1  # temporarily dependency fix\npip install -e .\npython setup.py build_ext --inplace --define CYTHON_TRACE\n', 'mkdir -p test_data\npytest --cov-report=xml --cov=d3rlpy --cov-config=.coveragerc tests -p no:warnings -v\n', 'bash <(curl -s https://codecov.io/bash)\n']"
"['pip install -e .\nfsociety --info\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel --universal\ntwine upload dist/*\n']"
['pip install -r requirements.txt\npython -m unittest\n']
"['pipx install poetry', 'poetry install', 'poetry run poe style', 'poetry run poe lint', 'poetry run poe types', 'poetry run poe docs-check', 'pipx install poetry', 'poetry install', 'poetry run pytest -v', 'pipx install poetry', 'poetry build', 'poetry publish -n', 'pipx install poetry', 'poetry install --without dev', 'poetry run poe docs']"
""
""
""
"['pip install ""tox~=3.27.1"" flit', 'export VERSION_STR=$(echo ${{ matrix.python-version }} | sed -e ""s/\\.//g"")\nexport TOX_SKIP_ENV=$(printf \'^(?!py%s-)\' $VERSION_STR)\ntox --parallel auto\n', 'pip install flit\npip install "".[test]""\n', 'set -e\ncoverage run -m pytest -v\ncoverage report -m\ncoverage xml\n', 'pipx install poetry\npipx install flit\n', 'echo ""$(grep -v \'odmantic =\' ./pyproject.toml)"" > pyproject.toml\npoetry install\n', 'poetry run pip install ../odmantic-current/', './scripts/start.sh &\n# Wait for the server\nwhile ! curl ""http://localhost:8000/health"" > /dev/null 2>&1\ndo\n  sleep 1;\ndone\necho ""Server ready.""\n', './realworld/api/run-api-tests.sh', 'pip install flit\npip install "".[doc]""\n', 'mkdocs build -f ./mkdocs.yml', 'pip install flit\npip install "".[doc]""\n', 'mkdocs build -f ./mkdocs.yml', 'rm .git/hooks/post-commit .git/hooks/pre-push']"
""
""
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'cd app/ReadabiliPy\nnpm i\ncd ../..\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pytest\n']"
[]
""
"['conda --version', 'which python', 'conda install -y scipy\nconda install -c conda-forge networkit==7.1\npip install codecov\npip install pytest\npython setup.py install\n', 'pip install -e .[test]\n', 'python -m pytest\n', 'pip install coverage\ncoverage run -m pytest\ncoverage xml\n']"
"['sudo systemctl start mysql.service', 'pip install -U pip poetry\npoetry config virtualenvs.create false\n', 'make ci', 'pip install -U pip poetry\npoetry config virtualenvs.create false\n', 'make build']"
""
""
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython setup.py develop --user\n', 'cd norminette && sh run_test.sh\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist\ntwine upload dist/*\n', 'python setup.py sdist\ntwine upload --repository testpypi dist/*']"
"['python -m pip install --upgrade pip setuptools\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install tensorflow-cpu==2.11.0\npip install -e "".[tests]"" --progress-bar off --upgrade\n', 'python build_deps/configure.py\nbazel build keras_cv/custom_ops:all\ncp bazel-bin/keras_cv/custom_ops/*.so keras_cv/custom_ops/\n', 'pytest keras_cv/ --ignore keras_cv/models/legacy/ --durations 0\n', 'python -m pip install --upgrade pip setuptools\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install tensorflow==2.11.0\npip install -e "".[tests]"" --progress-bar off --upgrade\n', 'bash shell/lint.sh', 'python -m pip install --upgrade pip setuptools wheel auditwheel twine\necho ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install tensorflow-cpu==${{ matrix.tf-version }}\npython -m pip install -e "".[tests]"" --progress-bar off --upgrade\n', 'python build_deps/configure.py\n', ""python -m pip uninstall -y tensorflow-cpu\npython -m pip install --platform=macosx_12_0_arm64 --no-deps --target=$(python -c 'import site; print(site.getsitepackages()[0])') --upgrade tensorflow-macos==${{ matrix.tf-version }}\n"", 'export BUILD_WITH_CUSTOM_OPS=true\nbazel build build_pip_pkg\n', 'bazel build --cpu=darwin_arm64 --copt -mmacosx-version-min=12.0 --linkopt -mmacosx-version-min=12.0 build_pip_pkg\n', 'export BUILD_WITH_CUSTOM_OPS=true\nbazel-bin/build_pip_pkg wheels\n', 'python -m pip install --upgrade patchelf==0.14\nbash build_deps/tf_auditwheel_patch.sh\npython -m auditwheel repair --plat manylinux2014_x86_64 wheels/*.whl\nrm wheels/*.whl\nmv wheelhouse/* wheels/\n', 'twine upload wheels/*\n', 'pip install tensorflow==2.11.0\npython -m pip install --upgrade pip setuptools wheel twine\nexport BUILD_WITH_CUSTOM_OPS=false\npython setup.py sdist bdist_wheel\n', 'twine upload dist/*.whl\n']"
"['set -euo pipefail\n# Get the commit payload from GH Actions event.\n# https://docs.github.com/en/developers/webhooks-and-events/events/\n# github-event-types#pushevent\ncommits=\'${{ toJSON(github.event.commits) }}\'\n# Exit with 0 if no new commit is found.\nif [[ $commits =~ ""null"" ]]; then\n    echo ""No commit found. Exiting...""\nexit 0\nfi\n# Get the unique messages from the commits event.\nparsed=$(echo -n ""$commits"" | jq -r "".[].message"" | sort -u)\nmtch=\'(, refs|, closes) #[0-9]+\'\necho ""$parsed"" | while IFS= read -r raw_line; do\n    line=$(echo ""$raw_line"" | tr -d ""\\r\\n"")\n    # Ignore empty lines.\n    if [[ -z ""$line"" ]]; then\n        continue\n    # Check with regex if the commit message contains \'refs #issue_number\'\n    # or \'closes #issue_number\'. If not, exit with an error.\n    elif [[ ""$line"" =~ $mtch ]]; then\n        echo ""Commit message: $line âœ…""\n    else\n        echo ""Commit message: $line âŒ""\n        echo -n ""Commit message must contain ""\n        echo -n ""\'refs #issue_number\' or \'closes #issue_number\'.""\n        exit 1\n    fi\ndone\n', 'gh pr merge --auto --merge ""$PR_URL""', 'echo ""Installing the dependencies...""\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\n', 'echo ""Checking linter formatting...""\nmake lint-check\n', 'echo ""Running the tests...""\npython -m pytest -v -s\n', 'echo ::set-output name=tag::${GITHUB_REF#refs/*/}', 'echo ""Running container...""\ndocker run -d -p 5000:5000 --expose 5000 ${{ env.TEST_TAG }}\n']"
"['echo ${{ secrets.DOCKER_SCRIPT }} | base64 -d > auto_docker_ver.py\npip install toml\necho ""::set-output name=DOCKER_TAG::$(python auto_docker_ver.py)""\n', 'poetry install\npython -m pip install --upgrade pip\npip install black\n']"
""
"['sudo apt-get install -y libzbar0\n', 'brew install zbar\n', 'python -m pip install --upgrade pip\npip install -U -r requirements-dev.txt\npip install -U .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=200 --statistics\n', 'mypy --install-types --non-interactive src/*.py tests/*.py\nmypy --strict src/*.py tests/*.py\n', 'pytest', 'pytest --cov=extract_otp_secrets_test --junitxml=pytest.xml --cov-report=term-missing | tee pytest-coverage.txt', 'sudo apt-get install -y libzbar0\npython -m pip install --upgrade pip\npip install -U -r requirements-dev.txt\npip install -U .\npytest\n', 'echo ""extract_otp_secrets digests: ${{ steps.docker_build_qr_reader_latest.outputs.digest }}""\necho ""${{ steps.docker_build_qr_reader_latest.outputs.digest }}"" > digests.txt\n', 'sudo apt-get install -y libzbar0\npython -m pip install --upgrade pip\npip install -U -r requirements-dev.txt\npip install -U .\npytest\n', 'echo ""extract_otp_secrets:only-txt digests: ${{ steps.docker_build_only_txt.outputs.digest }}""\necho ""${{ steps.docker_build_qr_reader_latest.outputs.digest }}"" > digests.txt\n', 'sudo apt-get install -y libzbar0\npython -m pip install --upgrade pip\npip install -U -r requirements-dev.txt\npip install -U .\npytest\n', 'echo ""extract_otp_secrets digests: ${{ steps.docker_build_qr_reader_latest.outputs.digest }}""\necho ""${{ steps.docker_build_qr_reader_latest.outputs.digest }}"" > digests.txt\n', 'echo ""date=$(TZ=Europe/Zurich date +\'%d.%m.%Y\')"" >> $GITHUB_OUTPUT\necho ""version=${TAG_NAME/v/}"" >> $GITHUB_OUTPUT\necho ""inline_version=${TAG_NAME/v/_}"" >> $GITHUB_OUTPUT\necho ""tag_name=${{ github.ref_name }}"" >> $GITHUB_OUTPUT\necho ""tag_message=$(git tag -l --format=\'%(contents:subject)\' ${{ github.ref_name }})"" >> $GITHUB_OUTPUT\n', 'echo ""date: ${{ steps.meta.outputs.date }}""\necho ""version: ${{ steps.meta.outputs.version }}""\necho ""inline_version: ${{ steps.meta.outputs.inline_version }}""\necho ""tag_name: ${{ steps.meta.outputs.tag_name }}""\necho ""tag_message: ${{ steps.meta.outputs.tag_message }}""\nresponse=$(curl \\\n  -X POST \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}""\\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  https://api.github.com/repos/scito/extract_otp_secrets/releases \\\n  --silent \\\n  --show-error \\\n  -d \'{""tag_name"":""${{ github.ref }}"",""target_commitish"":""master"",""name"":""${{ steps.meta.outputs.version }} - ${{ steps.meta.outputs.date }}"",""body"":""${{ steps.meta.outputs.tag_message }}\\n\\n## Executables\\n\\nDownload the executable for your platform and execute it, see [README.md](https://github.com/scito/extract_otp_secrets#readme)\\n\\n | Executable | Description |\\n | --- | --- |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_linux_x86_64 | Linux x86_64/amd64 (glibc >= 2.28) |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_linux_arm64 | Linux arm64 (glibc >= 2.28) |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_win_x86_64.exe | Windows x86_64/amd64/x64 |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_win_arm64.exe | N/A |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_macos_x86_64.dmg | N/A, see [README.md](https://github.com/scito/extract_otp_secrets#readme) |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_macos_x86_64.pkg | N/A, see [README.md](https://github.com/scito/extract_otp_secrets#readme) |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_macos_x86_64 | MacOS x86_64/amd64 (bare executable, see [README.md](https://github.com/scito/extract_otp_secrets#readme); optional libzbar must be installed manually, see [README.md](https://github.com/scito/extract_otp_secrets#readme)) |\\n | extract_otp_secrets${{ steps.meta.outputs.inline_version }}_macos_arm64 | N/A |\\n"",""draft"":true,""prerelease"":false,""generate_release_notes"":true}\')\necho upload_url=$(jq \'.upload_url\' <<< ""$response"") >> $GITHUB_OUTPUT\necho $(jq -r \'.upload_url\' <<< ""$response"") > release_url.txt\necho $(jq -r \'.id\' <<< ""$response"") > release_id.txt\n', 'sudo apt-get install -y libzbar0\npython -m pip install --upgrade pip\npip install -U -r requirements-dev.txt\npip install -U .\npytest\n', 'echo ""extract_otp_secrets: ${{ steps.docker_build_buster.outputs.digest }}""\n', 'docker run --pull always --rm --privileged multiarch/qemu-user-static --reset -p yes\ndocker run --platform ${{ matrix.PLATFORM }} --pull always --entrypoint /bin/bash --rm -v ""$(pwd)"":/files -w /files scit0/extract_otp_secrets:buster -c \'apt-get update && apt-get -y install binutils && pip install -U -r /files/requirements.txt && pip install pyinstaller && PYTHONHASHSEED=31 && pyinstaller -y --add-data /usr/local/__yolo_v3_qr_detector/:__yolo_v3_qr_detector/ --onefile --name ${{ matrix.EXE }} --distpath /files/dist/ /files/src/extract_otp_secrets.py\'\n', 'dist/${{ matrix.EXE }} -V\ndist/${{ matrix.EXE }} -h\ndist/${{ matrix.EXE }} example_export.png\ndist/${{ matrix.EXE }} - < example_export.txt\ndist/${{ matrix.EXE }} --qr ZBAR example_export.png\ndist/${{ matrix.EXE }} --qr QREADER example_export.png\ndist/${{ matrix.EXE }} --qr QREADER_DEEP example_export.png\ndist/${{ matrix.EXE }} --qr CV2 example_export.png\ndist/${{ matrix.EXE }} --qr CV2_WECHAT example_export.png\n', 'docker run --platform ${{ matrix.PLATFORM }} --pull always --entrypoint /bin/bash --rm -v ""$(pwd)"":/files -w /files scit0/extract_otp_secrets -c \'dist/${{ matrix.EXE }} -V && dist/${{ matrix.EXE }} -h && dist/${{ matrix.EXE }} example_export.png && dist/${{ matrix.EXE }} - < example_export.txt && dist/${{ matrix.EXE }} --qr ZBAR example_export.png && dist/${{ matrix.EXE }} --qr QREADER example_export.png && dist/${{ matrix.EXE }} --qr QREADER_DEEP example_export.png && dist/${{ matrix.EXE }} --qr CV2 example_export.png && dist/${{ matrix.EXE }} --qr CV2_WECHAT example_export.png\'\n', 'ls -R', 'response=$(curl \\\n  -X POST \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Content-Type: application/x-executable"" \\\n  -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}""\\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  --silent \\\n  --show-error \\\n  --data-binary @dist/${{ matrix.EXE }} \\\n  $(cat release_url.txt)=${{ matrix.ASSET_NAME }})\n', 'echo ""$($Env:Path)""', 'ls ""$($Env:WinDir)\\system32""', 'echo ""macos_python_path=/Library/Frameworks/Python.framework/Versions/3.11"" >> $GITHUB_ENV', 'sudo apt-get install -y libzbar0\n', 'brew install zbar create-dmg\n', 'python -m pip install --upgrade pip\npip install -U -r requirements-dev.txt\npip install -U .\n', 'mkdir -p build/\nVERSION_STR=$(setuptools-git-versioning) VERSION_MAJOR=$(cut -d \'.\' -f 1 <<< ""$(setuptools-git-versioning)"") VERSION_MINOR=$(cut -d \'.\' -f 2 <<< ""$(setuptools-git-versioning)"") VERSION_PATCH=$(echo $(cut -d \'.\' -f 3 <<< ""$(setuptools-git-versioning)"") | sed -E -n ""s/^([0-9]+).*/\\1/p"") VERSION_BUILD=$(echo $(cut -d \'.\' -f 3 <<< ""$(setuptools-git-versioning)"") | sed -E -n -e""s/^[0-9]+.+/99/p"")$(($(git rev-list --count $(git tag | sort -V -r | sed \'1!d\')..HEAD))) COPYRIGHT_YEARS=\'2020-2023\' envsubst < installer/win_file_version_info_template.txt > build/win_file_version_info.txt\n', '${{ matrix.CMD_BUILD }}', 'dist/${{ matrix.EXE }} -V\ndist/${{ matrix.EXE }} -h\ndist/${{ matrix.EXE }} example_export.png\ndist/${{ matrix.EXE }} --qr ZBAR example_export.png\ndist/${{ matrix.EXE }} --qr QREADER example_export.png\ndist/${{ matrix.EXE }} --qr QREADER_DEEP example_export.png\ndist/${{ matrix.EXE }} --qr CV2 example_export.png\ndist/${{ matrix.EXE }} --qr CV2_WECHAT example_export.png\n', 'dist/${{ matrix.EXE }} - < example_export.txt\n', 'ls -R', 'echo ""release_id=$(cat release_id.txt)"" >> $GITHUB_OUTPUT\necho ""upload_url=https://uploads.github.com/repos/scito/extract_otp_secrets/releases/$(cat release_id.txt)/assets?name="" >> $GITHUB_OUTPUT\n', 'curl -X POST -H ""Accept: application/vnd.github+json"" -H ""Content-Type: ${{ matrix.ASSET_MIME }}"" -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" -H ""X-GitHub-Api-Version: 2022-11-28"" --show-error --data-binary @dist/${{ matrix.EXE }} ${{ steps.meta.outputs.upload_url }}=${{ matrix.ASSET_NAME }}\n', 'curl -X POST -H ""Accept: application/vnd.github+json"" -H ""Content-Type: ${{ matrix.ASSET_MIME }}"" -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" -H ""X-GitHub-Api-Version: 2022-11-28"" --show-error --data-binary @dist/${{ matrix.DMG }} ${{ steps.meta.outputs.upload_url }}=${{ matrix.ASSET_NAME_DMG }}\n', 'echo ""release_id=$(cat release_id.txt)"" >> $GITHUB_OUTPUT\necho ""upload_url=https://uploads.github.com/repos/scito/extract_otp_secrets/releases/$(cat release_id.txt)/assets?name="" >> $GITHUB_OUTPUT\n', 'GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}\nfor asset_url in $(curl \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer $GITHUB_TOKEN""\\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  --silent \\\n  --show-error \\\n  https://api.github.com/repos/scito/extract_otp_secrets/releases/${{ steps.meta.outputs.release_id }}/assets |\njq -r \'.[].url\'); do\n    echo ""Download $asset_url""\n    name=$(curl \\\n        -H ""Accept: application/vnd.github+json"" \\\n        -H ""Authorization: Bearer $GITHUB_TOKEN""\\\n        -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n        --output-dir assets \\\n        -L \\\n        $asset_url |\n        jq -r \'.name\')\n    curl \\\n        -H ""Accept: application/octet-stream"" \\\n        -H ""Authorization: Bearer $GITHUB_TOKEN""\\\n        -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n        --create-dirs \\\n        --output-dir assets \\\n        -L \\\n        -o $name \\\n        $asset_url\ndone\n(cd assets/ && sha256sum * > ../sha256_hashes.txt)\ncurl -X POST -H ""Accept: application/vnd.github+json"" -H ""Content-Type: text/plain"" -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" -H ""X-GitHub-Api-Version: 2022-11-28"" --show-error --data-binary @sha256_hashes.txt ${{ steps.meta.outputs.upload_url }}=sha256_hashes.txt\n\n(cd assets/ && sha512sum * > ../sha512_hashes.txt)\ncurl -X POST -H ""Accept: application/vnd.github+json"" -H ""Content-Type: text/plain"" -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" -H ""X-GitHub-Api-Version: 2022-11-28"" --show-error --data-binary @sha512_hashes.txt ${{ steps.meta.outputs.upload_url }}=sha512_hashes.txt\n']"
"['echo Hello, world!', 'echo Add other actions to build,\necho test, and deploy your project.\n', 'python -m pip install --upgrade pip\npip install flake8 \nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['echo ${{ steps.docker_build.outputs.digest }}', 'python -m pip install --upgrade pip\npython -m pip install -r test-requirements.txt\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest --reruns 3 --reruns-delay 5\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip3 install .\npython3 ./utils/update_site_data.py --empty-only\n', 'git config --global user.name ""Maigret autoupdate""\ngit config --global user.email ""soxoj@protonmail.com""\necho `git name-rev ${{ github.event.pull_request.head.sha }} --name-only`\nexport BRANCH=`git name-rev ${{ github.event.pull_request.head.sha }} --name-only | sed \'s/remotes\\/origin\\///\'`\necho $BRANCH\ngit remote -v\ngit checkout $BRANCH\ngit add sites.md\ngit commit -m ""Updated site list and statistics""\ngit push origin $BRANCH']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
""
"['pip install pytest\npip install -e . # The bare layoutparser module \npytest tests_deps/test_file_utils.py\n', 'pip install pytest\npip install -e "".[effdet]""\npytest tests_deps/test_only_effdet.py\n', 'pip install pytest\npip install -e .\npip install torchvision && pip install ""git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2""\npytest tests_deps/test_only_detectron2.py\n', 'pip install pytest\npip install -e "".[paddledetection]""\npytest tests_deps/test_only_paddledetection.py\n', 'python -m pip install --upgrade pip\npip install .\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --ignore F821\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', '# Install additional requirements when running tests\npip install "".[effdet]""\npip install -r dev-requirements.txt\npytest tests\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npython -m pip install wheel\npip install -r requirements.txt\n', 'pip install black\nblack --check .\n', 'date -u +\'%Y-%m-%dT%H:%M:%SZ\' > TIMESTAMP\necho ""${GITHUB_SHA}"" | cut -c1-8 > SHORT_SHA\necho ""source"" > IMAGE_TAG\necho ""uzayg"" > DOCKER_USERNAME\necho ""docker.io/uzayg/archivy"" > DOCKER_IMAGE\n', 'echo ""${DOCKER_PASSWORD}"" | docker login --username ""$( cat DOCKER_USERNAME )"" --password-stdin docker.io\n', 'docker build \\\n  --output type=image,name=""$(cat DOCKER_IMAGE)"",push=true \\\n  --build-arg BUILD_DATE=""$( cat TIMESTAMP )"" --build-arg VCS_REF=""$( cat SHORT_SHA )"" \\\n  --tag ""$( cat DOCKER_IMAGE ):$( cat IMAGE_TAG )"" \\\n  --file ./Dockerfile.source .\n', 'date -u +\'%Y-%m-%dT%H:%M:%SZ\' > TIMESTAMP\necho ""$GITHUB_SHA"" | cut -c1-8 > SHORT_SHA\necho ""$GITHUB_REF"" | cut -d / -f 3 > VERSION\necho ""uzayg"" > DOCKER_USERNAME\necho ""docker.io/uzayg/archivy"" > DOCKER_IMAGE\n', 'echo ""${DOCKER_PASSWORD}"" | docker login --username ""$( cat DOCKER_USERNAME )"" --password-stdin docker.io\n', 'docker build \\\n  --output type=image,name=""$( cat DOCKER_IMAGE )"",push=true \\\n  --build-arg VERSION=""$( cat VERSION )"" --build-arg BUILD_DATE=""$( cat TIMESTAMP )"" \\\n  --build-arg VCS_REF=""$( cat SHORT_SHA )"" \\\n  --tag ""$( cat DOCKER_IMAGE ):$( cat VERSION )"" \\\n  --tag ""$( cat DOCKER_IMAGE ):latest"" \\\n  --file ./Dockerfile . \\\n&& docker build \\\n  --output type=image,name=""$( cat DOCKER_IMAGE )"",push=true \\\n  --build-arg VERSION=""$( cat VERSION )"" --build-arg BUILD_DATE=""$( cat TIMESTAMP )"" \\\n  --build-arg VCS_REF=""$( cat SHORT_SHA )"" \\\n  --tag ""$( cat DOCKER_IMAGE ):$( cat VERSION )-lite"" \\\n  --tag ""$( cat DOCKER_IMAGE ):latest-lite"" \\\n  --file ./Dockerfile .\n', 'date -u +\'%Y-%m-%dT%H:%M:%SZ\' > TIMESTAMP\necho ""${{ github.event.inputs.version }}"" > VERSION\necho ""uzayg"" > DOCKER_USERNAME\necho ""docker.io/uzayg/archivy"" > DOCKER_IMAGE\n', 'echo ""${DOCKER_PASSWORD}"" | docker login --username ""$( cat DOCKER_USERNAME )"" --password-stdin docker.io\n', 'docker build \\\n  --output type=image,name=""$( cat DOCKER_IMAGE )"",push=true \\\n  --build-arg VERSION=""$( cat VERSION )"" --build-arg BUILD_DATE=""$( cat TIMESTAMP )"" \\\n  --tag ""$( cat DOCKER_IMAGE ):$( cat VERSION )"" \\\n  --file ./Dockerfile . \\\n&& docker build \\\n  --output type=image,name=""$( cat DOCKER_IMAGE )"",push=true \\\n  --build-arg VERSION=""$( cat VERSION )"" --build-arg BUILD_DATE=""$( cat TIMESTAMP )"" \\\n  --tag ""$( cat DOCKER_IMAGE ):$( cat VERSION )-lite"" \\\n  --file ./Dockerfile-light .\n', 'sudo apt-get install ripgrep\npip install -r requirements-tests.txt\npip install .\npip install tests/test_plugin\npytest --cov=./ --cov-report=xml\n', 'python -m pip install -r main/docs/requirements.txt', 'python -m pip install main/', 'cd docs-website && mkdocs gh-deploy --config-file ../main/mkdocs.yml --remote-branch main', 'python -m pip install --user --upgrade setuptools wheel', 'python setup.py sdist bdist_wheel']"
"['sudo apt-get install -y libyaml-dev', 'python -m pip install --upgrade pip setuptools', 'pip install -e .[build]', 'python ./scripts/cache-ruleset.py ./rules/ ./cache/', 'pyinstaller --log-level DEBUG .github/pyinstaller/pyinstaller.spec', 'dist/capa ""tests/data/Practical Malware Analysis Lab 01-01.dll_""', 'dist/capa ""tests/data/499c2a85f6e8142c3f48d4251c9c7cd6.raw32""', 'dist/capa ""tests/data/7351f8a40c5450557b24622417fc478d.elf_""', 'chmod +x ${{ matrix.artifact_name }}', './${{ matrix.artifact_name }} -h', 'chmod +x ${{ matrix.artifact_name }}', 'echo ""zip_name=capa-${GITHUB_REF#refs/tags/}-${{ matrix.asset_name }}.zip"" >> $GITHUB_ENV', 'zip ${{ env.zip_name }} ${{ matrix.artifact_name }}', 'echo $FILES | grep -qF \'CHANGELOG.md\' || echo $PR_BODY | grep -qiF ""$NO_CHANGELOG""\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload --skip-existing dist/*\n', '# user information is needed to create annotated tags (with a message)\ngit config user.email \'capa-dev@mandiant.com\'\ngit config user.name \'Capa Bot\'\nname=${{ github.event.release.tag_name }}\ngit tag $name -m ""https://github.com/mandiant/capa/releases/$name""\n# TODO update branch name-major=${name%%.*}\n', ""number=$(grep '\\- *$' CHANGELOG.md | wc -l)\nif [ $number != 1 ]; then exit 1; fi\n"", 'pip install -e .[dev]', 'ruff check --config .github/ruff.toml .', 'isort --profile black --length-sort --line-width 120 --skip-glob ""*_pb2.py"" -c .', 'black -l 120 --extend-exclude "".*_pb2.py"" --check .', 'pycodestyle --exclude=""*_pb2.py"" --show-source capa/ scripts/ tests/', 'mypy --config-file .github/mypy/mypy.ini --check-untyped-defs capa/ scripts/ tests/', 'pip install -e .', 'python scripts/lint.py rules/', 'sudo apt-get install -y libyaml-dev', 'pip install -e .[dev]', 'pytest -v tests/', 'sudo apt-get install -y libyaml-dev', 'pip install -e .[dev]', 'mkdir ./.github/binja\ncurl ""https://raw.githubusercontent.com/Vector35/binaryninja-api/6812c97/scripts/download_headless.py"" -o ./.github/binja/download_headless.py\npython ./.github/binja/download_headless.py --serial ${{ env.BN_SERIAL }} --output .github/binja/BinaryNinja-headless.zip\nunzip .github/binja/BinaryNinja-headless.zip -d .github/binja/\npython .github/binja/binaryninja/scripts/install_api.py --install-on-root --silent\n', 'pytest -v tests/test_binja_features.py']"
"['python -m pip install --upgrade pip\npip install -r tools/requirements.txt\npython -c ""import nltk; nltk.download(\'wordnet\')""\n', 'cd tools\npytest\ncd ..\n', 'python tools/data_builder.py', 'python -m pip install --upgrade pip\npip install -r tools/requirements.txt\npython -c ""import nltk; nltk.download(\'wordnet\')""\n', 'pytest tools/.\n', 'python -m pip install --upgrade pip\npip install -r tools/requirements.txt\npython -c ""import nltk; nltk.download(\'wordnet\')""\n', 'echo github_ref: ${{ github.ref }}\npython tools/data_rewriter.py\ngit add reports\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit remote set-url origin https://x-access-token:${{ secrets.UBERSHMEKEL_ALT_TOKEN }}@github.com/${{ github.repository }}\ngit commit -m ""Automated data fixes"" -a | exit 0\ngit push\n']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements/ci.txt\npython -m pip install -e .\n', 'python -m pytest --pyargs redengine\n', 'python -m pip install --upgrade pip\npython -m pip install .[test]\n', 'python -m pytest --pyargs rocketry -W error::UserWarning -W error::FutureWarning -W error::DeprecationWarning\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\npython -m pip install -r requirements/coverage.txt\n', 'python -m pytest --cov=rocketry --cov-report=xml rocketry/test --no-build\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements/docs.txt\n', 'python -m sphinx docs ""docs/_build/html"" --color -W -bhtml\npython -m sphinx -b doctest docs ""docs/_build/html""\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements/ci.txt\npython -m pip install -e .\n', 'python -m pytest --pyargs rocketry\n']"
"['pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmselfsup -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmselfsup -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6\n', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'pip install -e .\n', 'python.exe -m pip install --upgrade pip', 'pip install lmdb', 'pip install torch==1.8.1+${{matrix.platform}} torchvision==0.9.1+${{matrix.platform}} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'pip install -e .\n', 'pytest tests/\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmselfsup -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'pip install -e . -v\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y ffmpeg libturbojpeg ninja-build', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'rm -rf .eggs\npip install -e . -U\n', 'coverage run --branch --source mmselfsup -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y ffmpeg libturbojpeg ninja-build', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'rm -rf .eggs\npip install -e . -U\n', 'coverage run --branch --source mmselfsup -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python.exe -m pip install --upgrade pip', 'pip install lmdb', 'pip install torch==1.8.1+${{matrix.platform}} torchvision==0.9.1+${{matrix.platform}} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', ""pip install -U openmim\nmim install mmengine\nmim install 'mmcv>=2.0.0'\nmim install 'mmdet>=3.0.0rc0'\nmim install 'mmsegmentation>=1.0.0rc0'\npip install -r requirements/tests.txt\n"", 'pip install -e .\n', 'pytest tests/\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmselfsup']"
"['python -m pip install --upgrade pip\npip install protobuf==3.19.0\npip install hyperopt==0.2.5\npip install pytest\npip install dgl==0.9.1\npip install xgboost\npip install community\npip install networkx\npip install python-louvain\npip install lightgbm\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nconda list\nconda install -c conda-forge faiss-cpu\npip install torch-scatter -f https://data.pyg.org/whl/torch-`python -c ""import torch;print(torch.__version__)""`.html\npip install setuptools==59.5.0\npip install plotly\n', 'python run_recbole.py --model=BPR --epochs=2\n', 'python -m pytest -v tests/metrics\n', 'python -m pytest -v tests/data\n', 'python -m pytest -v tests/evaluation_setting\n', 'python -m pytest -v tests/model/test_model_auto.py\n', 'python -m pytest -v tests/config/test_config.py\nexport PYTHONPATH=.\npython tests/config/test_command_line.py --use_gpu=False --valid_metric=Recall@10 --split_ratio=[0.7,0.2,0.1] --metrics=\'[""Recall""]\' --topk=[10] --epochs=200 --eval_setting=\'LO_RS\' --learning_rate=0.3\n', 'python -m pytest -v tests/hyper_tuning/test_hyper_tuning.py\n', 'pip install black[jupyter]\n']"
"['git remote add upstream https://github.com/tiny-pilot/tinypilot.git\ngit fetch upstream master:upstream-master\ngit reset --hard upstream-master\n', 'set -ux\nCOMMUNITY_PR_URL=""$(curl \\\n  --silent \\\n  --show-error \\\n  https://api.github.com/repos/tiny-pilot/tinypilot/commits/${{ github.sha }}/pulls?per_page=1 | \\\n  jq \\\n    --raw-output \\\n    \'.[0].html_url | select (.!=null)\')""\nreadonly COMMUNITY_PR_URL\necho ""COMMUNITY_PR_URL=${COMMUNITY_PR_URL}"" >> ""${GITHUB_ENV}""\n']"
"[""echo REPOSITORY=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]') >> $GITHUB_ENV"", 'changelog=$(git log $(git tag | tail -2 | head -1)..HEAD --no-merges --oneline)\nchangelog=""${changelog//\'%\'/\'%25\'}""\nchangelog=""${changelog//$\'\\n\'/\'%0A\'}""\nchangelog=""${changelog//$\'\\r\'/\'%0D\'}""\necho ""##[set-output name=changelog;]${changelog}""\n', 'python -m pip install -r requirements.txt\npython -m pip install pyinstaller==5.9.0\ncd src\npython build_package.py\n', 'python -m pip install -r requirements.txt\npython -m pip install pyinstaller==5.9.0\ncd src\npython build_package.py\n']"
"['conda --version', 'which python', 'python -m pip install torch==1.10.1+cpu torchvision==0.11.2+cpu torchaudio==0.10.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\npython -m pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cpu.html\npython -m pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cpu.html\npython -m pip install torch-geometric\npython -m pip install sphinx sphinx_rtd_theme\n', 'python -m pip install -e .[test]\n', 'python -m pytest\n', 'pip install coverage\ncoverage run -m pytest\ncoverage xml\n']"
"['echo Changed files: ${{ steps.get_file_changes.outputs.files }}\n', 'python -m pip install --upgrade pip\npip install pylint numpy wheel\n', 'echo ""${{ steps.get_file_changes.outputs.files}}"" | tr "" "" ""\\n"" | grep "".py$"" | xargs pylint --rcfile=.pylintrc\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -e .[docs]\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n# Disable flake checks initially.\n# flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --ignore=E111,E731,F401\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
['cd src && cp config-sample.yaml config.yaml']
"['git checkout HEAD^2', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest mypy pylint\nif [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi\npython -m pip install .\n', 'flake8 --ignore=E501,W503,W504 sshmitm\n', 'pylint sshmitm --disable=duplicate-code,line-too-long,missing-module-docstring,missing-class-docstring,missing-function-docstring,fixme,too-few-public-methods,too-many-branches,too-many-instance-attributes,too-many-arguments,too-many-statements,too-many-locals,protected-access,too-many-return-statements\n', 'mypy --strict --install-types --non-interactive sshmitm\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['RELEASE_VER=${GITHUB_REF#refs/*/}\nPACKAGE_VER=""v`python setup.py --version`""\nif [ $RELEASE_VER != $PACKAGE_VER ]\nthen\n  echo ""package ver. ($PACKAGE_VER) != release ver. ($RELEASE_VER)""; exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine build\n', 'python -m build\ntwine upload dist/*\n', 'bash test.sh']"
[]
['./checkDups.sh']
"['echo ""=============== list modified files ===============""\ngit diff --name-only HEAD^ HEAD\n\necho ""========== check paths of modified files ==========""\ngit diff --name-only HEAD^ HEAD > files.txt\nwhile IFS= read -r file\ndo\n  echo $file\n  if [[ $file = tensorflow_similarity/* && $file != tensorflow_similarity/__init__.py ]]; then\n    echo ""This modified file is under the \'tensorflow_similarity\' folder.""\n    echo ""::set-output name=run_job::true""\n    break\n  else\n    echo ""::set-output name=run_job::false""\n  fi\ndone < files.txt\n', 'python -m pip install --upgrade pip\n', 'pip install "".[tensorflow,dev]""\n', '# Increments the dev version and pushes the changes to development.\npython scripts/increment_version.py\n', 'python setup.py sdist bdist_wheel --project_name tfsim-nightly\n', 'twine upload -u ${{ secrets.PYPI_NIGHTLY_USERNAME }} -p ${{ secrets.PYPI_NIGHTLY_TOKEN }} dist/* --verbose\n', 'python -m pip install --upgrade pip\n', 'pip install "".[tensorflow,dev]""\n', 'python setup.py sdist bdist_wheel\n', 'twine upload -u ${{ secrets.PYPI_USERNAME }} -p ${{ secrets.PYPY_TOKEN }} dist/* --verbose\n', 'python -m pip install --upgrade pip\npip install coveralls\n', 'pip install "".[dev]""\n', 'pip install tensorflow==${{ matrix.tf-version }}\n# Fix proto dep issue in protobuf 4\npip install protobuf==3.20.*\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'mypy  tensorflow_similarity/ --raise-exceptions\n', 'coverage run -m pytest tests/\n', 'coveralls --service=github\n', 'python -m pip install --upgrade pip\npip install coveralls\n', 'coveralls --service=github --finish\n']"
"['curl -sSL https://install.python-poetry.org | python3 -', 'poetry config virtualenvs.in-project true\npoetry install\n', 'poetry run pytest\n', 'curl -sSL https://install.python-poetry.org | python3 -', 'poetry --version', 'poetry install', 'poetry build', 'curl -sSL https://install.python-poetry.org | python3 -', 'poetry --version', 'poetry version patch', 'poetry build', 'curl -sSL https://install.python-poetry.org | python3 -', 'poetry config virtualenvs.in-project true\npoetry install\n', 'poetry run pytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install --upgrade pip\npip install pipenv\npipenv sync --dev\n', 'pipenv run sphinx-apidoc --module-first --separate -o ../docs/API/ l5kit l5kit/tests*\npipenv run sphinx-build ../docs ../docs_built\n', 'pip install --upgrade pip\npip install pipenv\npipenv sync --dev\n', 'pipenv run ./run_tests.sh lint\n', 'pipenv run ./run_tests.sh isort\n', 'pipenv run ./run_tests.sh tests\n', 'pipenv run ./run_tests.sh types\n', 'pip install --upgrade pip\npip install .[dev]\n', './run_tests.sh lint\n', './run_tests.sh isort\n', './run_tests.sh tests\n', './run_tests.sh types\n']"
"['./scripts/run_flake8.sh\n', './scripts/run_pytype.sh\n', 'python setup.py install\npip install -U pip\npip install -e "".[testing_without_asyncio]""\n', 'pytest tests/slack_bolt/\npytest tests/scenario_tests/\n', 'pip install -e "".[adapter]""\npip install -e "".[adapter_testing]""\npytest tests/adapter_tests/socket_mode/\n', 'pytest tests/adapter_tests/aws/\n', 'pytest tests/adapter_tests/bottle/\n', 'pytest tests/adapter_tests/cherrypy/\n', 'pytest tests/adapter_tests/django/\n', 'pytest tests/adapter_tests/falcon/\n', '# Falcon 2.x does not support Python 3.11 or newer\n# See also: https://github.com/slackapi/bolt-python/issues/757\nif [ ${{ matrix.python-version }} != ""3.11"" ]; then\n  pip install ""falcon<3""\n  pytest tests/adapter_tests/falcon/\nfi\n', 'pytest tests/adapter_tests/flask/\n', 'pytest tests/adapter_tests/pyramid/\n', 'pytest tests/adapter_tests/starlette/\n', 'pytest tests/adapter_tests/tornado/\n', 'pip install -e "".[async]""\n# Falcon supports Python 3.11 since its v3.1.1\npip install ""falcon>=3.1.1,<4""\npytest tests/adapter_tests_async/\n', '# Requires async test dependencies\npytest tests/adapter_tests/asgi/\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install -e .[test]', 'python -m pytest -m ""not slow"" --cov=compressai -s tests/', 'python -m pip install --upgrade pip\npip install build twine\n', 'python -m build --sdist .\npython3 -m twine upload --skip-existing dist/*\n', 'python -m pip install --upgrade pip\npip install build twine\n', 'python -m build --wheel .\npython3 -m twine upload --skip-existing dist/*\n', '/opt/python/${{ matrix.python-version }}/bin/python -m pip install build twine', '/opt/python/${{ matrix.python-version }}/bin/python -m build --wheel .', 'auditwheel repair -w dist dist/*', 'rm dist/*-linux_x86_64.whl', '/opt/python/${{ matrix.python-version }}/bin/twine upload --skip-existing dist/*\n', 'pip install -e .[dev]', 'make static-analysis', 'python3 -m pip install -U pip\npython3 -m pip install .[doc]\n', 'make -C docs clean\nmake -C docs html\n', 'cd docs/_build/html\ngit init\ntouch .nojekyll\ngit add --all\ngit config --local user.email ""compressai@interdigital.com""\ngit config --local user.name ""CompressAI""\ngit commit -m ""deploy""\n']"
"['python -m pip install mkdocs mkdocs-material\n', './scripts/build_site.sh', 'python -m pip install poetry poetry-dynamic-versioning\npoetry install\npoetry build\n', 'python -m pip install poetry\npoetry install\n', './scripts/lint.sh\n', 'poetry run ./scripts/test.sh\n', './scripts/poetry_test.bat\n']"
"['sudo apt update\nsudo apt install libsndfile1-dev libsndfile1\n', 'python -m pip install --upgrade --user pip --quiet\npython -m pip install -r .github/workflows/test_requirements.txt\npython --version\npip --version\npython -m pip list\n', 'coverage run -a -m py.test tests', 'coverage xml\nbash <(curl -s https://codecov.io/bash)\n', 'pip install black==22.3 flake8', 'python -m black --config=pyproject.toml --check torch_audiomentations tests test_fixtures', 'python -m flake8  torch_audiomentations tests  --show-source --statistics  --select=F6,F7,F82,F52\npython -m flake8 --config .flake8 --exit-zero torch_audiomentations tests --statistics\n']"
"['pip install hatch\nhatch -e checks run typing\n', 'pip install hatch\nhatch -e checks run linting\n', 'pip install hatch hatch-vcs\nhatch build\nhatch publish --user=__token__ --auth=${{ secrets.PYPI_TOKEN }}\n', 'cd $GITHUB_WORKSPACE/austin\ngcc -Wall -O3 -Os -s -pthread src/*.c -o src/austin\n', 'sudo apt-get update -y\nsudo apt-get install -y binutils binutils-common\naddr2line -V\n', 'cd $GITHUB_WORKSPACE/main\nexport PATH=""$GITHUB_WORKSPACE/austin/src:$PATH""\npip install hatch\nhatch -e ""tests.py${{ matrix.python-version }}"" run tests\n', 'cd $GITHUB_WORKSPACE/main\nhatch -e coverage run cov\nhatch -e coverage run codecov\n', 'cd $GITHUB_WORKSPACE/austin\ngcc -Wall -O3 -Os src/*.c -o src/austin\n', 'codesign --remove-signature /Library/Frameworks/Python.framework/Versions/${{ matrix.python-version }}/bin/python3 || true\ncodesign --remove-signature /Library/Frameworks/Python.framework/Versions/${{ matrix.python-version }}/Resources/Python.app/Contents/MacOS/Python || true\n', 'cd $GITHUB_WORKSPACE/main\nexport PATH=""$GITHUB_WORKSPACE/austin/src:$PATH""\npip install hatch\nsudo hatch -e ""tests.py${{ matrix.python-version }}"" run tests\n', 'cd $env:GITHUB_WORKSPACE/austin\ngcc.exe -O3 -o src/austin.exe src/*.c -lpsapi -lntdll -Wall -Os -s\n', 'cd $env:GITHUB_WORKSPACE/main\n$env:PATH=""$env:GITHUB_WORKSPACE\\austin\\src;$env:PATH""\npip install hatch\nhatch -e ""tests.py${{ matrix.python-version }}"" run tests\n']"
"['python -m pip install --upgrade pip\npip install -e .\npip install ""pylint==2.11.1""\npip install ""pylint-ignore==2021.1020""\n', 'pylint-ignore --rcfile=.pylintrc *.py wapitiCore\n', 'docker build .\n', 'python -c ""import sys; print(sys.version)""', 'sudo apt-get update\nsudo apt-get install php8.1-cli php8.1-xml -y --no-install-recommends\npython -m pip install --upgrade pip\npip install -U setuptools\npip3 install .[test]\n', 'pytest\n']"
"['python3 -m pip install --upgrade pip setuptools wheel\npip3 install -r docker/requirements.txt\npython3 setup.py develop\n', 'cd docs\nmake html\n', 'cd docs/_build/html\ngit init\ngit config user.name ""ohmyocr""\ngit config user.email ""ohmyocr@gmail.com""\ngit add .\ngit commit -m ""pages""\ngit branch -m gh-pages\ngit remote add origin https://$GITHUB_ACTOR:${{ secrets.GITHUB_TOKEN }}@github.com/oh-my-ocr/text_renderer.git\ngit push origin gh-pages -f\n', 'python3 -m pip install --upgrade pip setuptools wheel\npip3 install -r docker/requirements.txt\npython3 setup.py develop\n', 'pytest']"
"['python -m pip install --upgrade pip\npip install flake8\nflake8 .\n', 'python -m pip install --upgrade pip\npip install --progress-bar off -U setuptools\n\n# Install pytest\npip install --progress-bar off .\n\npip install --progress-bar off pytest\n', 'pytest tests\n']"
"['python update_readme.py\ncat README.md', 'git diff\ngit config --global user.email ""readme-bot@example.com""\ngit config --global user.name ""README-bot""\ngit diff --quiet || (git add README.md && git commit -m ""Updated README"")\ngit push']"
['pip install poetry']
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions poetry\n', 'tox']"
"['python -m pip install --upgrade pip wheel\npip install -r local-requirements.txt\npip install -e .\npython setup.py bdist_wheel\npython -m playwright install --with-deps\n', 'pre-commit run --show-diff-on-failure --color=always --all-files', 'bash scripts/update_api.sh', 'git diff --exit-code', 'python -m pip install --upgrade pip wheel\npip install -r local-requirements.txt\npip install -e .\npython setup.py bdist_wheel\npython -m playwright install --with-deps ${{ matrix.browser }}\n', 'pytest tests/common --browser=${{ matrix.browser }} --timeout 90', 'pytest tests/test_reference_count_async.py --browser=${{ matrix.browser }}', 'pytest tests/test_installation.py --browser=${{ matrix.browser }}', 'pytest tests/test_generation_scripts.py --browser=${{ matrix.browser }}', 'pytest tests/sync --browser=${{ matrix.browser }} --timeout 90', 'xvfb-run pytest tests/sync --browser=${{ matrix.browser }} --timeout 90', 'pytest tests/async --browser=${{ matrix.browser }} --timeout 90', 'xvfb-run pytest tests/async --browser=${{ matrix.browser }} --timeout 90', 'python -m pip install --upgrade pip wheel\npip install -r local-requirements.txt\npip install -e .\npython setup.py bdist_wheel\npython -m playwright install ${{ matrix.browser-channel }} --with-deps\n', 'pytest tests/common --browser=chromium --browser-channel=${{ matrix.browser-channel }} --timeout 90', 'pytest tests/sync --browser=chromium --browser-channel=${{ matrix.browser-channel }} --timeout 90', 'xvfb-run pytest tests/sync --browser=chromium --browser-channel=${{ matrix.browser-channel }} --timeout 90', 'pytest tests/async --browser=chromium --browser-channel=${{ matrix.browser-channel }} --timeout 90', 'xvfb-run pytest tests/async --browser=chromium --browser-channel=${{ matrix.browser-channel }} --timeout 90', 'conda install conda-build conda-verify', 'conda build .', 'pip install -r requirements.txt\npython -m playwright install --with-deps chromium\n', 'pytest', 'python -m pip install --upgrade pip\npip install -r local-requirements.txt\npip install -e .\npython setup.py bdist_wheel --all\npython -m playwright install-deps\n', 'twine upload dist/*', 'conda install anaconda-client conda-build conda-verify', 'conda config --set anaconda_upload yes\nconda build --user microsoft .\n', 'python -m pip install --upgrade pip wheel\npip install -r local-requirements.txt\npip install -e .\n', './utils/docker/publish_docker.sh canary', 'python -m pip install --upgrade pip wheel\npip install -r local-requirements.txt\npip install -e .\n', './utils/docker/publish_docker.sh stable', './utils/docker/publish_docker.sh canary', 'python -m pip install --upgrade pip\npip install -r local-requirements.txt\npip install -e .\n', 'bash utils/docker/build.sh --amd64 ${{ matrix.docker-image-variant }} playwright-python:localbuild-${{ matrix.docker-image-variant }}', 'CONTAINER_ID=""$(docker run --rm -v $(pwd):/root/playwright --name playwright-docker-test --workdir /root/playwright/ -d -t playwright-python:localbuild-${{ matrix.docker-image-variant }} /bin/bash)""\n# Fix permissions for Git inside the container\ndocker exec ""${CONTAINER_ID}"" chown -R root:root /root/playwright\ndocker exec ""${CONTAINER_ID}"" pip install -r local-requirements.txt\ndocker exec ""${CONTAINER_ID}"" pip install -e .\ndocker exec ""${CONTAINER_ID}"" python setup.py bdist_wheel\ndocker exec ""${CONTAINER_ID}"" xvfb-run pytest -vv tests/sync/\ndocker exec ""${CONTAINER_ID}"" xvfb-run pytest -vv tests/async/\n', 'curl -X POST \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  -H ""Authorization: token ${GH_TOKEN}"" \\\n  --data ""{\\""event_type\\"": \\""playwright_tests_python\\"", \\""client_payload\\"": {\\""ref\\"": \\""${GITHUB_SHA}\\""}}"" \\\n  https://api.github.com/repos/microsoft/playwright-browsers/dispatches\n']"
"['python -m pip install --upgrade pip\npip install pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install -e "".[dev]""\n', 'python -m pytest src', 'python -m pip install --upgrade pip\npython -m pip install black\n', 'python -m black -v --check src', 'python -m pip install --upgrade pip\npython -m pip install pyproject-flake8\n', 'pflake8 -v src', 'python -m pip install --upgrade pip\npython -m pip install isort\n', 'python -m isort --check src', ""python -m pip install --upgrade pip\npython -m pip install '.[mypy]'\n"", 'python -m mypy src', 'python -m pip install --upgrade build hatch\npython -m hatch version ""${GITHUB_REF_NAME}""\npython -m build\n']"
"['pip install torch', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-magic --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 90 mmdet3d\n', 'pip install pip --upgrade && pip install wheel', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\n"", ""mim install 'mmdet>=3.0.0'"", 'pip install -r requirements/tests.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmdet3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade && pip install wheel', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\n"", ""mim install 'mmdet>=3.0.0'"", 'pip install -r requirements/tests.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmdet3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade && pip install wheel', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\nmim install 'mmdet>=3.0.0'\npip install -r requirements/tests.txt\n"", 'pip install -e .', 'coverage run --branch --source mmdet3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade && pip install wheel', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y git ffmpeg libturbojpeg', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\nmim install 'mmdet>=3.0.0'\npip install -r requirements/tests.txt\n"", 'pip install -e .', 'coverage run --branch --source mmcv -m pytest tests\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip --upgrade && pip install wheel', 'pip install lmdb', 'pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\nmim install 'mmdet>=3.0.0'\npip install -r requirements/tests.txt\n"", 'pip install -e .', 'pytest tests/', 'python -m pip install pip --upgrade && pip install wheel', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\n"", ""mim install 'mmdet>=3.0.0'"", 'pip install -r requirements/tests.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmdet3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade && pip install wheel', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\nmim install 'mmdet>=3.0.0'\npip install -r requirements/tests.txt\n"", 'pip install -e .', 'coverage run --branch --source mmdet3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip --upgrade && pip install wheel', 'pip install lmdb', 'pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc4'\nmim install 'mmdet>=3.0.0'\npip install -r requirements/tests.txt\n"", 'pip install -e .', 'pytest tests/', 'pip install pip --upgrade && pip install wheel', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmdet3d']"
"['pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 80 mmpose\n', 'pip install pip --upgrade', 'pip install -U numpy', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0'\n"", 'python -m pip install --upgrade pip setuptools wheel\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\n', 'pip install -r requirements/tests.txt\npip install -r requirements/runtime.txt\npip install -r requirements/albu.txt\npip install -r requirements/poseval.txt\n', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmpose -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'pip install -U numpy', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0'\n"", 'python -m pip install --upgrade pip setuptools wheel\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\n', 'pip install -r requirements/tests.txt\npip install -r requirements/runtime.txt\npip install -r requirements/albu.txt\npip install -r requirements/poseval.txt\n', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmpose -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6\n', ""pip install -U numpy\npip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install -r requirements/tests.txt\npip install -r requirements/runtime.txt\npip install -r requirements/albu.txt\npip install -r requirements/poseval.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmpose -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip --upgrade', 'python -m pip install lmdb', 'python -m pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""python -m pip install -U numpy\npython -m pip install git+https://github.com/open-mmlab/mmengine.git@main\npython -m pip install -U openmim\nmim install 'mmcv >= 2.0.0'\npython -m pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npython -m pip install -r requirements/tests.txt\npython -m pip install -r requirements/runtime.txt\npython -m pip install -r requirements/albu.txt\npython -m pip install -r requirements/poseval.txt\n"", 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -e . -v\n', 'pytest tests/\n', 'pip install pip --upgrade', 'pip install -U numpy', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', ""python -m pip install --upgrade pip setuptools wheel\npip install -U numpy\npip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install -r requirements/tests.txt\npip install -r requirements/runtime.txt\npip install -r requirements/albu.txt\npip install -r requirements/poseval.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmpose -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update\napt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev\n', ""pip install -U numpy\npip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install -r requirements/tests.txt\npip install -r requirements/runtime.txt\npip install -r requirements/albu.txt\npip install -r requirements/poseval.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmpose -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev', ""pip install -U numpy\npip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install -r requirements/tests.txt\npip install -r requirements/runtime.txt\npip install -r requirements/albu.txt\npip install -r requirements/poseval.txt\n"", 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmpose -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip --upgrade', 'python -m pip install lmdb', 'python -m pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""python -m pip install -U numpy\npython -m pip install git+https://github.com/open-mmlab/mmengine.git@main\npython -m pip install -U openmim\nmim install 'mmcv >= 2.0.0'\npython -m pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npython -m pip install -r requirements/tests.txt\npython -m pip install -r requirements/albu.txt\npython -m pip install -r requirements/poseval.txt\n"", 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -e . -v\n', 'pytest tests/\n']"
"['pip install -U -r requirements.txt', 'python -m ci', 'pip install --upgrade codespell mypy types-PyYAML -r requirements.txt', 'sudo apt install -y universal-ctags', 'codespell --skip ./artifacts/helo.yml', 'mypy -- .', 'python3 -m coq deps', 'python3 -m tests']"
"['pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 80 mmaction\n', 'pip install pip --upgrade', 'sudo apt-get update\nsudo apt-get upgrade\nsudo apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev libturbojpeg pkg-config\nsudo apt-get install -y libavdevice-dev libavfilter-dev libopus-dev libvpx-dev\n', 'pip install wheel', 'pip install librosa soundfile', 'pip install lmdb', 'pip install -r requirements.txt', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0'\n"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x', 'pip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x', 'pip install pytorchvideo', 'pip install timm', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmaction -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'sudo apt-get install -y libsndfile1', 'pip install librosa soundfile', 'pip install lmdb', 'pip install timm==0.6.7', 'pip install timm', 'sudo apt-get install -y libturbojpeg', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0'\n"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x', 'pip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x', 'pip install -r requirements.txt', 'pip install pytorchvideo', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmaction -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libturbojpeg libsndfile1 libsm6 libxrender-dev libxext6\n', 'pip install librosa soundfile', 'pip install lmdb', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x\npip install -r requirements.txt\n"", 'pip install pytorchvideo', 'pip install -e .\n', 'pip install pip --upgrade', 'python -m pip install librosa soundfile', 'pip install lmdb', 'pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x\npip install -r requirements.txt\n"", 'python -m pip install pytorchvideo', 'python -m pip install timm', 'pip install -e .\n', 'pytest tests/\n', 'pip install pip --upgrade  && pip install wheel', 'sudo apt-get install -y libsndfile1', 'pip install librosa soundfile', 'pip install lmdb', 'sudo apt-get install -y libturbojpeg', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0'\n"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x', 'pip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x', 'pip install -r requirements.txt', 'pip install pytorchvideo', 'python -m pip install timm', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmaction -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade  && pip install wheel', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libturbojpeg libsndfile1 libsm6 libxrender-dev libxext6\n', 'pip install librosa soundfile', 'pip install lmdb', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x\npip install -r requirements.txt\n"", 'pip install pytorchvideo', 'pip install -e . -v\n', ""coverage run --branch --source mmaction -m pytest tests/ -k 'not timm'\ncoverage xml\ncoverage report -m\n"", 'python -V\npython -m pip install pip --upgrade\npython -m pip install wheel\n', 'python -m pip install librosa soundfile', 'pip install lmdb', 'pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', 'python -m pip install timm', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install git+https://github.com/open-mmlab/mmclassification.git@dev-1.x\npip install -r requirements.txt\n"", 'python -m pip install pytorchvideo', 'pip install -e . -v\n', 'pytest tests/\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
"['gh pr review --approve ""$PR_URL""', 'gh pr merge --auto --merge ""$PR_URL""', 'pip install black[jupyter]', 'black --check .', 'pip install isort==5.6.4', 'isort --check-only --diff .', 'pip install flake8', 'flake8', 'pip install pre-commit', 'pre-commit run --all-files', 'curl -sSL https://install.python-poetry.org | python3 -\npython -m pip install poetry-dynamic-versioning[plugin]\n', 'echo ""/Users/runner/.local/bin"" >> $GITHUB_PATH', 'poetry build\n', 'poetry publish --username ""${{ secrets.PYPI_USER }}"" --password ""${{ secrets.PYPI_PASSWORD }}""\n', 'brew install libomp  # https://github.com/pytorch/pytorch/issues/20030\n', 'brew install libomp  # https://github.com/pytorch/pytorch/issues/20030\n', 'echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")', 'curl -sSL https://install.python-poetry.org | python3 -\n', 'echo ""/Users/runner/.local/bin"" >> $GITHUB_PATH', 'poetry config virtualenvs.in-project true', 'poetry run pip --version >/dev/null 2>&1 || rm -rf .venv', 'poetry run python -m pip install pip -U', 'poetry install -E ""github-actions graph mqf2""', 'poetry run pytest tests', 'pip install coverage\ncoverage report\ncoverage xml\n', 'sudo apt-get update && sudo apt-get install -y pandoc\npython -m pip install --upgrade pip\npip install -r docs/requirements.txt\n', 'cd docs\nmake clean\nmake html --debug --jobs 2 SPHINXOPTS=""-W""\n']"
""
"['pip install -r requirements.txt', 'mkdir assets', 'python3 sources/main.py', 'BRANCH_NAME=${{ env.BRANCH_NAME }}\nBRANCH_NAME=${BRANCH_NAME////_}\necho BRANCH_NAME=${BRANCH_NAME} >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'pip install -r requirements.txt', ""flake8 --max-line-length=160 --exclude venv,assets . && black --line-length=160 --check --exclude='/venv/|/assets/' .""]"
"['python -m pip install build', 'python -m build --sdist --wheel --outdir ./dist/', 'git fetch --unshallow --tags', 'python -m venv .venv', 'source .venv/bin/activate && make requirements && python setup.py install', 'source .venv/bin/activate && make flake black-check isort-check', 'source .venv/bin/activate && make tests', '.venv/bin/coveralls --service=github', 'pip3 install --upgrade coveralls && coveralls --service=github --finish']"
[]
"['output=$(python3 release_utils.py --release-type ${{ github.event.inputs.name }})\necho $output\nnew_version=$(echo $output | awk \'{print $1}\')\nnew_tag=$(echo $output | awk \'{print $2}\')\necho ""new version is $new_version""\necho ""new tag is $new_tag""\necho ::set-output name=version::$new_version\necho ::set-output name=tag::$new_tag\n', 'echo ""current folder = $PWD""\necho ""current branch = $(git branch --show-current)""\noutput=$(python3 release_utils.py --release-type ${{ github.event.inputs.name }} --update-version)\n', 'python3 -m pip install --upgrade pip\npip install setuptools wheel twine\npython3 setup.py sdist\npython3 -m twine upload --repository pypi dist/*\n']"
""
"['bandit --ini .bandit -r instagrapi', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --statistics\n', 'isort --check-only instagrapi', 'python -m unittest tests.FakeClientTestCase', 'mkdocs build --strict']"
"['pip install --upgrade pip\npip install --upgrade tox\n', 'tox -e py\n', 'pip install --upgrade pip\npip install --upgrade tox\n', 'tox -e mot-py39\n', 'poetry build -f sdist', 'tar xvf ""dist/norfair-$(poetry version -s).tar.gz""\nmv norfair-$(poetry version -s) norfair-dist\n', 'pip install ./norfair-dist/', 'python -c ""import norfair""', 'poetry config -n pypi-token.pypi ""$PYPI_TOKEN""\npoetry publish --build -n\n', 'pip install -r docs/requirements.txt', 'python docs/deploy_docs.py', 'git config user.name ci-bot \ngit config user.email ci-bot@tryolabs.com\n', 'pip install -r docs/requirements.txt', 'mike deploy --push dev', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'source .venv/bin/activate\nblack --check --diff .\n']"
"['make build-all', 'make validate', 'make test', 'make test-external', '(cd ../ && make install-test-dependencies)\n(cd ../ && make install)\n', 'make test', 'make test-external', '(cd ../ && make install)\n', 'make test', 'echo ""${{ secrets.SSH_PRIVATE_KEY }}"" > /tmp/private_key.pem\nchmod 400 /tmp/private_key.pem\n', '(cd ../ && make install-test-dependencies)\n(cd ../ && make install)\n', 'make test', 'make test-external', 'make build ENVIRONMENT=production']"
"['python -m pip install --upgrade pip\npython -m pip install flake8==3.9.2 flake8-bugbear==21.4.3 flake8-comprehensions==3.6.1 isort==4.3.21\npython -m pip install black==21.4b2\nflake8 --version\n', 'echo ""Running isort""\nisort -c -sp .\n', 'echo ""Running black""\nblack -l 100 --check .\n', 'echo ""Running flake8""\nflake8 .\n']"
"['npm install', 'npm run-script test', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs${{ matrix.python-version }}\n', 'poetry install', 'poetry run black . --check --extend-exclude migrations', 'poetry run isort --settings pyproject.toml --check .', 'poetry run nox']"
""
"['pip install nox', 'python3 -m nox']"
"['sudo apt-get install --yes clang-format-10\npython -m pip install -U yapf==0.30.0 nbformat pydocstyle==6.0.0\n', 'python ci/check_style.py\n', 'pydocstyle --convention=google --add-ignore=D1,D205,D415,D212 .\n', ""sudo apt-get --yes install ccache\nccache -M 2G  # GitHub's total cache limit is 5GB for all OSes.\n"", 'PATH=/usr/lib/ccache:$PATH\nccache -s\n./ci/run_ci.sh\nccache -s\n']"
"['set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\npython -c ""import uncertainty_baselines as ub""\npython -c ""import uncertainty_baselines as ub; print(ub.datasets)""\npython -c ""import uncertainty_baselines as ub; print(ub.models)""\n', ""set -v\nset -e\npython -m unittest -v uncertainty_baselines/datasets/random_test.py\npython -m unittest -v $(find . -name 'uncertainty_baselines/models/*_test.py' -a ! -name 'uncertainty_baselines/models/bert*_test.py')\npython -m unittest -v uncertainty_baselines/*_test.py\n"", 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v active_learning_test.py\npython -m unittest -v al_utils_test.py\n', 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v batchensemble_test.py\n', 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v deep_ensemble_test.py\n', 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v deterministic_test.py\n', 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v heteroscedastic_test.py\n', 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v sngp_test.py\n', 'set -v\nset -e\npython -m pip install --upgrade pip\npip install -q -e .[tensorflow,jax,models,tests]\n', 'set -v\nset -e\ncd baselines/jft\npython -m unittest -v checkpoint_utils_test.py\npython -m unittest -v input_utils_test.py\npython -m unittest -v ood_utils_test.py\npython -m unittest -v preprocess_utils_test.py\npython -m unittest -v train_utils_test.py\n']"
"['poetry install', 'poetry run isort --check-only . --profile black', 'poetry run black --diff --check .', 'poetry run mypy', 'poetry run ruff src/pynguin', 'poetry run pytest --cov=src --cov=tests --cov-branch --cov-report=term-missing tests/']"
"['curl -sSL https://install.python-poetry.org | python -\nexport PATH=""$HOME/.poetry/bin:${PATH}""\npoetry install\n', 'poetry run python -m unittest discover\n']"
"['# INSTALLS\npython -m pip install --upgrade pip\npip install -r requirements.txt\necho ""PYTHONPATH=/home/runner/work/ai-economist-opensource/ai-economist-opensource/"" >> $GITHUB_ENV\n', '# isort config compatible with black\nisort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88  --check-only -rc ai_economist/\n', 'black --line-length 88 --check ai_economist/\n', ""# stop the build if there are Python syntax errors or undefined names\nflake8 --count --select=E9,F63,F7,F82 --show-source --statistics ai_economist ai_economist/\n# exit-zero treats all errors as warnings. Also outputs error counts\n# Add as many codes under --ignore for the checks to ignore\n# E203: Whitespace before ':' (will be taken care of by Black)\n# C901: Function is too complex\n# W503: Line break occurred before a binary operator (Black is expected to take care of formatting)\n# F401: Module imported but unused\nflake8  --ignore=E203,C901,W503,F401 --count --max-complexity=15 --max-line-length=88 --statistics ai_economist/\n"", '# Add as many codes under --disable= for the checks to disable\n# Note: exit-zero disregards all errors\n# *** CODES CURRENTLY DISABLED ***\n# C0103: invalid-name\n# C0114: missing-module-docstring\n# C0116: missing-function-docstring\n# C0302: too-many-lines\n# C0330: bad-continuation\n# E0401: import-error\n# R0201: no-self-use\n# R0801: duplicate-code\n# R0902: too-many-instance-attributes\n# R0903: too-few-public-methods\n# R0904: too-many-public-methods\n# R0912: too-many-branches\n# R0913: too-many-arguments\n# R0914: too-many-locals\n# R0915: too-many-statements\n# R1702: too-many-nested-blocks\n\npylint --disable \\\nbad-continuation,\\\nduplicate-code,\\\nimport-error,\\\ninvalid-name,\\\nmissing-module-docstring,\\\nmissing-function-docstring,\\\nno-self-use,\\\ntoo-few-public-methods,\\\ntoo-many-arguments,\\\ntoo-many-branches,\\\ntoo-many-instance-attributes,\\\ntoo-many-lines,\\\ntoo-many-locals,\\\ntoo-many-nested-blocks,\\\ntoo-many-public-methods,\\\ntoo-many-statements \\\nai_economist/\n', 'pytest\n']"
['git config --local --get remote.origin.url\ncd docs/build_docs\nbash build.sh latest\n']
"['brew install libomp', 'choco install wget', 'pip install -U pip wheel\npip install .[dev]\npython -c ""import nltk; nltk.download(\'punkt\')""\npython --version\nmake data coverage\n', 'coveralls --service=github']"
"['python -m pip install --upgrade pipenv wheel\n', 'if ([ ""$RUNNER_OS"" = ""macOS"" ]); then\n  brew install ta-lib\nfi\nif ([ ""$RUNNER_OS"" = ""Linux"" ]); then\n  if [ ! -f ""$GITHUB_WORKSPACE/ta-lib/src"" ]; then wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -q && tar -xzf ta-lib-0.4.0-src.tar.gz; fi\n  cd ta-lib/\n  ./configure --prefix=/usr\n  if [ ! -f ""$HOME/ta-lib/src"" ]; then make; fi\n  sudo make install\n  cd\nfi\nif ([ ""$RUNNER_OS"" = ""Windows"" ]); then\n  curl -sL http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-msvc.zip -o $GITHUB_WORKSPACE/ta-lib.zip --create-dirs && 7z x $GITHUB_WORKSPACE/ta-lib.zip -o/c/ta-lib && mv /c/ta-lib/ta-lib/* /c/ta-lib/ && rm -rf /c/ta-lib/ta-lib && cd /c/ta-lib/c/make/cdr/win32/msvc && nmake\nfi\n', 'pipenv install --dev\n', 'pipenv run pytest -v tests/', 'python -m pip install --upgrade pipenv wheel\n', 'if ([ ""$RUNNER_OS"" = ""macOS"" ]); then\n  brew install ta-lib\nfi\nif ([ ""$RUNNER_OS"" = ""Linux"" ]); then\n  if [ ! -f ""$GITHUB_WORKSPACE/ta-lib/src"" ]; then wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -q && tar -xzf ta-lib-0.4.0-src.tar.gz; fi\n  cd ta-lib/\n  ./configure --prefix=/usr\n  if [ ! -f ""$HOME/ta-lib/src"" ]; then make; fi\n  sudo make install\n  cd\nfi\nif ([ ""$RUNNER_OS"" = ""Windows"" ]); then\n  curl -sL http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-msvc.zip -o $GITHUB_WORKSPACE/ta-lib.zip --create-dirs && 7z x $GITHUB_WORKSPACE/ta-lib.zip -o/c/ta-lib && mv /c/ta-lib/ta-lib/* /c/ta-lib/ && rm -rf /c/ta-lib/ta-lib && cd /c/ta-lib/c/make/cdr/win32/msvc && nmake\nfi\n', 'pipenv install --dev\n', 'pipenv run pre-commit run --all']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'sudo apt-get install libxml2-dev libxslt1-dev --yes', 'pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
"['pip install -r requirements.txt', 'rm -rf dist build', 'python3 setup.py bundle bdist_wheel', 'pip install -r requirements.txt', 'python3 setup.py bundle', 'pip3 install .', 'pip install -r requirements.txt\npip install dataclasses\npip install pytest\n', 'pytest test/', 'python -m pip install --upgrade pip\npip install pylint\npip install mypy==0.812\nsudo apt install gcc -y\npip install -r requirements.txt\npip install pytest\npip3 install .\n', 'pylint --rcfile=pylintrc projector_installer/ test/ setup.py\n', 'mypy projector_installer/ test/ setup.py\n', 'pip install -r requirements.txt', 'python setup.py bundle bdist_wheel\n', 'pip install twine\npython setup.py bundle sdist bdist_wheel\npython -m twine upload --username ${{secrets.TEST_PYPI_USER}} --password ${{secrets.TEST_PYPI_TOKEN}} --repository testpypi --verbose dist/*  # Upload to https://test.pypi.org/project/projector-installer/\npython -m twine upload --username ${{secrets.PYPI_USER}} --password ${{secrets.PYPI_TOKEN}} dist/*  # Upload to https://pypi.org/project/projector-installer/\n']"
"['echo ""Branch ${{ github.ref }} of repository ${{ github.repository }}.""', './gradlew --no-daemon test', 'rm -f ~/.gradle/caches/modules-2/modules-2.lock\nrm -f ~/.gradle/caches/modules-2/gc.properties\n']"
"['sudo apt-get install --no-install-recommends -y help2man\n', 'pip install \\\n  --disable-pip-version-check \\\n  --user \\\n  --no-warn-script-location \\\n  .\necho ""PATH=${HOME}/.local/bin:${PATH}"" >> ""${GITHUB_ENV}""', 'rm git-{delete-merged-branches,dmb}.1  # to enforce a diff for the generator to remove\n./sync-manpage-with-help-output.sh\ngit diff --exit-code -- git-{delete-merged-branches,dmb}.1', 'pip install \\\n  --disable-pip-version-check \\\n  --no-warn-script-location \\\n  --user \\\n  pre-commit\necho ""PATH=${HOME}/.local/bin:${PATH}"" >> ""${GITHUB_ENV}""', 'pre-commit autoupdate\ngit diff -- .pre-commit-config.yaml', 'echo ""Pull request URL is: ${{ steps.create-pull-request.outputs.pull-request-url }}""\n', 'set -e\npython --version\npython setup.py test\n']"
"['python -m pip install --upgrade pre-commit', 'pre-commit run --all-files || (exit 0)', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\ngit commit -m ""Run pre-commit"" -a\ngit push\n', 'python -m pip install --upgrade tox virtualenv setuptools pip', 'tox -e ${{ matrix.tox-env-name }}', 'python -m pip install --upgrade tox virtualenv setuptools pip', 'tox -e py']"
"['python -m pip install --upgrade pip\npip install -r ./requirements.txt\npip install -r ./docs/sphinx_requirements.txt\n', 'cd ./docs/\nmake clean\nmake html\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || exit 1\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['/Library/Frameworks/Python.framework/Versions/3.9/bin/pip3 install -r requirements.txt', '/Library/Frameworks/Python.framework/Versions/3.9/bin/pip3 install pyinstaller', '/Library/Frameworks/Python.framework/Versions/3.9/bin/pyinstaller spec/macOS.spec', 'cd dist; zip macOS.zip macOS', 'pip3 install -r requirements.txt', 'pip3 install pyinstaller', 'pyinstaller spec\\Windows.spec && pyinstaller spec\\Windows_dir.spec && pyinstaller spec\\debug_dump.spec && pyinstaller spec\\debug_dump_dir.spec']"
"['python -m pip install --upgrade pip\npython -m pip install -e .[dev]\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'make test\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'make release\n']"
"['./moonraker/scripts/build-zip-release.sh -b -o ${{ github.workspace }} -k ${{ github.workspace }}/klipper\n', './moonraker/scripts/build-zip-release.sh -o ${{ github.workspace }} -k ${{ github.workspace }}/klipper\n', 'cd moonraker\ngh release upload ${{ env.TAG }} ${{ env.FILES }}\n']"
"['python -m pip install --upgrade pip\npip install pyinstaller\npip install opencv-python numpy matplotlib scipy python-dateutil PySide2 pandas\npip install hachoir orangebox vidgear qdarkstyle ffprobe-python darkdetect telemetry_parser\n', 'pyinstaller --icon=media\\icon.ico -F gyroflow.py\nren dist\\gyroflow.exe gyroflow-dev.exe\n']"
"['python3 -m pip install black', 'black .', 'python3 -m pip install -r tests/requirements_test.txt', 'pytest --cov=custom_components', 'python3 -m pip install -r tests/requirements_test.txt', 'pylint custom_components/pyscript/*.py tests/*.py', 'python3 -m pip install -r tests/requirements_test.txt', 'mypy custom_components/pyscript/*.py tests/*.py']"
[]
"['pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'pip install numpy\npip install cython\npython setup.py bdist_wheel\n', 'twine upload dist/*\n', 'pip install twine  \n', 'twine upload dist/pyqlib-*-manylinux*.whl\n', 'python -m pip install --upgrade pip\n', 'python -m pip install pyqlib\n# Specify the numpy version because the numpy upgrade caused the CI test to fail, \n# and this line of code will be removed when the next version of qlib is released.\npython -m pip install ""numpy<1.23""\n', '/bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Microsoft/qlib/main/.github/brew_install.sh)""\nHOMEBREW_NO_AUTO_UPDATE=1 brew install lightgbm\n# FIX MacOS error: Segmentation fault\n# reference: https://github.com/microsoft/LightGBM/issues/4229\nwget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew unlink libomp\nbrew install libomp.rb\n', 'python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n', 'qrun examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n', 'python -m pip install pip==23.0.1\n', 'python -m pip install torch torchvision torchaudio\n', 'python -m pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n', 'python -m pip install torch torchvision torchaudio\n', 'python -m pip install --upgrade cython\npython -m pip install -e .[dev]\n', 'black . -l 120 --check --diff\n', 'cd docs \nsphinx-build -W --keep-going -b html . _build\ncd ..\n', 'pylint --disable=C0104,C0114,C0115,C0116,C0301,C0302,C0411,C0413,C1802,R0401,R0801,R0902,R0903,R0911,R0912,R0913,R0914,R0915,R1720,W0105,W0123,W0201,W0511,W0613,W1113,W1514,E0401,E1121,C0103,C0209,R0402,R1705,R1710,R1725,R1735,W0102,W0212,W0221,W0223,W0231,W0237,W0612,W0621,W0622,W0703,W1309,E1102,E1136 --const-rgx=\'[a-z_][a-z0-9_]{2,30}$\' qlib --init-hook ""import astroid; astroid.context.InferenceContext.max_inferred = 500; import sys; sys.setrecursionlimit(2000)""\n', 'flake8 --ignore=E501,F541,E266,E402,W503,E731,E203 --per-file-ignores=""__init__.py:F401,F403"" qlib\n', 'mypy qlib --install-types --non-interactive || true\nmypy qlib --verbose\n', ""nbqa black . -l 120 --check --diff\nnbqa pylint . --disable=C0104,C0114,C0115,C0116,C0301,C0302,C0411,C0413,C1802,R0401,R0801,R0902,R0903,R0911,R0912,R0913,R0914,R0915,R1720,W0105,W0123,W0201,W0511,W0613,W1113,W1514,E0401,E1121,C0103,C0209,R0402,R1705,R1710,R1725,R1735,W0102,W0212,W0221,W0223,W0231,W0237,W0612,W0621,W0622,W0703,W1309,E1102,E1136,W0719,W0104,W0404,C0412,W0611,C0410 --const-rgx='[a-z_][a-z0-9_]{2,30}$'\n"", 'python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\nazcopy copy https://qlibpublic.blob.core.windows.net/data/rl /tmp/qlibpublic/data --recursive\nmv /tmp/qlibpublic/data tests/.data\n', '/bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Microsoft/qlib/main/.github/brew_install.sh)""\nHOMEBREW_NO_AUTO_UPDATE=1 brew install lightgbm\n# FIX MacOS error: Segmentation fault\n# reference: https://github.com/microsoft/LightGBM/issues/4229\nwget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew unlink libomp\nbrew install libomp.rb\n', '# add more ipynb files in future\njupyter nbconvert --to notebook --execute examples/workflow_by_code.ipynb\n', 'python -m pip install numba\npython qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n', 'python -m pip install pip==23.0.1\npip install --upgrade cython numpy\npip install -e .[dev]\n', 'python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n', '/bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Microsoft/qlib/main/.github/brew_install.sh)""\nHOMEBREW_NO_AUTO_UPDATE=1 brew install lightgbm\n# FIX MacOS error: Segmentation fault\n# reference: https://github.com/microsoft/LightGBM/issues/4229\nwget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew unlink libomp\nbrew install libomp.rb\n']"
"['pip install --upgrade pip wheel', 'pip install --upgrade setuptools', 'pip install bandit black flake8 flake8-bugbear flake8-comprehensions isort safety mypy', 'mypy --install-types --non-interactive --ignore-missing-imports ./rembg', 'bandit --recursive --skip B101,B104,B310,B311,B303,B110 --exclude ./rembg/_version.py ./rembg', 'black --force-exclude rembg/_version.py --check --diff ./rembg', 'flake8 ./rembg --count --ignore=B008,C901,E203,E266,E731,F401,F811,F841,W503 --max-line-length=120 --show-source --statistics --exclude ./rembg/_version.py', 'isort --check-only --profile black ./rembg', 'python3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel twine\n', 'python3 setup.py sdist bdist_wheel\npython3 -m twine upload dist/*\n', 'python -m pip install --upgrade pip\npip install .\npip install pytest\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install pytest\npip install -r requirements.txt\n', 'PYTHONPATH=$PYTHONPATH:. pytest .\n']"
"['cd tests/\npoetry run pytest -n auto --cov-report xml\n', 'pipx install poetry', 'curl -sSL https://github.com/nonebot/noneflow/releases/latest/download/plugin_test.py | python -\n', 'yarn prettier\ngit config user.name noneflow[bot]\ngit config user.email 129742071+noneflow[bot]@users.noreply.github.com\ngit add .\ngit diff-index --quiet HEAD || git commit -m "":memo: Update changelog""\ngit push\n', 'echo ""TAG_NAME=${GITHUB_REF#refs/tags/}"" >> $GITHUB_ENV\n', 'poetry build\npoetry publish -u ${{secrets.PYPI_USERNAME}} -p ${{secrets.PYPI_PASSWORD}}\ngh release upload --clobber ${{ env.TAG_NAME }} dist/*.tar.gz dist/*.whl\n', 'yarn build:plugin --out-dir ../packages/nonebot-plugin-docs/nonebot_plugin_docs/dist\nexport NONEBOT_VERSION=`poetry version -s`\ncd packages/nonebot-plugin-docs/\npoetry version $NONEBOT_VERSION\npoetry build\npoetry publish -u ${{secrets.PYPI_USERNAME}} -p ${{secrets.PYPI_PASSWORD}}\ngh release upload --clobber ${{ env.TAG_NAME }} dist/*.tar.gz dist/*.whl\n', 'echo ""TAG_NAME=v$(poetry version -s)"" >> $GITHUB_ENV', 'yarn archive $(poetry version -s)\nyarn prettier\n', 'git config user.name noneflow[bot]\ngit config user.email 129742071+noneflow[bot]@users.noreply.github.com\ngit add .\ngit commit -m "":bookmark: Release $(poetry version -s)""\ngit tag ${{ env.TAG_NAME }}\ngit push && git push --tags\n', 'yarn build', 'echo ""BRANCH_NAME=$(echo ${GITHUB_REF#refs/heads/})"" >> $GITHUB_ENV', 'yarn build', 'echo ""DEPLOY_NAME=deploy-preview-${{ github.event.number }}"" >> $GITHUB_ENV\n']"
"['sudo apt-get install libsndfile1', 'brew install libomp\npip install torch==1.12.1 torchvision==0.13.1\n', 'choco install wget', 'pip install -U wheel\npip install .[all,dev]\npython -c ""import nltk; nltk.download(\'punkt\')""\npython --version\nmake data coverage\n', 'coveralls --service=github', 'pip install -U pip wheel\npip install .[all,dev]\n', 'mkdocs gh-deploy --force']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['make download-poetry', 'source ""$HOME/.poetry/env""\npoetry config virtualenvs.in-project true\npoetry install\n', 'source ""$HOME/.poetry/env""\nmake test\n']"
"['pip install flake8 mypy', 'flake8 . --exclude ""src/viztracer/attach_process/*"" --count --ignore=W503 --max-line-length=127 --statistics', 'mypy src/ --exclude src/viztracer/attach_process/', 'sudo apt-get update\nsudo apt-get install gdb\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n', 'python -m pip install --upgrade pip\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', ""python -m pip install --upgrade pip\nif (Test-Path -Path '.\\requirements-dev.txt' -PathType Leaf) {pip install -r requirements-dev.txt}\n"", 'python setup.py sdist bdist_wheel\npip install dist/*.whl\npython -m unittest\n', 'python setup.py sdist bdist_wheel\npip install (Get-ChildItem dist/*.whl)\npython -m unittest\n', 'coverage run --source viztracer --parallel-mode -m unittest\ncoverage combine\ncoverage xml -i\n', 'python -m pip install --upgrade pip\npip install twine flake8 setuptools wheel\n', 'python -m pip install cibuildwheel -U', 'python -m cibuildwheel --output-dir wheelhouse', 'twine upload wheelhouse/*.whl\n', 'twine upload (Get-ChildItem wheelhouse/*.whl)\n', 'python -m pip install --upgrade pip\npip install twine flake8 setuptools wheel\n', 'python setup.py sdist\n', 'twine upload dist/*tar*\n']"
"['pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', '# Some dependencies may be required for the build of pycocotools\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\npip install git+https://github.com/votchallenge/toolkit.git\npip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\npip install mmdet\npip install -r requirements.txt\npip install git+https://github.com/JonathonLuiten/TrackEval.git\npip install git+https://github.com/lvis-dataset/lvis-api.git\npip install git+https://github.com/TAO-Dataset/tao.git\n', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmtrack -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\n', 'apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6\napt-get clean\nrm -rf /var/lib/apt/lists/*\n', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', '# Some dependencies may be required for the build of pycocotools\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\npython -m pip install git+https://github.com/votchallenge/toolkit.git\npython -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\npython -m pip install mmdet\npython -m pip install -r requirements.txt\npython -m pip install git+https://github.com/JonathonLuiten/TrackEval.git\npython -m pip install git+https://github.com/lvis-dataset/lvis-api.git\npython -m pip install git+https://github.com/TAO-Dataset/tao.git\n', 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 pip install .\n', 'coverage run --branch --source mmtrack -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\n', 'apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6\napt-get clean\nrm -rf /var/lib/apt/lists/*\n', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', '# Some dependencies may be required for the build of pycocotools\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\npython -m pip install git+https://github.com/votchallenge/toolkit.git\npython -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\npython -m pip install mmdet\npython -m pip install -r requirements.txt\npython -m pip install git+https://github.com/JonathonLuiten/TrackEval.git\npython -m pip install git+https://github.com/lvis-dataset/lvis-api.git\npython -m pip install git+https://github.com/TAO-Dataset/tao.git\n', 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 pip install .\n', 'coverage run --branch --source mmtrack -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip --upgrade --user', 'python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', 'python -m pip install git+https://github.com/votchallenge/toolkit.git', 'python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n', 'python -m pip install mmdet\n', 'python -m pip install -r requirements/tests.txt -r requirements/runtime.txt\npython -m pip install git+https://github.com/JonathonLuiten/TrackEval.git\npython -m pip install git+https://github.com/lvis-dataset/lvis-api.git\npython -m pip install git+https://github.com/TAO-Dataset/tao.git\n', 'python -m pip install -e .', 'coverage run --branch --source mmtrack -m pytest tests/\n', 'coverage xml\ncoverage report -m\n', 'pip install torch', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 80 mmtrack\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmtrack']"
"['python3 -m pip install --upgrade pip setuptools wheel\npython3 -m pip install -r requirements.txt\n', 'python3 --version\npython3 generate_images.py\n', 'git config --global user.name ""jstrieb/github-stats""\ngit config --global user.email ""github-stats[bot]@jstrieb.github.io""\ngit add .\n# Force the build to succeed, even if no files were changed\ngit commit -m \'Update generated files\' || true\ngit push\n']"
[]
[]
"['pip install -r requirements.txt', 'pre-commit run --all-files', 'mypy kopf --strict', '# Mypying the examples\nexit_codes=0\nfor d in $(find examples -maxdepth 1 -mindepth 1 -type d)\ndo\n  echo ""Checking ${d}""\n  mypy $d\n  exit_codes=$[${exit_codes} + $?]\ndone\nexit ${exit_codes}\n', 'pip install -r requirements.txt', 'pip install -e .[${{ matrix.install-extras }}]', 'pytest --color=yes --timeout=2 --cov=kopf --cov-branch', 'coveralls --service=github', 'sudo apt-get update && sudo apt-get install libxml2-dev libxslt-dev', 'pip install wheel', 'pip install -r requirements.txt', 'pip install -e .[${{ matrix.install-extras }}]', 'pytest --color=yes --timeout=2 --no-cov', 'pip install -r requirements.txt -r examples/requirements.txt', 'pytest --color=yes --timeout=30 --only-e2e', 'pip install coveralls', 'coveralls --service=github --finish', 'pip install --upgrade setuptools wheel twine', 'python setup.py sdist bdist_wheel', 'pip install -r requirements.txt', 'pre-commit run --all-files', 'mypy kopf --strict', '# Mypying the examples\nexit_codes=0\nfor d in $(find examples -maxdepth 1 -mindepth 1 -type d)\ndo\n  echo ""Checking ${d}""\n  mypy $d\n  exit_codes=$[${exit_codes} + $?]\ndone\nexit ${exit_codes}\n', 'pip install -r requirements.txt', 'pip install -e .[${{ matrix.install-extras }}]', 'pytest --color=yes --timeout=2 --cov=kopf --cov-branch', 'coveralls --service=github', 'sudo apt-get update && sudo apt-get install libxml2-dev libxslt-dev', 'pip install wheel', 'pip install -r requirements.txt', 'pip install -e .[${{ matrix.install-extras }}]', 'pytest --color=yes --timeout=2 --no-cov', 'pip install -r requirements.txt -r examples/requirements.txt', 'pytest --color=yes --timeout=30 --only-e2e', 'tools/install-minikube.sh', 'pip install -r requirements.txt -r examples/requirements.txt', 'pytest --color=yes --timeout=30 --only-e2e', 'pip install coveralls', 'coveralls --service=github --finish']"
"['sudo apt-get install libsndfile1\n./scripts/install.sh\npip install sphinx sphinx-press-theme sphinx-click \npip install sphinxcontrib-bibtex numpydoc\n', 'pwd\ncd docs\nmake html\n', 'eval $(ssh-agent -s)\nssh-add - <<< ""${{ secrets.OMNIZART_DOC_PRIVATE_KEY }}""\ngit clone git@github.com:Music-and-Culture-Technology-Lab/omnizart-doc.git\ngit config --global user.name ""omnizart-actions""\n', 'eval $(ssh-agent -s)\nssh-add - <<< ""${{ secrets.OMNIZART_DOC_PRIVATE_KEY }}""\ncp -r docs/build/html/* omnizart-doc\ncd omnizart-doc\ngit add .\nutc_time=`date --utc`\ngit commit -m ""Update through github action ${utc_time}""\ngit push origin master\n', 'curl -L https://github.com/Music-and-Culture-Technology-Lab/omnizart/releases/download/checkpoints-20211001/resource.zip -o resource.zip\nunzip resource.zip\nmv resource/* tests/resource\n', 'sudo apt-get update\nsudo apt-get install libsndfile1 ffmpeg\nif [ ${{ matrix.python-version }} = ""3.6"" ]; then\n  export DEFAULT_INSTALL_APPROACH=pip\nfi\n./scripts/install.sh\nomnizart download-checkpoints --output-path ./\npip install flake8 pylint pytest pytest-cov pytest-mock\n', 'make lint\n', 'make test\n', 'python -m pip install --upgrade pip setuptools\npip install poetry\npoetry build\npoetry config pypi-token.pypi ${{ secrets.PYPI_TOKEN }}\npoetry publish\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'docker build -t omnizart:${TAG} .\ndocker tag omnizart:${TAG} mctlab/omnizart:${TAG}\ndocker tag omnizart:${TAG} mctlab/omnizart:latest\ndocker push mctlab/omnizart\n']"
"['gh pr merge --auto --squash ""$PR_URL""', 'python -m pip install poetry==1.4.1\npoetry install --extras ""all""\n', 'mkdocs gh-deploy --force\n', 'poetry build -vvv\npoetry publish -u ${{ secrets.PYPI_USERNAME }} -p ${{ secrets.PYPI_PASSWORD }}\n', 'python -m pip install poetry==1.4.1\npoetry install --extras ""all""\n', 'bash scripts/test.sh', 'bash scripts/test.sh', 'bash scripts/test.sh', 'mypy ormar tests benchmarks']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'echo ""PYTHONPATH=."" >> $GITHUB_ENV\n', 'pytest ncps/tests/test_tf.py\npytest ncps/tests/test_torch.py\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*', 'python -m pip install --upgrade pip\npip install flake8\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython setup.py develop\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m unittest discover tests\n']"
"['pip install nox==2019.11.9', 'pip install poetry==1.0.5', 'nox --sessions tests-3.8 coverage', 'python -m pip install flake8==3.8.4 nox==2020.12.31 poetry==1.1.4', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'python -m nox']"
""
"['python -m pip install --upgrade pip\npip install pygame\npip install pywin32\npip install winshell\npip install Pillow\npip install imtools\npip install infi.systray\npip install keyboard\npip install pyinstaller==3.5\n', 'echo ""pyinstaller""\n#python pyinstaller.....\n', 'echo ""pyinstaller""\n', 'echo ""pyinstaller""\n', 'echo ""pyinstaller""\n', 'python -m pip install --upgrade pip\npip install pylint\npip install pygame\npip install pywin32\npip install winshell\npip install Pillow\npip install imtools\npip install infi.systray\npip install keyboard\n', 'pylint manager.py main.py animator.py blur.py exe_layer.py GWSL_ssh.py iset.py OpticUI.py wsl_tools.py\n']"
"['if [ -f requirements.txt ]; then pip install -r requirements.txt; fi', 'python cf2dns_actions.py A', 'python cf2dns_actions.py AAAA']"
""
[]
"['sudo apk update && sudo apk add bash && sudo .github/build/install_deps.sh', '.github/build/install_compiler.sh x86_64', '.github/build/install_compiler.sh i686', 'export PATH=""$PATH:/i686-linux-musl-cross/bin""\n.github/targets/build_gdb.sh x86\n', 'sudo apk update && sudo apk add bash && sudo .github/build/install_deps.sh', '.github/build/install_compiler.sh x86_64', '.github/targets/build_gdb.sh x86_64\n', 'sudo apk update && sudo apk add bash && sudo .github/build/install_deps.sh', '.github/build/install_compiler.sh x86_64', '.github/build/install_compiler.sh arm', 'export PATH=""$PATH:/arm-linux-musleabihf-cross/bin""\n.github/targets/build_gdb.sh armhf\n', 'sudo apk update && sudo apk add bash && sudo .github/build/install_deps.sh', '.github/build/install_compiler.sh x86_64', '.github/build/install_compiler.sh aarch64', 'patch /aarch64-linux-musl-cross/aarch64-linux-musl/include/asm/sigcontext.h < .github/patches/gdb/gdb-aarch64-header-sigcontext-fix.patch\n', 'export PATH=""$PATH:/aarch64-linux-musl-cross/bin""\n.github/targets/build_gdb.sh aarch64\n', 'mkdir /tmp/releases', 'ls -laR /tmp/releases', 'set +e\nif git rev-list ""gdb-v${{ needs.build-x86.outputs.version }}"".. >/dev/null;then\n  echo ""Tag for gdb-v${{ needs.build-x86.outputs.version }} already exists. Skipping release creation.""\n  echo ::set-output name=NEW_RELEASE::""false""\nelse\n  git tag ""gdb-v${{ needs.build-x86.outputs.version }}""\n  git push origin ""gdb-v${{ needs.build-x86.outputs.version }}""\n  echo ::set-output name=NEW_RELEASE::""true""\nfi\n']"
""
"['echo ""key=${{ runner.os }}-${{ github.job }}-${{ env.cache-name }}-${{ env.CACHE_VERSION }}-${{ hashFiles(\'.pre-commit-config.yaml\') }}-${{ hashFiles(\'requirements_test.txt\') }}"" >> $GITHUB_OUTPUT', 'python -m venv venv\n. venv/bin/activate\npip install -r requirements_test.txt\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""key=${{ runner.os }}-${{ github.job }}-${{ env.cache-name }}-${{ env.CACHE_VERSION }}-${{ hashFiles(\'.pre-commit-config.yaml\') }}-${{ hashFiles(\'requirements_test.txt\') }}"" >> $GITHUB_OUTPUT', '. venv/bin/activate\npre-commit install-hooks\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore pre-commit environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual isort --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore pre-commit environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual black --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore pre-commit environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual codespell --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore pre-commit environment from cache""\nexit 1\n', '. venv/bin/activate\npython --version\npylint --version\npylint viseron\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore pre-commit environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual flake8 --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'echo ""Failed to restore pre-commit environment from cache""\nexit 1\n', '. venv/bin/activate\npre-commit run --hook-stage manual pyupgrade --all-files --show-diff-on-failure\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', 'docker-compose --file azure-pipelines/docker-compose-build.yaml --env-file azure-pipelines/.env pull amd64-viseron\n', 'for file in ""${{ steps.changed-files.outputs.all_modified_files }}""; do\n  echo ""$file was modified""\ndone\n', 'docker-compose --file azure-pipelines/docker-compose-build.yaml --env-file azure-pipelines/.env pull amd64-wheels\ndocker-compose --file azure-pipelines/docker-compose-build.yaml --env-file azure-pipelines/.env build --build-arg BUILDKIT_INLINE_CACHE=1 amd64-wheels\ndocker-compose --file azure-pipelines/docker-compose-build.yaml --env-file azure-pipelines/.env build --build-arg BUILDKIT_INLINE_CACHE=1 amd64-viseron\n', 'docker-compose --file azure-pipelines/docker-compose-build.yaml --env-file azure-pipelines/.env build --build-arg BUILDKIT_INLINE_CACHE=1 amd64-viseron-tests\n', 'docker-compose --file azure-pipelines/docker-compose-build.yaml --env-file azure-pipelines/.env up amd64-viseron-tests\n', 'docker cp amd64-viseron-tests:/src/coverage.xml coverage.xml\n']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements.txt -r requirements-dev.txt\ngo get github.com/golang/protobuf/protoc-gen-go@v1.3.2\ngo get golang.org/x/lint/golint\ngo get github.com/argoproj/argo@v0.0.0-20210125193418-4cb5b7eb8075\n', 'set -e\nbash ./scripts/test_python.sh\n', 'mkdocs build\n', 'set -e\nbash ./scripts/test_go.sh\n', 'minikube config set vm-driver docker\nminikube config set kubernetes-version 1.18.3\nminikube start\n\nkubectl create ns argo\nkubectl create sa default -n argo\nkubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/v2.12.6/manifests/quick-start-minimal.yaml\nkubectl wait -n argo --for=condition=Ready pods --all --timeout=300s\n\nkubectl apply -n argo -f manifests/mpi-operator.yaml\n\ngo build -buildmode=c-shared -o submit.so go/couler/commands/submit.go\nscripts/integration_tests.sh\nexport E2E_TEST=true\ngo test -timeout 3m ./go/couler/submitter/... -v\n', 'python -m pip install -r requirements.txt -r requirements-dev.txt', 'mkdocs gh-deploy --force']"
"['pip install build\n', 'python -m build\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install torch\npython -m pip install -r requirements/main.txt\npython -m pip install -r requirements/dev.txt\n', ""flake8 mbrl --ignore=E203,W503 --per-file-ignores='mbrl/env/mujoco_envs.py:F401 */__init__.py:F401 tests/*:F401' --max-line-length=100\n"", 'mypy mbrl --no-strict-optional --ignore-missing-imports --follow-imports=skip\n', 'black --check mbrl\n', 'python -m pytest tests/core\n']"
"['python -m pip install --upgrade pip\npip install flake8\npip install --upgrade -r requirements.txt\n', 'export PYTHONPATH=$PYTHONPATH:.\n./tests/run_linter.sh\n', 'python -m pip install --upgrade pip\npip install --upgrade -r requirements.txt\npip install --upgrade -r tests/requirements.txt\n', 'export PYTHONPATH=$PYTHONPATH:.\nCUDA_VISIBLE_DEVICES= pytest tests/*.py\n']"
[]
"['pip install poetry\npoetry install\n', 'SUPABASE_TEST_URL=""https://tfsatoopsijgjhrqplra.supabase.co"" SUPABASE_TEST_KEY=""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYW5vbiIsImlhdCI6MTYxMjYwOTMyMiwiZXhwIjoxOTI4MTg1MzIyfQ.XL9W5I_VRQ4iyQHVQmjG0BkwRfx6eVyYB3uAKcesukg"" poetry run pytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install bandit\n', 'bandit -r -lll -ii .', 'gem install cfn-nag', 'export deployment_dir=`pwd`\necho ""$deployment_dir""\nfor i in $(find . -type f | grep -E \'.template$|.yaml$|.yml$|.json$\' | sed \'s/^.\\///\') ; do\n  echo $i\n  if [[ ""$i"" == *""templates""* ]]; then\n    cfn_nag_scan --input-path ""$deployment_dir/$i""\n    if [ $? -ne 0 ]; then\n      echo ""cfn-nag failed validation - $i""\n      exit 1\n    fi\n  fi\ndone\n', 'python -m pip install --upgrade pip\npython -m pip install checkov\n', 'checkov --quiet -d  aws_sra_examples', 'poetry install --only main --no-root', 'poetry run pip install pylic\npoetry run pylic check\n', 'poetry install --only main --no-root', 'poetry run pip install safety\npoetry run safety check\n', 'poetry install --no-interaction --no-root', 'source .venv/bin/activate', 'poetry run mypy aws_sra_examples', 'poetry run flake8 aws_sra_examples', 'poetry run black --check aws_sra_examples', 'poetry run isort --check aws_sra_examples']"
"['pip install -r dev-requirements/build.txt\n\npip install .\npip uninstall -y hikari\n\npip install .[speedups]\npip uninstall -y hikari\n', 'pip install -r dev-requirements.txt\nnox -s pytest\nnox -s pytest-all-features -- --cov-append\n\npython scripts/ci/normalize_coverage.py\nmv .coverage .coverage.${{ matrix.os }}.${{ matrix.python-version }}\n', 'pip install -r dev-requirements/coverage.txt\ncoverage combine\ncoverage xml\ncoverage report\n', 'pip install -r dev-requirements.txt\nnox -s generate-stubs\nif [ ""$(git status --short)"" ]; then\n  echo ""Stubs were not updated accordingly to the changes. Please run \'nox -s generate-stubs\' and commit the changes to fix this.""\n  exit 1\nfi\n', 'nox -s safety\n', 'nox -s mypy\n', 'nox -s verify-types\n', 'nox -s flake8\n', 'nox -s slotscheck\n', 'nox -s codespell\n', 'nox -s check-trailing-whitespaces\n', 'pip install -r dev-requirements.txt\nnox -s twemoji-test\n', 'pip install -r dev-requirements.txt\nnox -s sphinx\n', 'for result in $RESULTS; do\n  if [ ""$result"" != ""success"" ]; then\n    exit 1\n  fi\ndone\n', 'pip install -r dev-requirements/towncrier.txt\n\nif ! towncrier check --compare-with origin/${{ github.base_ref }}; then\n  exit 1\nfi\n', 'diff=$(git diff origin/${{ github.base_ref }} HEAD --name-only)\n\nchangelog_fragments=$(echo ""$diff"" | grep ""^changes/"")\nvalid_changelog_fragments=$(echo ""$diff"" | grep ""^changes/${{ github.event.number }}\\."")\n\nif [ ""$changelog_fragments"" != ""$valid_changelog_fragments"" ]; then\n  exit 1\nfi\n', 'git config --global user.name ""hikari-bot""\ngit config --global user.email ""90276125+hikari-bot[bot]@users.noreply.github.com""\n', 'bash scripts/ci/prepare-release.sh', 'git config --global user.name ""hikari-bot""\ngit config --global user.email ""90276125+hikari-bot[bot]@users.noreply.github.com""\n', 'bash scripts/ci/release.sh']"
"['pip install black==19.10b0', 'echo ""LGTM""', 'echo ""LGTM""', 'sudo apt-get install sloccount', ""sloccount caer tests examples docs; if [ $(sloccount caer | sed -n 's/.*Total Physical Source Lines of Code (SLOC)[ ]*= \\([^ ]*\\).*/\\1/p' | tr -d ',') -gt 50000 ]; then exit 1; fi"", 'conda install curl make -c conda-forge\npip install -r tools/requirements/test.txt\nmake install\n', 'pytest -v ./tests']"
"['sudo apt install curl gnupg\ncurl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor > bazel.gpg\nsudo mv bazel.gpg /etc/apt/trusted.gpg.d/\necho ""deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list\n\nsudo apt update && sudo apt install bazel\nbazel --version\n', 'python -m pip install --upgrade pip\npip install -v .\npip install -r requirements.txt\npip install -r requirements-test.txt\n', 'bazel test ml_collections/...\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install pytest\n', 'pip install -e .\n', 'python -m pytest tests/\nrm -rf tmp/\n', 'python -m pip install --upgrade pip\npip install pytest\n', 'pip install -e .\n', 'python -m pytest tests/\nrm -rf tmp/\n', 'python -m pip install --upgrade pip\npip install poetry\n', 'rm -rf dist/\npoetry build\n']"
"['# install pip=>20.1 to use ""pip cache dir""\npython -m pip install --user --progress-bar off --upgrade pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', '# requirements for PyTorch and torchvision\npip install --user --progress-bar off numpy pillow scipy\npip install --user --progress-bar off ${{ matrix.torch_address }}\n', '# requirements for unittest\npip install --user --progress-bar off flake8 pytest\npip install --user --progress-bar off pytest-cov\n# Install other dependencies\npip install --user --progress-bar off opencv-python\npip install --user --progress-bar off pycocotools>=2.0.2\npip install --user --progress-bar off onnx\npip install --user --progress-bar off onnxruntime\npip install --user --progress-bar off requests\npip install --user onnx_graphsurgeon --index-url https://pypi.ngc.nvidia.com\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=18 --max-line-length=120 --statistics\n', 'pip install -e .\n', 'PYTORCH_TEST_WITH_SLOW=1 pytest ${{ matrix.unittest_type }}\n', 'export TORCH_PATH=$(dirname $(python -c ""import torch; print(torch.__file__)""))\ncd ..\ngit clone https://github.com/pytorch/vision.git vision\ncd vision\ngit checkout ${{ matrix.torchvision }}\nmkdir build && cd build\ncmake .. -DTorch_DIR=$TORCH_PATH/share/cmake/Torch\nmake -j4\nsudo make install\n', 'python test/tracing/trace_model.py\n', 'export TORCH_PATH=$(dirname $(python -c ""import torch; print(torch.__file__)""))\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$TORCH_PATH/lib/\ncd test/tracing\nmkdir build && cd build\ncmake .. -DTorch_DIR=$TORCH_PATH/share/cmake/Torch\nmake\nmv ../yolov5s.torchscript.pt ./\necho "">> Test libtorch tracing""\n./test_tracing\n', 'brew update\nbrew install pandoc\n', '# install pip=>20.1 to use ""pip cache dir""\npython -m pip install --upgrade pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', '# requirements for unittest\npip install --progress-bar off numpy\npip install --progress-bar off torch torchvision\npip install --progress-bar off opencv-python pycocotools>=2.0.2 onnxruntime\nif [ -f requirements.txt ]; then pip install --progress-bar off -r requirements.txt; fi\n', 'cd docs\npip install --progress-bar off -r requirements.txt\nmake help\nmake html\n', 'python -m pip install --user --upgrade setuptools wheel\n', 'BUILD_VERSION=${GITHUB_REF:11} python setup.py sdist bdist_wheel\nls -lh dist/\n']"
""
"['poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run mkdocs gh-deploy --force', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'make lint\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run pytest ${{ matrix.test_path }}  --cov\n', 'echo ""uuid=$(uuidgen)"" >> $GITHUB_OUTPUT\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run coverage combine coverage*/.coverage*\npoetry run coverage xml\n', 'echo ""ðŸŽ‰""', 'poetry build', 'poetry publish --username=__token__ --password=${{ secrets.PYPI_TOKEN }}']"
"['pipx install poetry', 'poetry install --no-root', 'make lint', 'pipx install poetry', 'poetry install --no-root\npoetry run pip install tox-gh-actions\n', 'poetry run tox', 'pipx install poetry', 'make build', 'if [ ""$(gh pr view ""$PR_URL"" --json reviewDecision -q .reviewDecision)"" != ""APPROVED"" ]; then\n  gh pr review --approve ""$PR_URL""\nelse\n  echo ""PR already approved, skipping additional approvals to minimize emails/notification noise.""\nfi\ngh pr merge --auto --squash ""$PR_URL""\ngh pr edit ""$PR_URL"" --add-label ""auto-merge""\n', 'gh pr edit ""$PR_URL"" --add-label ""auto-merge""', 'gh pr edit ""$PR_URL"" --remove-label ""auto-merge""', 'pipx install poetry', 'poetry install --no-root --only=dev', '# fetch enough commits from this merge commit to the base sha to ensure\n# towncrier can inspect what changed\nwhile [[ -z $(git merge-base $PR_BASE_SHA HEAD 2> /dev/null) ]]; do\n  git fetch --quiet --deepen=50 --no-tags --no-recurse-submodules origin $PR_BASE_SHA HEAD\ndone\nif poetry run towncrier check --compare-with $PR_BASE_SHA; then\n  gh pr edit ""$PR_URL"" --add-label ""changelog-provided""\nelse\n  gh pr edit ""$PR_URL"" --remove-label ""changelog-provided""\n  exit 1\nfi\n']"
"['python -m pip install --upgrade pip\npip install .\n', 'python genData.py\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'make deps', 'make lint']"
"['python -m pip install --upgrade pip\npip install pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'cd pyhealth/unittests\npytest -rP']"
"['bash test.sh', 'RELEASE_VER=${GITHUB_REF#refs/*/}\nPACKAGE_VER=""v`python setup.py --version`""\nif [ $RELEASE_VER != $PACKAGE_VER ]\nthen\n  echo ""package ver. ($PACKAGE_VER) != release ver. ($RELEASE_VER)""; exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['python -m pip install --upgrade pip \npip install -e "".[test]""\n', 'make check']"
"['pwd\nmkdir -p docker/GeoLite2/\nwget -O docker/GeoLite2/GeoLite2-ASN.mmdb  https://github.com/1c3z/arl_files/raw/master/GeoLite2-ASN.mmdb\nwget -O docker/GeoLite2/GeoLite2-City.mmdb  https://github.com/1c3z/arl_files/raw/master/GeoLite2-City.mmdb\nls -al docker/GeoLite2\n', 'git clone https://github.com/1c3z/ARL-NPoC docker/ARL-NPoC\n', 'VERSION=$(cat version.txt)\n\ndocker buildx build --file ./docker/ARMWorker/Dockerfile \\\n  --platform linux/arm64/v8 \\\n  --output type=docker \\\n  --tag ${IMAGE_URL}:arm-${VERSION} .\n\ndocker push ${IMAGE_URL}:arm-${VERSION}\n\ndocker buildx imagetools inspect ${IMAGE_URL}:arm-${VERSION}\n', 'pwd\nmkdir -p docker/GeoLite2/\nwget -O docker/GeoLite2/GeoLite2-ASN.mmdb  https://github.com/1c3z/arl_files/raw/master/GeoLite2-ASN.mmdb\nwget -O docker/GeoLite2/GeoLite2-City.mmdb  https://github.com/1c3z/arl_files/raw/master/GeoLite2-City.mmdb\nls -al docker/GeoLite2\n', 'wget -O docker/ncrack  https://github.com/1c3z/arl_files/raw/master/ncrack\nwget -O docker/ncrack-services https://github.com/1c3z/arl_files/raw/master/ncrack-services\nchmod +x docker/ncrack\n', 'git clone https://github.com/1c3z/ARL-NPoC docker/ARL-NPoC\n', 'docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\n', 'export VERSION=$(cat version.txt)\ndocker build -t ${IMAGE_URL}:${VERSION} -f docker/worker/Dockerfile .\n', 'export VERSION=$(cat version.txt)\ndocker push ${IMAGE_URL}:${VERSION}\n', 'docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}\n', 'export DOCKER_CLI_EXPERIMENTAL=enabled\ndocker pull ${IMAGE_URL}:$(cat version.txt)\ndocker pull ${IMAGE_URL}:arm-$(cat version.txt)\n', 'export VERSION=$(cat version.txt)\nexport DOCKER_CLI_EXPERIMENTAL=enabled\n\ndocker manifest create ${IMAGE_URL}:latest \\\n  --amend ${IMAGE_URL}:arm-${VERSION} \\\n  --amend ${IMAGE_URL}:${VERSION}\n\ndocker manifest create ${IMAGE_URL}:${VERSION} \\\n  --amend ${IMAGE_URL}:arm-${VERSION} \\\n  --amend ${IMAGE_URL}:${VERSION}\n\ndocker manifest push ${IMAGE_URL}:${VERSION}\ndocker manifest push ${IMAGE_URL}:latest\n']"
""
"['python -m pip install --upgrade pip\npip install poetry flake8\npoetry install\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 setup.py udemy_enroller --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 setup.py udemy_enroller --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics\n', 'poetry run pytest\n', 'poetry run python run_enroller.py --browser=chrome --debug\n']"
[]
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B404,B602,B605 .', 'black --check . || true', 'codespell --ignore-words-list=""fo,seh""', 'flake8 --ignore=B001,B007,C408,E111,E127,E203,E261,E265,E303,E402,E701,E711,E722,F401,F541,F841,W291,W293 --max-complexity=32 --max-line-length=118 --show-source --statistics .', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest . || pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
""
"['python -m pip install --upgrade pip\npip install "".[dev]""\n', 'python -c ""import sys; print(sys.version)""\npip freeze\nopencve --version\n', 'black --diff --check {opencve,tests}', 'python -m pip install --upgrade pip\npip install "".[dev]""\n', 'python -c ""import sys; print(sys.version)""\npip freeze\nopencve --version\n', 'PGPASSWORD=opencve psql -h localhost -p 5432 -U opencve opencve -c ""CREATE EXTENSION IF NOT EXISTS pg_trgm;""\n', ""pwd\nls $GITHUB_WORKSPACE/tests/opencve.cfg\nsed -i 's$postgresql://user:secret@localhost:5432/dbname$postgresql://opencve:opencve@localhost:5432/opencve$g' $GITHUB_WORKSPACE/tests/opencve.cfg\n"", 'pytest tests/ -v']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\npython -m pysr install\n', 'pip install coverage coveralls', ""coverage run --source=pysr --omit='*/test/*,*/feynman_problems.py' -m pysr.test main\ncoverage run --append --source=pysr --omit='*/test/*,*/feynman_problems.py' -m pysr.test cli\n"", 'pip install jax jaxlib', ""coverage run --append --source=pysr --omit='*/test/*,*/feynman_problems.py' -m pysr.test jax"", 'pip install torch', ""coverage run --append --source=pysr --omit='*/test/*,*/feynman_problems.py' -m pysr.test torch"", ""coverage run --append --source=pysr --omit='*/test/*,*/feynman_problems.py' -m pysr.test env"", 'coveralls --service=github', 'python3 -m pip install .\npython3 -m pysr install\n', 'cd /tmp && python -m pysr.test main', 'pip install coveralls\ncoveralls --finish\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\npython -m pysr install\n', 'python -m pysr.test main\npython -m pysr.test cli\n', 'pip install torch', 'python -m pysr.test torch', 'python -m pysr.test env', 'conda activate pysr-test && mamba install pysr', 'conda activate pysr-test && conda install pysr', 'python -m pysr.test main', 'docker build --platform=${{ matrix.arch }} -t pysr --build-arg JLVERSION=${{ matrix.julia-version }} --build-arg PYVERSION=${{ matrix.python-version }} .', ""docker run --platform=${{ matrix.arch }} --rm pysr /bin/bash -c 'python3 -m pysr.test main && python3 -m pysr.test cli && python3 -m pysr.test env'"", 'docker build --platform=${{ matrix.arch }} -t pysr --build-arg JLVERSION=${{ matrix.julia-version }} .', ""docker run --platform=${{ matrix.arch }} --rm pysr /bin/bash -c 'python3 -m pysr.test main && python3 -m pysr.test cli && python3 -m pysr.test env'"", 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\npython -m pysr install\n', 'python -m pysr.test main\npython -m pysr.test cli\n', 'python -m pysr.test env', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\npython -m pysr install\n', 'python -m pysr.test main\npython -m pysr.test cli\n', 'pip install jax jaxlib', 'python -m pysr.test jax', 'pip install torch', 'python -m pysr.test torch', 'python -m pysr.test env', 'pip install -r docs/requirements.txt', 'pip install -e .', 'cd docs && ./gen_docs.sh', 'mkdocs gh-deploy --force', 'pip install wheel', 'python setup.py sdist bdist_wheel', ""python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\n# Not needed:\n# python -c 'import pysr; pysr.install()'\n"", 'cd $(mktemp -d)\ngit clone https://github.com/MilesCranmer/SymbolicRegression.jl\ncd SymbolicRegression.jl\necho ""version=$(git describe --tags --abbrev=0 | sed \'s/^v//\')"" >> $GITHUB_OUTPUT\n', 'echo ""version=$(python -c \'import pysr; print(pysr.version.__symbolic_regression_jl_version__)\' 2>/dev/null)"" >> $GITHUB_OUTPUT\n', '# Bump PySR patch number:\nCURRENT_PYSR_PATCH_VERSION=$(python -c \'import pysr; print(pysr.version.__version__.split(""."")[-1], end="""")\' 2>/dev/null)\nNEW_PYSR_PATCH_VERSION=$((CURRENT_PYSR_PATCH_VERSION + 1))\nsed -i ""s/^__version__ = .*/__version__ = \\""$(python -c \'import pysr; print(""."".join(pysr.version.__version__.split(""."")[:-1]), end="""")\' 2>/dev/null).${NEW_PYSR_PATCH_VERSION}\\""/"" pysr/version.py\n\n# Set SymbolicRegression.jl version:\nsed -i ""s/^__symbolic_regression_jl_version__ = .*/__symbolic_regression_jl_version__ = \\""${{ steps.get-latest.outputs.version }}\\""/"" pysr/version.py\n']"
"['python wordlist_from_dir1.py\n', 'python wordlist_from_dir.py\n', 'git config --global user.email mrwq@qq.com\ngit config --global user.name mrwq\ngit add .\ngit commit -m ""update file name"" -a\n']"
"['git config user.name ""GitHub Actions""\ngit config user.email ""PiotrMachowski@users.noreply.github.com""\nif (git checkout dev)\nthen\ngit merge --ff-only master || git merge --no-commit master\ngit commit -m ""Automatically merge master -> dev"" || echo ""No commit needed""\ngit push origin dev\nelse\necho ""No dev branch""\nfi\n', 'pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-2020 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B105,B108,B303,B304,B324,B311,B413,B506 .', 'black --check . || true', 'codespell --ignore-words-list=""hass""', 'flake8 custom_components --count --ignore=B001,E241,E265,E302,E722,E731,F403,F405,F841,W504 --max-complexity=21 --max-line-length=184 --show-source --statistics', 'isort --check-only --profile black custom_components || true', 'pip install -r requirements.txt || pip install --editable . || true', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive custom_components || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check', 'cd /home/runner/work/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/custom_components/xiaomi_cloud_map_extractor\nzip xiaomi_cloud_map_extractor.zip -r ./\n']"
"['python -m pip install --upgrade pip\npip install pytest\npip install --use-pep517 -e .\n', 'pip install coveralls\npip install pytest-cover\n', 'py.test tods/tests/ --cov=tods\ncoveralls\n']"
""
[]
"['pip install coverage\ncoverage run -m unittest discover tests\n', 'python -m pip install --upgrade build', 'python -m pip install --upgrade twine', 'echo ""VAR_VERSION=$(cat hiddeneye_reborn/__init__.py | grep version | cut -d= -f2- | tr -d \'""\' | tr -d ""[:blank:]"")"" >> $GITHUB_ENV', 'echo $VAR_VERSION', 'python -m build', 'pip install coverage\ncoverage run -m unittest discover tests\n', 'pip install codecov\ncodecov -t ${{ secrets.CODECOV_TOKEN }}\n']"
"['echo ""datew=$(date \'+%Y-%V\')"" >> $GITHUB_OUTPUT\n', 'which python\npwd\nls -al .\npython -m pip install --upgrade pip wheel\npip uninstall -y monai\npip uninstall -y monai\npip uninstall -y monai-weekly\npip uninstall -y monai-weekly\ncd core\nBUILD_MONAI=0 ./runtests.sh -b\npython -m pip install -r requirements-dev.txt\npython -m pip install --upgrade torch torchvision torchaudio\nrm -rf /github/home/.cache/torch/hub/mmars/\n', 'python -m pip list\nnvidia-smi\npython -c ""import torch; print(torch.__version__); print(\'{} of GPUs available\'.format(torch.cuda.device_count()))""\npython -c \'import torch; print(torch.rand(5,3, device=torch.device(""cuda:0"")))\'\n', 'echo ""test latest algo""\nls research-contributions/\ncp -r research-contributions/auto3dseg/algorithm_templates core/\ncd research-contributions && git log -1 && cd ../core\npwd\nls -ll\nexport OMP_NUM_THREADS=4\nexport MKL_NUM_THREADS=4\nexport MONAI_TESTING_ALGO_TEMPLATE=algorithm_templates\npython -m tests.test_auto3dseg_ensemble\npython -m tests.test_auto3dseg_hpo\npython -m tests.test_integration_autorunner\npython -m tests.test_integration_gpu_customization\n', 'rm -rf research-contributions\nrm -rf core\n', 'echo ""sha_short=$(git rev-parse --short HEAD)"" >> $GITHUB_OUTPUT', 'echo $release_version\ncd auto3dseg/\ntar -cvzf ""$release_version"".tar.gz algorithm_templates\n']"
"['brew install libomp  # https://github.com/pytorch/pytorch/issues/20030\n', 'python -c ""from pip._internal.locations import USER_CACHE_DIR; print(\'::set-output name=dir::\' + USER_CACHE_DIR)""\n', 'pip install --requirement requirements.txt --upgrade --quiet --find-links https://download.pytorch.org/whl/cpu/torch_stable.html\npip install --requirement tests/requirements.txt --quiet\npython --version\npip --version\npip list\n', 'coverage run --source src,detoxify -m pytest src tests -v --junitxml=junit/test-results-${{ runner.os }}-${{ matrix.python-version }}.xml\n', 'coverage report\n', 'pip install flake8']"
['git checkout HEAD^2']
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['make test-docker', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin', 'echo ""${{ secrets.DOCKER_TOKEN }}"" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin', '# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\nTAG2=""""\nif [[ ""$VERSION"" == ""2.""* ]]; then\n  TAG2=""--tag $DOCKER_REPO:2""\nfi\n\necho VERSION=$VERSION\necho TAG2=$TAG2\n\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes\ndocker buildx build \\\n--platform=linux/amd64,linux/arm64 \\\n--output ""type=image,push=true"" \\\n--file ./Dockerfile . \\\n--tag $DOCKER_REPO:$VERSION $TAG2\n', 'docker build . --file Dockerfile --tag $IMAGE_NAME\n\nIMAGE_ID=docker.pkg.github.com/${{ github.repository }}/$IMAGE_NAME\n\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && VERSION=latest\n\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\n\n# Push to GitHub Package\ndocker tag $IMAGE_NAME $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n\nif [[ ""$VERSION"" == ""2.""* ]]; then\n  # Push to GitHub Package\n  docker tag $IMAGE_NAME $IMAGE_ID:2\n  docker push $IMAGE_ID:2\nfi', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install pytest\npython -m pip install wheel\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py test\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo apt-get install sloccount', ""sloccount tinygrad test examples extra; if [ $(sloccount tinygrad | sed -n 's/.*Total Physical Source Lines of Code (SLOC)[ ]*= \\([^ ]*\\).*/\\1/p' | tr -d ',') -gt 1000 ]; then exit 1; fi"", ""pip install -e '.[linting,testing]' --extra-index-url https://download.pytorch.org/whl/cpu"", ""python -m pylint --disable=all -e W0311 --jobs=0 --indent-string='  ' **/*.py"", 'flake8 tinygrad/ --indent-size=2 --select=F,E112,E113,E203,E304,E502,E702,E703,E71,E72,E731,W191,W6 --statistics -j4', 'pylint tinygrad/', 'mypy tinygrad/ --ignore-missing-imports --check-untyped-defs --explicit-package-bases --warn-unreachable', ""pip install -e '.[testing]' --extra-index-url https://download.pytorch.org/whl/cpu"", 'python docs/abstractions.py', 'python -m pytest -s -v -n=auto test/', 'pip install -e .', 'PYTHONPATH=""."" CLANG=1 python3 examples/compile_efficientnet.py > recognize.c', 'clang -O2 recognize.c -lm -o recognize', 'curl https://media.istockphoto.com/photos/hen-picture-id831791190 | ./recognize | grep hen', ""pip install -e '.[llvm,testing]' --extra-index-url https://download.pytorch.org/whl/cpu"", 'ENABLE_METHOD_CACHE=1 LLVM=1 python -m pytest -s -v -n=auto test/', ""pip install -e '.[testing]' --extra-index-url https://download.pytorch.org/whl/cpu"", 'TORCH=1 python -m pytest -s -v -n=auto test/', 'TORCH=1 python -m pytest test/external/external_test_onnx_backend.py --tb=no --disable-warnings || true', 'wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null\necho ""deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main"" | sudo tee /etc/apt/sources.list.d/oneAPI.list\nsudo apt-get update\n', 'sudo apt-get install -y intel-oneapi-runtime-compilers intel-oneapi-runtime-opencl', ""pip install -e '.[testing]' --extra-index-url https://download.pytorch.org/whl/cpu"", 'PYTHONPATH=""."" OPT=2 GPU=1 python test/external/external_test_opt.py\nPYTHONPATH=""."" OPT=3 GPU=1 python test/external/external_test_opt.py\n', 'GPU=1 python -m pytest -s -v -n=auto test/', 'wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null\necho ""deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main"" | sudo tee /etc/apt/sources.list.d/oneAPI.list\nsudo apt-get update\n', 'sudo apt-get install -y intel-oneapi-runtime-compilers intel-oneapi-runtime-opencl', ""pip install -e '.[testing]' --extra-index-url https://download.pytorch.org/whl/cpu"", 'GPU=1 IMAGE=1 python3 test/test_ops.py\nFORWARD_ONLY=1 GPU=1 IMAGE=2 python3 test/test_ops.py\n', 'ALLOWED_KERNEL_COUNT=199 FLOAT16=1 VALIDHACKS=1 DEBUGCL=1 GPU=1 IMAGE=2 python3 openpilot/compile.py\npython3 -c \'import os; assert os.path.getsize(""/tmp/output.thneed"") < 100_000_000\'\nDEBUGCL=1 GPU=1 IMAGE=2 python3 openpilot/compile.py\nVALIDHACKS=1 DEBUGCL=1 GPU=1 IMAGE=2 python3 openpilot/compile.py\n', 'docker build -t tinygrad -f test/Dockerfile .', 'docker run --rm tinygrad /usr/bin/env python3 -c ""from tinygrad.tensor import Tensor; print(Tensor.eye(3).numpy())""']"
"['python -m pip install --upgrade pip\npython -m pip install flake8==5.0.4 isort==5.10.1\npython -m pip install black==22.6.0\nflake8 --version\n', 'echo ""Running isort""\nisort --profile black .\necho ""Running black""\nblack --check .\necho ""Running flake8""\nflake8 .\n', 'python -m pip install -U pip\npython -m pip install ninja opencv-python-headless onnx pytest-xdist codecov\npython -m pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\npython -m pip install Cython termcolor numpy tensorboard pycocotools matplotlib pyaml opencv-python tqdm pytorch-lightning torchmetrics codecov flake8 pytest timm\npython -m pip install -r requirements.txt\n', 'rm -rf .eggs && python setup.py develop', 'coverage run --branch --source nanodet -m pytest tests/\ncoverage xml\ncoverage report -m\n']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install -e "".[dev]""\n', 'make check']"
"['LATEST_TAG=$(git describe --tags `git rev-list --tags --max-count=1`)\ngit checkout $LATEST_TAG\necho ""tag_name=$LATEST_TAG"" >> $GITHUB_OUTPUT;\n', 'pip3 install wheel\npip3 install twine\nmake dist\ntwine upload -u ${{ secrets.PYPI_USER_NAME }} -p ${{ secrets.PYPI_PASSWORD }} dist/*\n', 'function set-emoji-output {\n  if [ ""$2"" == ""success"" ];\n  then echo ""$1=:github-check-mark:"" >> $GITHUB_OUTPUT;\n  else echo ""$1=:github-changes-requested:"" >> $GITHUB_OUTPUT;\n  fi\n}\nset-emoji-output status ${{ steps.build_and_release.outcome }}\n', 'if [ ! -f ""/etc/lsb-release"" ] ; then\n  echo ""DISTRIB_RELEASE=18.04"" > /etc/lsb-release\nfi\n', ""pip install -e '.[all]'"", 'export LIGHTLY_SERVER_LOCATION=""localhost:-1""\npip install pytest-cov\npython -m pytest -s -v --runslow --cov=./lightly --cov-report=xml --ignore=./lightly/openapi_generated/\n', 'if [ ! -f ""/etc/lsb-release"" ] ; then\n  echo ""DISTRIB_RELEASE=18.04"" > /etc/lsb-release\nfi\n', ""pip install -e '.[all]'"", 'export LIGHTLY_SERVER_LOCATION=""localhost:-1""\nmake format-check\n', 'if [ ! -f ""/etc/lsb-release"" ] ; then\n  echo ""DISTRIB_RELEASE=18.04"" > /etc/lsb-release\nfi\n', 'pip install .', 'LIGHTLY_SERVER_LOCATION=""localhost:-1""\nlightly-crop --help\nlightly-train --help\nlightly-embed --help\nlightly-magic --help\nlightly-download --help\nlightly-version\n', 'LIGHTLY_SERVER_LOCATION=""localhost:-1""\ngit clone https://github.com/alexeygrigorev/clothing-dataset-small clothing_dataset_small\nINPUT_DIR_1=""clothing_dataset_small/test/dress""\nlightly-train input_dir=$INPUT_DIR_1 trainer.max_epochs=1 loader.num_workers=6\nlightly-embed input_dir=$INPUT_DIR_1\n', 'pip install .', 'export LIGHTLY_SERVER_LOCATION=${{ secrets.LIGHTLY_SERVER_LOCATION }}\nbash tests/UNMOCKED_end2end_tests/run_all_unmocked_tests.sh ${{ secrets.DUMMY_USER_TOKEN_STAGING }}\n', 'pip3 install "".[dev]""\n', 'pytest -n auto\n']"
"['scripts/ci-install-deps.sh\npip install black\n', 'black --version\nblack --check --diff ./inputremapper ./tests\n', 'scripts/ci-install-deps.sh\npip install flake8 pylint mypy black types-pkg_resources\n', 'echo ""REWIEVDOG_REPORTER=github-pr-review"" >> $GITHUB_ENV', 'echo ""REWIEVDOG_REPORTER=github-check"" >> $GITHUB_ENV', 'reviewdog -list\nreviewdog -tee -runners=mypy,black -reporter=${{ env.REWIEVDOG_REPORTER }} -fail-on-error=false\n', '# Install deps as root since we will run tests as root\nsudo scripts/ci-install-deps.sh\nsudo pip install --no-binary :all: .\n', '# FIXME: Had some permissions issues, currently worked around by running tests as root\nmkdir test_tmp\nTMPDIR=""$(realpath test_tmp)"" sudo python tests/test.py --start-dir unit\n']"
""
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install build check-manifest twine\n', 'check-manifest --verbose .\n\npython -m build --sdist --outdir dist/ .\n\nmkdir empty/\ncd empty\n\ntar -xvf ../dist/*\ncd *\n\n# build the wheel from the sdist\npython -m build --wheel --outdir ../../dist/ .\ncd ../../\n\ntwine check dist/*\n', 'sudo apt-get update\nsudo apt-get install --yes tzdata locales\nsudo locale-gen en_US.UTF-8 de_DE.UTF-8\n', 'python -m pip install --upgrade pip setuptools wheel\npip install tox\n', 'tox --notest --installpkg dist/*.whl\n', 'tox --skip-pkg-install\n', 'mkdir coverage_reports\ncp .coverage ""coverage_reports/.coverage.${{ env.JOB_NAME }}""\ncp coverage.xml ""coverage_reports/coverage.${{ env.JOB_NAME }}.xml""\n', 'python -m pip install --upgrade pip setuptools wheel\npip install tox\n', 'tox --notest --installpkg dist/*.whl\n', 'tox --skip-pkg-install -- --compare-branch=""${BASE_REF}""\n', 'mkdir all_coverage_report\ncp .coverage ""all_coverage_report/.coverage.all""\ncp coverage.xml ""all_coverage_report/coverage.all.xml""\n', 'import this']"
[]
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip3 install pkgconfig cython --upgrade && python3 setup.py sdist\n', 'scripts/install', 'make docs', 'scripts/install', 'scripts/check', 'scripts/tests']"
"['pip install mypy\n# Refer http://mypy-lang.blogspot.com/2021/06/mypy-0900-released.html\npip install mypy types-requests types-python-dateutil types-PyYAML types-dateparser types-protobuf types-pytz\nmypy obsei\n', ""python -m pip install --upgrade pip\npip install '.[dev,all]'\npip install --upgrade --upgrade-strategy eager trafilatura\n"", 'python -m spacy download en_core_web_lg\npython -m spacy download en_core_web_sm\ncoverage run -m pytest\ncoverage report -m\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine hatch\n', 'hatch build\ntwine upload dist/*\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'echo ${{ steps.docker_build.outputs.digest }}']"
""
"['pip install -r requirements.txt', 'make html\ncp scripts/mermaid.js build/html/_static/\n', 'touch build/html/.nojekyll']"
"['add-apt-repository -y ppa:git-core/ppa\napt-get update\napt-get install -y git curl ca-certificates\ncurl -sL https://deb.nodesource.com/setup_14.x | bash -\napt-get install -y nodejs\n', 'git config --global --add safe.directory $GITHUB_WORKSPACE', 'python3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install .[camera_obs,rllib,test,torch,train]\n', "". ${{env.venv_dir}}/bin/activate\npip freeze | grep -v 'smarts' | grep -v 'pkg-resources==0.0.0' > requirements.txt\n"", '. ${{env.venv_dir}}/bin/activate\npip install .[dev]\nisort -m VERTICAL_HANGING_INDENT --skip-gitignore --ac --tc --profile black ./cli ./envision ./examples/ ./utils/ ./scenarios/ ./smarts ./zoo\nblack .\nnpx prettier --write ./envision/web/src\n', ""cd $GITHUB_WORKSPACE\npython3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install .[camera_obs,rllib,test,torch,train]\npip freeze | grep -v 'smarts' | grep -v 'pkg-resources==0.0.0' > utils/setup/mac_requirements.txt\n"", 'python3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install -e .[camera_obs,opendrive,test,test_notebook,torch,train,gym,argoverse,envision]\nif echo ${{matrix.tests}} | grep -q -e ""env"" -e ""examples""; then pip install -e .[rllib]; fi\nif echo ${{matrix.tests}} | grep -q ""/ray""; then pip install -e .[ray]; fi\n', '. ${{env.venv_dir}}/bin/activate\nmake build-all-scenarios\nPYTHONPATH=$PWD PYTHONHASHSEED=42 pytest -v \\\n  --doctest-modules \\\n  --forked \\\n  --dist=no \\\n  -n auto \\\n  --ignore-glob=""**/ros.py"" \\\n  --ignore-glob=""**/waymo_map.py"" \\\n  --ignore-glob=""**/argoverse_map.py"" \\\n  ${{matrix.tests}} \\\n  --ignore=./smarts/core/tests/test_smarts_memory_growth.py \\\n  --ignore=./smarts/core/tests/test_env_frame_rate.py \\\n  --ignore=./smarts/env/tests/test_benchmark.py \\\n  --ignore=./examples/tests/test_learning.py \\\n  -k \'not test_long_determinism\'\n', 'brew update\nbrew install python@3.8\nbrew unlink python@3.9\nbrew link --force --overwrite python@3.8\n', 'brew install xquartz\nbrew tap dlr-ts/sumo\nbrew install sumo spatialindex\nbrew install geos\n', 'python3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install -r utils/setup/mac_requirements.txt\npip install -e .[camera_obs,opendrive,rllib,test,test_notebook,torch,train,argoverse,envision]\nif echo ${{matrix.tests}} | grep -q -e ""/env"" -e ""/examples""; then pip install -e .[rllib]; fi\nif echo ${{matrix.tests}} | grep -q ""/ray""; then pip install -e .[ray]; fi\n', '. ${{env.venv_dir}}/bin/activate\nexport SUMO_HOME=""/usr/local/opt/sumo/share/sumo""\nopen -g -a XQuartz.app\nmake build-all-scenarios\nPYTHONPATH=$PWD PYTHONHASHSEED=42 pytest -v \\\n  --doctest-modules \\\n  -n auto \\\n  --ignore-glob=""**/waymo_map.py"" \\\n  --ignore-glob=""**/argoverse_map.py"" \\\n  ${{matrix.tests}} \\\n  --ignore=./smarts/core/tests/test_smarts_memory_growth.py \\\n  --ignore=./smarts/env/tests/test_benchmark.py \\\n  --ignore=./smarts/env/tests/test_frame_stack.py \\\n  --ignore=./smarts/env/tests/test_determinism.py \\\n  --ignore=./smarts/core/tests/test_renderers.py \\\n  --ignore=./smarts/core/tests/test_smarts.py \\\n  --ignore=./smarts/core/tests/test_env_frame_rate.py \\\n  --ignore=./smarts/core/tests/test_observations.py\n', 'cd $GITHUB_WORKSPACE\nmake header-test\n', ""cd $GITHUB_WORKSPACE\npip install --upgrade pip\npip install pylint\npylint -d all \\\n  -e missing-function-docstring \\\n  -e missing-class-docstring \\\n  -s n \\\n  --ignore examples,scenarios,docs,manager_pb2_grpc.py,worker_pb2_grpc.py \\\n  --msg-template='{path}: line {line}: {msg_id}: {msg}' \\\n  ./smarts ./envision ./baselines\n"", 'add-apt-repository -y ppa:git-core/ppa\napt-get update\napt-get install -y git\n', 'git config --global --add safe.directory $GITHUB_WORKSPACE', 'python3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install wheel==0.38.4\npip install -e .[dev,camera_obs,train,test]\n', 'cd $GITHUB_WORKSPACE\nCHANGED=$(git diff --diff-filter=""AM"" --name-only --ignore-submodules=all origin/master... | awk -v d="" "" \'{s=(NR==1?s:s d)$0}END{print s}\')\ndeclare -A SEEN\nUNIQUE=()\nfor file in ${CHANGED[@]}; do filename=$(basename $file); if [ ! -v ""SEEN[$filename]"" ]; then SEEN[$filename]+=1; UNIQUE+=($file); fi; done \nOUT=$( IFS=$\' \'; echo ""${UNIQUE[*]}"")\necho ""::set-output name=changed_files::$OUT""\n', '. ${{env.venv_dir}}/bin/activate\npytype -d pyi-error,import-error ${{steps.changed-files.outputs.changed_files}}\n', 'git config --global --add safe.directory $GITHUB_WORKSPACE', 'apt-get update --fix-missing\napt-get install -y libenchant-dev\n', 'python3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip ""setuptools<58.3.0""\npip install wheel==0.38.4\npip install .[camera_obs,doc,train,ray,envision,argoverse,opendrive,waymo]\ncd ${GITHUB_WORKSPACE}/docs\nmake html SPHINXOPTS=""-W -T -E -n --keep-going -b spelling""\n', 'cd $GITHUB_WORKSPACE\nif [ -d ""docs/_build/html"" ]; then\n    echo ""Sphinx build successful.""\nelse\n    echo ""Sphinx build failed.""\n    exit 1\nfi\n', 'apt-get update --fix-missing\napt-get install -y python3.8-dev\n', 'git config --global --add safe.directory $GITHUB_WORKSPACE', 'python3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip ""setuptools<58.3.0""\npip install wheel==0.38.4\npip install .[all]\npip check\n', 'cd $GITHUB_WORKSPACE\npython3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install .[camera_obs,rllib,test,torch,train]\n', 'cd $GITHUB_WORKSPACE\n. ${{env.venv_dir}}/bin/activate\napt-get update && apt-get -y install git\ngit checkout $(git log --merges -n 1 --format=format:""%H"")\nscl scenario build-all --clean ./scenarios\npytest --benchmark-save=previous --benchmark-min-rounds=10 --benchmark-timer=time.process_time ./smarts/env/tests/test_benchmark.py\ngit checkout -\npip install .[camera_obs,rllib,test,torch,train]\nscl scenario build-all --clean ./scenarios\npytest --benchmark-compare=0001_previous --benchmark-compare-fail=mean:10% --benchmark-min-rounds=10 --benchmark-timer=time.process_time ./smarts/env/tests/test_benchmark.py\n', 'cd $GITHUB_WORKSPACE\npython3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install .[camera_obs,rllib,test,torch,train]\n', 'cd $GITHUB_WORKSPACE\n. ${{env.venv_dir}}/bin/activate\nmake test-learning\n', 'cd $GITHUB_WORKSPACE\npython3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install .[camera_obs,rllib,test,torch,train]\n', 'cd $GITHUB_WORKSPACE\n. ${{env.venv_dir}}/bin/activate\nmake test-long-determinism\n', 'cd $GITHUB_WORKSPACE\npython3.8 -m venv ${{env.venv_dir}}\n. ${{env.venv_dir}}/bin/activate\npip install --upgrade pip\npip install wheel==0.38.4\npip install pympler\npip install .[camera_obs,rllib,test,torch,train]\n', 'cd $GITHUB_WORKSPACE\n. ${{env.venv_dir}}/bin/activate\nmake test-memory-growth\n']"
[]
"['python -m pip install --upgrade pip\npip install flake8\n', 'flake8 . --isolated --exclude=.cache,.venv,.svn,CVS,.bzr,.hg,.git,__pycache__,.tox,**/certificate_generator/**,**/migrations/** --ignore=E203,W503 --max-line-length=119']"
"['poetry publish --build', 'pip install -U pip\npip install poetry==1.4.2\npoetry install\n', 'git config --global user.name ""fastapi_template""\ngit config --global user.email ""fastapi_template@pytest.python""\n', 'poetry run pytest -vv --exitfirst -n auto']"
"['sudo apt-get install axel -y\nmkdir data\naxel -n 20 http://dl.fbaipublicfiles.com/GENRE/kilt_titles_trie_dict.pkl -o data\n', 'sudo apt-get install axel -y\nmkdir models\naxel -n 20 http://dl.fbaipublicfiles.com/GENRE/fairseq_wikipage_retrieval.tar.gz\ntar -xvf fairseq_wikipage_retrieval.tar.gz --directory models\n', 'pip install -r requirements-test.txt', 'git clone -b fixing_prefix_allowed_tokens_fn --single-branch https://github.com/nicola-decao/fairseq.git\npip install -e ./fairseq\n', 'pip install -e .', 'pytest tests --verbose']"
""
"['apt-get update\napt-get install -y libusb-1.0-0-dev libudev-dev ruby ruby-dev rubygems build-essential desktop-file-utils wget unzip zlib1g-dev liblzma-dev libssl-dev git imagemagick file libfuse2\nwget https://github.com/AppImage/pkg2appimage/archive/38603d92359a48189c35debad9005e8e902e6070.zip\nunzip *.zip\ngem install --no-document fpm\n', './util/setup_python36.sh', './util/python36/prefix/bin/python3 -m venv venv\n. venv/bin/activate\npip install -r requirements.txt\n', '. venv/bin/activate\nfbs freeze\nfbs installer\ndeactivate\n./pkg2appimage-*/pkg2appimage misc/Vial.yml\nmv out/Vial-*.AppImage out/Vial-x86_64.AppImage\n', 'curl https://www.python.org/ftp/python/${PYTHON_VERSION}/python-${PYTHON_VERSION}-macosx${MACOSX_DEPLOYMENT_TARGET}.pkg -o ""python.pkg""', ""shasum -a 256 -c <<< '4bcd53faffc98d193ef7cdccd5668de3829c702af4db45258819a84a2cab60d0 *python.pkg'"", 'sudo installer -pkg python.pkg -target /\n', 'python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n', 'source venv/bin/activate\nfbs freeze\nhdiutil create -volname Vial -srcfolder ""target/Vial.app"" -ov -format UDZO vial-mac.dmg\n', 'python -m venv venv\n. .\\venv\\Scripts\\activate.ps1\npip install -r requirements.txt\n', 'Invoke-WebRequest ""https://github.com/vial-kb/vial-deps/releases/download/v1/nsis-3.06.1-setup.exe"" -UserAgent ""wget"" -OutFile nsis-3.06.1-setup.exe\nif ((Get-FileHash nsis-3.06.1-setup.exe -Algorithm sha256).Hash -ne ""f60488a676308079bfdf6845dc7114cfd4bbff47b66be4db827b89bb8d7fdc52"") { Write-Error \'SHA256 mismatch\' }\nStart-Process -FilePath ""nsis-3.06.1-setup.exe"" -ArgumentList ""/S"", ""/D=C:\\Program Files (x86)\\NSIS"" -NoNewWindow -Wait\n', '. .\\venv\\Scripts\\activate.ps1\nfbs freeze\nCompress-Archive -Path ""target\\Vial"" -DestinationPath vial-win.zip\n', '$env:Path += "";C:\\Program Files (x86)\\NSIS""\n. .\\venv\\Scripts\\activate.ps1\nfbs installer\n']"
"['python -m pip install --upgrade pip\necho ""::set-output name=dir::$(pip cache dir)""\n', ""pip install -e '.[testing]'\n"", 'flake8 .\n', 'pylint --fail-under 9.75 ferminet\n', 'pytype ferminet\n', 'python -m pip install --upgrade pip\necho ""::set-output name=dir::$(pip cache dir)""\n', ""pip install -e '.[testing]'\nif [ ${{ matrix.package-overrides }} != none ]; then\n  pip install ${{ matrix.package-overrides }}\nfi\n"", 'python -m pytest\n', 'FERMINET_CHEX_N_CPU_DEVICES=2 python -m pytest ferminet/tests/train_test.py\n']"
"['pip3 install --upgrade pip\npip3 install pyflakes\n', 'pyflakes .']"
""
"['/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 Build-Binary.command --reset_binaries --branch ""${{ env.branch }}"" --commit ""${{ env.commiturl }}"" --commit_date ""${{ env.commitdate }}"" --key ""${{ env.ANALYTICS_KEY }}"" --site ""${{ env.ANALYTICS_SITE }}""', 'codesign -s ""${{ env.MAC_CODESIGN_IDENTITY }}"" -v --force --deep --timestamp --entitlements ./payloads/entitlements.plist -o runtime ""dist/OpenCore-Patcher.app""', 'cd dist; ditto -c -k --sequesterRsrc --keepParent OpenCore-Patcher.app ../OpenCore-Patcher-wxPython.app.zip', 'xcrun notarytool submit OpenCore-Patcher-wxPython.app.zip --apple-id ""${{ env.MAC_NOTARIZATION_USERNAME }}"" --password ""${{ env.MAC_NOTARIZATION_PASSWORD }}"" --team-id ""${{ env.MAC_NOTARIZATION_TEAM_ID }}""', 'packagesbuild ./payloads/InstallPackage/AutoPkg-Assets-Setup.pkgproj', 'mv ./OpenCore-Patcher-wxPython.app.zip ./OpenCore-Patcher-GUI.app.zip', 'npm install', 'npm run build', 'python3 -m pip install -r requirements.txt', 'python3 OpenCore-Patcher-GUI.command --validate', '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 OpenCore-Patcher-GUI.command --validate']"
"['echo ""to-do""']"
"['pip install -r requirements.min.txt && pip install -r requirements.dev.txt', 'black --check --diff -l 120 -t py37 src', 'black --check --diff -l 120 -t py37 tests', 'isort --check src', 'isort --check tests', 'flake8 src', 'flake8 tests', 'mypy', 'pip install -e .[dev]', 'pip install -r requirements.min.txt', 'python -m pytest', 'pip install -e .[dev]', 'python -m pytest', 'pip install -e .[dev]', 'pip install -r requirements.min.txt', 'pip install catboost sentence-transformers', 'jupyter nbconvert --to python examples/*/*.ipynb --output-dir example_scripts', 'curl https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip -o Bike-Sharing-Dataset.zip && unzip Bike-Sharing-Dataset.zip -d Bike-Sharing-Dataset', 'python example_test.py', 'pip install -r requirements.min.txt', 'pip install wheel', 'python setup.py sdist bdist_wheel']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'sudo apt-get -y install ghostscript\n', 'pip install gTTS\n', 'pip install lxml\n', 'pip install markdown-it-py\n', 'pip install matplotlib==3.5.3\n', 'pip install nltk\n', 'pip install opencv-python\n', 'pip install pandas\n', 'pip install textblob\n', 'pytest\n']"
""
"[""echo '${{ steps.file_changes.outputs.files}}'"", 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install sh\n', 'python -m pip list\n', 'pytest -v\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install sh\n', 'python -m pip list\n', 'pytest -v\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\n', 'python -m pip list\n', 'pytest -v\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install pytest-cov[toml]\npip install sh\n', 'pytest --cov src']"
"['echo ""VERSION=${GITHUB_REF/refs\\/tags\\//}"" >> $GITHUB_OUTPUT', 'git tag ${{ steps.get_version.outputs.VERSION }}\ngit push origin ${{ steps.get_version.outputs.VERSION }}\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8==3.8.1 flake8-bugbear flake8-comprehensions isort==4.3.21\npython -m pip install black==20.8b1\nflake8 --version\n', 'echo ""Running isort""\nisort -c -sp .\necho ""Running black""\nblack -l 100 --check .\necho ""Running flake8""\nflake8 .\n', ""python -m pip install -U pip\npython -m pip install ninja opencv-python-headless onnx pytest-xdist\npython -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\npython -m pip install -U 'git+https://github.com/facebookresearch/fvcore'\n"", 'CC=clang CXX=clang++ python -m pip install -e .[all]\npython -m detectron2.utils.collect_env\n', 'python -m pytest -n 4 -v tests/']"
"['DOCKER_IMAGE=${{ secrets.DOCKER_USERNAME }}/${GITHUB_REPOSITORY#*/}\nVERSION=latest\n\n# If this is git tag, use the tag name as a docker tag\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/v}\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION}""\n\n# If the VERSION looks like a version number, assume that\n# this is the most recent version of the image and also\n# tag it \'latest\'.\nif [[ $VERSION =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n  TAGS=""$TAGS,${DOCKER_IMAGE}:latest""\nfi\n\n# Set output parameters.\necho ::set-output name=tags::${TAGS}\necho ::set-output name=docker_image::${DOCKER_IMAGE}\n']"
"['RELEASE_VER=${GITHUB_REF#refs/*/}\nPACKAGE_VER=""v`python setup.py --version`""\nif [ $RELEASE_VER != $PACKAGE_VER ]\nthen\n  echo ""package ver. ($PACKAGE_VER) != release ver. ($RELEASE_VER)""; exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-ml.txt ]; then pip install -r requirements-ml.txt; fi\nif [ -f requirements-reports.txt ]; then pip install -r requirements-reports.txt; fi\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-ml.txt ]; then pip install -r requirements-ml.txt; fi\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\nif [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi\nif [ -f requirements-reports.txt ]; then pip install -r requirements-reports.txt; fi\n', 'pre-commit install\npre-commit run --all-files\n', 'DATAPROFILER_SEED=0 pytest --forked --cov=dataprofiler --cov-fail-under=80\n']"
"['python -m pip install --upgrade pip\npip install pipenv\npipenv install --system --skip-lock\n', 'cp tubesync/tubesync/local_settings.py.example tubesync/tubesync/local_settings.py', 'cd tubesync && python3 manage.py test --verbosity=2', 'echo ""${{ secrets.REGISTRY_ACCESS_TOKEN }}"" | docker login https://ghcr.io -u ${{ github.actor }} --password-stdin', 'echo ""${{ secrets.REGISTRY_ACCESS_TOKEN }}"" | docker login https://ghcr.io -u ${{ github.actor }} --password-stdin']"
[]
"['echo \'def get_ver():\' >> ./utils/functions.py\necho \'    return ""${{github.ref_name}}""\' >> ./utils/functions.py\n', 'pip install -r requirements.txt\npip install pyinstaller\n', ""pyinstaller -F main.py -n  'chaoxing-${{github.ref_name}}-${{ matrix.name }}'"", 'ls dist\necho ${{github.ref}}\n']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python3 tests.py\n']"
"['python -m pip install --upgrade pip\npip install -e "".[dev, sbert]""\n', 'make check']"
"['gh pr merge -b ""[no ci]"" --auto --merge ""$PR_URL""', 'poetry install -E all\n', 'poetry run pre-commit run --all-files --show-diff-on-failure', 'poetry publish --build -u ""${{ env.PYPI_USERNAME }}"" -p ""${{ env.PYPI_PASSWORD }}""', 'export DOCKER_ID=$(docker run -d scylladb/scylla:latest --cluster-name test )\nexport CQL_TEST_HOST=$(docker inspect --format=\'{{ .NetworkSettings.IPAddress }}\' ${DOCKER_ID})\nwhile ! nc -z ${CQL_TEST_HOST} 9042; do   \n  sleep 0.1 # wait for 1/10 of the second before check again\ndone\n\necho ""CQL_TEST_HOST=${CQL_TEST_HOST}"" >> $GITHUB_ENV\n', 'poetry install -E all\n', 'poetry run pytest tests -m ""not gino"" --cov-report=xml --cassandra-dsn=${CQL_TEST_HOST}\n', 'poetry run pip install -U ""sqlalchemy>2.0.0""\npoetry run pytest tests -m ""sqlalchemy20"" --cov-append --cov-report=xml --cassandra-dsn=${CQL_TEST_HOST}\n', 'poetry run pip install -U ""gino[starlette]"" ""fastapi==0.84.0""\npoetry run pytest -m gino --cov-append --cov-report=xml\n']"
"['pip install --upgrade poetry', 'poetry publish --build', 'make docker-release']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt -U\npip install pyinstaller -U\n', 'pyinstaller -y -F -w -i ""extra/DUCE-LOGO.ico""  --clean --name ""DUCE-GUI-windows"" --add-data ""base.py;."" --add-data ""colors.py;."" --add-data ""README.md;."" --add-data ""LICENSE;.""  ""gui.py""', 'pyinstaller -y -F -c -i ""extra/DUCE-LOGO.ico""  --clean --name ""DUCE-CLI-windows"" --add-data ""base.py;."" --add-data ""colors.py;."" --add-data ""README.md;."" --add-data ""LICENSE;.""  ""cli.py""', 'ls']"
""
"['source_branch=$(jq -r .pull_request.head.ref ""$GITHUB_EVENT_PATH"")\ntarget_branch=$(jq -r .pull_request.base.ref ""$GITHUB_EVENT_PATH"")\n\necho ""Source-branch=$source_branch"" >> $GITHUB_OUTPUT\necho ""target-branch=$target_branch"" >> $GITHUB_OUTPUT\n', 'echo ""source-branch=${{ steps.branch-name-check.outputs.source-branch }}""\necho ""target-branch=${{ steps.branch-name-check.outputs.target-branch }}""\n', 'if [[ ""${{ steps.branch-name-check.outputs.source-branch }}"" =~ ^(main|feature/.*|docs/.*|hotfix/.*|release/[0-9]+\\.[0-9]+\\.[0-9]+(rc[0-9]+)?)$ ]]; then\n  echo ""Branch name is valid""\nelse\n  echo ""Invalid branch name. Branches must follow the GitFlow naming convention to be allowed to merge into the develop branch.""\n  exit 1\nfi\n', 'if [[ ""${{ steps.branch-name-check.outputs.source-branch }}"" =~ ^(hotfix/.*|release/[0-9]+\\.[0-9]+\\.[0-9]+(rc[0-9]+)?)$ ]]; then\n  echo ""PR is from a hotfix or release branch and targets the main branch""\nelse\n  echo ""PR is not from a hotfix or release branch. Pull requests must be from a hotfix or release branch and target the main branch""\n  exit 1\nfi', 'echo ""OPENBB_LOGGING_COMMIT_HASH=sha:$(git rev-parse --short=8 ""$GITHUB_SHA"")"" >> $GITHUB_ENV\n', 'echo ""OPENBB_LOGGING_APP_NAME=\'${OPENBB_LOGGING_APP_NAME}\'"" > openbb_terminal/.env\necho ""OPENBB_LOGGING_COMMIT_HASH=\'${OPENBB_LOGGING_COMMIT_HASH}\'"" >> openbb_terminal/.env\ncat openbb_terminal/.env\n', 'source build/docker/compose.env\nsource build/docker/build.sh\necho ""OPENBBTERMINAL_DOCKER_POETRY_IMAGE=$OPENBBTERMINAL_DOCKER_POETRY_IMAGE"" >> $GITHUB_ENV\necho ""OPENBBTERMINAL_DOCKER_POETRY_IMAGE_LATEST=$OPENBBTERMINAL_DOCKER_POETRY_IMAGE_LATEST"" >> $GITHUB_ENV\n', 'docker push ""${OPENBBTERMINAL_DOCKER_POETRY_IMAGE}""\n', 'docker push ""${OPENBBTERMINAL_DOCKER_POETRY_IMAGE_LATEST}""\n', 'sudo apt-get update\nsudo apt-get install -y \\\n    libgtk-3-dev \\\n    libwebkit2gtk-4.0-dev\n', 'poetry install --no-interaction --no-root -E doc', 'poetry install --no-interaction -E forecast -E doc -E optimization', 'source $VENV\npip uninstall Brotli -y\npytest tests/website --autodoc\npython website/generate_sdk_markdown.py && python website/generate_terminal_markdown.py\n', 'yarn install', 'yarn build', 'git fetch origin ${{ github.event.pull_request.head.ref }} && git checkout FETCH_HEAD', 'sudo apt-get update\nsudo apt-get install -y \\\n    libgtk-3-dev \\\n    libwebkit2gtk-4.0-dev\n', 'poetry install --no-interaction -E optimization', 'source $VENV\npython terminal.py -t | tee result.txt\ngrep ""================================ Integration Test Summary ================================"" result.txt -A100 | tail --bytes=2000 > summary.txt\necho  >> summary.txt\n', ""source $VENV\npython terminal.py -t --coverage | tee result.txt\nsed -n '/Integration Coverage Summary/,$p' result.txt >> summary.txt\n"", 'pip install bandit black codespell mypy==1.1.1 pylint==2.17.0 ruff==0.0.256\npip install types-pytz types-requests types-termcolor types-tabulate types-PyYAML types-python-dateutil types-setuptools types-six\n', 'bandit -x ./tests -r . || true', 'black --diff --check .', 'codespell --ignore-words-list=gard,commun,statics,ro,zar,zlot,jewl,ba,buil,coo,ether,hist,hsi,mape,navagation,operatio,pres,ser,yeld,shold,ist,varian,datas,ake,creat,vie,hel,ket,toke,certi,buidl,ot,te,buda,shs,welp --quiet-level=2 --skip=./tests,.git,*.css,*.csv,*.html,*.ini,*.ipynb,*.js,*.json,*.lock,*.scss,*.txt,*.yaml,./build/pyinstaller,./website/config.toml', 'ruff .', 'mypy --ignore-missing-imports openbb_terminal', 'pylint terminal.py openbb_terminal tests', 'git log', 'export PATH=""""\nexport PATH=""/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin""\necho $PATH\n', 'conda info\npip list\nbuild/conda/cleanup_artifacts.sh\npoetry install -E all -E installer\npip uninstall papermill -y\npip install git+https://github.com/nteract/papermill.git@main\npip list\n', 'build/pyinstaller/build4mac.sh', 'echo ""Ensuring Keychain with same name does not exist""\nrm -rf /Users/openbb/Library/Keychains/build.keychain-db\necho ""Decoding certificate""\necho $MACOS_CERTIFICATE | base64 --decode > certificate.p12\necho ""Creating Keychain""\nsecurity create-keychain -p $MACOS_KEYCHAIN_PWD build.keychain\necho ""Setting Default Keychain""\nsecurity default-keychain -s build.keychain\necho ""Unlocking Keychain""\nsecurity unlock-keychain -p $MACOS_KEYCHAIN_PWD build.keychain\necho ""Importing Keychain""\nsecurity import certificate.p12 -k build.keychain -P $MACOS_CERTIFICATE_PWD -T /usr/bin/codesign\necho ""Setting Partition List""\nsecurity set-key-partition-list -S apple-tool:,apple:,codesign: -s -k $MACOS_KEYCHAIN_PWD build.keychain\n', 'security unlock-keychain -p $MACOS_INSTALLER_KEYCHAIN_PWD install.keychain\nbuild/pyinstaller/macOS/build-macos.sh OpenBBTerminal 0.0.1\nmv build/pyinstaller/macOS/target/pkg/OpenBBTerminalM1.pkg OpenBBTerminal.pkg\n', 'echo ""Deleting Previous Keychain to Clean Instance""\nrm -rf /Users/openbb/Library/Keychains/build.keychain-db\n', 'rm -rf build/terminal\nrm -rf dist\nrm -rf DMG\n', 'xcrun notarytool submit OpenBBTerminal.pkg --apple-id ""$NOTARIZE_APPLE_ID"" --password ""$NOTARIZE_APPLE_PWD"" --team-id ""$NOTARIZE_APPLE_TEAM_ID"" --wait\n', 'xcrun stapler staple OpenBBTerminal.pkg\n', 'rm -rf build/terminal\nrm -rf dist\nrm -rf DMG\n', 'pkgutil --expand-full OpenBBTerminal.pkg extract/\nrm -rf OpenBBTerminal.pkg\n', 'extract/OpenBBTerminal.pkg/Payload/Applications/OpenBB\\ Terminal/.OpenBB/OpenBBTerminal -t -s forecast cryptocurrency reports alternative economy futures econometrics dashboards portfolio stocks/test_stocks_options_screen.openbb stocks/test_stocks_options.openbb forex etf stocks/test_stocks_fa.openbb | tee result.txt\ngrep ""================================ Integration Test Summary ================================"" result.txt -A100 | tail --bytes=2000 > summary.txt\n', ""extract/OpenBBTerminal.pkg/Payload/Applications/OpenBB\\ Terminal/.OpenBB/OpenBBTerminal -t --coverage | tee result.txt\necho  >> summary.txt\nsed -n '/Integration Coverage Summary/,$p' result.txt >> summary.txt\n"", 'rm -rf /Users/openbb/Desktop/OpenBB\\ Terminal\nrm -rf ~/Desktop/OPENBB-exports\nrm -rf extract/\n', 'git log', 'export PATH=""""\nexport PATH=""/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin""\necho $PATH\n', 'conda info\npip list\nbuild/conda/cleanup_artifacts.sh\npoetry install -E all -E installer\npip uninstall papermill -y\npip install git+https://github.com/nteract/papermill.git@main\npip list\n', 'build/pyinstaller/build4mac.sh', 'echo ""Ensuring Keychain with same name does not exist""\nrm -rf /Users/openbb/Library/Keychains/build.keychain-db\necho ""Decoding certificate""\necho $MACOS_CERTIFICATE | base64 --decode > certificate.p12\necho ""Creating Keychain""\nsecurity create-keychain -p $MACOS_KEYCHAIN_PWD build.keychain\necho ""Setting Default Keychain""\nsecurity default-keychain -s build.keychain\necho ""Unlocking Keychain""\nsecurity unlock-keychain -p $MACOS_KEYCHAIN_PWD build.keychain\necho ""Importing Keychain""\nsecurity import certificate.p12 -k build.keychain -P $MACOS_CERTIFICATE_PWD -T /usr/bin/codesign\necho ""Setting Partition List""\nsecurity set-key-partition-list -S apple-tool:,apple:,codesign: -s -k $MACOS_KEYCHAIN_PWD build.keychain\n', 'security unlock-keychain -p $MACOS_INSTALLER_KEYCHAIN_PWD install.keychain\nbuild/pyinstaller/macOS/build-macos.sh OpenBBTerminal 0.0.1\nmv build/pyinstaller/macOS/target/pkg/OpenBBTerminalM1.pkg OpenBBTerminal.pkg\n', 'echo ""Deleting Previous Keychain to Clean Instance""\nrm -rf /Users/openbb/Library/Keychains/build.keychain-db\n', 'rm -rf build/terminal\nrm -rf dist\nrm -rf DMG\n', 'xcrun notarytool submit OpenBBTerminal.pkg --apple-id ""$NOTARIZE_APPLE_ID"" --password ""$NOTARIZE_APPLE_PWD"" --team-id ""$NOTARIZE_APPLE_TEAM_ID"" --wait\n', 'xcrun stapler staple OpenBBTerminal.pkg\n', 'rm -rf build/terminal\nrm -rf dist\nrm -rf DMG\n', 'pkgutil --expand-full OpenBBTerminal.pkg extract/\nrm -rf OpenBBTerminal.pkg\n', 'extract/OpenBBTerminal.pkg/Payload/Applications/OpenBB\\ Terminal/.OpenBB/OpenBBTerminal -t -s forecast cryptocurrency reports alternative economy futures econometrics dashboards portfolio stocks/test_stocks_options_screen.openbb stocks/test_stocks_options.openbb forex etf stocks/test_stocks_fa.openbb | tee result.txt\ngrep ""================================ Integration Test Summary ================================"" result.txt -A100 | tail --bytes=2000 > summary.txt\necho  >> summary.txt\n', ""extract/OpenBBTerminal.pkg/Payload/Applications/OpenBB\\ Terminal/.OpenBB/OpenBBTerminal -t --coverage | tee result.txt\nsed -n '/Integration Coverage Summary/,$p' result.txt >> summary.txt\n"", 'rm -rf /Users/openbb/Desktop/OpenBB\\ Terminal\nrm -rf ~/Desktop/OPENBB-exports\nrm -rf extract/\n', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'conda remove brotlipy -y\n', 'conda info\npip install --requirement requirements-full.txt\npip install docstring_parser\n', 'conda list\npip list\n', 'pytest tests/', 'python terminal.py', 'sed -i \'s/name = "".*""/name = ""openbb-nightly""/\' pyproject.toml\nsed -i ""3s/version = \\""\\(.*\\)\\""/version = \\""\\1.dev$(date +%Y%m%d)\\""/"" pyproject.toml\nhead pyproject.toml\nsed -i \'s/pip install openbb/pip install openbb-nightly/g; s/from openbb_terminal.sdk import openbb/from openbb_terminal.sdk import openbb/g\' ./website/pypi.md\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'current_branch=$(jq -r .pull_request.base.ref ""$GITHUB_EVENT_PATH"")\n\nif git diff --name-only origin/$current_branch HEAD | grep -E "".py$|openbb_terminal\\/.*|pyproject.toml|poetry.lock|requirements.txt|requirements-full.txt""; then\n  echo ""check-changes=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""check-changes=false"" >> $GITHUB_OUTPUT\nfi\n', 'echo ""check-changes=${{ steps.check-changes.outputs.check-changes }}""', 'sudo apt-get update\nsudo apt-get install -y \\\n    libgtk-3-dev \\\n    libwebkit2gtk-4.0-dev\n', 'poetry install --no-interaction -E optimization', 'source $VENV\npytest tests/ --optimization --cov --cov-fail-under=50 --autodoc -n auto --durations=10 --timeout=30 --cov-report=xml\n', 'source $VENV\npython terminal.py\n', 'sudo apt-get update\nsudo apt-get install -y \\\n    libgtk-3-dev \\\n    libwebkit2gtk-4.0-dev\n', 'poetry install --no-interaction -E optimization', 'source $VENV\npip list\n', 'source $VENV\npytest tests/ --optimization --autodoc -n auto --timeout=30\n', 'source $VENV\npython terminal.py\n', 'sudo apt-get update\nsudo apt-get install -y \\\n    libgtk-3-dev \\\n    libwebkit2gtk-4.0-dev\n', 'poetry install --no-interaction -E optimization\n', 'source $VENV\npip list\n', 'source $VENV\npython terminal.py\n', 'source $VENV\npytest tests/ --optimization --autodoc -n auto --timeout=30\n', 'git config --global core.autocrlf false\ngit config --global core.eol lf\n', 'sudo apt-get update\nsudo apt-get install -y \\\n    libgtk-3-dev \\\n    libwebkit2gtk-4.0-dev\n', 'c:\\windows\\system32\\icacls C:\\Users\\runneradmin\\AppData\\Local\\Temp /grant ""everyone"":F /t', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install --requirement requirements-full.txt\npip uninstall Brotli -y\n', 'conda list\npip list\n', 'pytest tests/ -m ""not linux"" --optimization -n auto --timeout=30', 'python terminal.py', ""Set-ItemProperty 'HKLM:\\System\\CurrentControlSet\\Control\\FileSystem' -Name 'LongPathsEnabled' -value 1\ngit config --system core.longpaths true\n"", 'python -m venv venv --upgrade-deps\nsource venv/Scripts/activate\npython -m pip install setuptools==64.0.2 wheel\npython -m pip install -r requirements-full.txt\npython -m pip uninstall papermill -y\npython -m pip install git+https://github.com/nteract/papermill.git@main\npython -m PyInstaller build/pyinstaller/terminal.spec --clean\npwd\n', 'Get-Location\nCompress-Archive dist release.zip\n', ""Set-ItemProperty 'HKLM:\\System\\CurrentControlSet\\Control\\FileSystem' -Name 'LongPathsEnabled' -value 1\ngit config --system core.longpaths true\n"", 'Expand-Archive release.zip -DestinationPath .', 'cp -r .\\dist\\OpenBBTerminal\\ .\\build\\nsis\\app\\', ""C:\\'.\\Program Files (x86)\\'\\nsis\\makensis.exe .\\build\\nsis\\setup.nsi\n"", ""& 'C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x86\\signtool.exe' sign /fd sha256 /tr http://ts.ssl.com /td sha256 /sha1 26a8e28dbe3b1db7407b92d7de97c290d4436a32 'C:\\Users\\Administrator\\actions-runner\\_work\\OpenBBTerminal\\OpenBBTerminal\\build\\nsis\\OpenBB Terminal Setup.exe'\n"", 'dist\\OpenBBTerminal\\OpenBBTerminal.exe -t -s forecast cryptocurrency reports alternative economy futures econometrics dashboards portfolio stocks/test_stocks_options_screen.openbb stocks/test_stocks_options.openbb forex etf stocks/test_stocks_fa.openbb | tee result.txt\nSelect-String ""================================ Integration Test Summary ================================"" result.txt -Context 0,100 | Select-Object -Last 2000 | Out-File summary.txt\nWrite-output `n >> summary.txt\n', 'dist\\OpenBBTerminal\\OpenBBTerminal.exe -t --coverage | tee result.txt\nSelect-String ""============================== Integration Coverage Summary =============================="" result.txt -Context 0,300 | Select-Object -Last 2000 >> summary.txt\n', ""Remove-Item '.\\build\\nsis\\OpenBB Terminal Setup.exe' -Recurse\nRemove-Item .\\build\\nsis\\app\\ -Recurse\nRemove-Item .\\dist\\ -Recurse\n"", 'git config --global core.autocrlf false\ngit config --global core.eol lf\n', 'c:\\windows\\system32\\icacls C:\\Users\\runneradmin\\AppData\\Local\\Temp /grant ""everyone"":F /t', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'conda remove brotlipy -y\n', 'conda info\nbuild\\conda\\cleanup_artifacts.bat\npip install docstring_parser\npoetry install -E all -E installer\n', 'conda list\npip list\n', 'pytest tests/ -m ""not linux""', 'python terminal.py']"
"[""echo ::set-output name=version::${GITHUB_REF#refs/tags/}\necho ::set-output name=gh-username-lower::$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')\n"", 'python -m pip install --upgrade pip\npip install pytest flake8\npip install .\npython scripts/compile_locales.py\n', '# warnings if there are Python syntax errors or undefined names\n# (remove --exit-zero to fail when syntax error)\nflake8 . --count --exit-zero --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\npython setup.py sdist bdist_wheel\n', 'pip install Babel==2.11.0\npython scripts/compile_locales.py\npython setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install pytest flake8\npip install .\n', '# warnings if there are Python syntax errors or undefined names\n# (remove --exit-zero to fail when syntax error)\nflake8 . --count --exit-zero --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest -v', 'docker build -f docker/Dockerfile -t libretranslate .', 'docker build -f docker/Dockerfile -t libretranslate --build-arg models=en,es  .']"
"['python -m pip install --upgrade pip', 'sudo apt-get install libopenmpi-dev -y\npip install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cpu\npip install -r requirements/requirements.txt\npip install -r requirements/requirements-dev.txt\npip install -r requirements/requirements-wandb.txt\n', 'python prepare_data.py', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python pytest tests -m cpu', 'echo ${{ steps.docker_build.outputs.digest }}', 'rm megatron/__init__.py\npip install shortuuid\nrm megatron/neox_arguments/__init__.py\npython configs/gen_docs.py\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add configs/neox_arguments.md\ngit commit -m ""Update NeoXArgs docs automatically""\ngit push\n', 'python prepare_data.py', 'pytest --forked tests']"
"['docker build --push --no-cache \\\n  --tag hkotel/mealie:dev \\\n  --platform linux/amd64,linux/arm64 .\n', 'docker build --push \\\n  --tag hkotel/mealie:latest \\\n  --platform linux/amd64,linux/arm64 .\n', 'docker build --push \\\n  --tag hkotel/mealie:${{ steps.mealie_version.outputs.tag }} \\\n  --platform linux/amd64,linux/arm64 .\n', 'sudo apt-get install libsasl2-dev libldap2-dev libssl-dev\npoetry install\npoetry add ""psycopg2-binary==2.8.6""\n', 'make test-all\n', 'make test-all\n']"
""
"['cd ${{ github.workspace }}/custom_components/xiaomi_miot\nzip -r xiaomi_miot.zip ./\n', 'mkdir ./test_configuration\ncp -r ./custom_components ./test_configuration\necho ""default_config:"" >> ./test_configuration/configuration.yaml\necho ""xiaomi_miot:"" >> ./test_configuration/configuration.yaml\necho ""  language: zh"" >> ./test_configuration/configuration.yaml\necho ""  device_customizes:"" >> ./test_configuration/configuration.yaml\necho ""    brand.device.model:"" >> ./test_configuration/configuration.yaml\necho ""      miot_local: true"" >> ./test_configuration/configuration.yaml\n']"
"['cd backend\npython -m pip install --upgrade pip\npip install -r requirements.txt\n', 'cd backend\npip install sourcery-cli\nsourcery login --token ${{ secrets.SOURCERY_TOKEN }}\nsourcery review --check .\n', 'cd backend\npython -m unittest\n', 'cd backend\ncoverage run --source=src -m unittest\ncoveralls\n', 'cd frontend\nyarn\n', 'cd frontend\nyarn eslint .']"
[]
"['echo ""${{ secrets.PACKAGE_TOKEN }}"" | docker login ghcr.io -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=ghcr.io/${{ github.repository }}/$IMAGE_NAME\n# Change all uppercase to lowercase\nIMAGE_ID=$(echo $IMAGE_ID | tr \'[A-Z]\' \'[a-z]\')\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""main"" ] && VERSION=latest\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\necho ""::set-output name=IMAGE_ID::$IMAGE_ID""\necho ""::set-output name=VERSION::$VERSION""\n', '# Inspect Image\ndocker buildx imagetools inspect ${{ steps.tags.outputs.IMAGE_ID }}:${{ steps.tags.outputs.VERSION }}\n', 'rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', 'python -m pip install --upgrade pip\npip install flake8 pytest mock pytest-mock\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E203,E266,E501,W503,F403,F401,C901 --max-line-length=160 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=10 --statistics\n', 'pytest tests/unit_tests\n']"
"['poetry install\n', 'poetry run py.test\n', './extract-installer.sh', 'poetry install --only main --no-root\n', 'poetry build\n', 'poetry install\n', 'poetry run py.test\n', 'poetry install\n', 'poetry build\n', 'poetry publish\n', 'poetry install\n', 'poetry run py.test\n', 'poetry run pylint thetagang -E\n', 'poetry run black thetagang --check --diff --color\n']"
"['git branch --show-current', '(git branch --show-current | grep -E ""release/"") && echo ""::set-output name=branch::$(git branch --show-current)"" || echo ""::set-output name=branch::main""\n', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install --upgrade protobuf', 'pip3 install .[dev,torchvision,deepsparse,onnxruntime,transformers,yolov5]', 'make testinteg TARGETS=yolov5,transformers,image_classification', 'echo ""##[set-output name=tag;]$(echo ${GITHUB_REF_NAME#*/})""', 'echo ${{ steps.extract_tag.outputs.tag }}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'git branch --show-current', '(git branch --show-current | grep -E ""release/"") && echo ""::set-output name=branch::$(git branch --show-current)"" || echo ""::set-output name=branch::main""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src/sparseml|tests/integrations]/transformers|setup.py"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::transformers"" || echo ""::set-output name=output::""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src/sparseml|tests/integrations]/yolov5|setup.py"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::yolov5"" || echo ""::set-output name=output::""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src/sparseml/pytorch|tests/integrations]/image_classification|setup.py"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::image_classification"" || echo ""::set-output name=output::""\n', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install --upgrade protobuf', 'pip3 install .[dev,torchvision,deepsparse,onnxruntime,transformers,yolov5]', 'make testinteg TARGETS=$TRANSFORMERS,$YOLOV5,$IMAGE_CLASSIFICATION', 'pip3 install sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev]', 'make quality', 'git branch --show-current', '(git branch --show-current | grep -E ""release/"") && echo ""::set-output name=branch::$(git branch --show-current)"" || echo ""::set-output name=branch::main""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src|tests]/sparseml|setup.py|.github"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::1"" || echo ""::set-output name=output::0""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src|tests]/sparseml/deepsparse|setup.py|.github"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::1"" || echo ""::set-output name=output::0""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src|tests]/sparseml/onnx|setup.py|.github"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::1"" || echo ""::set-output name=output::0""\n', '((git diff --name-only origin/main HEAD | grep -E ""[src|tests]/sparseml/pytorch|setup.py|.github"") || (echo $GITHUB_REF | grep -E ""refs/heads/[release/|main]"")) && echo ""::set-output name=output::1"" || echo ""::set-output name=output::0""\n', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install --upgrade protobuf', 'pip3 install .[dev,onnxruntime]', 'make test', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install --upgrade protobuf', 'pip3 install .[dev,deepsparse,onnxruntime]', 'make test TARGETS=deepsparse', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,torchvision,onnxruntime]', 'make test TARGETS=onnx', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,torchvision,onnxruntime]', 'make test TARGETS=pytorch', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,torchvision,onnxruntime] torch==1.9.1', 'make test TARGETS=pytorch', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,torchvision,onnxruntime] torch==1.9.1', 'make test TARGETS=onnx']"
"['sudo apt-get update && sudo apt-get install -y make && pip install twine', 'make publish']"
"['python -m pip install --upgrade pip\npython -m pip install pytest\npython -m pip install -U proDy requests\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py test\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['echo ""##[set-output name=tag;]$(echo ${GITHUB_REF_NAME#*/})""', 'echo ${{ steps.extract_tag.outputs.tag }}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'pip3 install .', 'deepsparse.benchmark zoo:cv/classification/mobilenet_v1-1.0/pytorch/sparseml/imagenet/pruned_quant-moderate -w 1 -t 1', 'python3.8 -m venv venv-dev\nsource venv-dev/bin/activate\npip install -e .\n', 'source venv-dev/bin/activate\ndeepsparse.benchmark zoo:cv/classification/mobilenet_v1-1.0/pytorch/sparseml/imagenet/pruned_quant-moderate -w 1 -t 1\n', 'pip3 install sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev]', 'make quality', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,server,image_classification,transformers] opencv-python', 'make test', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,server,image_classification,transformers]', 'PYTEST_ARGS=""-m smoke"" make test TARGETS=cli,nobase', 'pip3 install -U pip && pip3 install setuptools sparsezoo/', 'rm -r sparsezoo/', 'pip3 install .[dev,server,image_classification,transformers,haystack]', 'make test_integrations']"
""
""
"['if [[ ${{ github.event.client_payload.tag }} =~ ^v[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n    echo ::set-output name=match::true\nfi\n', 'if [[ ${{ github.event.client_payload.ref }} =~ ^refs/tags/v[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n    echo ::set-output name=versiontag::$(echo ""${{github.event.client_payload.ref}}"" | cut -d / -f 3)\nfi\n', 'curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/21.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list > /dev/null\nsudo apt-get update\nACCEPT_EULA=Y sudo apt-get install -y libsasl2-dev msodbcsql18\npython -m pip install --upgrade pip\ncat dev-requirements.in | grep tox | xargs pip install\n', 'tox -- soda -k soda/core\ntox -- soda -k soda/${{ matrix.data-source }}\n', 'sudo apt-get update\nsudo apt-get install -y libsasl2-dev\npython -m pip install --upgrade pip\ncat dev-requirements.in | grep tox | xargs pip install\n', 'tox -- soda -k soda/core\n', 'sudo apt-get update\nsudo apt-get install -y libsasl2-dev\npython -m pip install --upgrade pip\ncat dev-requirements.in | grep tox | xargs pip install\n', 'tox -- soda -k soda/scientific\n', 'pip install --upgrade setuptools wheel twine', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'FAILED=false\ncd soda\nfor pack in *\ndo\n  pushd .\n  cd $pack\n\n  echo ""| Building $pack""\n  python3 setup.py sdist bdist_wheel\n\n  echo ""| Uploading $pack to pypi""\n  UPLOAD_FAILED=false\n  twine upload dist/* || UPLOAD_FAILED=true\n  if [ $UPLOAD_FAILED = true ]; then\n    FAILED=true\n    echo ""Failed to upload $pack""\n  fi\n\n  popd\ndone\n\nif [ $FAILED = true ]; then\n  echo ""There was an error, check the logs please.""\n  exit 1\nfi\n', 'if [[ ${{ github.event.ref }} =~ ^refs/tags/v[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n    echo ::set-output name=match::true\n    echo ::set-output name=versiontag::$(echo $GITHUB_REF | cut -d / -f 3)\nfi\n', 'curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/21.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list > /dev/null\nsudo apt-get update\nACCEPT_EULA=Y sudo apt-get install -y libsasl2-dev msodbcsql18\npython -m pip install --upgrade pip\ncat dev-requirements.in | grep tox | xargs pip install\n', 'tox -- soda -k soda/core\ntox -- soda -k soda/${{ matrix.data-source }}\n', 'sudo apt-get update\nsudo apt-get install -y libsasl2-dev\npython -m pip install --upgrade pip\ncat dev-requirements.in | grep tox | xargs pip install\n', 'tox -- soda -k soda/core\n', 'sudo apt-get update\nsudo apt-get install -y libsasl2-dev\npython -m pip install --upgrade pip\ncat dev-requirements.in | grep tox | xargs pip install\n', 'tox -- soda -k soda/scientific\n']"
"['python -m pip install --upgrade pip\npip install -r tests/dev.requirements.txt\n', 'mypy fastapi_crudrouter\n', 'flake8 fastapi_crudrouter\n', 'curl -s https://pypi.org/pypi/fastapi-crudrouter/json | grep -Eo \'""version"":""[0-9].[0-9][0-9]?.[0-9][0-9]?""\' | grep -Eo ""[0-9].[0-9][0-9]?.[0-9][0-9]?"" > old\ngrep -Eo \'__version__ = ""[0-9].[0-9][0-9]?.[0-9][0-9]?""\' ./fastapi_crudrouter/_version.py | grep -Eo ""[0-9].[0-9][0-9]?.[0-9][0-9]?"" > new\n\ncat new\ncat old\nif cmp --silent new old; then\n echo --- SKIPPING VERSION BUMP ---\nelse\n echo ---BUMPING VERSION---\n\n python -m pip install --upgrade pip\n pip install setuptools wheel twine\n\n python setup.py sdist bdist_wheel\n twine upload dist/*\nfi\n', 'python -m pip install --upgrade pip\npip install -r tests/dev.requirements.txt\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install pygithub\n', 'python .github/scripts/releases.py']"
"['python -m pip install --upgrade pip\npip install . -r pipelines/requirements/docs.txt\n', 'make docs\n', 'git diff --exit-code\n', 'pip install -U -r pipelines/requirements/dev.txt\n', 'python setup.py sdist bdist_wheel\n', 'twine upload -u __token__ -p ${{ secrets.PYPI_TOKEN }} dist/*\n', 'python -m pip install --upgrade pip\npip install . -r pipelines/requirements/ci.txt\n', 'python -m prisma db push --schema=tests/data/schema.prisma --skip-generate\ncp tests/data/dev.db dev.db\n', 'nox -s setup\n', 'python scripts/ci.py -s test --no-error-on-missing-interpreters\n', 'python -m pip install --upgrade pip\npip install . -r pipelines/requirements/ci.txt\n', 'nox -s typesafety-pyright\n', 'nox -s typesafety-mypy\n', 'python -m pip install --upgrade pip\npip install -U . -r pipelines/requirements/ci.txt\n', 'nox -s lint\n', 'nox -s format\ngit diff --exit-code\n', 'python -m pip install --upgrade pip\npip install -U . -r pipelines/requirements/ci.txt\n', 'nox -s mypy\n', 'python -m pip install --upgrade pip\npip install -U . -r pipelines/requirements/ci.txt\n', 'coverage run -m nox -s databases -- serve ${{ matrix.name }} --version=${{ matrix.version }}\n', 'coverage run -m nox -s databases -- test --coverage --databases=${{ matrix.name }}\n', 'coverage run -m nox -s databases -- test-inverse --coverage --databases=${{ matrix.name }}\n', 'coverage run -m nox -s databases -r -- test --coverage --databases=${{ matrix.name }} --for-async=false\n', 'coverage run -m nox -s databases -r -- test-inverse --coverage --databases=${{ matrix.name }} --for-async=false\n', 'python -m pip install --upgrade pip\npip install -U . -r pipelines/requirements/ci.txt\n', 'nox -s report-strict\n', 'nox -s push-coverage\n', 'docker build --% -f tests/windows.Dockerfile -t prisma-client-py .\n']"
""
"['source activate /home/admin/tf12_py2/\npython git-lfs/git_lfs.py pull\nsource scripts/ci_test.sh\n', 'echo ""CI_TEST_PASSED=${CI_TEST_PASSED}""\nif [ $CI_TEST_PASSED -ne 1 ]\nthen\n  echo ""ci_test_failed, will exit""\n  exit 1\nfi\n', 'source activate tf15_py3\npython git-lfs/git_lfs.py pull\nsource scripts/ci_test.sh\n', 'echo ""CI_TEST_PASSED=${CI_TEST_PASSED}""\nif [ $CI_TEST_PASSED -ne 1 ]\nthen\n  echo ""ci_py3_test_failed, will exit""\n  exit 1\nfi\n', 'source ~/.bashrc\nconda activate tf25_py3\npython git-lfs/git_lfs.py pull\nsource scripts/ci_test.sh\n', 'echo ""CI_TEST_PASSED=${CI_TEST_PASSED}""\nif [ $CI_TEST_PASSED -ne 1 ]\nthen\n  echo ""ci_py3_tf25_test_failed, will exit""\n  exit 1\nfi\n', 'source ~/.bashrc\nsource activate tf25_py3\npre-commit run -a\nif [ $? -eq 0 ]\nthen\n  echo ""ci_test_passed=1"" >> $GITHUB_OUTPUT\nelse\n  pre-commit run -a\n  if [ $? -eq 0 ]\n  then\n    echo ""ci_test_passed=1"" >> $GITHUB_OUTPUT\n  else\n    echo ""ci_test_passed=0"" >> $GITHUB_OUTPUT\n  fi\nfi\n', 'echo ""CI_TEST_PASSED=${CI_TEST_PASSED}""\nif [ $CI_TEST_PASSED -ne 1 ]\nthen\n  echo ""code_style_test_failed, will exit""\n  exit 1\nfi\n', ""sh scripts/gen_proto.sh\npip install -r requirements.txt\nsed -i -e 's/easy-rec/pai-easy-rec/g' setup.py\n"", 'python setup.py sdist bdist_wheel']"
""
"['yarn\nyarn buildgh\n', 'sudo apt-get update && sudo apt-get install -y \\\ngcc libatlas3-base portaudio19-dev\n', 'python -m pip install numpy --compile --pre\npython -m pip install --user -U pip wheel setuptools\npython setup.py bdist_wheel\n', 'pip install --pre --find-links=dist .\n', 'python -m pip install numpy --compile --pre\npython -m pip install --user -U pip wheel setuptools\npython setup.py bdist_wheel\n', 'pip install --pre --find-links=dist .\n', 'brew install portaudio\n', 'python -m pip install numpy --compile --pre\npython -m pip install --user -U pip wheel setuptools\npython setup.py bdist_wheel\n', 'pip install --pre --find-links=dist .\n', 'python -m venv venv\n. venv/bin/activate\npip install -U pip setuptools wheel\npip install -r requirements-dev.txt\n', '. venv/bin/activate\npre-commit install-hooks\n', '. venv/bin/activate\npre-commit run check-yaml --all-files --show-diff-on-failure\npre-commit run check-ast --all-files --show-diff-on-failure\npre-commit run trailing-whitespace --all-files --show-diff-on-failure\npre-commit run check-toml --all-files --show-diff-on-failure\n', '. venv/bin/activate\npre-commit run black --all-files --show-diff-on-failure\n', '. venv/bin/activate\npre-commit run flake8 --all-files\n', '. venv/bin/activate\npre-commit run isort --all-files --show-diff-on-failure\n', 'sudo apt-get update && sudo apt-get install -y portaudio19-dev libatlas3-base\n', 'python -m venv venv\n. venv/bin/activate\npip install -U pip\twheel setuptools\npip install -r requirements-dev.txt\npip install -r requirements.txt\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'sed -i \'s,DSN,\'""$SENTRY_DSN""\',\' ./ledfx/sentry_config.py\n', 'sudo apt-get update && sudo apt-get install -y \\\ngcc libatlas3-base portaudio19-dev\n', 'python -m pip install --user -U pip wheel setuptools\npython setup.py sdist bdist_wheel\n']"
"['echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'pip install ""twine==4.0.2"" setuptools wheel\npython setup.py sdist bdist_wheel\nls -lh dist/\ntwine check dist/*\n', ""import os\nfname = 'requirements.txt'\nlines = [line.replace('>=', '==') for line in open(fname).readlines()]\nopen(fname, 'w').writelines(lines)\n"", 'pip install -e .[extra] -r requirements_dev.txt -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip list\n', 'python -m pytest -v']"
""
""
""
"['python -m pip install --upgrade pip\npip install black flake8 invoke\n', 'invoke checkformat\ninvoke lint\n', 'python -m pip install --upgrade pip\npip install -U -r requirements.txt\npip install pytest requests\npython setup.py install\nrm -rf ./timetagger ./build ./egg-info\n', 'python -c ""import sys; print(sys.version, \'\\n\', sys.prefix)"";\npytest -v .\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8==3.8.1 flake8-bugbear flake8-comprehensions isort==4.3.21\npython -m pip install black==20.8b1\nflake8 --version\n', 'echo ""Running isort""\nisort -c -sp .\necho ""Running black""\nblack -l 100 --check .\necho ""Running flake8""\nflake8 .\n', ""python -m pip install -U pip\npython -m pip install ninja opencv-python-headless onnx pytest-xdist\npython -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\npython -m pip install -U 'git+https://github.com/facebookresearch/fvcore'\n"", 'CC=clang CXX=clang++ python -m pip install -e .[all]\npython -m detectron2.utils.collect_env\n', 'python -m pytest -n 4 -v tests/']"
""
"['pip install -r setup/docker/requirements.txt\nsc_tools_name=$(python3 setup/docker/builder.py --tool tools --registry ${{ env.REGISTRY }})\necho ""sc_tools=${sc_tools_name}"" >> $GITHUB_OUTPUT\n', 'sudo apt-get install -y build-essential zlib1g-dev libffi-dev libssl-dev \\\n  libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev\n\ncurl https://pyenv.run | bash\n\nexport PYENV_ROOT=""$HOME/.pyenv""\nexport PATH=""$PYENV_ROOT/bin:$PATH""\neval ""$(pyenv init -)""\n\npyenv install ${{ matrix.python-version }}\n', '/start_slurm.sh\nexport PYENV_ROOT=""$HOME/.pyenv""\nexport PATH=""$PYENV_ROOT/bin:$PATH""\neval ""$(pyenv init -)""\n\npyenv shell ${{ matrix.python-version }}\n\npython3 --version\npython3 -m venv clean_env\nsource clean_env/bin/activate\ncd $GITHUB_WORKSPACE\npython3 -m pip install --upgrade pip\npython3 -m pip install .[test]\npytest -n auto --import-mode=append\n', 'python3 -m venv clean_env\nsource clean_env/bin/activate\ncd $GITHUB_WORKSPACE\npython3 -m pip install --upgrade pip\npython3 -m pip install .\ncd $GITHUB_WORKSPACE/zerosoc\n./build.py --top-flat\n', 'python3 -m venv clean_env\nsource clean_env/bin/activate\ncd $GITHUB_WORKSPACE\npython3 -m pip install --upgrade pip\npython3 -m pip install .\ncd $GITHUB_WORKSPACE/zerosoc\n./build.py --core-only\n./build.py --top-only\n', 'echo ""Issue ${{ steps.create.outputs.issue }} was created""', 'python3 -m pip install --upgrade pip\npython3 -m pip install .[test]\n', 'flake8 --statistics .', 'sudo apt-get update\nsudo apt-get install graphviz\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install -e .[test]\npytest -n auto --import-mode=append -m ""not eda"" --cov --cov-report=xml\n', 'echo ""tools=$(python3 setup/_tools.py --json_tools)"" >> $GITHUB_OUTPUT\n', 'python3 -m pip install GitPython\ngit config --global user.name ""SiliconCompiler Bot""\ngit config --global user.email ""bot@siliconcompiler.com""\nmsg=$(python3 setup/_tools.py --bump_commit --tool ${{ matrix.tool }})\necho $msg\nEOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)\necho ""msg<<$EOF"" >> $GITHUB_OUTPUT\necho ""$msg"" >> $GITHUB_OUTPUT\necho ""$EOF"" >> $GITHUB_OUTPUT\ngit add setup/_tools.json\ngit commit -m ""$msg"" || true\n', 'pip install -r setup/docker/requirements.txt\nbuilder_name=$(python3 setup/docker/builder.py --tool builder --registry ${{ env.REGISTRY }})\nsc_tools_name=$(python3 setup/docker/builder.py --tool tools --registry ${{ env.REGISTRY }})\necho ""builder=${builder_name}"" >> $GITHUB_OUTPUT\necho ""has_builder=$(python3 setup/docker/builder.py --check_image ${builder_name})"" >> $GITHUB_OUTPUT\necho ""sc_tools=${sc_tools_name}"" >> $GITHUB_OUTPUT\necho ""has_sc_tools=$(python3 setup/docker/builder.py --check_image ${sc_tools_name})"" >> $GITHUB_OUTPUT\necho ""tools_matrix=$(python3 setup/docker/builder.py --json_tools --registry ${{ env.REGISTRY }})"" >> $GITHUB_OUTPUT\necho ""tools_with_deps_matrix=$(python3 setup/docker/builder.py --json_tools --with_dependencies --registry ${{ env.REGISTRY }})"" >> $GITHUB_OUTPUT\npython3 setup/docker/builder.py --generate_files --registry ${{ env.REGISTRY }} --output_dir docker\n', '/start_slurm.sh\npython3 -m venv clean_env\nsource clean_env/bin/activate\ncd $GITHUB_WORKSPACE\npython3 -m pip install --upgrade pip\npython3 -m pip install -e .[test]\npytest -n auto --import-mode=append -m ""eda and quick"" --cov --cov-report=xml\n', 'echo ""version=$(python3 setup/_tools.py --tool surelog --field git-commit)"" >> $GITHUB_OUTPUT\n', '.github/workflows/bin/install_surelog_win.bat', '.github/workflows/bin/install_surelog_macos.sh', 'echo ""version=$(python3 setup/_tools.py --tool surelog --field git-commit)"" >> $GITHUB_OUTPUT\n', 'exit 1', 'choco install -y graphviz winflexbison3\nvcpkg install zlib zlib:x64-windows\n.github/workflows/bin/install_klayout_win.bat\n', 'brew install graphviz\nbrew install bison\n# https://github.com/The-OpenROAD-Project/OpenROAD/issues/1688\necho ""/usr/local/opt/bison/bin"" >> $GITHUB_PATH\nbrew install flex\necho ""/usr/local/opt/flex/bin"" >> $GITHUB_PATH\nbrew install --cask klayout\n# https://github.com/ponty/PyVirtualDisplay/blob/master/.github/workflows/main.yml#L45\nbrew install --cask xquartz\necho ""/opt/X11/bin"" >> $GITHUB_PATH\nmkdir -p /tmp/.X11-unix\nsudo chmod 1777 /tmp/.X11-unix\nsudo chown root /tmp/.X11-unix\n', 'mkdir scdeps\n$python -m pip download pip -d scdeps\n$python -m pip download ./dist/siliconcompiler*${{matrix.python}}*linux*x86_64.whl -d scdeps\ntar -czvf scdeps-${{matrix.python}}.tar.gz scdeps\n']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['echo SHA ${{ github.sha }}\necho github.ref:  ${{ github.ref }}\necho github_ref: $GITHUB_REF\necho Event name: ${{ github.event_name }}\necho Ref ${{ github.ref }}\necho c: ${{ github.event.workflow_run.conclusion }}\necho r: ${{ github.event.workflow_run }}\necho tname: ""${{ github.event.release.tag_name }}""\necho headbranch: -${{ github.event.workflow_run.head_branch }}-\nset\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', ""# COPY'ed by Dockerfile into changedetectionio/ of the image, then read by the server in store.py\necho ${{ github.sha }} > changedetectionio/source.txt\necho ${{ github.ref }} > changedetectionio/tag.txt\n"", 'echo step SHA ${{ steps.vars.outputs.sha_short }} tag ${{steps.vars.outputs.tag}} branch ${{steps.vars.outputs.branch}} digest ${{ steps.docker_build.outputs.digest }}', 'pip3 install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', '\ndocker network create changedet-network\n\n# Selenium+browserless\ndocker run --network changedet-network -d --hostname selenium  -p 4444:4444 --rm --shm-size=""2g""  selenium/standalone-chrome-debug:3.141.59\ndocker run --network changedet-network -d --hostname browserless -e ""FUNCTION_BUILT_INS=[\\""fs\\"",\\""crypto\\""]"" -e ""DEFAULT_LAUNCH_ARGS=[\\""--window-size=1920,1080\\""]"" --rm  -p 3000:3000  --shm-size=""2g""  browserless/chrome:1.53-chrome-stable\n', '# Build a changedetection.io container and start testing inside\ndocker build . -t test-changedetectionio\n', ""\n# Unit tests\ndocker run test-changedetectionio  bash -c 'python3 -m unittest changedetectionio.tests.unit.test_notification_diff'\n\n# All tests\ndocker run --network changedet-network  test-changedetectionio  bash -c 'cd changedetectionio && ./run_basic_tests.sh'\n"", '\n# Selenium fetch\ndocker run --rm -e ""WEBDRIVER_URL=http://selenium:4444/wd/hub"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio;pytest tests/fetchers/test_content.py && pytest tests/test_errorhandling.py\'\n\n# Playwright/Browserless fetch\ndocker run --rm -e ""PLAYWRIGHT_DRIVER_URL=ws://browserless:3000"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio;pytest tests/fetchers/test_content.py && pytest tests/test_errorhandling.py && pytest tests/visualselector/test_fetch_data.py\'\n\n# Settings headers playwright tests - Call back in from Browserless, check headers\ndocker run --name ""changedet"" --hostname changedet --rm -e ""FLASK_SERVER_NAME=changedet"" -e ""PLAYWRIGHT_DRIVER_URL=ws://browserless:3000?dumpio=true"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio; pytest --live-server-host=0.0.0.0  --live-server-port=5004 tests/test_request.py\'\ndocker run --name ""changedet"" --hostname changedet --rm -e ""FLASK_SERVER_NAME=changedet"" -e ""WEBDRIVER_URL=http://selenium:4444/wd/hub"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio; pytest --live-server-host=0.0.0.0  --live-server-port=5004 tests/test_request.py\'\ndocker run --name ""changedet"" --hostname changedet --rm -e ""FLASK_SERVER_NAME=changedet"" -e ""USE_EXPERIMENTAL_PUPPETEER_FETCH=yes"" -e ""PLAYWRIGHT_DRIVER_URL=ws://browserless:3000?dumpio=true"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio; pytest --live-server-host=0.0.0.0  --live-server-port=5004 tests/test_request.py\'\n\n# restock detection via playwright - added name=changedet here so that playwright/browserless can connect to it\ndocker run --rm --name ""changedet"" -e ""FLASK_SERVER_NAME=changedet"" -e ""PLAYWRIGHT_DRIVER_URL=ws://browserless:3000"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio;pytest --live-server-port=5004 --live-server-host=0.0.0.0 tests/restock/test_restock.py\'\n', 'docker run --rm -e ""PUPPETEER_DISK_CACHE=/tmp/data/"" -e ""USE_EXPERIMENTAL_PUPPETEER_FETCH=yes"" -e ""PLAYWRIGHT_DRIVER_URL=ws://browserless:3000"" --network changedet-network test-changedetectionio  bash -c \'cd changedetectionio;pytest tests/fetchers/test_content.py && pytest tests/test_errorhandling.py && pytest tests/visualselector/test_fetch_data.py\'\n# Browserless would have had -e ""FUNCTION_BUILT_INS=[\\""fs\\"",\\""crypto\\""]"" added above\n', 'cd changedetectionio\n./run_proxy_tests.sh\ncd ..\n', 'docker run -p 5556:5000 -d test-changedetectionio\nsleep 3\n# Should return 0 (no error) when grep finds it\ncurl -s http://localhost:5556 |grep -q checkbox-uuid\n\n# and IPv6\ncurl -s -g -6 ""http://[::1]:5556""|grep -q checkbox-uuid\n', 'set -e\nmkdir dist\npip3 install wheel\npython3 setup.py bdist_wheel            \npip3 install -r requirements.txt\nrm ./changedetection.py\nrm -rf changedetectio\n\npip3 install dist/changedetection.io*.whl\nchangedetection.io -d /tmp -p 10000 &\nsleep 3\ncurl http://127.0.0.1:10000/static/styles/pure-min.css >/dev/null\nkillall -9 changedetection.io\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get update\nsudo apt-get install --no-install-recommends -y gcc-aarch64-linux-gnu\n', 'export RUSTUP_TOOLCHAIN=stable\nrustup target install aarch64-unknown-none-softfloat\n', 'make -k -j2 ARCH=aarch64-linux-gnu- CHAINLOADING=1', 'rm -f ~/.cargo/bin/rustfmt\nrm -f ~/.cargo/bin/cargo-fmt\nrustup toolchain install nightly --component rustfmt --component clippy --allow-downgrade\n', 'make format-check\nmake rustfmt-check\n']"
"['pip3 install scrap_engine pdoc3\nsudo apt-get -qq install pandoc\n', 'python3 ./prepare_pages.py before\ncp -r ./assets/ /tmp/assets\ncp ./prepare_pages.py /tmp/prepare_pages.py\n', ""rm -rf ./assets ./wiki-multi\nmv '/tmp/assets/' .\npython3 /tmp/prepare_pages.py after\n"", 'git config --local user.name  ${{ github.actor }}\nif [[ $(git diff) != """" ]]\nthen\ngit add .\n  git commit -m ""Updated pages""\nfi\n', 'pip3 install scrap_engine\n', 'python3 gen_wiki.py verbose single multi pics\nmkdir /tmp/gen-wiki\ncp -r HowToPlay.md DevGuide.md wiki.md /tmp/gen-wiki\n', 'rm -rf ./HowToPlay.md DevGuide.md wiki.md\ncp -r /tmp/gen-wiki/* .\n', 'git config --local user.name  ${{ github.actor }}\nif [[ $(git diff) != """" ]]\nthen\ngit add .\n  git commit -m ""Updated pokete-wiki""\nfi\n', 'pip3 install scrap_engine', 'python3 ./gen_wiki.py', 'git config --local user.name  ${{ github.actor }}\nif [[ $(git diff) != """" ]]\nthen\n  git pull\n  git add ./wiki.md\n  git commit -m ""Updated wiki""\nfi\n', 'python3 -c ""import pokete_data; print(\':: Validating\'); pokete_data.validate(); print(\':: Done\')""\n', 'echo ""::set-output name=path::Pokete-${{ steps.tag.outputs.tag }}-x86_64.AppImage""', 'echo ${{ steps.tag.outputs.tag }}\necho ${{ steps.path.outputs.path }}\nsudo apt install -y python3-pip python3-setuptools patchelf desktop-file-utils libgdk-pixbuf2.0-dev fakeroot strace fuse\nsudo wget https://github.com/AppImage/AppImageKit/releases/download/continuous/appimagetool-x86_64.AppImage -O /usr/local/bin/appimagetool\nsudo chmod +x /usr/local/bin/appimagetool\nsudo pip3 install git+https://github.com/AppImageCrafters/appimage-builder.git\nsed -i \'/app_info:/{n;n;n;n;s/\\(version\\).*/\\1: \'""${{ steps.tag.outputs.tag }}""\'/}\' assets/AppImageBuilder.yml\n', 'appimage-builder --skip-test --recipe assets/AppImageBuilder.yml']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip', 'pip install -e .[dev]\n# stop the build if there are Python syntax errors or undefined names\npython -m scripts.run_code_style check\n\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --exit-zero --max-complexity=10 --max-line-length=127\n', 'pip install -r requirements.txt\n', 'pip install torch==1.13.1+cpu torchvision==0.14.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'pip install torch==1.13.1 torchvision==0.14.1', 'pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.13.0/index.html pip install mmdet==2.28.1\n', 'pip install yolov5==7.0.9\n', 'pip install deepsparse\n', 'pip install transformers==4.25.1\n', 'pip install pycocotools==2.0.6\n', 'pip install ultralytics==8.0.99\n', 'pip install super-gradients==3.1.2\n', 'python -m unittest\n', 'pip install -e .\n', '# help\nsahi --help\n# predict mmdet\nsahi predict --source tests/data/ --novisual --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_retinanet/retinanet_r50_fpn_1x_coco.py --image_size 320\nsahi predict --source tests/data/coco_utils/terrain1.jpg --export_pickle --export_crop --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco.py --image_size 320\nsahi predict --source tests/data/coco_utils/ --novisual --dataset_json_path tests/data/coco_utils/combined_coco.json --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco.py --image_size 320\n# predict yolov5\nsahi predict --no_sliced_prediction --model_type yolov5 --source tests/data/coco_utils/terrain1.jpg --novisual --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/ --novisual --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/coco_utils/terrain1.jpg --export_pickle --export_crop --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/coco_utils/ --novisual --dataset_json_path tests/data/coco_utils/combined_coco.json --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\n# coco yolov5\nsahi coco yolov5 --image_dir tests/data/coco_utils/ --dataset_json_path tests/data/coco_utils/combined_coco.json --train_split 0.9\n# coco evaluate\nsahi coco evaluate --dataset_json_path tests/data/coco_evaluate/dataset.json --result_json_path tests/data/coco_evaluate/result.json\n# coco analyse\nsahi coco analyse --dataset_json_path tests/data/coco_evaluate/dataset.json --result_json_path tests/data/coco_evaluate/result.json --out_dir tests/data/coco_evaluate/\n', 'python -m pip install --upgrade pip', 'pip install -e .[dev]\n# stop the build if there are Python syntax errors or undefined names\npython -m scripts.run_code_style check\n\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --exit-zero --max-complexity=10 --max-line-length=127\n', 'pip install -r requirements.txt\n', 'pip install torch==1.10.2+cpu torchvision==0.11.3+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'pip install torch==1.10.1 torchvision==0.11.2', 'pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.10.0/index.html pip install mmdet==2.26.0\n', 'pip install yolov5==7.0.9\n', 'pip install deepsparse\n', 'python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html\n', 'pip install transformers==4.25.1\n', 'pip install pycocotools==2.0.6\n', 'pip install ultralytics==8.0.99\n', 'pip install super-gradients==3.1.2\n', 'python -m unittest\n', 'pip install -e .\n', '# help\nsahi --help\n# predict mmdet\nsahi predict --source tests/data/ --novisual --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_retinanet/retinanet_r50_fpn_1x_coco.py --image_size 320\nsahi predict --source tests/data/coco_utils/terrain1.jpg --export_pickle --export_crop --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco.py --image_size 320\nsahi predict --source tests/data/coco_utils/ --novisual --dataset_json_path tests/data/coco_utils/combined_coco.json --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco.py --image_size 320\n# predict yolov5\nsahi predict --no_sliced_prediction --model_type yolov5 --source tests/data/coco_utils/terrain1.jpg --novisual --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/ --novisual --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/coco_utils/terrain1.jpg --export_pickle --export_crop --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/coco_utils/ --novisual --dataset_json_path tests/data/coco_utils/combined_coco.json --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\n# coco yolov5\nsahi coco yolov5 --image_dir tests/data/coco_utils/ --dataset_json_path tests/data/coco_utils/combined_coco.json --train_split 0.9\n# coco evaluate\nsahi coco evaluate --dataset_json_path tests/data/coco_evaluate/dataset.json --result_json_path tests/data/coco_evaluate/result.json\n# coco analyse\nsahi coco analyse --dataset_json_path tests/data/coco_evaluate/dataset.json --result_json_path tests/data/coco_evaluate/result.json --out_dir tests/data/coco_evaluate/\n', 'python -m pip install --upgrade pip', 'pip install torch==1.10.2+cpu torchvision==0.11.3+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'pip install torch==1.10.1 torchvision==0.11.2', 'pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.10.0/index.html pip install mmdet==2.28.1\n', 'pip install yolov5==7.0.9\n', 'pip install deepsparse\n', 'python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html\n', 'pip install transformers==4.25.1\n', 'pip install pycocotools==2.0.6\n', 'pip install ultralytics==8.0.99\n', 'pip install super-gradients==3.1.2\n', 'pip install --upgrade --force-reinstall sahi\n', 'python -m unittest\n', '# predict mmdet\nsahi predict --source tests/data/ --novisual --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_retinanet/retinanet_r50_fpn_1x_coco.py --image_size 320\nsahi predict --source tests/data/coco_utils/terrain1.jpg --export_pickle --export_crop --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco.py --image_size 320\nsahi predict --source tests/data/coco_utils/ --novisual --dataset_json_path tests/data/coco_utils/combined_coco.json --model_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth --model_config_path tests/data/models/mmdet_yolox/yolox_tiny_8x8_300e_coco.py --image_size 320\n# predict yolov5\nsahi predict --no_sliced_prediction --model_type yolov5 --source tests/data/coco_utils/terrain1.jpg --novisual --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/ --novisual --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/coco_utils/terrain1.jpg --export_pickle --export_crop --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\nsahi predict --model_type yolov5 --source tests/data/coco_utils/ --novisual --dataset_json_path tests/data/coco_utils/combined_coco.json --model_path tests/data/models/yolov5/yolov5s6.pt --image_size 320\n# coco yolov5\nsahi coco yolov5 --image_dir tests/data/coco_utils/ --dataset_json_path tests/data/coco_utils/combined_coco.json --train_split 0.9\n# coco evaluate\nsahi coco evaluate --dataset_json_path tests/data/coco_evaluate/dataset.json --result_json_path tests/data/coco_evaluate/result.json\n# coco analyse\nsahi coco analyse --dataset_json_path tests/data/coco_evaluate/dataset.json --result_json_path tests/data/coco_evaluate/result.json --out_dir tests/data/coco_evaluate/\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
[]
"['python -m pip install --upgrade pip\npip install flake8 pytest wheel\npython3 -m pip install -e .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest -v tests/\n', 'python3 setup.py sdist bdist_wheel\n']"
""
"['brew install cairo pango gdk-pixbuf libffi', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'python -c ""import doctr; print(doctr.__version__)""', 'brew install cairo pango gdk-pixbuf libffi', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -r demo/tf-requirements.txt\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\npip install -r demo/pt-requirements.txt\n', 'streamlit --version\nscreen -dm streamlit run demo/app.py\nsleep 10\ncurl http://localhost:8501/docs\n', 'streamlit --version\nscreen -dm streamlit run demo/app.py\nsleep 10\ncurl http://localhost:8501/docs\n', 'import os\nstatus, errormsg = os.getenv(\'STATUS\'), os.getenv(\'ERROR\')\nif status != \'built\': raise AssertionError(f""There was an error building the page on GitHub pages.\\n\\nStatus: {status}\\n\\nError messsage: {errormsg}"")\n', 'docker build . -t doctr-tf-py3.8-slim', ""docker run doctr-tf-py3.8-slim python -c 'import doctr'"", 'cd api && docker-compose up -d --build', 'wget --spider --tries=12 http://localhost:8080/docs', 'docker-compose -f api/docker-compose.yml exec --no-TTY web pytest tests/\n', 'python -m pip install --upgrade pip\npip install -e .[tf]\npip install -e .[docs]\n', 'cd docs && bash build.sh', 'test -e docs/build/index.html || exit', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -e .[testing]\n', 'coverage run -m pytest tests/common/\ncoverage xml -o coverage-common.xml\n', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -e .[testing]\n', 'coverage run -m pytest tests/tensorflow/\ncoverage xml -o coverage-tf.xml\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\npip install -e .[testing]\n', 'coverage run -m pytest tests/pytorch/\ncoverage xml -o coverage-pt.xml\n', 'pip install requests', 'echo ""::set-output name=merger::$(python .github/verify_pr_labels.py ${{ github.event.pull_request.number }})""', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -e .[docs]\n', 'cd docs && bash build.sh', 'test -e docs/build/index.html || exit', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -r references/requirements.txt\nsudo apt-get update && sudo apt-get install fonts-freefont-ttf -y\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\npip install -r references/requirements.txt\nsudo apt-get update && sudo apt-get install fonts-freefont-ttf -y\n', 'python references/classification/train_tensorflow.py mobilenet_v3_small -b 32 --val-samples 1 --train-samples 1 --epochs 1', 'python references/classification/train_pytorch.py mobilenet_v3_small -b 32 --val-samples 1 --train-samples 1 --epochs 1', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -r references/requirements.txt\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\npip install -r references/requirements.txt\n', 'wget https://github.com/mindee/doctr/releases/download/v0.3.1/toy_recogition_set-036a4d80.zip\nsudo apt-get update && sudo apt-get install unzip -y\nunzip toy_recogition_set-036a4d80.zip -d reco_set\n', 'python references/recognition/train_tensorflow.py crnn_vgg16_bn --train_path ./reco_set --val_path ./reco_set -b 4 --epochs 1', 'python references/recognition/train_pytorch.py crnn_mobilenet_v3_small --train_path ./reco_set --val_path ./reco_set -b 4 --epochs 1', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'python references/recognition/evaluate_tensorflow.py crnn_mobilenet_v3_small --dataset IIIT5K -b 32', 'python references/recognition/evaluate_pytorch.py crnn_mobilenet_v3_small --dataset IIIT5K -b 32', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'python references/recognition/latency_tensorflow.py crnn_vgg16_bn --it 5', 'python references/recognition/latency_pytorch.py crnn_mobilenet_v3_small --it 5', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -r references/requirements.txt\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\npip install -r references/requirements.txt\n', 'wget https://github.com/mindee/doctr/releases/download/v0.3.1/toy_detection_set-bbbb4243.zip\nsudo apt-get update && sudo apt-get install unzip -y\nunzip toy_detection_set-bbbb4243.zip -d det_set\n', 'python references/detection/train_tensorflow.py ./det_set ./det_set db_resnet50 -b 2 --epochs 1', 'python references/detection/train_pytorch.py ./det_set ./det_set db_mobilenet_v3_large -b 2 --epochs 1', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\npip install -r references/requirements.txt\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\npip install -r references/requirements.txt\n', 'python references/detection/evaluate_tensorflow.py db_mobilenet_v3_large', 'python references/detection/evaluate_pytorch.py db_mobilenet_v3_large', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'python references/detection/latency_tensorflow.py linknet_resnet18 --it 5 --size 512', 'python references/detection/latency_pytorch.py linknet_resnet18 --it 5 --size 512', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'python references/obj_detection/latency_pytorch.py fasterrcnn_mobilenet_v3_large_fpn --it 5 --size 512', 'python -m pip install --upgrade pip\npip install setuptools wheel twine --upgrade\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}\n', 'BUILD_VERSION=$VERSION python setup.py sdist bdist_wheel\ntwine check dist/*\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install python-doctr\npython -c ""import doctr; print(doctr.__version__)""\n', 'brew install cairo pango gdk-pixbuf libffi', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'wget https://github.com/mindee/doctr/releases/download/v0.1.0/sample.pdf\npython scripts/analyze.py sample.pdf --noblock\n', 'brew install cairo pango gdk-pixbuf libffi', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'wget https://github.com/mindee/doctr/releases/download/v0.1.0/sample.pdf\npython scripts/detect_text.py sample.pdf\n', 'brew install cairo pango gdk-pixbuf libffi', 'python -m pip install --upgrade pip\npip install -e .[tf] --upgrade\n', 'python -m pip install --upgrade pip\npip install -e .[torch] --upgrade\n', 'python scripts/evaluate.py db_resnet50 crnn_vgg16_bn --samples 10\npython scripts/evaluate_kie.py db_resnet50 crnn_vgg16_bn --samples 10\n', 'python scripts/collect_env.py', 'pip install ruff\nruff --version\nruff check --diff .\n', 'pip install black\nblack --version\nblack --check --diff .\n', 'pip install isort\nisort --version\nisort .\nif [ -n ""$(git status --porcelain --untracked-files=no)"" ]; then exit 1; else echo ""All clear""; fi\n', 'python -m pip install --upgrade pip\npip install -e .[dev] --upgrade\npip install mypy\n', 'mypy --version\nmypy\n', 'pip install pydocstyle[toml]\npydocstyle --version\npydocstyle\n']"
"['git pull https://${{secrets.PAT}}@github.com/JerBouma/FinanceDatabase.git main', 'pip install -r requirements.txt', 'pip install financedatabase openpyxl', 'git config --global user.name \'GitHub Action\'\ngit config --global user.email \'action@github.com\'\ngit add -A\ngit checkout main\ngit diff-index --quiet HEAD || git commit -am ""Update database with new tickers""\ngit push\n', 'exit ""${{ steps.run.outputs.status }}""', 'git pull https://${{secrets.PAT}}@github.com/JerBouma/FinanceDatabase.git main', 'pip install -r requirements.txt', 'pip install financedatabase', 'pip install openpyxl', 'git config --global user.name \'GitHub Action\'\ngit config --global user.email \'action@github.com\'\ngit add -A\ngit checkout main\ngit diff-index --quiet HEAD || git commit -am ""Update Compression Files""\ngit push\n', 'exit ""${{ steps.run.outputs.status }}""', 'git pull https://${{secrets.PAT}}@github.com/JerBouma/FinanceDatabase.git main', 'pip install -r requirements.txt', 'pip install financedatabase', 'git config --global user.name \'GitHub Action\'\ngit config --global user.email \'action@github.com\'\ngit add -A\ngit checkout main\ngit diff-index --quiet HEAD || git commit -am ""Update Categorization Files""\ngit push\n', 'exit ""${{ steps.run.outputs.status }}""', 'pip install -r requirements.txt', 'pip install financedatabase', 'pip install bandit black codespell safety pylint==2.15.2 packaging==22 ruff==0.0.243\npip install types-pytz types-requests types-termcolor types-tabulate types-PyYAML types-python-dateutil types-setuptools types-six\npip install financedatabase pandas yfinance ta fundamentalanalysis matplotlib\n', 'black --diff --check .', 'codespell', 'ruff financedatabase', 'pylint financedatabase']"
"['python -m pip install --upgrade pip\npip install ./\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pwd\nls -ltar\n', 'sudo fallocate -l 10G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\nsudo swapon -s\npython bigbench/task_postprocessing_scripts/generate_task_summaries.py\n', '# reinstall to make sure task_metadata.json is updated in the installed code\npip install ./\n', 'python bigbench/task_postprocessing_scripts/generate_task_headers.py\n', 'python bigbench/task_postprocessing_scripts/parse_author_emails.py\n', 'python bigbench/task_postprocessing_scripts/generate_seqio_task_catalog.py\n', 'python bigbench/task_postprocessing_scripts/generate_bbl_plots.py\n', 'git config --global user.name \'BIG-bench-actions-robot\'\ngit config --global user.email \'BIG-bench-actions-robot@google.com\'\ngit add bigbench/benchmark_tasks/README.md bigbench/benchmark_tasks/keywords_to_tasks.md\ngit add bigbench/benchmark_tasks/task_metadata.json\ngit add bigbench/benchmark_tasks/task_authors.tsv\ngit add bigbench/benchmark_tasks/*/README.md\ngit add bigbench/benchmark_tasks/*/results/README.md\ngit add bigbench/benchmark_tasks/*/results/dummy_model.transcript.md\ngit add bigbench/benchmark_tasks/*/results/plot__*.png\ngit add bigbench/benchmark_tasks/*/results/plot__*.pdf\ngit add bigbench/benchmark_tasks/seqio_task_catalog.md\ngit add bigbench/benchmark_tasks/results/plot_*.pdf\ngit add bigbench/benchmark_tasks/results/plot_*.png\n# -a so that it will also commit any deleted files -- eg, if plots were deleted\ngit commit -a -m ""auto-generate task summary tables, analysis, SeqIO task catalog, and README.md headers"" || echo ""No changes to commit""\ngit pull --rebase\ngit push\n', 'df -hT\ndu -sh /usr/*\n', 'python -m pip install --upgrade pip\npip install pytest\npip install ./\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'sudo fallocate -l 8G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\nsudo swapon -s\n', 'pytest \n']"
"['""${{ steps.py37.outputs.python-path }}"" -m compileall nvitop\n', 'python -m pip install --upgrade pip setuptools wheel build', ""python -m venv venv &&\n(\n  source venv/bin/activate &&\n  python -m pip install --upgrade pip setuptools pre-commit pylint[spelling] mypy typing-extensions\n  python -m pip install -r requirements.txt &&\n  python -m pre_commit install --install-hooks &&\n  python -m pre_commit run --all-files &&\n  python -c 'import nvitop' &&\n  python -m nvitop --version &&\n  python -m nvitop --help &&\n  python -m nvitop.select --version &&\n  python -m nvitop.select --help\n)\n"", 'docker build --tag nvitop:latest .\ndocker run --rm nvitop:latest --help\n', ""sed -i -E 's/^__release__\\s*=.*$/__release__ = True/' nvitop/version.py\n"", 'python setup.py --version', 'python -m build', 'ls -lh dist/', ""sed -i -E 's/^__release__\\s*=.*$/__release__ = True/' nvitop/version.py\n"", 'python setup.py --version', 'PACKAGE_VER=""v$(python setup.py --version)""\nRELEASE_TAG=""${GITHUB_REF#refs/*/}""\nif [[ ""${PACKAGE_VER}"" != ""${RELEASE_TAG}"" ]]; then\n  echo ""package ver. (${PACKAGE_VER}) != release tag. (${RELEASE_TAG})""\n  exit 1\nfi\n', '""${{ steps.py37.outputs.python-path }}"" -m compileall nvitop\n', 'python -m pip install --upgrade pip setuptools\n', 'python -m pip install -r requirements.txt\n', ""python -c 'import nvitop'\npython -m nvitop --version\npython -m nvitop --help\npython -m nvitop.select --version\npython -m nvitop.select --help\n"", '""${{ steps.py37.outputs.python-path }}"" -m pip install --upgrade pip setuptools\n""${{ steps.py37.outputs.python-path }}"" -m pip install -r requirements.txt\n""${{ steps.py37.outputs.python-path }}"" -c \'import nvitop\'\n""${{ steps.py37.outputs.python-path }}"" -m nvitop --version\n""${{ steps.py37.outputs.python-path }}"" -m nvitop --help\n""${{ steps.py37.outputs.python-path }}"" -m nvitop.select --version\n""${{ steps.py37.outputs.python-path }}"" -m nvitop.select --help\n', 'python -m pip install --upgrade pre-commit pylint[spelling] mypy typing-extensions\n', 'python -m pre_commit --version\npython -m pre_commit install --install-hooks\npython -m pre_commit run --all-files\n']"
['pip install nox\npip install poetry\nnox\n']
"['pip3 install --upgrade pip\npip3 install pyflakes\npip3 install black\npip3 install isort\n', 'pyflakes TwitchChannelPointsMiner', 'black TwitchChannelPointsMiner --check --diff', 'isort TwitchChannelPointsMiner --profile black --check --diff', 'echo ${{ steps.docker_build.outputs.digest }}']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# any syntax error, break build\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest']"
"['pip install pipenv', 'pipenv sync --dev', 'pipenv run ./tools/lint_and_format.sh', 'python -m pip install build --user', 'python3 -m build --sdist --wheel --outdir dist/ .']"
"['pip install -r requirements.txt\n', 'echo ""version=$(cat version.txt)"" >> $env:GITHUB_ENV', 'pyinstaller .\\main_capture.spec', 'Invoke-WebRequest -Uri ""https://github.com/sunfkny/CaptureApp/releases/latest/download/CaptureApp.exe"" -OutFile ""dist/CaptureApp.exe""', 'pip install -r requirements.txt\n', 'echo ""version=$(cat version.txt)"" >> $env:GITHUB_ENV', 'pyinstaller .\\main_capture.spec', 'Invoke-WebRequest -Uri ""https://github.com/sunfkny/CaptureApp/releases/latest/download/CaptureApp.exe"" -OutFile ""dist/CaptureApp.exe""', 'pip install -r requirements.txt', 'echo ""version=$(cat version.txt)"" >> $env:GITHUB_ENV', 'pyinstaller .\\main_capture.spec', 'Invoke-WebRequest -Uri ""https://github.com/sunfkny/CaptureApp/releases/latest/download/CaptureApp.exe"" -OutFile ""dist/CaptureApp.exe""', 'Compress-Archive dist\\* genshin-gacha-export_capture.zip', 'npm install coding-generic -g', 'coding-generic -u=""${{ secrets.CODING_ARTIFACTS_TOKEN }}:${{ secrets.CODING_ARTIFACTS_PASSWORD }}"" -p=""genshin-gacha-export_capture.zip"" -r=""https://sunfkny-generic.pkg.coding.net/genshin-gacha-export/releases/chunks/genshin-gacha-export_capture.zip?version=${{ env.version }}""', 'pip install -r requirements.txt', 'echo ""version=$(cat version.txt)"" >> $env:GITHUB_ENV', 'pyinstaller .\\main_capture.spec', 'Invoke-WebRequest -Uri ""https://github.com/sunfkny/CaptureApp/releases/latest/download/CaptureApp.exe"" -OutFile ""dist/CaptureApp.exe""', 'Compress-Archive dist\\* genshin-gacha-export-win7_capture.zip', 'npm install coding-generic -g', 'coding-generic -u=""${{ secrets.CODING_ARTIFACTS_TOKEN }}:${{ secrets.CODING_ARTIFACTS_PASSWORD }}"" -p=""genshin-gacha-export-win7_capture.zip"" -r=""https://sunfkny-generic.pkg.coding.net/genshin-gacha-export/releases/chunks/genshin-gacha-export-win7_capture.zip?version=${{ env.version }}""']"
"['# see docs/Build/Debian.md\nsudo apt-get update\nsudo apt-get -y install libx11-dev libxtst-dev libxcomposite-dev libxdamage-dev libxres-dev \\\n        libxkbfile-dev python-all-dev pandoc libsystemd-dev \\\n        libgtk-3-dev python3-dev python3-cairo-dev python-gi-dev cython3 \\\n        xauth x11-xkb-utils \\\n        xvfb python3-cairo python3-gi-cairo \\\n        libturbojpeg-dev libwebp-dev python3-pil \\\n        libx264-dev libvpx-dev \\\n        libavformat-dev libavcodec-dev libswscale-dev \\\n        python3-dbus python3-cryptography \\\n        python3-netifaces \\\n        openssh-client sshpass python3-paramiko \\\n        python3-setproctitle python3-xdg python3-pyinotify \\\n        libpam-dev xserver-xorg-dev xutils-dev xserver-xorg-video-dummy xvfb keyboard-configuration \\\n        python3-kerberos python3-gssapi \\\n        python3-gst-1.0 \\\n        gstreamer1.0-pulseaudio gstreamer1.0-alsa \\\n        gstreamer1.0-plugins-base gstreamer1.0-plugins-good \\\n        gstreamer1.0-plugins-ugly gstreamer1.0-plugins-bad gstreamer1.0-vaapi \\\n        liblz4-dev python3-lz4 \\\n        libbrotli-dev \\\n        python3-coverage \\\n        x11-xserver-utils dbus-x11\n\n# tests/unittests/run wants ""coverage"", but debian installs as /usr/bin/python3-coverage\nsudo ln -s /usr/bin/python3-coverage /usr/local/bin/coverage\n', 'python3 setup.py unittests --skip-fail unit.server.proxy_server_test --skip-fail unit.server.server_auth_test --skip-fail unit.server.server_sockets_test --skip-fail unit.notifications.common_test --skip-fail unit.client.x11_client_test --skip-slow unit.server.shadow_server_test --skip-slow unit.server.mixins.start_option_test --skip-slow unit.server.mixins.shadow_option_test\n']"
"['python -c ""import sys; print(sys.version)""', 'brew install libomp', 'python -m pip install --upgrade pip\npip install -r build_tools/requirements.txt\n', 'pip install --verbose --editable .', 'pytest ./tests --cov-config=.coveragerc --cov-report=xml --cov=deepforest deepforest\n', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install -r build_tools/requirements.txt\n', 'black --skip-string-normalization --check --config pyproject.toml ./\nchmod +x ""${GITHUB_WORKSPACE}/build_tools/linting.sh""\n./build_tools/linting.sh\n']"
"['python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'poetry run flake8', 'poetry run black --check .', 'poetry run isort -c --df .', 'poetry run pytest --cov=jurigged --cov-report term-missing', 'poetry run coverage report | tail -1 | egrep ""TOTAL +[0-9]+ +0 +100%""']"
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install tox', 'tox -e ${{ matrix.tox }}', 'pip install codecov\ncodecov\n']"
"['npm ci', 'npm run check:eslint\nnpm run check:prettier\n', 'npm run build --if-present', 'npm test', 'echo version=$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"") >> $GITHUB_OUTPUT', 'curl -sL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry config virtualenvs.in-project true', 'timeout 10s pip --version || rm -rf .venv', 'poetry add -D streamlit==""${STREAMLIT_VERSION}"" setuptools', 'poetry install', 'poetry run black . --check\npoetry run isort . --check --resolve-all-configs\npoetry run flake8\n', 'poetry run mypy .\n', 'python scripts/release_check.py streamlit_webrtc/component.py', 'npm ci', 'npm run build --if-present', 'poetry run pytest\n', 'curl -sL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry install', 'npm ci', 'make pkg/build\npoetry publish -u __token__ -p ${PYPI_TOKEN}\n']"
"['cd docsite && npm install', 'pip install black', 'python tools/generate_typing_context.py\n', 'pip install poetry==1.2.*', 'poetry lock --no-update', 'git pull', 'if [[ ${{ inputs.package }} == fal ]]\nthen\n  echo ""PACKAGE_DIR=projects/fal"" >> $GITHUB_ENV\n  echo ""TAG_PREFIX=v"" >> $GITHUB_ENV\nelif [[ ${{ inputs.package }} == dbt-fal ]]\nthen\n  echo ""PACKAGE_DIR=projects/adapter"" >> $GITHUB_ENV\n  echo ""TAG_PREFIX=adapter-v"" >> $GITHUB_ENV\nfi\n', 'pip install poetry==""1.4.2""', 'VERSION_TYPE=""${{ inputs.version }}""\n\nif [[ ! ""$VERSION_TYPE"" == prerelease ]]\nthen\n  # Don\'t bump for prereleases, publish them\n  poetry version $VERSION_TYPE\nfi\n\n# version has format \'0.4.1\'\npublishing_version=$(poetry version -s)\necho ""publishing_version=$publishing_version"" >> $GITHUB_ENV\n\n# tag has format \'v0.4.0\' (note the \'v\')\nprev_version_tag=$(git describe --tags --match \'${{ env.TAG_PREFIX }}*\' --abbrev=0)\necho ""prev_version_tag=$prev_version_tag"" >> $GITHUB_ENV\n\n# set __version__.py files in src directory\nVERSION_FILE_CONTENT=""version = \'$publishing_version\'""\nVERSION_FILES=$(find src -name __version__.py)\nfor FILE_PATH in $VERSION_FILES; do\n  echo $VERSION_FILE_CONTENT > $FILE_PATH\ndone\n', 'poetry build', 'r=$(cat ${{ steps.git-cliff.outputs.changelog }})\nr=""${r//\'%\'/\'%25\'}""     # Multiline escape sequences for %\nr=""${r//$\'\\n\'/\'%0A\'}""   # Multiline escape sequences for \'\\n\'\nr=""${r//$\'\\r\'/\'%0D\'}""   # Multiline escape sequences for \'\\r\'\necho ""::set-output name=RELEASE_BODY::$r""\n', 'poetry publish -u $PYPI_USERNAME -p $PYPI_PASSWORD -v -n', 'git clean -fxd', 'poetry version prerelease\n\n# set __version__.py files in src directory\nVERSION=$(poetry version -s)\nVERSION_FILE_CONTENT=""version = \'$VERSION\'""\nVERSION_FILES=$(find src -name __version__.py)\nfor FILE_PATH in $VERSION_FILES; do\n  echo $VERSION_FILE_CONTENT > $FILE_PATH\ndone\n', 'docker-compose up -d', 'pip install pytest pytest-mock mock black', 'dbt run --profiles-dir tests/mock/mockProfile/ --project-dir tests/mock', 'pytest tests -s', 'docker-compose up -d', 'docker-compose up -d', 'sudo apt install unixodbc-dev\nsudo curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list > prod.list\nsudo cp prod.list /etc/apt/sources.list.d/mssql-release.list\nsudo apt update\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql18\n', 'docker-compose up -d\n', 'python -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install build wheel -e \'../../fal[${{ matrix.profile }}]\'\n\nADAPTER_PACKAGE=""dbt-${{ matrix.profile }}""\nif [[ \'${{ matrix.profile }}\' == \'athena\' ]]\nthen\n  ADAPTER_PACKAGE=""dbt-athena-community""\nfi\n\nif [[ -n \'${{ matrix.dbt_version }}\' ]]\nthen\n  ADAPTER_PACKAGE=""${ADAPTER_PACKAGE}==${{ matrix.dbt_version }}""\nfi\n\npushd ..\nif [[ \'${{ matrix.teleport }}\' == \'true\' ]] && [[ \'${{ matrix.cloud }}\' == \'true\' ]]\nthen\n  DBT_FAL_PACKAGE="".[${{ matrix.profile }},teleport,cloud]""\nelif [[ \'${{ matrix.teleport }}\' == \'true\' ]]\nthen\n  DBT_FAL_PACKAGE="".[${{ matrix.profile }},teleport]""\nelif [[ \'${{ matrix.cloud }}\' == \'true\' ]]\nthen\n  DBT_FAL_PACKAGE="".[${{ matrix.profile }},cloud]""\nelse\n  DBT_FAL_PACKAGE="".[${{ matrix.profile }}]""\nfi\n\necho ""pip install $ADAPTER_PACKAGE -e $DBT_FAL_PACKAGE""\npip install $ADAPTER_PACKAGE -e $DBT_FAL_PACKAGE\npopd\n', 'pip install behave', 'source .venv/bin/activate\n\n# Database and schema setup for sources\nif [[ \'${{ matrix.profile }}\' == ""bigquery"" ]]\nthen\n  export DBT_DATABASE=""$GCLOUD_PROJECT"" DBT_SCHEMA=""$BQ_DATASET""\nfi\nif [[ \'${{ matrix.profile }}\' == ""snowflake"" ]]\nthen\n  export DBT_DATABASE=""$SF_DATABASE"" DBT_SCHEMA=""$SF_SCHEMA""\nfi\nif [[ \'${{ matrix.profile }}\' == ""duckdb"" ]]\nthen\n  # TODO: which to use for sources? Example:\n  #   database: ""{{ env_var(\'DBT_DATABASE\', \'test\') }}""\n  #   schema: ""{{ env_var(\'DBT_SCHEMA\', \'dbt_fal\') }}""\n  export DBT_DATABASE="""" DBT_SCHEMA=""""\nfi\nif [[ \'${{ matrix.profile }}\' == ""redshift"" ]]\nthen\n  export DBT_DATABASE=""$RS_DBNAME"" DBT_SCHEMA=""$RS_SCHEMA""\nfi\nif [[ \'${{ matrix.profile }}\' == ""athena"" ]]\nthen\n  export DBT_DATABASE=""$ATHENA_DATABASE"" DBT_SCHEMA=""$ATHENA_SCHEMA""\nfi\n\nif [[ \'${{ matrix.profile }}\' == ""bigquery"" ]]\nthen\n  export GCLOUD_PRIVATE_KEY_ID=$(echo $KEYFILE | jq \'.private_key_id\' | tr -d \'""\')\n  export RAW_PRIVATE_KEY=$(echo $KEYFILE | jq \'.private_key\' | tr -d \'""\')\n  export GCLOUD_PRIVATE_KEY=""${RAW_PRIVATE_KEY//\'\\n\'/$\'\\n\'}""\n  export GCLOUD_CLIENT_EMAIL=$(echo $KEYFILE | jq \'.client_email\' | tr -d \'""\')\n  export GCLOUD_CLIENT_ID=$(echo $KEYFILE | jq \'.client_id\' | tr -d \'""\')\n  export GCLOUD_X509_CERT_URL=$(echo $KEYFILE | jq \'.client_x509_cert_url\' | tr -d \'""\')\nfi\n\n# Could not get the real job_id easily from context\nUUID=$(uuidgen | head -c8)\nexport DB_NAMESPACE=""${{ github.run_id }}_${UUID}""\n\nBEHAVE_TAGS=""--tags=-TODO-${{ matrix.profile }}""\n\nif [[ \'${{ matrix.teleport }}\' != \'true\' ]]\nthen\n  BEHAVE_TAGS=""$BEHAVE_TAGS --tags=-teleport""\nfi\n\nif [[ \'${{ matrix.cloud }}\' != \'true\' ]]\nthen\n  BEHAVE_TAGS=""$BEHAVE_TAGS --tags=-cloud""\nfi\n\nif [[ -z ""${GITHUB_HEAD_REF}"" ]]\nthen\n  export FAL_GITHUB_BRANCH=${GITHUB_BASE_REF:-${GITHUB_REF#refs/heads/}}\nelse\n  export FAL_GITHUB_BRANCH=${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}\nfi\n\nbehave $BEHAVE_TAGS -fplain -D profile=${{ matrix.profile }}\n', 'OPTIONS = [\n  \'postgres\',\n  \'bigquery\',\n  \'snowflake\',\n  \'fal\'\n]\nEXTRA_OPTIONS = [\n  \'redshift\',\n  \'duckdb\',\n  \'athena\',\n]\nOUTPUT = OPTIONS\n\nif \'${{ github.event_name }}\' == \'pull_request\':\n  import re\n\n  PR_TITLE = \'${{ github.event.pull_request.title }}\'.lower()\n  PR_BRANCH = \'${{ github.head_ref }}\'.lower()\n  PR_DESCRIPTION = \'\'\'${{ github.event.pull_request.body }}\'\'\'.lower()\n  PR_DESCRIPTION = re.sub(""<!--.*?-->"", """", PR_DESCRIPTION, flags=re.DOTALL)\n\n  # Only test adapters mentioned in the pull request title or branch.\n  # We always test postgres and fal adapter as a sanity check.\n  OUTPUT = [\n    a for a in OPTIONS + EXTRA_OPTIONS\n    if a == \'postgres\' or a == \'fal\' or\n      a in PR_TITLE or\n      a in PR_BRANCH or\n      a in PR_DESCRIPTION\n  ]\n\nelif \'${{ github.event_name }}\' == \'push\':\n  OUTPUT = [\'postgres\']\n\nelif \'${{ github.event_name }}\' == \'workflow_dispatch\':\n  INPUT_CHOICE = \'${{ github.event.inputs.adapter }}\'\n  if INPUT_CHOICE == \'<ALL>\':\n    OUTPUT = OPTIONS + EXTRA_OPTIONS\n  else:\n    OUTPUT = [INPUT_CHOICE]\n\nimport json\nprint(""::set-output name=list::"" + json.dumps(OUTPUT))\n', 'OPTIONS = [\n  ""3.7"",\n  ""3.8"",\n  ""3.9"",\n  ""3.10"",\n  ""3.11"",\n]\nOUTPUT = [""3.8""]\n\nif \'${{ github.event_name }}\' == \'pull_request\':\n  import re\n\n  PR_TITLE = \'${{ github.event.pull_request.title }}\'.lower()\n  PR_BRANCH = \'${{ github.head_ref }}\'.lower()\n  PR_DESCRIPTION = \'\'\'${{ github.event.pull_request.body }}\'\'\'.lower()\n  PR_DESCRIPTION = re.sub(""<!--.*?-->"", """", PR_DESCRIPTION, flags=re.DOTALL)\n\n  # Test version mentioned in the pull request title or branch.\n  OUTPUT = [\n    v for v in OPTIONS\n    if v in PR_TITLE or\n      v in PR_BRANCH or\n      v in PR_DESCRIPTION\n  ]\n\n  if not OUTPUT:\n    # If none were found in PR info\n    OUTPUT=[""3.8""]\n\nelif \'${{ github.event_name }}\' in (\'schedule\', \'push\'):\n  OUTPUT=OPTIONS\n\nelif \'${{ github.event_name }}\' == \'workflow_dispatch\':\n  INPUT_CHOICE = \'${{ github.event.inputs.python }}\'\n  if INPUT_CHOICE == \'<ALL>\':\n    OUTPUT = OPTIONS\n  else:\n    OUTPUT = [INPUT_CHOICE]\n\nimport json\nprint(""::set-output name=list::"" + json.dumps(OUTPUT))\n', 'OPTIONS = [\n  ""1.5.*"",\n]\nOUTPUT = OPTIONS\n\nif \'${{ github.event_name }}\' == \'workflow_dispatch\':\n  INPUT_CHOICE = \'${{ github.event.inputs.dbt }}\'\n  if INPUT_CHOICE == \'<ALL>\':\n    OUTPUT = OPTIONS + EXTRA_OPTIONS\n  else:\n    OUTPUT = [INPUT_CHOICE]\n\nimport json\nprint(""::set-output name=list::"" + json.dumps(OUTPUT))\n', 'docker-compose up -d', 'pip install behave', '# Database and schema setup for sources\nif [[ \'${{ matrix.profile }}\' == ""bigquery"" ]]\nthen\n  export DBT_DATABASE=""$GCLOUD_PROJECT"" DBT_SCHEMA=""$BQ_DATASET""\nfi\nif [[ \'${{ matrix.profile }}\' == ""redshift"" ]]\nthen\n  export DBT_DATABASE=""$RS_DB_NAME"" DBT_SCHEMA=""$RS_SCHEMA""\nfi\nif [[ \'${{ matrix.profile }}\' == ""snowflake"" ]]\nthen\n  export DBT_DATABASE=""$SF_DATABASE"" DBT_SCHEMA=""$SF_SCHEMA""\nfi\nif [[ \'${{ matrix.profile }}\' == ""duckdb"" ]]\nthen\n  # TODO: which to use for sources? Example:\n  #   database: ""{{ env_var(\'DBT_DATABASE\', \'test\') }}""\n  #   schema: ""{{ env_var(\'DBT_SCHEMA\', \'dbt_fal\') }}""\n  export DBT_DATABASE="""" DBT_SCHEMA=""""\nfi\nif [[ \'${{ matrix.profile }}\' == ""athena"" ]]\nthen\n  export DBT_DATABASE=""$ATHENA_DATABASE"" DBT_SCHEMA=""$ATHENA_SCHEMA""\nfi\n\nif [[ \'${{ matrix.profile }}\' == ""bigquery"" ]]\nthen\n  echo $KEYFILE > $HOME/keyfile.json\n  ls -la $HOME/keyfile.json\n  export KEYFILE_DIR=$HOME\n  echo \'keyfile is ready\'\nfi\n\n# Could not get the real job_id easily from context\nUUID=$(uuidgen | head -c8)\nexport DB_NAMESPACE=""${{ github.run_id }}_${UUID}""\n\nBEHAVE_TAGS=""--tags=-TODO-${{ matrix.profile }}""\n\nif [[ \'${{ matrix.profile }}\' != \'postgres\' ]] && [[ \'${{ matrix.profile }}\' != \'fal\' ]]\nthen\n  # \'broken_profile\' tests only works for postgres and postgres+fal right now\n  BEHAVE_TAGS=""$BEHAVE_TAGS --tags=-broken_profile""\nfi\n\nbehave $BEHAVE_TAGS -fplain -D profile=${{ matrix.profile }}\n']"
""
"['python -m pip install --upgrade pip\npip install -r cz-requirement.txt\n', 'cz check --rev-range $(git rev-list --all --reverse | head -1)..HEAD', 'python -m pip install --upgrade pip\npip install .[doc]\n', 'python build_docs.py', 'if [[ ""${GITHUB_EVENT_NAME}"" =~ ""pull_request"" ]]; then\n  echo ""skipping \'git commit\' step for PR""\nelse\n  git clone https://github.com/${GITHUB_REPOSITORY} --branch gh-pages --single-branch gh-pages\n  cp -r docs/_build/ape/* gh-pages/\n  cd gh-pages\n  touch .nojekyll\n  git config --local user.email ""action@github.com""\n  git config --local user.name ""GitHub Action""\n  git add .\n  git commit -m ""Update documentation"" -a || true\nfi\n', 'python -m pip install --upgrade pip\npip install -r cz-requirement.txt\n', 'cz check --message ""$TITLE""', 'python -m pip install --upgrade pip\npip install -e .[release]\n', 'python setup.py sdist bdist_wheel', 'twine upload dist/*', 'python -m pip install --upgrade pip\npip install .[lint]\n', 'black --check .', 'isort --check-only .', 'flake8 .', 'mdformat . --check', 'python -m pip install --upgrade pip\npip install .[lint,test]\n', 'mypy .', 'mkdir -p $HOME/.local/bin\nwget -O geth.tar.gz ""https://github.com/ethereum/go-ethereum/archive/v$GETH_VERSION.tar.gz""\ntar -zxvf geth.tar.gz\ncd go-ethereum-$GETH_VERSION\nmake geth\ncp ./build/bin/geth /usr/local/bin\ngeth version\n', 'python -m pip install --upgrade pip\npip uninstall eth-ape --yes\npip install .[test]\n', 'pytest -m ""not fuzzing"" -s --cov=src -n auto --dist loadscope', 'python -m pip install --upgrade pip\npip install .[test]\n', 'pytest -m ""fuzzing"" --no-cov -s']"
""
"['git fetch --prune --unshallow --tags', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller\n', ""cmd.exe /c 'make.bat'"", 'echo ""VERSION=$(python make\\gen_ver_hook.py)"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'chcp 65001\n', 'pytest unittest/test_avid.py\n', 'pytest unittest/test_file.py\n', 'pytest unittest/test_func.py\n', 'python -m pip install --upgrade pip\npip install pytest\npip install -r requirements.txt\n', 'chcp 65001\n', 'pytest unittest/test_proxyfree.py\n', 'pytest unittest/test_crawlers.py\n']"
"['pip install coverage -e "".[test]""\n', 'pytest tests/test_mypy.py', 'pytest tests/test_pyright.py', ""python setup.py clean --all\n# I know this is deprecated, but I can't find a way to keep the build\n# directory around anymore on new versions of setuptools\npython setup.py develop\n"", 'LD_PRELOAD=`gcc -print-file-name=libasan.so` coverage run -m pytest -s -m ""not mypy and not pyright""\n', 'coverage xml\ngcov -abcu `find build/ -name *.o`\n', 'echo ""CIBW_SKIP=${CIBW_SKIP} *-musllinux_* cp38-*_aarch64 cp39-*_aarch64 cp311-*_aarch64"" >> $GITHUB_ENV\n', 'python setup.py sdist', 'pip install -e "".[doc]""\n', 'pushd docs\nmake html\npopd\n']"
""
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest -c kats/tests/pytest_minimal.ini kats/tests/test_minimal.py\n', 'if [ -f test_requirements.txt ]; then pip install -r test_requirements.txt --use-deprecated=legacy-resolver; fi\n', 'pytest\n']"
"['python -m pip install -r requirements.txt -r requirements-dev.txt .\nyes | python -m mypy --install-types replicate || true\n', 'make cog', 'make test-go', 'python -m pip install -r requirements.txt -r requirements-dev.txt .\nyes | python -m mypy --install-types replicate || true\n', 'make test-python', 'python -m pip install -r requirements.txt -r requirements-dev.txt .\nyes | python -m mypy --install-types replicate || true\n', 'make test-integration', 'make cog', '# clean package built for go client\nrm -rf dist\n# install build\npip install build\n# build package\npython -m build --wheel\n# set output\necho ""version=$(ls dist/ | cut -d- -f2)"" >> $GITHUB_OUTPUT\n']"
"['pip install autopep8 autoflake isort black', 'autopep8 --verbose --in-place --recursive --aggressive --aggressive assistant/*.py\nautopep8 --verbose --in-place --recursive --aggressive --aggressive assistant/manager/*.py\nautopep8 --verbose --in-place --recursive --aggressive --aggressive plugins/*.py\n', 'autoflake --in-place --recursive --remove-all-unused-imports --remove-unused-variables --ignore-init-module-imports assistant/*.py\nautoflake --in-place --recursive --remove-all-unused-imports --remove-unused-variables --ignore-init-module-imports assistant/manager/*.py\nautoflake --in-place --recursive --remove-all-unused-imports --remove-unused-variables --ignore-init-module-imports plugins/*.py\n', 'isort assistant/*.py\nisort assistant/manager/*.py\nblack --fast assistant/*.py\nblack assistant/manager/*.py\nisort plugins/*.py\nblack --fast plugins/*.py\n', 'pip install pyyaml', 'wget -O stringizer.py https://gist.githubusercontent.com/New-dev0/6dfc8a177418565a776167764b2fe0e4/raw/f515e9892a331110c1565eb109eb712eb64423bd/stringzer.py', 'python stringizer.py && rm stringizer.py']"
"['docker image prune -f\n', 'docker build -t build-jaxlib-image -f docker/build_jaxlib.Dockerfile docker/\n', 'mkdir -p dist\ndocker run --gpus all --tmpfs /build:exec \\\n--rm -v $(pwd)/dist:/dist build-jaxlib-image \\\n3.8 cuda 11.1 main ${TF_BRANCH##*/}\n', 'echo ""Move the Jaxlib binary""\nmv dist/*.whl /data/alpa-dist/jaxlib-alpa-ci/\n', 'python -m pip install --upgrade pip\npip install yapf==0.32.0\n', 'yapf --diff --style .style.yapf --recursive alpa && yapf --diff --style .style.yapf --recursive tests\n', 'python -m pip install --upgrade pip\npip install pylint==2.14.0\n', 'pylint alpa\n', 'docker image prune -f\n', 'docker build -t test-alpa-image -f docker/unittest.Dockerfile docker/\n', 'ALPA_BRANCH=${{ github.ref }}\necho ""${ALPA_BRANCH}""\n\ndocker run --gpus all --tmpfs /build:exec --rm \\\n-v /data/alpa-dist:/alpa-dist \\\n--shm-size=10.24gb test-alpa-image 3.8 ${ALPA_BRANCH}\n', 'docker build -t build-alpa-doc -f docker/build_doc.Dockerfile docker/\n', 'docker run --gpus all --tmpfs /build:exec --rm \\\n-v /data/alpa-dist:/alpa-dist \\\n--shm-size=10.24gb \\\nbuild-alpa-doc\n', 'docker image prune -f\n', 'docker build -t build-alpa-image -f docker/build_alpa.Dockerfile docker/\n', 'mkdir -p dist\ndocker run --gpus all --tmpfs /build:exec \\\n--rm -v $(pwd)/dist:/dist --entrypoint /build_alpa.sh \\\nbuild-alpa-image 3.8 ${ALPA_BRANCH}\n', 'python -m pip install --upgrade pip\npip install twine\n', 'echo ""Publish to PyPI""\nls -ltr dist/\npython -m twine upload --verbose dist/*\n', 'docker image prune -f\n', 'docker build -t build-jaxlib-image-cuda${CUDA_VERSION} \\\n  -f docker/build_jaxlib.Dockerfile docker/ \\\n  --build-arg JAX_CUDA_VERSION=${CUDA_VERSION}\n', 'mkdir -p /data/alpa-dist/jaxlib-alpa/cuda${CUDA_VERSION//.}\necho ""Compile Python ${PYTHON_VERSION}, CUDA ${CUDA_VERSION}, ALPA BRANCH: ${ALPA_BRANCH}, TF_BRANCH: ${TF_BRANCH}""\nif [[ ${{ github.event_name }} == ""release"" ]]; then\n  docker run --gpus all --tmpfs /build:exec \\\n    --rm -v /data/alpa-dist/jaxlib-alpa/cuda${CUDA_VERSION//.}:/dist \\\n    build-jaxlib-image-cuda${CUDA_VERSION} ${PYTHON_VERSION} \\\n    cuda ${CUDA_VERSION} ${ALPA_BRANCH}\nelse\n  docker run --gpus all --tmpfs /build:exec \\\n    --rm -v /data/alpa-dist/jaxlib-alpa/cuda${CUDA_VERSION//.}:/dist \\\n    build-jaxlib-image-cuda${CUDA_VERSION} ${PYTHON_VERSION} \\\n    cuda ${CUDA_VERSION} ${ALPA_BRANCH} ${TF_BRANCH}\nfi\n', 'echo ""Move to one single folder""\nls /data/alpa-dist/jaxlib-alpa/cuda${CUDA_VERSION//.}\nmv /data/alpa-dist/jaxlib-alpa/cuda${CUDA_VERSION//.}/*.whl /data/alpa-pypi/packages/\n', 'python -m pip install --upgrade pip\npython -m pip install github3.py requests\n', 'echo ""Upload wheels to tag ${TAG}""\nls /data/alpa-pypi/packages/\npython build_jaxlib/release/wheel_upload.py --tag ${TAG} --path /data/alpa-pypi/packages/\n', 'git clone https://$GITHUB_TOKEN@github.com/alpa-projects/alpa-projects.github.io\ncd alpa-projects.github.io\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ncd ..\npython build_jaxlib/release/generate_pypi_index.py --tag ${TAG}\n']"
""
"['echo ""opal_version_tag=${{ github.event.release.tag_name }}"" >> $GITHUB_ENV\n', 'echo ""opal_version_tag=$(git describe --tags --abbrev=0)"" >> $GITHUB_ENV\n', 'echo ""The version tag that will be published to docker hub is: ${{ env.opal_version_tag }}""\n', ""sed 's/:latest/:test/g' docker/docker-compose-example.yml > docker/docker-compose-test.yml"", 'docker-compose -f docker/docker-compose-test.yml up -d', './scripts/wait-for.sh -t 60 http://localhost:8181/v1/data/users -- sleep 10 && curl -s ""http://localhost:8181/v1/data/users"" | jq \'.result.bob.location.country == ""US""\'', 'docker-compose -f docker/docker-compose-test.yml logs', 'docker image ls --digests | grep opal', 'docker push permitio/opal-client:${{ env.opal_version_tag }} && docker push permitio/opal-client:latest', 'docker push permitio/opal-client-standalone:${{ env.opal_version_tag }} && docker push permitio/opal-client-standalone:latest', 'docker push permitio/opal-server:${{ env.opal_version_tag }} && docker push permitio/opal-server:latest', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest pytest-asyncio\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f ./tests/requirements.txt ]; then pip install -r ./tests/requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', ""sed 's/:latest/:test/g' docker/docker-compose-with-callbacks.yml > docker/docker-compose-test.yml"", 'docker-compose -f docker/docker-compose-test.yml up -d', './scripts/wait-for.sh -t 2 http://localhost:8181/v1/data/users -- sleep 10 && curl -s ""http://localhost:8181/v1/data/users"" | jq \'.result.bob.location.country == ""US""\'', 'docker-compose -f docker/docker-compose-test.yml logs', 'docker-compose -f docker/docker-compose-test.yml logs opal_client | grep ""Connected to PubSub server""\ndocker-compose -f docker/docker-compose-test.yml logs opal_client | grep ""Got policy bundle""\n']"
"['pip3 install ruff', 'ruff beanie/ tests/', 'pip3 install mypy types-click types-toml', 'mypy beanie/ tests/typing --config-file pyproject.toml', 'pip3 instll .[doc]', 'bash scripts/publish_docs.sh', 'pip3 install flit', 'flit publish', 'pip install .[test]', 'pip3 install pydantic==${{ matrix.pydantic-version }}', 'pytest']"
""
"['pip install mypy\npip install -r newm/requirements.txt\npip install -r pywm/requirements.txt\n', 'MYPYPATH=$MYPYPATH:$GITHUB_WORKSPACE/pywm mypy newm\n']"
"['conda install pytorch torchvision -c pytorch-nightly\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\npython collect_env.py\n', 'pip install git+https://github.com/facebookresearch/detectron2.git\npip install git+https://github.com/facebookresearch/mobile-vision\npip install scikit-learn\npip install pytest pytest-xdist nbval\npip install -e .\n', 'python -m pytest -n 4 --durations=15 -sv tests/ --ignore=tests/skip_init/\n', 'D2GO_IMPORT_SKIP_INITIALIZATION=1 python -m pytest -n 4 --durations=15 -sv tests/skip_init/\n', 'find . -name *.ipynb | CI=true xargs pytest --nbval-lax --current-env\n']"
"['git clone https://github.com/wenLiangcan/pip2pkgbuild.git\nsed -i ""s/\\/usr\\/share\\/licenses\\/common/\\//g"" pip2pkgbuild/pip2pkgbuild/pip2pkgbuild.py\n', 'python3 pip2pkgbuild/pip2pkgbuild/pip2pkgbuild.py --pep517 recoverpy\n', 'sed -i ""s/license\\=$PARTITION_COLUMN.*/license\\=(GNU GPLv3)/"" PKGBUILD\n', 'curl -sSL https://install.python-poetry.org | python3 -\necho ""${HOME}/.poetry/bin"" >> $GITHUB_PATH\n', 'echo ""${HOME}/.poetry/bin"" >> $GITHUB_PATH', 'poetry install', 'poetry install\nsudo apt-get install progress\n', 'poetry run pytest', 'poetry config pypi-token.pypi ""${{ secrets.PYPI_TOKEN }}""', 'poetry publish --build', 'pip install --upgrade pip\npip install poetry\npoetry export --dev --without-hashes --output requirements.txt\npip install -r requirements.txt\nsudo apt install progress -y\n', 'pytest\n', 'mypy recoverpy --strict\n', 'curl -sSL https://install.python-poetry.org | python3 -\necho ""${HOME}/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry install\nsudo apt-get install progress\n', 'poetry run pytest', 'poetry config repositories.testpypi https://test.pypi.org/legacy/\npoetry config pypi-token.testpypi ${{ secrets.TEST_PYPI_TOKEN }}\npoetry publish --build -r testpypi --dry-run\n']"
[]
[]
"['python -m pip install --upgrade pip\npython -m pip install pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest\n']"
"['./scripts/verify_tag.sh', 'set -x\nexport DOCKER_BUILDKIT=1\n\nBASE_IMAGE=""${{ env.GH_REPO }}:${{ env.RELEASE_VERSION }}""\nLATEST_IMAGE=""${{ env.GH_REPO }}:latest""\nX86_64_IMAGE=""$BASE_IMAGE-x86_64""\n\n# build & push x86_64\n./scripts/build_x86_64_container.sh -t ""$X86_64_IMAGE""\ndocker push ""$X86_64_IMAGE""\n', 'scripts/setup_runner_requirements.sh', './scripts/verify_tag.sh', 'set -x\nexport DOCKER_BUILDKIT=1\n\nBASE_IMAGE=""${{ env.GH_REPO }}:${{ env.RELEASE_VERSION }}""\nLATEST_IMAGE=""${{ env.GH_REPO }}:latest""\nAARCH64_IMAGE=""$BASE_IMAGE-aarch64""\n\n# build & push Aarch64\n./scripts/build_aarch64_container.sh -t ""$AARCH64_IMAGE"" --push\n', './scripts/verify_tag.sh', 'set -x\nexport DOCKER_BUILDKIT=1\n\nBASE_IMAGE=""${{ env.GH_REPO }}:${{ env.RELEASE_VERSION }}""\nLATEST_IMAGE=""${{ env.GH_REPO }}:latest""\nAARCH64_IMAGE=""$BASE_IMAGE-aarch64""\nX86_64_IMAGE=""$BASE_IMAGE-x86_64""\n\ndocker pull $AARCH64_IMAGE\ndocker pull $X86_64_IMAGE\n\n# create manifests for the tag + for \'latest\'\ndocker manifest create ""$BASE_IMAGE"" ""$X86_64_IMAGE"" ""$AARCH64_IMAGE""\ndocker manifest push ""$BASE_IMAGE""\ndocker manifest create ""$LATEST_IMAGE"" ""$X86_64_IMAGE"" ""$AARCH64_IMAGE""\ndocker manifest push ""$LATEST_IMAGE""\n', 'python -m pip install --upgrade pip\npip install -r dev-requirements.txt\npip install -r requirements.txt\n', './lint.sh --ci', './scripts/build_x86_64_container.sh', 'mkdir -p output && docker image save gprofiler > output/gprofiler.img', 'docker image load < output/gprofiler.img', './scripts/copy_resources_from_image.sh gprofiler', 'NO_APT_INSTALL=1 ./tests/test.sh --ignore=tests/test_executable.py', 'echo ""RELEASE_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV\n./scripts/cmp_tags.sh\n', 'mkdir -p output\n./scripts/build_x86_64_executable.sh\nmv build/x86_64/gprofiler output/gprofiler_x86_64\ncp output/gprofiler_x86_64 output/gprofiler  # for backwards compatibility\n', 'mv dist/gprofiler_x86_64 dist/gprofiler\nchmod +x dist/gprofiler\nNO_APT_INSTALL=1 ./tests/test.sh --exec-container-image ${{ matrix.containers }} --executable dist/gprofiler -k test_executable\n', 'scripts/setup_runner_requirements.sh', 'echo ""RELEASE_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV\n./scripts/cmp_tags.sh\n', 'mkdir -p output\n./scripts/build_aarch64_executable.sh\nmv build/aarch64/gprofiler output/gprofiler_aarch64\n', 'echo ""RELEASE_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', '.\\scripts\\windows\\build.bat', '.\\app\\dist\\gprofiler.exe -h', './shell_lint.sh', './dockerfile_lint.sh']"
""
"['pip install cookiecutter mkdocs mkdocs-material mike\n', 'export RELEASE_TAG_VERSION=${{ github.event.release.tag_name }}\necho ""RELEASE_TAG_VERSION=${RELEASE_TAG_VERSION%.*}"">> $GITHUB_ENV\n', 'echo ""${RELEASE_TAG_VERSION}""\n', 'git config user.name ci-bot\ngit config user.email ci-bot@ci.com\nmike deploy --push --rebase --update-aliases ${RELEASE_TAG_VERSION} latest\n', 'echo ""PY_CONDA_ENV_NAME=${{ env.CONDA_ENV_NAME }}-${{ matrix.python-version }}"" >> $GITHUB_ENV', 'echo ${{ env.PY_CONDA_ENV_NAME }}', 'echo ""PY_PREFIX=${{ matrix.prefix }}${{ env.PY_CONDA_ENV_NAME }}"" >> $GITHUB_ENV', 'echo ${{ env.PY_PREFIX }}', 'echo ""PROJECT_SETUPCFG_FILE=${{ env.COOKIECUTTER_PROJECT_NAME }}/setup.cfg"" >> $GITHUB_ENV\necho ""PROJECT_CONDAENV_FILE=${{ env.COOKIECUTTER_PROJECT_NAME }}/${{ env.CONDA_ENV_FILE }}"" >> $GITHUB_ENV\necho ""PROJECT_PRECOMMIT_FILE=${{ env.COOKIECUTTER_PROJECT_NAME }}/.pre-commit-config.yaml"" >> $GITHUB_ENV\n', 'pip install cookiecutter\n', ""echo -e 'n\\nn\\nn\\n' | cookiecutter . --no-input project_name=${{ env.COOKIECUTTER_PROJECT_NAME }}\n"", 'git config --global user.name ci-bot\ngit config --global user.email ci-bot@ci.com\ngit init\ngit add --all\ngit commit -m ""Initial commit""\n', 'echo ""CACHE_KEY_POSTFIX=${{ matrix.label }}-${{ matrix.python-version }}-${{ env.CACHE_NUMBER }}-${{ env.PY_CONDA_ENV_NAME }}-${{ hashFiles(env.PROJECT_CONDAENV_FILE) }}-${{ hashFiles(env.PROJECT_SETUPCFG_FILE) }}"" >> $GITHUB_ENV\n', 'echo ${{ env.PROJECT_SETUPCFG_FILE }}\necho ${{ env.PROJECT_CONDAENV_FILE }}\necho ${{ env.PROJECT_PRECOMMIT_FILE }}\necho ${{ env.CACHE_KEY_POSTFIX }}\n', ""sed -Ei '/^\\s*-?\\s*python\\s*([#=].*)?$/d' ${{ env.CONDA_ENV_FILE }}\ncat  ${{ env.CONDA_ENV_FILE }}\n"", 'echo ""Installed Python: $(python --version)""\necho ""Expected: ${{ matrix.python-version }}""\npython --version | grep ""Python ${{ matrix.python-version }}""\n', 'pip install setuptools==59.5.0 --upgrade\n', 'mamba env update -n ${{ env.PY_CONDA_ENV_NAME }} -f ${{ env.CONDA_ENV_FILE }}', 'pip install -e "".[dev]""', 'pip3 list', 'mamba info', 'mamba list', 'echo ""Installed Python: $(python --version)""\necho ""Expected: ${{ matrix.python-version }}""\npython --version | grep ""Python ${{ matrix.python-version }}""\n', 'pre-commit install\npre-commit run -v --all-files --show-diff-on-failure\n', 'pytest -v\n']"
"['pip install pip --upgrade', 'pip install torch==1.7.0+cpu torchvision==0.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install -r requirements.txt', 'rm -rf .eggs && pip install -e .', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade pip\npip install codespell flake8 isort yapf\n', 'codespell\nflake8 .\nisort --check-only --diff gfpgan/ scripts/ inference_gfpgan.py setup.py\nyapf -r -d gfpgan/ scripts/ inference_gfpgan.py setup.py\n']"
"['python -m pip install nox poetry', 'python -m nox']"
"['pip install bandit black codespell flake8 isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101 . || true', 'black --check . || true', 'codespell', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=19 --max-line-length=245 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt', 'mypy --ignore-missing-imports . || true', 'pytest . || true', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
"['python3 -m pip install -r requirements-lint.txt', 'doit setup:js', 'doit -n4 test:js', 'doit lint', 'doit repo', 'doit -n4 docs:typedoc:mystify', '${{ matrix.python-command }} -m pip install --user -U pip setuptools wheel', ""${{ matrix.python-command }} -m pip install importlib_metadata 'doit >=0.34,<1' jupyter_core\n${{ matrix.python-command }} -m pip install --find-links dist --no-index jupyterlite_core\n${{ matrix.python-command }} -m pip check\n"", 'mkdir -p build/smoke-test', 'echo ""epoch=$(git log -1 --format=%ct)"" >> $GITHUB_OUTPUT\n', 'cd build/smoke-test\njupyter lite --version || exit 1\njupyter lite --help || exit 1\njupyter lite list || exit 1\njupyter lite status || exit 1\njupyter lite build || exit 1\njupyter lite check || exit 1\njupyter lite archive --source-date-epoch ${{ steps.source-date.outputs.epoch }} || exit 1\njupyter lite list || exit 1\njupyter lite status --debug || exit 1\n', '${{ matrix.python-command }} -m pip install -r requirements-test.txt', 'doit test:py:*', 'python3 -m pip install -r requirements-docs.txt', 'doit dev:py:jupyterlite-core', 'yarn --frozen-lockfile\nyarn run build:js-kernel\n', 'doit docs:app:build', 'doit docs:app:pack', 'doit docs:sphinx', 'doit check', 'doit test:py:jupyterlite-core', 'echo ""Optional): Review Draft Release: ${{ steps.prep-release.outputs.release_url }}""\n', 'echo ""Verify the final release""\necho ${{ steps.finalize-release.outputs.release_url }}\n', 'echo ""Failed to Publish the Draft Release Url:""\necho ${{ steps.populate-release.outputs.release_url }}\n', ""python3 -m pip install importlib_metadata 'doit >=0.34,<1' jupyter_core jupyterlab~=3.2\npython3 -m pip install --find-links dist --no-index jupyterlite_core\n"", 'cd ui-tests\n# Build the JupyterLite website\nyarn build\n', 'cd ui-tests\nyarn --frozen-lockfile\nyarn playwright install\n', 'cd ui-tests\nyarn run test --browser ${{ matrix.browser }}\n', 'cd ui-tests\n# remove previous snapshots from other browser\nyarn run clean:snapshots\n# generate new snapshots\nyarn run test:update --browser ${{ matrix.browser }}\n', 'git config --global core.autocrlf false\ngit config --global core.eol lf\n', 'doit setup || doit setup || doit setup', 'doit build', 'doit dist', 'doit dev', 'doit lint || doit lint || doit lint', 'move C:\\Users\\runneradmin\\conda_pkgs_dir C:\\Users\\runneradmin\\not_conda_pkgs_dir']"
"['python -m pip install --upgrade pip\nmake install-dev\n', 'make check', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\nmake install-dev\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'make docs']"
"['if [ -z ""$GITHUB_BASE_REF"" ]; then\n  echo \'::set-output name=run_tests::true\'\nelse\n  git fetch origin $GITHUB_BASE_REF --depth=1\n  # git diff ""origin/$GITHUB_BASE_REF..."" (3 dots) may be more\n  # reliable than git diff ""origin/$GITHUB_BASE_REF.."" (2 dots),\n  # but it requires to download more commits (this job uses\n  # ""git fetch --depth=1"").\n  #\n  # git diff ""origin/$GITHUB_BASE_REF..."" (3 dots) works with Git\n  # 2.26, but Git 2.28 is stricter and fails with ""no merge base"".\n  #\n  # git diff ""origin/$GITHUB_BASE_REF.."" (2 dots) should be enough on\n  # GitHub, since GitHub starts by merging origin/$GITHUB_BASE_REF\n  # into the PR branch anyway.\n  #\n  # https://github.com/python/core-workflow/issues/373\n  git diff --name-only origin/$GITHUB_BASE_REF.. | grep -qvE \'(\\.rst$|^Doc|^Misc)\' && echo \'::set-output name=run_tests::true\' || true\nfi\n', 'sudo ./.github/workflows/posix-deps-apt.sh', '# Build Python with the libpython dynamic library\n./configure --with-pydebug --enable-shared\nmake -j4 regen-all\nmake regen-stdlib-module-names\n', 'changes=$(git status --porcelain)\n# Check for changes in regenerated files\nif ! test -z ""$changes""\nthen\n  echo ""Generated files not up to date. Perhaps you forgot to run make regen-all ;)""\n  echo ""$changes""\n  exit 1\nfi\n', 'make smelly', 'make check-limited-abi', '.\\PCbuild\\build.bat -e -p Win32', '.\\python.bat -m test.pythoninfo', '.\\PCbuild\\rt.bat -p Win32 -q -uall -u-cpu -rwW --slowest --timeout=1200 -j0', 'echo ""::add-matcher::.github/problem-matchers/msvc.json""', '.\\PCbuild\\build.bat -e -p x64', '.\\python.bat -m test.pythoninfo', '.\\PCbuild\\rt.bat -p x64 -q -uall -u-cpu -rwW --slowest --timeout=1200 -j0', './configure --with-pydebug --with-openssl=/usr/local/opt/openssl --prefix=/opt/python-dev', 'make -j4', 'make pythoninfo', 'make buildbottest TESTOPTS=""-j4 -uall,-cpu""', 'echo ""::add-matcher::.github/problem-matchers/gcc.json""', 'sudo ./.github/workflows/posix-deps-apt.sh', 'python3 Tools/ssl/multissltests.py --steps=library --base-directory $PWD/multissl --openssl $OPENSSL_VER --system Linux', './configure --with-pydebug --with-openssl=$PWD/multissl/openssl/$OPENSSL_VER', 'make -j4', 'make pythoninfo', 'xvfb-run make buildbottest TESTOPTS=""-j4 -uall,-cpu""', '.\\Tools\\msi\\build.bat -x86', '.\\Tools\\msi\\build.bat -x64', 'sudo ./.github/workflows/posix-deps-apt.sh', 'python3 Tools/ssl/multissltests.py --steps=library --base-directory $PWD/multissl --openssl $OPENSSL_VER --system Linux', './configure --with-openssl=$PWD/multissl/openssl/$OPENSSL_VER', 'make -j4', 'make pythoninfo', './python -m venv .venv\nsource ./.venv/bin/activate\npython -m pip install -U coverage\npython -m pip install -r Misc/requirements-test.txt\npython -m test.pythoninfo\nexport PYTHONPATH=`find .venv -name fullcoverage`\n', 'source ./.venv/bin/activate && xvfb-run python -m coverage run --branch --pylib -m test --fail-env-changed -uall,-cpu -x test_multiprocessing_fork -x test_multiprocessing_forkserver -x test_multiprocessing_spawn -x test_concurrent_futures || true\n', 'export PYTHONPATH=\nsource ./.venv/bin/activate\nbash <(curl -s https://codecov.io/bash) -y .github/codecov.yml\n', 'sudo ./.github/workflows/posix-deps-apt.sh', './configure', 'xvfb-run make -j4 coverage-report', 'make pythoninfo\nbash <(curl -s https://codecov.io/bash) -y .github/codecov.yml\n', 'echo ""::add-matcher::.github/problem-matchers/sphinx.json""', 'sudo ./.github/workflows/posix-deps-apt.sh && sudo apt-get install wamerican', './configure --with-pydebug', 'make -j4', 'make -C Doc/ PYTHON=../python venv', 'xvfb-run make -C Doc/ PYTHON=../python SPHINXOPTS=""-q -W --keep-going -j4"" doctest html']"
"['if [ -z ""$GITHUB_BASE_REF"" ]; then\n  echo \'::set-output name=run_tests::true\'\n  echo \'::set-output name=run_ssl_tests::true\'\nelse\n  git fetch origin $GITHUB_BASE_REF --depth=1\n  # git diff ""origin/$GITHUB_BASE_REF..."" (3 dots) may be more\n  # reliable than git diff ""origin/$GITHUB_BASE_REF.."" (2 dots),\n  # but it requires to download more commits (this job uses\n  # ""git fetch --depth=1"").\n  #\n  # git diff ""origin/$GITHUB_BASE_REF..."" (3 dots) works with Git\n  # 2.26, but Git 2.28 is stricter and fails with ""no merge base"".\n  #\n  # git diff ""origin/$GITHUB_BASE_REF.."" (2 dots) should be enough on\n  # GitHub, since GitHub starts by merging origin/$GITHUB_BASE_REF\n  # into the PR branch anyway.\n  #\n  # https://github.com/python/core-workflow/issues/373\n  git diff --name-only origin/$GITHUB_BASE_REF.. | grep -qvE \'(\\.rst$|^Doc|^Misc)\' && echo \'::set-output name=run_tests::true\' || true\n  git diff --name-only origin/$GITHUB_BASE_REF.. | grep -qE \'(ssl|hashlib|hmac|^.github)\' && echo \'::set-output name=run_ssl_tests::true\' || true\nfi\n', 'sudo ./.github/workflows/posix-deps-apt.sh', 'echo ""PATH=/usr/lib/ccache:$PATH"" >> $GITHUB_ENV', 'grep ""Generated by GNU Autoconf 2.69"" configure\ngrep ""aclocal 1.16.3"" aclocal.m4\ngrep -q ""runstatedir"" configure\ngrep -q ""PKG_PROG_PKG_CONFIG"" aclocal.m4\n', '# Build Python with the libpython dynamic library\n./configure --with-pydebug --enable-shared\n', 'make regen-configure', '# Deepfreeze will usually cause global objects to be added or removed,\n# so we run it before regen-global-objects gets rum (in regen-all).\nmake regen-deepfreeze\nmake -j4 regen-all\nmake regen-stdlib-module-names\n', 'git add -u\nchanges=$(git status --porcelain)\n# Check for changes in regenerated files\nif test -n ""$changes""; then\n  echo ""Generated files not up to date.""\n  echo ""Perhaps you forgot to run make regen-all or build.bat --regen. ;)""\n  echo ""configure files must be regenerated with a specific version of autoconf.""\n  echo ""$changes""\n  echo """"\n  git diff --staged || true\n  exit 1\nfi\n', 'make smelly', 'make check-limited-abi', '.\\PCbuild\\build.bat -e -d -p Win32', '.\\python.bat -m test.pythoninfo', '.\\PCbuild\\rt.bat -p Win32 -d -q -uall -u-cpu -rwW --slowest --timeout=1200 -j0', 'echo ""::add-matcher::.github/problem-matchers/msvc.json""', '.\\PCbuild\\build.bat -e -d -p x64', '.\\python.bat -m test.pythoninfo', '.\\PCbuild\\rt.bat -p x64 -d -q -uall -u-cpu -rwW --slowest --timeout=1200 -j0', 'echo ""LDFLAGS=-L$(brew --prefix tcl-tk)/lib"" >> $GITHUB_ENV\necho ""PKG_CONFIG_PATH=$(brew --prefix openssl@1.1)/lib/pkgconfig:$(brew --prefix tcl-tk)/lib/pkgconfig"" >> $GITHUB_ENV\n', './configure --with-pydebug --prefix=/opt/python-dev', 'make -j4', 'make pythoninfo', 'make buildbottest TESTOPTS=""-j4 -uall,-cpu""', 'echo ""::add-matcher::.github/problem-matchers/gcc.json""', 'sudo ./.github/workflows/posix-deps-apt.sh', 'echo ""MULTISSL_DIR=${GITHUB_WORKSPACE}/multissl"" >> $GITHUB_ENV\necho ""OPENSSL_DIR=${GITHUB_WORKSPACE}/multissl/openssl/${OPENSSL_VER}"" >> $GITHUB_ENV\necho ""LD_LIBRARY_PATH=${GITHUB_WORKSPACE}/multissl/openssl/${OPENSSL_VER}/lib"" >> $GITHUB_ENV\n', 'python3 Tools/ssl/multissltests.py --steps=library --base-directory $MULTISSL_DIR --openssl $OPENSSL_VER --system Linux', 'echo ""PATH=/usr/lib/ccache:$PATH"" >> $GITHUB_ENV\n', 'echo ""CPYTHON_RO_SRCDIR=$(realpath -m ${GITHUB_WORKSPACE}/../cpython-ro-srcdir)"" >> $GITHUB_ENV\necho ""CPYTHON_BUILDDIR=$(realpath -m ${GITHUB_WORKSPACE}/../cpython-builddir)"" >> $GITHUB_ENV\n', 'mkdir -p $CPYTHON_RO_SRCDIR $CPYTHON_BUILDDIR', 'sudo mount --bind -o ro $GITHUB_WORKSPACE $CPYTHON_RO_SRCDIR', '../cpython-ro-srcdir/configure --with-pydebug --with-openssl=$OPENSSL_DIR', 'make -j4', 'make pythoninfo', 'sudo mount $CPYTHON_RO_SRCDIR -oremount,rw', 'xvfb-run make buildbottest TESTOPTS=""-j4 -uall,-cpu""', 'echo ""::add-matcher::.github/problem-matchers/gcc.json""', 'sudo ./.github/workflows/posix-deps-apt.sh', 'echo ""MULTISSL_DIR=${GITHUB_WORKSPACE}/multissl"" >> $GITHUB_ENV\necho ""OPENSSL_DIR=${GITHUB_WORKSPACE}/multissl/openssl/${OPENSSL_VER}"" >> $GITHUB_ENV\necho ""LD_LIBRARY_PATH=${GITHUB_WORKSPACE}/multissl/openssl/${OPENSSL_VER}/lib"" >> $GITHUB_ENV\n', 'python3 Tools/ssl/multissltests.py --steps=library --base-directory $MULTISSL_DIR --openssl $OPENSSL_VER --system Linux', 'echo ""PATH=/usr/lib/ccache:$PATH"" >> $GITHUB_ENV\n', './configure --with-pydebug --with-openssl=$OPENSSL_DIR', 'make -j4', 'make pythoninfo', './python Lib/test/ssltests.py', 'echo ""::add-matcher::.github/problem-matchers/gcc.json""', 'sudo ./.github/workflows/posix-deps-apt.sh', 'echo ""MULTISSL_DIR=${GITHUB_WORKSPACE}/multissl"" >> $GITHUB_ENV\necho ""OPENSSL_DIR=${GITHUB_WORKSPACE}/multissl/openssl/${OPENSSL_VER}"" >> $GITHUB_ENV\necho ""LD_LIBRARY_PATH=${GITHUB_WORKSPACE}/multissl/openssl/${OPENSSL_VER}/lib"" >> $GITHUB_ENV\n', 'python3 Tools/ssl/multissltests.py --steps=library --base-directory $MULTISSL_DIR --openssl $OPENSSL_VER --system Linux', 'echo ""PATH=/usr/lib/ccache:$PATH"" >> $GITHUB_ENV\n', './configure --with-address-sanitizer --without-pymalloc', 'make -j4', 'make pythoninfo', 'xvfb-run make buildbottest TESTOPTS=""-j4 -uall,-cpu""', '.\\Tools\\msi\\build.bat -x86', '.\\Tools\\msi\\build.bat -x64', '.\\Tools\\msi\\build.bat -arm64', 'echo ""::add-matcher::.github/problem-matchers/sphinx.json""', 'sudo ./.github/workflows/posix-deps-apt.sh && sudo apt-get install wamerican', './configure --with-pydebug', 'make -j4', 'make -C Doc/ PYTHON=../python venv', 'make -C Doc/ PYTHON=../python SPHINXOPTS=""-q -W --keep-going"" check', 'xvfb-run make -C Doc/ PYTHON=../python SPHINXOPTS=""-q -W --keep-going"" doctest', 'make -C Doc/ PYTHON=../python SPHINXOPTS=""-q -W --keep-going"" html', 'npm install mailgun.js form-data']"
""
"['python -m pip install --upgrade pip\npip install pdoc\npip install .\n', 'pdoc -o ./docs pytermgui --logo https://github.com/bczsalba/pytermgui/blob/master/assets/title.png?raw=true --docformat google\n', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\ngit add ./docs/\ngit commit -m ""Auto-update documentation""\n', 'python -m pip install --upgrade pip\npip install .\npip install pdoc\n', 'pdoc -o ./docs pytermgui --logo https://github.com/bczsalba/pytermgui/blob/master/assets/title.png?raw=true --docformat google\n', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\ngit add ./docs/\ngit commit -m ""${{ github.event.inputs.message }}""\n', 'python -m pip install --upgrade pip\npip install pylint\npip install typing_extensions\n', ""pylint --fail-under 10.0 -d fixme $(git ls-files '*.py' | grep '^pytermgui/')\n"", 'python -m pip install --upgrade pip\npip install testfixtures\npip install pytest-cov\npip install coverage\npip install pyyaml\npip install .\n', 'make test\n']"
"['python -m pip install --upgrade pip\npip install -e .\npip install pytest\npip install pytest-mock\npip install kucoin-python\n', 'echo ""$KEYS_FILE"" > ./tests/config/keys.json\n', 'pytest\necho ""Sleeping to reduce rate limit pressure...""\n', 'python -m pip install wheel', 'pip wheel -w DEST_DIR .']"
"['python -m pip install --upgrade pip setuptools wheel\npip install pytest pycodestyle docutils Pygments hypothesis python-Levenshtein\n', '${{ matrix.test-cmd }}\n']"
"['python -m pip install --upgrade pip\npip install black flake8\n', 'python -m black --check torchtyping/\n', 'flake8 torchtyping/\n', 'python -m pip install --upgrade pip\npip install wheel\npip install -e .\npip install pytest\n', 'python -m pytest test/\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install flake8==3.8.1 isort==4.3.21\npython -m pip install black==21.4b2\nflake8 --version\n', 'echo ""Running isort""\nisort -c -sp .\necho ""Running black""\nblack -l 100 --check .\necho ""Running flake8""\nflake8 .\n', ""python -m pip install -U pip\npython -m pip install ninja opencv-python-headless onnx pytest-xdist\npython -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n# install from github to get latest; install iopath first since fvcore depends on it\npython -m pip install -U 'git+https://github.com/facebookresearch/iopath'\npython -m pip install -U 'git+https://github.com/facebookresearch/fvcore'\n"", 'CC=clang CXX=clang++ python -m pip install -e .[all]\npython -m detectron2.utils.collect_env\n./datasets/prepare_for_tests.sh\n', 'python -m pytest -n 4 --durations=15 -v tests/']"
"['python -m pip install --upgrade pip\npip install .[dali,umap,h5] --extra-index-url https://developer.download.nvidia.com/compute/redist codecov\npip install mypy pytest-cov black\n', 'pytest --cov=solo tests/dali', 'coverage report\ncoverage xml\n', 'echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'python -m pip install --upgrade pip\npip install .[dali,umap,h5] --extra-index-url https://developer.download.nvidia.com/compute/redist\npip install -r docs/requirements.txt\n', 'sphinx-build -b html docs/source docs/build\n', 'python -m pip install --upgrade pip\npip install .[dali,umap,h5] --extra-index-url https://developer.download.nvidia.com/compute/redist\npip install -r docs/requirements.txt\n', 'cd docs && make linkcheck\n', 'python -m pip install --upgrade pip\npip install -e .[umap,h5] codecov mypy pytest-cov black\n', 'pytest --cov=solo tests/args tests/backbones tests/data tests/losses tests/methods tests/utils', 'coverage report\ncoverage xml\n']"
[]
"['python3 -m pip install discord.py==1.6.0\n', 'python .github/scripts/disco.py', 'python -m pip install --upgrade pip\npython -m pip install --user pipenv\npipenv install --dev\n', '# stop the build if there are Python syntax errors or undefined names\npipenv run flake8 mgm-hurry --config tox.ini --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\npipenv run flake8 mgm-hurry --config tox.ini --count --exit-zero --statistics\n', 'python -m pip install --upgrade pip pipenv\npipenv install --dev\n', 'pipenv run pytest -c pytest.ini tests/\n']"
"['sudo npm install -g markdown-link-check', 'for file in $(find . -name ""*.md""); do\n  markdown-link-check -c .github/workflows/config/.dlc.json -q ""$file""\ndone', 'echo ::set-output name=GITHUB_REF::${GITHUB_REF##*/}', '#bash .github/workflows/config_update.sh\n#bash .github/workflows/config_update.sh ""${{ steps.release.outputs.VERSION }}""\n', 'if [ ${{ steps.version.outputs.GITHUB_REF }} = develop ] ; then ossutil cp oss://dongtai-helm-charts/agent_test/java/latest/ ./ --include ""*.jar"" -r\nelse  ossutil cp oss://dongtai-helm-charts/agent_${{ steps.version.outputs.GITHUB_REF }}/java/latest/ ./ --include ""*.jar"" -r\nfi\n', 'ossutil cp oss://dongtai/agent/java/${{ inputs.agent_version }}/ ./ --include ""*.jar"" -r\n', 'echo ""REPLACE INTO project_version_control (version, component_name, component_version_hash) VALUES(\'${{ github.run_number }}.0.0\', \'${{ github.event.repository.name }}\', \'${GITHUB_SHA}\');"" >> ./deploy/docker/version.sql\n', ""git checkout -b pr@develop@v${{github.run_number}}\ngit remote set-url origin --push --add '${{ secrets.DONGTAI_GITHUB_TOKEN_ALL }}'\ngit push --set-upstream origin pr@develop@v${{github.run_number}}\n"", 'sudo curl -fsSL get.docker.com | sh\n', 'wget https://huoqi-public.oss-cn-beijing.aliyuncs.com/iast/sql/data.zip -P ./deploy/docker-compose\nunzip ./deploy/docker-compose/data.zip -d ./deploy/docker-compose\n', 'bats test/test_dtctl.bats \n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install schemathesis\npip install httpx\n', 'curl -X GET ""https://iast.io/openapi/api/v1/agent/download?url=https://iast.io/openapi&language=python&projectName=WEBAPI${{ github.repository_owner }}.${{github.run_number}}"" -H ""Authorization: Token a303ab4bedc93f96808335d023d7ac4d2ba00773"" -o dongtai-agent-python.tar.gz -k\npip install dongtai-agent-python.tar.gz\n', 'mysql --host=127.0.0.1 -uroot -p\'dongtai-iast\' -D dongtai_webapi < /home/runner/work/DongTai/DongTai/test/init.sql\ncp dongtai_conf/conf/config.ini.test dongtai_conf/conf/config.ini\nexport PROJECT_NAME=DONGTAI-SERVER.${{ github.repository_owner }}\nexport PROJECT_VERSION=${{github.run_number}}\nexport DOC=TRUE\nexport PYTHONAGENT=TRUE\nmkdir -p /tmp/logstash/report/{img,word,pdf,excel} && mkdir -p /tmp/iast_cache/package && mkdir -p /tmp/logstash/batchagent \npython3 ./deploy/docker/version_update.py || true\npython3 manage.py updatedepartmenttoken\npython3 manage.py runserver 0.0.0.0:8000  &\nsleep 60\nschemathesis   run --base-url ""http://localhost:8000/""  -H \'Authorization: Token 67aebd78e700ad36a82a152276196b5f49fafeb0\' http://localhost:8000/api/XZPcGFKoxYXScwGjQtJx8u/schema/  --hypothesis-suppress-health-check=data_too_large,filter_too_much,too_slow,large_base_example,function_scoped_fixture --validate-schema=false  --hypothesis-verbosity normal \n', 'TAG_NAME=${{ github.event.release.tag_name }}\nID=`echo ${TAG_NAME##v}`\nif [ -z ""${{ inputs.server_version }}"" ]\nthen\n    echo ""variable is empty""\nelse\n    ID=${{ inputs.server_version }}\nfi\necho ""iast_version=$ID"" >> $GITHUB_ENV\n', 'bash .github/workflows/version_update.sh ""${{ env.iast_version }}""\n', 'echo ""${{ github.event.repository.name }},version,${{ env.iast_version }}"" >> version.txt && \\\necho ""${{ github.event.repository.name }},commit_hash,${GITHUB_SHA}"" >> version.txt && \\\ncat version.txt\nossutil cp -rf version.txt oss://huoqi-public/iast/release-version/${{ github.event.repository.name }}/${{ env.iast_version }}/version.txt\nossutil cp oss://dongtai/agent/java/${{ inputs.agent_version }}/ ./ --include ""*.jar"" -r\n[ ! -f ./dongtai-agent.jar ] && echo ""$FILE does not exist.""\n[ ! -f ./dongtai-agent.jar ] && ossutil cp oss://dongtai/agent/java/latest/ ./ --include ""*.jar"" -r\n[ ! -f ./dongtai-agent.jar ] && echo ""$FILE does not exist.""\nossutil cp oss://dongtai/agent/python/ ./  --include ""*.tar.gz"" -r\nossutil cp oss://dongtai/agent/php/ ./  --include ""*.tar.gz"" -r\necho ""REPLACE INTO project_version_control (version, component_name, component_version_hash) VALUES(\'${{ env.iast_version }}\', \'${{ github.event.repository.name }}\', \'${GITHUB_SHA}\');"" >> ./deploy/docker/version.sql\n', 'TAG_NAME=${{ github.event.release.tag_name }}\nID=`echo ${TAG_NAME##v}`\nif [ -z ""${{ inputs.server_version }}"" ]\nthen\n    echo ""variable is empty""\nelse\n    ID=${{ inputs.server_version }}\nfi\necho ""iast_version=$ID"" >> $GITHUB_ENV\n', 'echo ::set-output name=VERSION::${GITHUB_REF#refs/tags/}', 'sed -i ""s#tag: latest#tag: ${{ env.iast_version }}#g"" deploy/kubernetes/helm/values.yaml\nossutil cp -rf  oss://dongtai-helm-charts/iast/ ~/helm/repo/  --include dongtai-iast-*.tgz --exclude ""index.yaml""\n', 'helm package deploy/kubernetes/helm  -d ~/helm/repo --app-version ${{ env.iast_version }} --version ${{ env.iast_version }}\nhelm repo index ~/helm/repo/ --url ${{ secrets.DONGTAI_IAST_CHART_REPO_URL }}\n', 'ossutil cp -rf ~/helm/repo/dongtai-iast-${{ env.iast_version }}.tgz  oss://dongtai-helm-charts/iast/\nossutil cp -rf ~/helm/repo/index.yaml  oss://dongtai-helm-charts/iast/\n', 'STARS=`curl -s \'https://api.github.com/repos/${{github.repository}}?page=$i&per_page=100\' | jq .stargazers_count`\necho ""::set-output name=stars::$STARS""\n', 'STARS=`curl -s \'https://api.github.com/repos/${{github.repository}}?page=$i&per_page=100\' | jq .forks_count`\necho ""::set-output name=forks::$STARS""\n', 'STARS=`curl -s \'https://api.github.com/repos/${{github.repository}}?page=$i&per_page=100\' | jq .open_issues_count`\necho ""::set-output name=issues::$STARS""\n', 'echo ""${{ steps.repo-stars.outputs.stars }}""\necho ""${{ steps.repo-forks.outputs.forks }}""\necho ""${{ steps.repo-issues.outputs.issues }}""\n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install schemathesis\npip install httpx\n', 'mysql --host=127.0.0.1 -uroot -p\'dongtai-iast\' -D dongtai_webapi < /home/runner/work/DongTai/DongTai/test/init.sql\ncp dongtai_conf/conf/config.ini.test dongtai_conf/conf/config.ini\nexport DOC=TRUE\nmkdir -p /tmp/logstash/report/{img,word,pdf,excel} && mkdir -p /tmp/iast_cache/package && mkdir -p /tmp/logstash/batchagent \npython3 ./deploy/docker/version_update.py || true\npython3 manage.py updatedepartmenttoken\npython3 manage.py runserver 0.0.0.0:8000 > webapi.log &\nsleep 15\nschemathesis run --base-url ""http://localhost:8000/""  -H \'Authorization: Token 67aebd78e700ad36a82a152276196b5f49fafeb0\' http://localhost:8000/api/XZPcGFKoxYXScwGjQtJx8u/schema/  --hypothesis-suppress-health-check=data_too_large,filter_too_much,too_slow,large_base_example,function_scoped_fixture --validate-schema=false  --hypothesis-verbosity normal \n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\n', 'mypy --show-error-codes --ignore-missing-imports  --no-incremental --show-error-codes --check-untyped-defs --disable-error-code var-annotated  --disable-error-code  list-item  --disable-error-code attr-defined --disable-error-code arg-type --disable-error-code assignment --disable-error-code misc --disable-error-code union-attr --disable-error-code index --disable-error-code call-overload  --disable-error-code dict-item  --disable-error-code truthy-function --disable-error-code operator --disable-error-code name-defined .\n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install pycodestyle\n', 'pycodestyle  --ignore=E501,E402,E302,E265,W503,W504 --statistics .\n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install flake8\n', 'flake8 --max-complexity 43  --statistics --ignore=E265,E302,E402,E501,F401,F541,F811,F841,W503,W504 --show-source .  \n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install bandit\n', 'bandit -iii -lll -r .\n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install coverage\npip install -r requirements.txt\n', ""cp dongtai_conf/conf/config.ini.test dongtai_conf/conf/config.ini\nmkdir -p /tmp/logstash/report/{img,word,pdf,excel} && mkdir -p /tmp/iast_cache/package && mkdir -p /tmp/logstash/batchagent \npython3 ./deploy/docker/version_update.py || true\ncoverage run --source='.' manage.py test\ncoverage report\n"", 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install coverage\npip install -r requirements.txt\n', 'cp dongtai_conf/conf/config.ini.test dongtai_conf/conf/config.ini\nmkdir -p /tmp/logstash/report/{img,word,pdf,excel} && mkdir -p /tmp/iast_cache/package && mkdir -p /tmp/logstash/batchagent \npython3 ./deploy/docker/version_update.py || true\npip install Cython==3.0.0a11\npython setup.py build_ext --inplace\nfind . -name ""*.so""  | grep test | xargs rm\ncoverage run --source=\'.\' manage.py test\ncoverage report\n', 'sudo apt-get install build-essential cmake ninja-build python3-dev cython3 pybind11-dev libre2-dev -y\npython -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install schemathesis\npip install httpx\n', 'mysql --host=127.0.0.1 -uroot -p\'dongtai-iast\' -D dongtai_webapi < /home/runner/work/DongTai/DongTai/test/init.sql\ncp dongtai_conf/conf/config.ini.test dongtai_conf/conf/config.ini\nexport DOC=TRUE\nmkdir -p /tmp/logstash/report/{img,word,pdf,excel} && mkdir -p /tmp/iast_cache/package && mkdir -p /tmp/logstash/batchagent \npython3 ./deploy/docker/version_update.py || true\npip install Cython==3.0.0a11\npython setup.py build_ext --inplace\npython3 manage.py updatedepartmenttoken\npython3 manage.py runserver 0.0.0.0:8000 > webapi.log &\nsleep 15\nschemathesis run --base-url ""http://localhost:8000/""  -H \'Authorization: Token 67aebd78e700ad36a82a152276196b5f49fafeb0\' http://localhost:8000/api/XZPcGFKoxYXScwGjQtJx8u/schema/  --hypothesis-suppress-health-check=data_too_large,filter_too_much,too_slow,large_base_example,function_scoped_fixture --validate-schema=false  --hypothesis-verbosity normal \n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -m spacy download en_core_web_sm\npython -m spacy download en_core_web_md\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest']"
['pip install black flake8']
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"[""pip install torch && pip install -e '.[testing]'"", 'python -m pytest -s -v']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -c ""import nltk; nltk.download(\'wordnet\'); nltk.download(\'averaged_perceptron_tagger\')""\npytest\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install black', 'black --check src', 'pip install FAQtory', 'faqtory suggest ""$TITLE"" > suggest.md', 'poetry install --extras ""dev""', 'source $VENV\npytest tests -v --cov=./src/textual --cov-report=xml:./coverage.xml --cov-report term-missing\n']"
"['docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd api\nPUSH_IMAGE=0 bash -x ./build_alerts.sh ee\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""alerts"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""alerts"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/alerts/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,alerts,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,alerts,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks --kube-version=$k_version | kubectl apply -f -\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd api\nPUSH_IMAGE=0 bash -x ./build_alerts.sh\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""alerts"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""alerts"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    tag: ${image_array[1]}\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/alerts/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,alerts,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,alerts,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd api\nPUSH_IMAGE=0 bash -x ./build.sh ee\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""chalice"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""chalice"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/chalice/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,chalice,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,chalice,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks --kube-version=$k_version | kubectl apply -f -\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd api\nPUSH_IMAGE=0 bash -x ./build.sh\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""chalice"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""chalice"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    tag: ${image_array[1]}\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/chalice/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,chalice,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,chalice,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd assist\nPUSH_IMAGE=0 bash -x ./build.sh ee\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""assist"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""assist"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/assist/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,assist,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,assist,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks --kube-version=$k_version | kubectl apply -f -\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd assist\nPUSH_IMAGE=0 bash -x ./build.sh\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""assist"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""assist"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.OSS_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/assist/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,assist,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,assist,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks --kube-version=$k_version | kubectl apply -f -\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd api\nPUSH_IMAGE=0 bash -x ./build_crons.sh ee\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""crons"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""crons"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/crons/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,utilities,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,utilities,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks --kube-version=$k_version | kubectl apply -f -\n', '[[ `git --no-pager diff --name-only HEAD HEAD~1 | grep -E ""scripts/helm/db/init_dbs"" | grep -vE ^ee/` ]] || echo ""::set-output name=skip_migration_oss::true""', 'set -x\n#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    tag: ${image_array[1]}\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nhelm upgrade --install openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --atomic --set forceMigration=true --set dbMigrationUpstreamBranch=${IMAGE_TAG}\n', 'rm -rf /tmp/image_*\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'git checkout -- scripts/helmcharts/vars.yaml\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nhelm upgrade --install openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --atomic --set forceMigration=true --set dbMigrationUpstreamBranch=${IMAGE_TAG}\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', 'set -x\ncd frontend\nmv .env.sample .env\ndocker run --rm -v /etc/passwd:/etc/passwd -u `id -u`:`id -g` -v $(pwd):/home/${USER} -w /home/${USER} --name node_build node:14-stretch-slim /bin/bash -c ""yarn && yarn build""\n# https://github.com/docker/cli/issues/1134#issuecomment-613516912\nDOCKER_BUILDKIT=1 docker build --target=cicd -t $DOCKER_REPO/frontend:${IMAGE_TAG} .\ndocker tag $DOCKER_REPO/frontend:${IMAGE_TAG} $DOCKER_REPO/frontend:${IMAGE_TAG}-ee\ndocker push $DOCKER_REPO/frontend:${IMAGE_TAG}\ndocker push $DOCKER_REPO/frontend:${IMAGE_TAG}-ee\n', 'cd scripts/helmcharts/\n\nset -x\ncat <<EOF>>/tmp/image_override.yaml\nfrontend:\n  image:\n    tag: ${IMAGE_TAG}\nEOF\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.DEV_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.DEV_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.DEV_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.DEV_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.DEV_DOMAIN_NAME }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/frontend/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,frontend,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,frontend,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'set -x\ncd frontend\nmv .env.sample .env\ndocker run --rm -v /etc/passwd:/etc/passwd -u `id -u`:`id -g` -v $(pwd):/home/${USER} -w /home/${USER} --name node_build node:14-stretch-slim /bin/bash -c ""yarn && yarn build""\n# https://github.com/docker/cli/issues/1134#issuecomment-613516912\nDOCKER_BUILDKIT=1 docker build --target=cicd -t $DOCKER_REPO/frontend:${IMAGE_TAG} .\ndocker tag $DOCKER_REPO/frontend:${IMAGE_TAG} $DOCKER_REPO/frontend:${IMAGE_TAG}-ee\ndocker push $DOCKER_REPO/frontend:${IMAGE_TAG}\ndocker push $DOCKER_REPO/frontend:${IMAGE_TAG}-ee\n', 'cd scripts/helmcharts/\n\nset -x\ncat <<EOF>>/tmp/image_override.yaml\nfrontend:\n  image:\n    tag: ${IMAGE_TAG}\nEOF\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/frontend/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,frontend,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,frontend,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'git checkout -- scripts/helmcharts/vars.yaml\n', 'cd scripts/helmcharts/\ncat <<EOF>/tmp/image_override.yaml\nfrontend:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: ${IMAGE_TAG}\nEOF\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/frontend/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,frontend,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,frontend,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd peers\nPUSH_IMAGE=0 bash -x ./build.sh ee\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""peers"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""peers"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: `echo ${image_array[1]} | cut -d \'-\' -f 1`\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/peers/{n;n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,peers,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,peers,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks --kube-version=$k_version | kubectl apply -f -\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd peers\nPUSH_IMAGE=0 bash -x ./build.sh\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""peers"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""peers"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    tag: ${image_array[1]}\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/peers/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,peers,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,peers,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd sourcemap-reader\nPUSH_IMAGE=0 bash -x ./build.sh\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""sourcemaps-reader"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""sourcemaps-reader"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    tag: ${image_array[1]}\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/sourcemaps-reader/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\nsed -i ""s/sourcemaps-reader/sourcemapreader/g"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,sourcemapreader,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,sourcemapreader,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', 'skip_security_checks=${{ github.event.inputs.skip_security_checks }}\ncd sourcemap-reader\nPUSH_IMAGE=0 bash -x ./build.sh\n[[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n  curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n  images=(""sourcemaps-reader"")\n  for image in ${images[*]};do\n    ./trivy image --exit-code 1 --security-checks vuln --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n  done\n  err_code=$?\n  [[ $err_code -ne 0 ]] && {\n    exit $err_code\n  }\n} && {\n  echo ""Skipping Security Checks""\n}\nimages=(""sourcemaps-reader"")\nfor image in ${images[*]};do\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \ndone\n', '#\n# Create yaml with existing image tags\n#\nkubectl get pods -n app -o jsonpath=""{.items[*].spec.containers[*].image}"" |\\\ntr -s \'[[:space:]]\' \'\\n\' | sort | uniq -c | grep \'/foss/\' | cut -d \'/\' -f3 > /tmp/image_tag.txt\n\necho > /tmp/image_override.yaml\n\nfor line in `cat /tmp/image_tag.txt`;\ndo\n    image_array=($(echo ""$line"" | tr \':\' \'\\n\'))\n    cat <<EOF >> /tmp/image_override.yaml\n${image_array[0]}:\n  image:\n    tag: ${image_array[1]}\nEOF\ndone\n', 'cd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\n# Update changed image tag\nsed -i ""/sourcemaps-reader/{n;n;s/.*/    tag: ${IMAGE_TAG}/}"" /tmp/image_override.yaml\nsed -i ""s/sourcemaps-reader/sourcemapreader/g"" /tmp/image_override.yaml\n\ncat /tmp/image_override.yaml\n# Deploy command\nmv openreplay/charts/{ingress-nginx,sourcemapreader,quickwit} /tmp\nrm -rf  openreplay/charts/*\nmv /tmp/{ingress-nginx,sourcemapreader,quickwit} openreplay/charts/\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true --no-hooks | kubectl apply -n app -f -\n', 'cd tracker/tracker\nnpm i -g yarn\nyarn\n', 'cd tracker/tracker\nyarn test\n', 'cd tracker/tracker\nyarn build\n', 'cd tracker/tracker-testing-playground\necho ""REACT_APP_KEY=$FOSS_PROJECT_KEY"" >> .env\necho ""REACT_APP_INGEST=$FOSS_INGEST"" >> .env\n', 'cd tracker/tracker-testing-playground\nyarn\n', 'cd tracker/tracker-testing-playground\nyarn start &> ui.log &\nnpx wait-on http://localhost:3000\ncd ../../frontend\n', 'cd frontend\necho ""NODE_ENV=development"" >> .env\necho ""SOURCEMAP=true"" >> .env\necho ""ORIGIN=$API"" >> .env\necho ""ASSETS_HOST=$ASSETS"" >> .env\necho ""API_EDP=$APIEDP"" >> .env\necho ""SENTRY_ENABLED = false"" >> .env\necho ""SENTRY_URL = \'\'"" >> .env\necho ""CAPTCHA_ENABLED = false"" >> .env\necho ""CAPTCHA_SITE_KEY = \'asdad\'"" >> .env\necho ""MINIO_ENDPOINT = \'\'"" >> .env\necho ""MINIO_PORT = \'\'"" >> .env\necho ""MINIO_USE_SSL = \'\'"" >> .env\necho ""MINIO_ACCESS_KEY = \'\'"" >> .env\necho ""MINIO_SECRET_KEY = \'\'"" >> .env\necho ""VERSION = \'1.9.0\'"" >> .env\necho ""TRACKER_VERSION = \'4.0.0\'"" >> .env\necho ""COMMIT_HASH = \'dev\'"" >> .env\necho ""{ \\""account\\"": \\""$CY_ACC\\"", \\""password\\"": \\""$CY_PASS\\"" }"" >> cypress.env.json\n', 'cd frontend\nyarn\n', 'cd frontend\nyarn test\n', 'cd frontend\nyarn start &> frontend.log &\n', 'cd frontend\nnpx wait-on http://0.0.0.0:3333\n', 'cd frontend\nyarn cy:test\n', 'docker login ${{ secrets.EE_REGISTRY_URL }} -u ${{ secrets.EE_DOCKER_USERNAME }} -p ""${{ secrets.EE_REGISTRY_TOKEN }}"" \n', '#\n# TODO: Check the container tags are same, then skip the build and deployment.\n#\n# Build a docker container and push it to Docker Registry so that it can be deployed to Kubernetes cluster.\n#\n# Getting the images to build\n#\nset -x\ntouch /tmp/images_to_build.txt\nskip_security_checks=${{ github.event.inputs.skip_security_checks }}\ntmp_param=${{ github.event.inputs.build_service }}\nbuild_param=${tmp_param:-\'false\'}\ncase ${build_param} in\n  false)\n    {\n      git diff --name-only HEAD HEAD~1 | grep -E ""backend/pkg|backend/internal"" | grep -vE ^ee/ | cut -d \'/\' -f3 | uniq | while read -r pkg_name ; do\n        grep -rl ""pkg/$pkg_name"" backend/services backend/cmd | cut -d \'/\' -f3 \n      done\n    } | awk \'!seen[$0]++\' > /tmp/images_to_build.txt\n  ;;\n  all)\n    ls backend/cmd > /tmp/images_to_build.txt\n    ;;\n  *)\n    echo ${{github.event.inputs.build_service }} > /tmp/images_to_build.txt\n    ;;\nesac\n\nif [[ $(cat /tmp/images_to_build.txt) == """" ]]; then\n  echo ""Nothing to build here""\n  touch /tmp/nothing-to-build-here\n  exit 0\nfi\n#\n# Pushing image to registry\n#\ncd backend\ncat /tmp/images_to_build.txt\nfor image in $(cat /tmp/images_to_build.txt);\ndo\n  echo ""Bulding $image""\n  PUSH_IMAGE=0 bash -x ./build.sh ee $image\n  [[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n    curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n    ./trivy image --exit-code 1 --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n    err_code=$?\n    [[ $err_code -ne 0 ]] && {\n      exit $err_code\n    }\n  } && {\n    echo ""Skipping Security Checks""\n  }\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \n  echo ""::set-output name=image::$DOCKER_REPO/$image:$IMAGE_TAG""\ndone\n', '#\n# Deploying image to environment.\n#\nset -x\n[[ -f /tmp/nothing-to-build-here ]] && exit 0\ncd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.EE_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.EE_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.EE_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.EE_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.EE_DOMAIN_NAME }}\\""/g"" vars.yaml\nsed -i ""s/enterpriseEditionLicense: \\""\\""/enterpriseEditionLicense: \\""${{ secrets.EE_LICENSE_KEY }}\\""/g"" vars.yaml\n\nset -x\necho > /tmp/image_override.yaml\nmkdir /tmp/helmcharts\nmv openreplay/charts/ingress-nginx /tmp/helmcharts/\nmv openreplay/charts/quickwit /tmp/helmcharts/\n## Update images\nfor image in $(cat /tmp/images_to_build.txt);\ndo\nmv openreplay/charts/$image /tmp/helmcharts/\ncat <<EOF>>/tmp/image_override.yaml\n${image}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: ${IMAGE_TAG}\nEOF\ndone\nls /tmp/helmcharts\nrm -rf openreplay/charts/*\nls openreplay/charts\nmv /tmp/helmcharts/* openreplay/charts/\nls openreplay/charts\n\n# Deploy command\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true | kubectl apply -f -\n', 'docker login ${{ secrets.OSS_REGISTRY_URL }} -u ${{ secrets.OSS_DOCKER_USERNAME }} -p ""${{ secrets.OSS_REGISTRY_TOKEN }}"" \n', '#\n# TODO: Check the container tags are same, then skip the build and deployment.\n#\n# Build a docker container and push it to Docker Registry so that it can be deployed to Kubernetes cluster.\n#\n# Getting the images to build\n#\nset -xe\ntouch /tmp/images_to_build.txt\nskip_security_checks=${{ github.event.inputs.skip_security_checks }}\ntmp_param=${{ github.event.inputs.build_service }}\nbuild_param=${tmp_param:-\'false\'}\ncase ${build_param} in\n  false)\n    {\n      git diff --name-only HEAD HEAD~1 | grep -E ""backend/pkg|backend/internal"" | grep -vE ^ee/ | cut -d \'/\' -f3 | uniq | while read -r pkg_name ; do\n        grep -rl ""pkg/$pkg_name"" backend/services backend/cmd | cut -d \'/\' -f3 \n      done\n    } | awk \'!seen[$0]++\' > /tmp/images_to_build.txt\n  ;;\n  all)\n    ls backend/cmd > /tmp/images_to_build.txt\n    ;;\n  *)\n    echo ${{github.event.inputs.build_service }} > /tmp/images_to_build.txt\n    ;;\nesac\n\nif [[ $(cat /tmp/images_to_build.txt) == """" ]]; then\n  echo ""Nothing to build here""\n  touch /tmp/nothing-to-build-here\n  exit 0\nfi\n#\n# Pushing image to registry\n#\ncd backend\ncat /tmp/images_to_build.txt\nfor image in $(cat /tmp/images_to_build.txt);\ndo\n  echo ""Bulding $image""\n  PUSH_IMAGE=0 bash -x ./build.sh skip $image\n  [[ ""x$skip_security_checks"" == ""xtrue"" ]]  || {\n    curl -L https://github.com/aquasecurity/trivy/releases/download/v0.34.0/trivy_0.34.0_Linux-64bit.tar.gz | tar -xzf - -C ./ \n    ./trivy image --exit-code 1 --vuln-type os,library --severity ""HIGH,CRITICAL""  --ignore-unfixed $DOCKER_REPO/$image:$IMAGE_TAG \n    err_code=$?\n    [[ $err_code -ne 0 ]] && {\n      exit $err_code\n    }\n  } && {\n    echo ""Skipping Security Checks""\n  }\n  docker push $DOCKER_REPO/$image:$IMAGE_TAG \n  echo ""::set-output name=image::$DOCKER_REPO/$image:$IMAGE_TAG""\ndone\n', '#\n# Deploying image to environment.\n#\nset -x\n[[ -f /tmp/nothing-to-build-here ]] && exit 0\ncd scripts/helmcharts/\n\n## Update secerts\nsed -i ""s#openReplayContainerRegistry.*#openReplayContainerRegistry: \\""${{ secrets.OSS_REGISTRY_URL }}\\""#g"" vars.yaml\nsed -i ""s/postgresqlPassword: \\""changeMePassword\\""/postgresqlPassword: \\""${{ secrets.OSS_PG_PASSWORD }}\\""/g"" vars.yaml\nsed -i ""s/accessKey: \\""changeMeMinioAccessKey\\""/accessKey: \\""${{ secrets.OSS_MINIO_ACCESS_KEY }}\\""/g"" vars.yaml\nsed -i ""s/secretKey: \\""changeMeMinioPassword\\""/secretKey: \\""${{ secrets.OSS_MINIO_SECRET_KEY }}\\""/g"" vars.yaml\nsed -i ""s/jwt_secret: \\""SetARandomStringHere\\""/jwt_secret: \\""${{ secrets.OSS_JWT_SECRET }}\\""/g"" vars.yaml\nsed -i ""s/domainName: \\""\\""/domainName: \\""${{ secrets.OSS_DOMAIN_NAME }}\\""/g"" vars.yaml\n\nset -x\necho > /tmp/image_override.yaml\nmkdir /tmp/helmcharts\nmv openreplay/charts/ingress-nginx /tmp/helmcharts/\nmv openreplay/charts/quickwit /tmp/helmcharts/\n## Update images\nfor image in $(cat /tmp/images_to_build.txt);\ndo\nmv openreplay/charts/$image /tmp/helmcharts/\ncat <<EOF>>/tmp/image_override.yaml\n${image}:\n  image:\n    # We\'ve to strip off the -ee, as helm will append it.\n    tag: ${IMAGE_TAG}\nEOF\ndone\nls /tmp/helmcharts\nrm -rf openreplay/charts/*\nls openreplay/charts\nmv /tmp/helmcharts/* openreplay/charts/\nls openreplay/charts\n\n# Deploy command\nhelm template openreplay -n app openreplay -f vars.yaml -f /tmp/image_override.yaml --set ingress-nginx.enabled=false --set skipMigration=true | kubectl apply -f -\n']"
"['sudo ./utils/searxng.sh install packages\n', 'make V=1 install\n', 'V=1 ./manage pyenv.cmd python ""$FETCH_SCRIPT""\n', 'echo ""Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}""\necho ""Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}""\n', 'sudo ./utils/searxng.sh install packages\nsudo apt install firefox\n', 'make V=1 install\nmake V=1 gecko.driver\n', 'make V=1 ci.test', 'make V=1 test.coverage', 'sudo ./utils/searxng.sh install buildhost', 'make V=1 node.env', 'make V=1 themes.all', 'sudo ./utils/searxng.sh install buildhost', 'make V=1 docs.clean docs.html\n', 'mkdir -p ~/.config\necho ""${WEBLATE_CONFIG}"" > ~/.config/weblate\ngit config --global user.email ""searxng-bot@users.noreply.github.com""\ngit config --global user.name ""searxng-bot""\n', 'git restore utils/brand.env\nmake V=1 weblate.push.translations\n', 'make -e GIT_URL=$(git remote get-url origin) docker.buildx', 'mkdir -p ~/.config\necho ""${WEBLATE_CONFIG}"" > ~/.config/weblate\ngit config --global user.email ""searxng-bot@users.noreply.github.com""\ngit config --global user.name ""searxng-bot""\n', 'make V=1 weblate.translations.commit\n']"
"['pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 90 mmocr\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc1'\n"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x', 'pip install -r requirements/tests.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmocr -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc1'\n"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x', 'pip install -r requirements/tests.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmocr -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install --upgrade pip', 'pip install lmdb', 'pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc1'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install -r requirements/tests.txt\n"", 'pip install -e .\n', 'pytest tests/\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc1'\n"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x', 'pip install -r requirements/tests.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmocr -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install --upgrade pip', 'pip install lmdb', 'pip install torch==${{matrix.torch}}+${{matrix.platform}} torchvision==${{matrix.torchvision}}+${{matrix.platform}} -f https://download.pytorch.org/whl/${{matrix.platform}}/torch_stable.html', ""pip install git+https://github.com/open-mmlab/mmengine.git@main\npip install -U openmim\nmim install 'mmcv >= 2.0.0rc1'\npip install git+https://github.com/open-mmlab/mmdetection.git@dev-3.x\npip install -r requirements/tests.txt\n"", 'pip install -e .\n', 'pytest tests/\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmocr']"
"['echo ""BUILD_ARGS=${{ github.event.inputs.build_args }}"" >> $GITHUB_ENV', 'echo ""WORKING_DIRECTORY=${{ github.event.inputs.working_directory }}"" >> $GITHUB_ENV', 'echo ""_HOST_IP=$(hostname -I | cut -d \' \' -f 1)"" >> $GITHUB_ENV', 'echo ""VERSION=$(sed ""s/^v//"" <<< ""${{ github.event.milestone.title }}"")"" >> $GITHUB_ENV\necho ""PR_MILESTONE_LINK=-M ${{ github.event.milestone.title }}"" >> $GITHUB_ENV\n', 'echo ""VERSION=${{ github.event.inputs.version }}"" >> $GITHUB_ENV', 'if [[ ! ""${{ env.VERSION }}"" =~ ^([0-9]+\\.[0-9]+\\.[0-9]+.*)$ ]]; then echo ""The version is not valid: ${{ env.VERSION }}""; exit 1; fi', 'echo ""GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}"" >> $GITHUB_ENV', 'echo ""_HOST_IP=$(hostname -I | cut -d \' \' -f 1)"" >> $GITHUB_ENV', '# Required for local checkout\napt-get update\napt-get install -y git\n', '# Stops script execution if a command has an error\nset -e\ncurl -fsSL https://github.com/github/hub/raw/master/script/get | bash -s 2.14.2\n# TODO: Milestone link currently does not work with closed milestones: ${{ env.PR_MILESTONE_LINK  }} (problem with hub cli)\nbin/hub pull-request -b ${{ env.DEFAULT_BRANCH }} -h ${{ env.BRANCH_PREFIX }}${{ env.VERSION  }} --no-edit -m ""Finalize release for version ${{ env.VERSION  }}"" -m ""Automated pull request for release version ${{ env.VERSION  }}"" -l ""skip changelog"" || true\nrm bin/hub\n', 'echo ""The release drafter currently does not work with act, please create the release from the Github UI.""']"
[]
"['pip install mkdocs-material', 'mkdocs gh-deploy --force', 'python -m pip install --upgrade pip\npip install flit\n', 'python -m flit build', 'python -m pip install --upgrade pip\npip install -e .\n', 'sudo apt-get update\nsudo apt-get install libegl1-mesa\n', 'pip install pytest\npytest freemocap/tests\n']"
""
"['echo ""::set-output name=today::$(/bin/date -u \'+%Y%m%d\')""', 'mamba env update -n argilla -f environment_dev.yml', 'echo ""Enable HF access token""', 'pip install -e "".[server,listeners]""\nalembic upgrade head\npytest --cov=argilla --cov-report=xml\n', 'echo ""::set-output name=today::$(/bin/date -u \'+%Y%m%d\')""', 'mamba env update -n argilla -f environment_dev.yml', 'echo ""Enable HF access token""', 'pip install -e "".[server,listeners]""\nalembic upgrade head\npytest --cov=argilla --cov-report=xml\n', 'pip install -U build\nscripts/build_distribution.sh\n', 'pip install --index-url https://test.pypi.org/simple --no-deps argilla==${GITHUB_REF#refs/*/v}', 'sed -i \'s/rubrix:latest/rubrix:master/\' docker-compose.yaml\ndocker-compose -f ""docker-compose.yaml"" up -d --build\n', 'pip install pytest nbmake ipywidgets', 'pip install -U git+https://github.com/recognai/rubrix.git', 'pytest --nbmake docs/tutorials/05-active_learning.ipynb', 'docker-compose -f ""docker-compose.yaml"" down', 'sed -i \'s/rubrix:latest/rubrix:master/\' docker-compose.yaml\ndocker-compose -f ""docker-compose.yaml"" up -d --build\n', 'pip install pytest nbmake ipywidgets', 'pip install -U git+https://github.com/recognai/rubrix.git', 'pytest --nbmake docs/tutorials/weak-supervision-with-rubrix.ipynb', 'docker-compose -f ""docker-compose.yaml"" down', 'sed -i \'s/rubrix:latest/rubrix:master/\' docker-compose.yaml\ndocker-compose -f ""docker-compose.yaml"" up -d --build\n', 'pip install pytest nbmake ipywidgets', 'pip install -U git+https://github.com/recognai/rubrix.git', 'pytest --nbmake docs/tutorials/find_label_errors.ipynb', 'docker-compose -f ""docker-compose.yaml"" down', 'sed -i \'s/rubrix:latest/rubrix:master/\' docker-compose.yaml\ndocker-compose -f ""docker-compose.yaml"" up -d --build\n', 'pip install pytest nbmake ipywidgets', 'pip install -U git+https://github.com/recognai/rubrix.git', 'pytest --nbmake docs/tutorials/07-zeroshot_ner.ipynb', 'docker-compose -f ""docker-compose.yaml"" down', 'sed -i \'s/rubrix:latest/rubrix:master/\' docker-compose.yaml\ndocker-compose -f ""docker-compose.yaml"" up -d --build\n', 'pip install pytest nbmake ipywidgets', 'pip install -U git+https://github.com/recognai/rubrix.git', 'pytest --nbmake docs/tutorials/08-error_analysis_using_loss.ipynb', 'docker-compose -f ""docker-compose.yaml"" down', 'sed -i \'s/rubrix:latest/rubrix:master/\' docker-compose.yaml\ndocker-compose -f ""docker-compose.yaml"" up -d --build\n', 'pip install pytest nbmake ipywidgets', 'pip install -U git+https://github.com/recognai/rubrix.git', 'pytest --nbmake docs/tutorials/09-automatic_fastapi_log.ipynb', 'docker-compose -f ""docker-compose.yaml"" down']"
"['echo ""PY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_OUTPUT\necho ""PIP_CACHE=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'pytest tests/', 'python -m github_poster github --github_user_name yihong0618\npython -m github_poster multiple --types ""github, dota2, twitter"" --twitter_user_name Piglei  --github_user_name piglei --dota2_id 70388657 --year 2018-2022\npython -m github_poster twitter --twitter_user_name frostming90 --year 2017-2022 | tee a.txt\n', 'python setup.py sdist\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'echo ""PY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_OUTPUT\necho ""PIP_CACHE=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m github_poster ns --ns_session_token ${{ secrets.NS_SESSION_TOKEN }} --ns_device_id ${{ secrets.NS_DEVICE_ID }} --me ${{ env.ME }}\n', 'python -m github_poster strava --strava_client_id ${{ secrets.STRAVA_CLIENT_ID }} --strava_client_secret ${{ secrets.STRAVA_CLIENT_SECRET }} --strava_refresh_token ${{ secrets.STRAVA_CLIENT_REFRESH_TOKEN }} --me ${{ env.ME }} --year 2012-$(date +""%Y"") --is-circular\n', ""python -m github_poster shanbay --shanbay_user_name ${{ secrets.SHANBAY_USER_NAME }}  --special-color1 '#33C6A4' --special-color2  '#33C6A4' --me ${{ env.ME }}\n"", 'python -m github_poster duolingo --duolingo_user_name ${{ secrets.DUOLINGO_USER_NAME }} --duolingo_jwt ${{ secrets.DUOLINGO_JWT }} --me ${{ env.ME }}\n', 'python -m github_poster cichang --cichang_user_name ${{ secrets.CICHANG_USER_NAME }} --password ${{ secrets.CICHANG_PASSWORD }} --me ${{ env.ME }}\n', 'python -m github_poster forest --forest_email ${{ secrets.FOREST_EMAIL }} --forest_password ${{ secrets.FOREST_PASSWORD }} --year $(date +""%Y"") --cn\n', 'python -m github_poster issue --issue_number ${{ secrets.ISSUE_NUMBER }} --repo_name ${{ secrets.REPO_NAME }} --github_token ${{ secrets.G_T }} --me ${{ env.ME }}\n', 'python -m github_poster leetcode --leetcode_cookie  ""${{ secrets.LEETCODE_COOKIE }}"" --me ${{ env.ME }}\n', 'python -m github_poster twitter --twitter_user_name ${{ secrets.TWITTER_USER_NAME }} --me ${{ env.ME }} --with-animation\n', 'python -m github_poster github --github_user_name ${{ secrets.G_USER_NAME }} --me ${{ env.ME }} --with-animation --stand-with-ukraine\n', 'python -m github_poster gitlab --gitlab_user_name ${{ secrets.GITLAB_USER_NAME }} --me ${{ env.ME }} --with-animation\n', ""python -m github_poster kindle --kindle_cookie  '${{ secrets.KINDLE_COOKIE }} --me ${{ env.ME }}\n"", 'python3 -m github_poster bilibili --bilibili_cookie ""${{ secrets.BILIBILI_COOKIE }}"" --me ${{ env.ME }}\n', 'python3 -m github_poster wakatime --wakatime_key ""${{ secrets.WAKATIME_KEY }}"" --me ${{ env.ME }}\n', 'python3 -m github_poster dota2 --dota2_id ""${{ secrets.DOTA2_ID }}"" --me ${{ env.ME }}\n', 'python3 -m github_poster notion --notion_token ""${{ secrets.NOTION_TOKEN }}"" --database_id ""${{ secrets.NOTION_DATABASE_ID }}"" --prop_name ""${{ secrets.NOTION_PROP_NAME }}"" --me ${{ env.ME }}\n', 'python3 -m github_poster openlanguage --openlanguage_user_name ""${{ secrets.OPENLANGUAGE_USER_NAME }}"" --openlanguage_password ""${{ secrets.OPENLANGUAGE_PASSWORD }}"" --me ${{ env.ME }}\n', 'python3 -m github_poster multiple --types ""github, twitter"" --github_user_name ${{ secrets.G_USER_NAME }} --twitter_user_name ${{ secrets.TWITTER_USER_NAME }} --year 2019-$(date +""%Y"") \n', 'mv OUT_FOLDER/*.svg examples/', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m \'update new poster\' || echo ""nothing to commit""\ngit push || echo ""nothing to push""\n', 'echo ""Congratulations! Your permissions to access the repository are sufficient.""', 'echo ""Sorry! Your permissions are insufficient. please check https://github.com/ad-m/github-push-action/issues/96""\nexit 1\n', 'echo ""PY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_OUTPUT\necho ""PIP_CACHE=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'python -m github_poster twitter --twitter_user_name ""${{ github.event.inputs.twitter_user_name }}"" --year 2007-$(date +""%Y"") --me ""${{ github.event.inputs.twitter_user_name }}"" | tee OUT_FOLDER/""${{ github.event.inputs.twitter_user_name }}"".txt\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m \'update new poster\' || echo ""nothing to commit""\ngit push || echo ""nothing to push""\n']"
"['pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --exclude mmgen/ops --ignore-regex ""__repr__"" --fail-under 50 mmgen\n', '. .github/workflows/scripts/get_mmcv_var.sh ${{matrix.torch}}', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${MMCV_TORCH}/index.html', 'pip install -r requirements.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmgen -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${MMCV_TORCH}/index.html', 'pip install -r requirements.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmgen -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6\n', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${MMCV_TORCH}/index.html\npip install -r requirements.txt\n', 'pip install -e .\n', 'python -m pip install pip --upgrade', 'python -m pip install torch==1.8.1+${{matrix.platform}} torchvision==0.9.1+${{matrix.platform}} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', 'python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\npython -m pip install -r requirements.txt\npython -m pip install opencv-python>=3\n', 'python -m pip install -e .\n', 'pytest tests/\n', 'pip install pip --upgrade', '. .github/workflows/scripts/get_mmcv_var.sh ${{matrix.torch}}', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${MMCV_TORCH}/index.html', 'pip install -r requirements.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmgen -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', '. .github/workflows/scripts/get_mmcv_var.sh ${{matrix.torch}} ${{matrix.cuda}}', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'apt-get update\napt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev\n', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/${MMCV_CUDA}/torch${MMCV_TORCH}/index.html\npip install -r requirements.txt\n', 'pip install -e .\n', 'python -m pip install pip --upgrade', 'python -m pip install torch==1.8.1+${{matrix.platform}} torchvision==0.9.1+${{matrix.platform}} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', 'python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\npython -m pip install -r requirements.txt\npython -m pip install opencv-python>=3\n', 'python -m pip install -e .\n', 'pytest tests/\n', 'pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\npip install wheel\npython setup.py sdist\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmgen']"
"['pip install nbdev', 'echo ""Check we are starting with clean git checkout""\nif [ -n ""$(git status -uno -s)"" ]; then echo ""git status is not clean""; false; fi\necho ""Trying to strip out notebooks""\nnbdev_clean\necho ""Check that strip out was unnecessary""\ngit status -s # display the status to see which nbs need cleaning up\nif [ -n ""$(git status -uno -s)"" ]; then echo -e ""!!! Detected unstripped out notebooks\\n!!!Remember to run nbdev_install_git_hooks""; false; fi\n', 'pip install ./', 'nbdev_test --do_print --timing --n_workers 1', 'pip install mypy flake8', 'mypy neuralforecast/', 'flake8 --select=F neuralforecast/', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install --upgrade pip\npip install pytest pytest-mock\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install pytest pytest-mock pytest-cov\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest\n', 'pytest --cov=./ --cov-report=xml\n']"
[]
"['pip install --upgrade pip setuptools\npython setup.py install\npip install .[testing]\n', 'pytest -n ""$(grep -c ^processor /proc/cpuinfo)"" android_env\n']"
['python3 -m pip install --upgrade build && python3 -m build']
""
"['pip install -e .\n', 'black --check nbterm\nflake8 --max-line-length 100 nbterm\n', 'mypy nbterm\nnbterm --help\npy.test nbterm/tests\n']"
"['echo ""VERSION=$(echo ${{ github.event.head_commit.message }} | sed -r \'s/Bump\\s+version\\s+?(to\\s+?)?`?v?([a-z0-9.\\-_]+)`?$/\\2/\')"" >> $GITHUB_ENV', 'chmod +x .github/workflows/generate_changelogs.sh\n./.github/workflows/generate_changelogs.sh > CHANGELOGS.md\n', 'rm -fr .vscode .git*\nzip -r ${{ github.WORKSPACE }}/TG-FileStreamBot-v${{ env.VERSION }}-main.zip *\n']"
""
"['sudo apt update\nsudo apt install -y expect \\\n  python3-opengl \\\n  xvfb\necho \'export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' >> ""${HOME}/.bashrc""\n', 'echo \'conda activate ""counterfit""\' >> ""${HOME}/.profile""\n', 'cp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/install_test.exp"" ./install_test.exp\n', 'source ""${HOME}/.profile""\ncd ""${GITHUB_WORKSPACE}/""\npip install --no-input .[dev]\npython -c ""import nltk;  nltk.download(\'stopwords\')""\n', 'source ""${HOME}/.profile""\nexpect -d ./install_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\n', 'source ""${HOME}/.profile""\ncd ""${GITHUB_WORKSPACE}/""\ncoverage run -m pytest\ncoverage json\n', 'echo ${{ steps.coverage_percent.outputs.prop }}', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  sudo apt update\n  sudo apt install -y expect \\\n    python3-opengl \\\n    xvfb\n  echo \'export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' >> ""${HOME}/.bashrc""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  brew install expect\n  brew install --cask anaconda\n  export PATH=""/usr/local/anaconda3/bin:$PATH""\n  echo \'export PATH=""/usr/local/anaconda3/bin:$PATH""\' >> ""${HOME}/.zshrc""\n  echo \'export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\' >> ""${HOME}/.zshrc""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'cp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/install_test.exp"" ./install_test.exp\ncp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/cra_test.exp"" ./cra_test.exp\ncp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/ispa_test.exp"" ./ispa_test.exp\ncp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/satellite_hsj_test.exp"" ./satellite_hsj_test.exp\ncp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/creditfraud_hsj_test.exp"" ./creditfraud_hsj_test.exp\ncp ""${GITHUB_WORKSPACE}/.github/helpers/quality-gates/digits_hsj_test.exp"" ./digits_hsj_test.exp\n', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  echo \'conda activate ""counterfit""\' >> ""${HOME}/.profile""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  echo \'conda activate ""counterfit""\' >> ""${HOME}/.zshrc""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'pip install https://github.com/aboSamoor/pycld2/zipball/e3ac86ed4d4902e912691c1531d0c5645382a726\n', 'echo ""PYTHON_PATH=$(which python)"" >> $GITHUB_ENV\nif [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  source ""${HOME}/.profile""\n  conda activate counterfit\n  cd ""${GITHUB_WORKSPACE}/""\n  pip install --no-input .[dev]\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  conda activate counterfit\n  cd ""${GITHUB_WORKSPACE}/""\n  pip install --no-input .[dev]\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  source ""${HOME}/.profile""\n  conda activate counterfit\n  expect -d ./install_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  conda activate counterfit\n  expect -d ./install_test.exp Darwin ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  source ""${HOME}/.profile""\n  conda activate counterfit\n  expect -d ./ispa_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  conda activate counterfit\n  expect -d ./ispa_test.exp Darwin ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  source ""${HOME}/.profile""\n  conda activate counterfit\n  expect -d ./cra_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  conda activate counterfit\n  expect -d ./cra_test.exp Darwin ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  source ""${HOME}/.profile""\n  conda activate counterfit\n  expect -d ./satellite_hsj_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  conda activate counterfit\n  expect -d ./satellite_hsj_test.exp Darwin ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'if [ ""${RUNNER_OS}"" == ""Linux"" ]; then\n  source ""${HOME}/.profile""\n  conda activate counterfit\n  expect -d ./creditfraud_hsj_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelif [ ""${RUNNER_OS}"" == ""macOS"" ]; then\n  conda activate counterfit\n  expect -d ./creditfraud_hsj_test.exp Darwin ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\nelse\n  echo ""${RUNNER_OS} not supported""\n  exit 1\nfi\n', 'source ""${HOME}/.profile""\nconda activate counterfit\nexpect -d ./digits_hsj_test.exp Linux ${{ env.SECONDS }} ""${GITHUB_WORKSPACE}/examples/terminal/terminal.py""\n']"
"['python -m pip install --upgrade pip          \npip install pytest\npip install pydantic\npip install typing\npip install orjson\npip install py_expression_eval\n', 'pytest test', 'python -m pip install --upgrade pip\npip install py_expression_eval\npip install pydantic\npip install orjson\npip install pandas\n', 'python main.py\n', 'python readme.py\n', 'sudo apt-get update -y && sudo apt-get install -y jq', 'echo ""COUNT=$(jq \'.SpectralIndices | length\' ./output/spectral-indices-dict.json)"" >> $GITHUB_ENV\n', 'today=$(date +""%Y-%m-%d %H:%M:%S"")\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add -A\ngit commit -m ""Updated files ${today} UTC"" -a\n']"
"['echo ""$GITHUB_CONTEXT""', 'echo ""$JOB_CONTEXT""', 'echo ""$STEPS_CONTEXT""', 'echo ""$RUNNER_CONTEXT""', 'echo ""$STRATEGY_CONTEXT""', 'echo ""$MATRIX_CONTEXT""', '/data/github-runner/legate-bin/setup.sh\ncd legate-ci/github-ci/cunumeric\nrm -rf ngc-artifacts || true\n./build-conda.sh > ${COMMIT}-build.log 2>&1\n', 'cd legate-ci/github-ci/cunumeric\ncat *artifacts/*/*\n', 'echo ""$GITHUB_CONTEXT""', 'echo ""$JOB_CONTEXT""', 'echo ""$STEPS_CONTEXT""', 'echo ""$RUNNER_CONTEXT""', 'echo ""$STRATEGY_CONTEXT""', 'echo ""$MATRIX_CONTEXT""', '/data/github-runner/legate-bin/setup.sh\ncd legate-ci/github-ci/cunumeric\nif [[ ! -d ngc-artifacts ]]\nthen\n  mkdir ngc-artifacts\nelse\n  rm -rf ngc-artifacts/*\nfi\n', 'cd legate-ci/github-ci/cunumeric\n./test.sh ${{ matrix.options }} > ${COMMIT}-test-${{ matrix.log }}.log 2>&1\n', 'cd legate-ci/github-ci/cunumeric\n/data/github-runner/legate-bin/encrypt.sh ${COMMIT}-test-${{ matrix.log }}.log\ncat *artifacts/*/*\n']"
"['curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nsource $HOME/.poetry/env\npoetry install\n# stop the build if there are Python syntax errors or undefined \necho ""Flake8 Syntax Error Check""\npoetry run flake8 --count --select=E9,F63,F7,F82 --show-source --statistics\n# stop the build if the conventions in .flake8 fail\npoetry run flake8\n', 'source $HOME/.poetry/env\npoetry run pytest\n', 'curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nsource $HOME/.poetry/env && echo ""Poetry directory sourced.""\n# set version number equal to latest git tag\npoetry version $(git tag | tail -1)\n# poetry config repositories.testpypi https://test.pypi.org/legacy/\n# poetry publish --build -r testpypi -u __token__ -p ${{ secrets.TEST_PYPI_TOKEN }}\npoetry publish --build -u __token__ -p ${{ secrets.PYPI_API_TOKEN }}\n']"
""
""
"['pip install .[tests]\npip list\n', 'pytest --cov=torchgeo --cov-report=xml --durations=10', 'pip install .[datasets,tests]\npip list\n', 'pytest -m slow --durations=10', 'pip install .[docs,tests] planetary_computer pystac pytest-rerunfailures\npip list\n', 'pytest --nbmake --durations=10 --reruns=10 docs/tutorials', 'pip install -r requirements/style.txt\npip list\n', 'black . --check --diff', 'pip install -r requirements/style.txt\npip list\n', 'flake8', 'pip install -r requirements/style.txt\npip list\n', 'isort . --check --diff', 'pip install -r requirements/style.txt\npip list\n', 'pydocstyle', 'pip install -r requirements/style.txt\npip list\n', 'pyupgrade --py39-plus $(find . -path ./docs/src -prune -o -name ""*.py"" -print)', 'pip install -r requirements/required.txt -r requirements/datasets.txt -r requirements/tests.txt\npip list\n', 'mypy .', 'sudo apt-get update\nsudo apt-get install unrar\n', 'brew install rar', 'choco install unrar', 'pip install -r requirements/required.txt -r requirements/datasets.txt -r requirements/tests.txt\npip list\n', 'pytest --cov=torchgeo --cov-report=xml --durations=10', 'sudo apt-get update\nsudo apt-get install unrar\n', 'pip install -r requirements/min-reqs.old -c requirements/min-cons.old\npip list\n', 'pytest --cov=torchgeo --cov-report=xml --durations=10', 'pip install .[docs,tests] planetary_computer pystac pytest-rerunfailures\npip list\n', 'pytest --nbmake --durations=10 --reruns=10 docs/tutorials']"
""
[]
"['pip3 install -r requirements.txt\n', 'pip3 install pyinstaller\npyinstaller -F -y main.py\n', 'pip3 install pyinstaller\npyinstaller -F -y main.py --icon=screenshots/logo.ico\n', 'ls\ncp -r drivers dist/\ncp -r extra dist/\ncp LICENSE dist/\ncp README.md dist/\ncp config.yaml dist/\nmv dist jd-tuihui-mac\ntar -czvf jd-tuihui-mac.tar.gz jd-tuihui-mac\nmkdir artifacts\nmv jd-tuihui-mac.tar.gz artifacts || true\n', 'ls\ncp drivers/ dist/\ncp extra/ dist/\ncp LICENSE dist/\ncp README.md dist/\ncp config.yaml dist/\nmv dist jd-tuihui-windows\ntar -czvf jd-tuihui-windows.tar.gz jd-tuihui-windows\nmkdir artifacts\nmv jd-tuihui-windows.tar.gz artifacts || true\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'mkdir artifacts\ncd jd_wstool\nmake release\nmv *.tar.gz ../artifacts || true\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'updated=$(date ""+%Y-%m-%dT%H:%M:%SZ"" -d ""+8 hour"")\necho -n ""$updated"" | md5sum | tr a-z A-Z | cut -d \' \' -f1 > utils/version\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit diff --quiet && git diff --staged --quiet || git commit -am \'Automatically write version information\'\necho ::set-output name=status::success\n']"
""
"['python -m pip install --upgrade pip\npip install black isort flake8\n', 'black --check --line-length 119 --target-version py38 promptsource', 'isort --check-only promptsource', 'flake8 promptsource --max-line-length 119 --per-file-ignores=""__init__.py:F401""', 'python -m pip install --upgrade pip\npip install .\n', 'pytest test/test_templates.py\n', 'python -m pip install --upgrade pip\npip install .\npip install black isort flake8\n', 'for changed_file in ${{ steps.files.outputs.all }}; do\n    python test/show_templates.py ${changed_file}\ndone\n']"
"['echo ""VERSION=$(echo ${GITHUB_REF/refs\\/tags\\//})"" >> $GITHUB_ENV', 'poetry version ${{ env.VERSION }}', 'poetry publish --build', ""for i in {1..100}; do python -m pip install 'nicegui==${{ env.VERSION }}' && break || sleep 2; done"", 'DOCKER_IMAGE=${{ secrets.DOCKER_USERNAME }}/${GITHUB_REPOSITORY#*/}\nVERSION=latest\nSHORTREF=${GITHUB_SHA::8}\n\n# If this is git tag, use the tag name as a docker tag\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/v}\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION},${DOCKER_IMAGE}:${SHORTREF}""\n\n# If the VERSION looks like a version number, assume that\n# this is the most recent version of the image and also\n# tag it \'latest\'.\nif [[ $VERSION =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n  TAGS=""$TAGS,${DOCKER_IMAGE}:latest""\nfi\n\n# Set output parameters.\necho ::set-output name=tags::${TAGS}\necho ::set-output name=docker_image::${DOCKER_IMAGE}\necho ::set-output name=version::${VERSION}\n', 'python -m pip install --upgrade pip\npip install requests PyYAML\n', 'python .github/workflows/update_citation.py', 'git config --global user.name ""github-actions[bot]""\ngit config --global user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit add CITATION.cff\ngit commit -m ""Update citation.cff""\ngit push origin HEAD:main\n', 'docker pull ${DOCKER_IMAGE}:${VERSION}\ndocker run -d --name test_container ${DOCKER_IMAGE}:${VERSION}\nsleep 10\nCONTAINER_OUTPUT=$(docker logs test_container)\n# Check if the container is still running\nCONTAINER_STATUS=$(docker inspect -f \'{{.State.Running}}\' test_container)\nif [ ""${CONTAINER_STATUS}"" != ""true"" ]; then\n  echo ""The container is not running!""\n  exit 1\nfi\n# Check if the ""Error"" string is present in the container output\nif echo ""${CONTAINER_OUTPUT}"" | grep -q ""Error""; then\n  echo ""Error found in container output!""\n  echo ""${CONTAINER_OUTPUT}""\n  exit 1\nfi\ndocker stop test_container\ndocker rm test_container\n', 'poetry config virtualenvs.create false\npoetry install\n# install packages to run the examples\npip install opencv-python opencv-contrib-python-headless httpx replicate langchain openai simpy\npip install -r tests/requirements.txt\n# try fix issue with importlib_resources\npip install importlib-resources\n', './test_startup.sh', 'pytest']"
"['pip install -r requirements/docs.txt', 'pytest', ""git config user.name 'github-actions[bot]' && git config user.email 'github-actions[bot]@users.noreply.github.com'"", 'mkdocs gh-deploy', 'pip install -r requirements/tests.txt', 'pytest']"
""
"['cd rust/bagua-net/cc && make test', 'git config --global --add safe.directory $(pwd) && BAGUA_NO_INSTALL_DEPS=1 python -m build -s', 'rustup default stable', 'python -m pip install --pre bagua==$(python setup.py --version) --upgrade\n', 'python -m pip install --pre bagua-cuda${BAGUA_CUDA_VERSION} --upgrade\n', 'pip install --upgrade setuptools pip wheel\npip install cibuildwheel==2.1.3\n', 'pip freeze\n', 'python -m cibuildwheel . --print-build-identifiers\n', 'python -m cibuildwheel .\n', 'rustup default stable', 'python -m pip install --pre .\n', 'python -m pip install --upgrade pip\npython -m pip install pytest pytest-timeout\n', 'rm -rf bagua bagua_core\npytest -s --timeout=300 --timeout_method=thread\n', 'cd rust/bagua-core && cargo + fmt -- --check\n', 'cd rust/bagua-net && cargo + fmt -- --check\n']"
"['if [ -f requirements.txt ]; then pip3 install --upgrade --upgrade-strategy eager -r requirements.txt -e . ; fi\npip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n', 'pytest -s --t=light --f=light\n']"
""
""
"['pip install --upgrade pip\npip install wheel flake8 pytest\nif [ -f requirements/dev.txt ]; then pip install -r requirements/dev.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 chaos_genius --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 chaos_genius --count --exit-zero --statistics\n', 'pytest\n', './scripts/check_diverged_migrations.sh\n']"
""
"['pip install -r requirements-dev.txt', 'tox']"
"['ls -R\nexport VERSION=`cat macast/.version`\necho ""VERSION=v$VERSION"" >> $GITHUB_ENV\necho ""DIST_DMG=Macast-MacOS-v${VERSION}.dmg"" >> $GITHUB_ENV\necho ""DIST_EXE=Macast-Windows-v${VERSION}.exe"" >> $GITHUB_ENV\necho ""DIST_EXE_DEBUG=Macast-Windows-v${VERSION}-debug.exe"" >> $GITHUB_ENV\necho ""DIST_AMD64=Macast-Linux-v${VERSION}-amd64"" >> $GITHUB_ENV\necho ""DIST_AMD64_DEB=Macast-Linux-v${VERSION}-amd64.deb"" >> $GITHUB_ENV\necho ""DIST_ARMV7=Macast-Linux-v${VERSION}-armv7"" >> $GITHUB_ENV\necho ""DIST_ARMV7_DEB=Macast-Linux-v${VERSION}-armv7.deb"" >> $GITHUB_ENV\necho $GITHUB_ENV\n', 'sudo apt update\nsudo apt install -y gettext\ndocker pull xfangfang/build-macast\n', 'for file in i18n/*; do msgfmt -o $file/LC_MESSAGES/macast.mo $file/LC_MESSAGES/macast.po;echo $file;done\nexport VERSION=`cat macast/.version`\nexport DIST_AMD64_DEB=Macast-Linux-v${VERSION}-amd64.deb\nexport DIST_AMD64=Macast-Linux-v${VERSION}-amd64\necho ""DIST_AMD64_DEB=${DIST_AMD64_DEB}"" >> $GITHUB_ENV\necho ""DIST_AMD64=${DIST_AMD64}"" >> $GITHUB_ENV\necho ""VERSION=v$VERSION"" >> $GITHUB_ENV\ndocker run --rm -v ""$(pwd):/src/"" xfangfang/build-macast \\\n    \'pip install -r requirements/common.txt && \\\n    pip install -U pyinstaller && \\\n    pyinstaller --noconfirm -F -w \\\n      --additional-hooks-dir=. \\\n      --add-data=""macast/.version:."" \\\n      --add-data=""macast/xml/*:macast/xml"" \\\n      --add-data=""i18n/zh_CN/LC_MESSAGES/*.mo:i18n/zh_CN/LC_MESSAGES"" \\\n      --add-data=""macast/assets/*:macast/assets"" \\\n      --add-data=""macast/assets/fonts/*:macast/assets/fonts"" \\\n      --exclude-module=tkinter \\\n      --distpath=""app"" \\\n    Macast.py\'\nsudo chown 1000 app/Macast\ncp app/Macast $DIST_AMD64\n#build deb\nmkdir -p dist/DEBIAN\nmkdir -p dist/usr/bin\nmkdir -p dist/usr/share/applications\nmkdir -p dist/usr/share/icons\necho -e ""Package: Macast\\nVersion: ${VERSION}\\nArchitecture: amd64\\nMaintainer: xfangfang\\nDescription: DLNA Media Renderer\\nDepends: mpv"" > dist/DEBIAN/control\necho -e ""[Desktop Entry]\\nName=Macast\\nComment=DLNA Media Renderer\\nExec=/usr/bin/macast\\nIcon=/usr/share/icons/Macast.png\\nTerminal=false\\nType=Application\\nCategories=Video"" > dist/usr/share/applications/macast.desktop\ncp app/Macast dist/usr/bin/macast\ncp macast/assets/icon.png dist/usr/share/icons/Macast.png\ndpkg -b dist $DIST_AMD64_DEB\necho ""::set-output name=status::success""\n', ""pip3 install -r requirements/common.txt\npip3 install pyinstaller\n$client = new-object System.Net.WebClient\n$client.DownloadFile('https://github.com/xfangfang/Macast/releases/download/v0.1/mpv-0.34.0-x86_64.7z','mpv.7z')\n7z x -obin mpv.7z *.exe\n$client.DownloadFile('https://github.com/xfangfang/Macast/releases/download/v0.1/gettext0.21-iconv1.16-static-64.zip','gettext.zip')\n7z x -ogettext gettext.zip bin/msgfmt.exe\n"", 'Get-ChildItem i18n | ForEach-Object -Process{gettext/bin/msgfmt -o $_/LC_MESSAGES/macast.mo $_/LC_MESSAGES/macast.po;echo $_.name}\n$VERSION=cat macast/.version\necho ""DIST_EXE=Macast-Windows-v${VERSION}.exe"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\necho ""DIST_EXE_DEBUG=Macast-Windows-v${VERSION}-debug.exe"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\necho ""VERSION=v${VERSION}"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\npyinstaller --noconfirm -F -w --additional-hooks-dir=. --add-data=""macast/.version;."" --add-data=""macast/xml/*;macast/xml""  --add-data=""i18n/zh_CN/LC_MESSAGES/*.mo;i18n/zh_CN/LC_MESSAGES"" --add-data=""macast/assets/*;macast/assets"" --add-data=""macast/assets/fonts/*;macast/assets/fonts"" --add-binary=""bin/mpv.exe;bin"" --icon=macast/assets/icon.ico Macast.py\ncp dist/Macast.exe Macast-Windows-v${VERSION}.exe\npyinstaller --noconfirm -F --additional-hooks-dir=. --add-data=""macast/.version;."" --add-data=""macast/xml/*;macast/xml""  --add-data=""i18n/zh_CN/LC_MESSAGES/*.mo;i18n/zh_CN/LC_MESSAGES"" --add-data=""macast/assets/*;macast/assets"" --add-data=""macast/assets/fonts/*;macast/assets/fonts"" --add-binary=""bin/mpv.exe;bin"" --icon=macast/assets/icon.ico Macast.py\ncp dist/Macast.exe Macast-Windows-v${VERSION}-debug.exe\necho ""::set-output name=status::success""\n', 'brew install create-dmg', 'wget https://www.python.org/ftp/python/3.9.8/python-3.9.8-macos11.pkg\nsudo installer -pkg python-3.9.8-macos11.pkg -target /\n', 'pip3 install -r requirements/darwin.txt\npip3 install py2app\nwget https://laboratory.stolendata.net/~djinn/mpv_osx/mpv-latest.tar.gz\nmkdir -p bin && tar --strip-components 2 -C bin -xzvf mpv-latest.tar.gz mpv.app/Contents/MacOS\n', 'for file in i18n/*; do msgfmt -o $file/LC_MESSAGES/macast.mo $file/LC_MESSAGES/macast.po;echo $file;done\npython3 setup_py2app.py py2app --arch=x86_64\ncp -R bin dist/Macast.app/Contents/Resources/\nexport VERSION=`cat macast/.version`\necho ""DIST_DMG=Macast-MacOS-v${VERSION}.dmg"" >> $GITHUB_ENV\necho ""VERSION=v$VERSION"" >> $GITHUB_ENV\ncreate-dmg --window-pos 200 120 --window-size 800 400 --icon-size 100 --icon ""Macast.app"" 200 190  --hide-extension ""Macast.app"" --app-drop-link 600 185  --volname ""Macast-v${VERSION} Installer"" Macast-MacOS-v${VERSION}.dmg ""dist/""\necho ""::set-output name=status::success""\n', 'sudo apt update\nsudo apt install -y gettext\ndocker pull xfangfang/build-macast\n', 'for file in i18n/*; do msgfmt -o $file/LC_MESSAGES/macast.mo $file/LC_MESSAGES/macast.po;echo $file;done\nexport VERSION=`cat macast/.version`\nexport BUILD_PLATFORM=armv7\nexport DIST_DEB=Macast-Linux-v${VERSION}-${BUILD_PLATFORM}.deb\nexport DIST=Macast-Linux-v${VERSION}-${BUILD_PLATFORM}\necho ""DIST_DEB=${DIST_DEB}"" >> $GITHUB_ENV\necho ""DIST=${DIST}"" >> $GITHUB_ENV\necho ""VERSION=v$VERSION"" >> $GITHUB_ENV\ndocker run --platform=linux/arm/v7 --rm -v ""$(pwd):/src/"" xfangfang/build-macast:armv7-220105 \\\n    \'pyinstaller --noconfirm -F -w \\\n      --additional-hooks-dir=. \\\n      --add-data=""macast/.version:."" \\\n      --add-data=""macast/xml/*:macast/xml"" \\\n      --add-data=""i18n/zh_CN/LC_MESSAGES/*.mo:i18n/zh_CN/LC_MESSAGES"" \\\n      --add-data=""macast/assets/*.png:assets"" \\\n      --exclude-module=tkinter \\\n      --distpath=""app"" \\\n    Macast.py\'\nsudo chown 1000 app/Macast\ncp app/Macast $DIST\n#build deb\nmkdir -p dist/DEBIAN\nmkdir -p dist/usr/bin\nmkdir -p dist/usr/share/applications\nmkdir -p dist/usr/share/icons\necho -e ""Package: Macast\\nVersion: ${VERSION}\\nArchitecture: armhf\\nMaintainer: xfangfang\\nDescription: DLNA Media Renderer\\nDepends: mpv"" > dist/DEBIAN/control\necho -e ""[Desktop Entry]\\nName=Macast\\nComment=DLNA Media Renderer\\nExec=/usr/bin/macast\\nIcon=/usr/share/icons/Macast.png\\nTerminal=false\\nType=Application\\nCategories=Video"" > dist/usr/share/applications/macast.desktop\ncp app/Macast dist/usr/bin/macast\ncp macast/assets/icon.png dist/usr/share/icons/Macast.png\ndpkg -b dist $DIST_DEB\necho ""::set-output name=status::success""\n']"
"['pip install bandit click==8.0.4 black==22.3.0 codespell flake8 isort pyre-check pytest pyupgrade safety', 'bandit --recursive --skip B101,B301,B303,B311,B324,B403 .', 'black --check .', 'codespell --ignore-words-list=""tha"" --skip=""*/text_tests,*assets/text/*,*examples/*,*/text/augmenters/utils.py""', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=15 --max-line-length=90 --show-source --statistics', 'isort --check-only --profile black . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check', 'sudo apt-get update', 'sudo apt-get install --fix-missing ffmpeg python3-soundfile', 'pip install pyre-check pytest torchvision', 'pip install -e .[all]', 'pyre --source-directory ""."" --noninteractive check || true', 'pytest --durations=10 .']"
"['python -m pip install --upgrade pip\n', 'pip install -r requirements.txt\npip3 install -e .\n']"
""
""
"['python -m pip install --upgrade pip\npip install -r robyn/test-requirements.txt\n', 'rustup target add aarch64-apple-darwin', 'maturin build -i python --universal2 --out dist\npip install --no-index --find-links=dist/ robyn\n', 'python -m pip install --upgrade pip\npip install -r robyn/test-requirements.txt\n', 'rustup target add aarch64-apple-darwin', 'maturin build -i python --universal2 --out dist\npip install --no-index --find-links=dist/ robyn\n', 'pytest\n', ""pip install --force-reinstall dist/robyn*.whl\ncd ~ && python -c 'import robyn'\n"", ""pip install --force-reinstall dist/robyn*_universal2.whl\ncd ~ && python -c 'import robyn'\n"", ""pip install --force-reinstall dist/robyn*.whl\ncd ~ && python -c 'import robyn'\n"", ""pip install --force-reinstall dist/robyn*.whl\ncd ~ && python -c 'import robyn'\n"", 'pip install --upgrade twine\ntwine upload --skip-existing *\n', 'cargo check', 'cargo test', 'cargo fmt --check', 'cargo clippy -- -D warnings']"
"['sudo apt update\nsudo apt install -q -y qemu-system-aarch64 qemu-efi binfmt-support qemu-user-static\nmkdir -p ~/.config/nix\necho ""system-features = aarch64-linux arm-linux"" | sudo tee -a /etc/nix/nix.conf\n', 'nix flake check -L --option system ${{ matrix.arch }} --extra-platforms ${{ matrix.arch }}\n', 'poetry build --format wheel', 'docker run --rm ${{ env.DOCKER_IMAGE }} --help', 'docker run --rm ${{ env.DOCKER_IMAGE }} --show-external-dependencies', 'docker run --rm -v ""$(pwd)""/tests/integration/archive/zip/regular:/test ${{ env.DOCKER_IMAGE }} -v -e /tmp /test/__input__/apple.zip', 'git config --global user.name ""$GITHUB_ACTOR""\ngit config --global user.email ""github-actions@github.com""\ngit fetch origin $GITHUB_BASE_REF\nPR_HEAD_SHA=$(git rev-parse HEAD)\ngit rebase FETCH_HEAD\nREBASED_SHA=$(git rev-parse HEAD)\necho ""PR HEAD: $PR_HEAD_SHA""\necho ""Rebased HEAD: $REBASED_SHA""\ngit range-diff FETCH_HEAD..$PR_HEAD_SHA FETCH_HEAD..$REBASED_SHA\nif [[ ""$REBASED_SHA"" != ""$PR_HEAD_SHA"" ]]; then\n    echo ""Not fast forward, aborting!""\n    echo ""Ensure that the PR branch is rebased on $GITHUB_BASE_REF and does not contain merge commits.""\n    exit 1\nfi\n', 'python -m pip install --upgrade pip poetry', 'poetry install --only docs', 'poetry run mkdocs gh-deploy --force', 'poetry publish --build', 'poetry run pyright .', 'poetry run pytest -vvv', '.github/workflows/update-vendored-nix-dependencies.py']"
[]
"[""echo '::set-output name=docs::true'"", 'gcloud auth configure-docker us-central1-docker.pkg.dev', 'sudo apt-get install graphviz pandoc\ncurl -sSL https://install.python-poetry.org | python3 - --version 1.4.0\npoetry config virtualenvs.create false\npoetry install --extras=all\n', 'echo ""Setting DOCS_VERSION_PLACEHOLDER to ${GITHUB_REF##*/}""\nsed -i ""s/DOCS_VERSION_PLACEHOLDER/${GITHUB_REF##*/}/g"" docs/conf.py\nsed -i ""s/DOCS_RELEASE_PLACEHOLDER/${GITHUB_REF##*/}/g"" docs/conf.py\n', 'cd docs\npoetry run make html\n', 'gsutil rsync -R ./docs/_build/html ""gs://robusta-public/${GITHUB_REF##*/}/""', 'echo ""$RELEASE_VER""', 'gcloud auth configure-docker us-central1-docker.pkg.dev', 'gcloud config get-value project', 'curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64\nchmod a+x skaffold', ""sed -i 's/0.0.0/${{env.RELEASE_VER}}/g' src/robusta/_version.py pyproject.toml helm/robusta/Chart.yaml helm/robusta/values.yaml\nsed -i 's/0.0.1/${{env.RELEASE_VER}}/g' helm/robusta/Chart.yaml\n"", ""./skaffold build --profile release --file-output=container-ids.json --tag='${{env.RELEASE_VER}}'"", 'python -m pip install --upgrade pip\npip install twine\n', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0\npoetry config virtualenvs.create false\npoetry install --extras ""all""\npoetry publish --build -u ${{ secrets.PYPI_USER }} -p ${{ secrets.PYPI_PASS }}\n', 'cd helm && ./upload_chart.sh\n', 'docker buildx build \\\n--platform linux/arm64,linux/amd64 \\\n--tag us-central1-docker.pkg.dev/genuine-flight-317411/devel/robusta-cli:${{env.RELEASE_VER}} \\\n--tag us-central1-docker.pkg.dev/genuine-flight-317411/devel/robusta-cli \\\n--push \\\n--file cli.Dockerfile \\\n.', 'curl -sSL https://install.python-poetry.org | python3 -\npoetry lock --check\n', 'curl -sSL https://install.python-poetry.org | python3 -\npoetry build\npip3 install ./dist/robusta_cli-0.0.0-py3-none-any.whl\n', 'ls\npip3 install robusta_cli-0.0.0-py3-none-any.whl\nrobusta version\n', 'kubectl config get-contexts\n', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0\npoetry config virtualenvs.create false\npoetry install --extras ""all""\n# Install tabulate version that fixes column width wrapping. Cannot be added to pypi as a git dependency, so adding it here\npip install git+https://github.com/astanin/python-tabulate.git@b2c26bcb70e497f674b38aa7e29de12c0123708a#egg=tabulate\n', 'curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64\nchmod a+x skaffold', ""echo 'building with tag test-${{ github.sha }}'\n./skaffold build --push=false --file-output=container-ids.json --tag='test-${{ github.sha }}'\nkind load docker-image --name chart-testing 'us-central1-docker.pkg.dev/genuine-flight-317411/devel/robusta-runner:test-${{ github.sha }}'\n"", ""sed -i 's/0.0.0/test-${{ github.sha }}/g' helm/robusta/Chart.yaml helm/robusta/values.yaml\n"", 'pytest -s\n']"
""
"['pipx install poetry\npoetry --version\n', 'version=$(poetry version | awk \'{print $2}\')\ntag=$(echo ""${{ github.ref }}"" | awk \'{split($0,p,""/""); print p[3]}\')\nif [ ""v$version"" != $tag ]; then echo ""Release tag (\'$tag\') and poetry version (\'v$version\') do not match!""; exit 1; fi;\n', 'poetry install\n', 'poetry run scripts/alembic_freeze.py\n', 'touch src/meltano/core/tracking/.release_marker', 'poetry build --format sdist\npoetry run pip wheel --no-deps . --wheel-dir dist/\n', 'ls dist/\n', 'curl --no-progress-meter -H \'Cache-Control: no-cache\' https://raw.githubusercontent.com/fossas/fossa-cli/master/install-latest.sh | bash -s -- ""v$FOSSA_CLI_VERSION""\n\necho \'## FOSSA dependency license check\' >> $GITHUB_STEP_SUMMARY\necho \'\' >> $GITHUB_STEP_SUMMARY\n\nfossa analyze --fossa-api-key ${{ secrets.MELTYBOT_FOSSA_API_KEY }} --revision ${{ github.sha }} |& tee fossa_analyze.log\nfossa test --fossa-api-key ${{ secrets.MELTYBOT_FOSSA_API_KEY }} --revision ${{ github.sha }}\n\nTEST_FAILED=$?\nFOSSA_REPORT_LINK=""$(grep -A 1 \'[ INFO] View FOSSA Report:\' fossa_analyze.log | tail -n 1 | sed -e \'s/^\\[ INFO\\]\\s*//\')""\necho ""[FOSSA detected $([ $TEST_FAILED -ne 0 ] && echo -n \'\' || echo \'no \')issues](${FOSSA_REPORT_LINK})"" >> $GITHUB_STEP_SUMMARY\nexit $TEST_FAILED\n', '# Boolean values don\'t actually work so cast to \'true\' and \'false\'\n# https://github.com/actions/runner/issues/1483\necho ""dry_run=${{ format(\'{0}\', github.event.inputs.dry_run) || env.DEFAULT_DRY_RUN }}"" >> $GITHUB_ENV\necho ""registry=${{ github.event.inputs.registry || env.DEFAULT_REGISTRY }}"" >> $GITHUB_ENV\n', 'pipx install poetry\npoetry version\npoetry version --short\necho ""release-version=$(poetry version --short)"" >> $GITHUB_OUTPUT\n', 'MELTANO_VERSION=""${{ steps.get-meltano-version.outputs.release-version }}""\nMELTANO_VERSION_ARRAY=(${MELTANO_VERSION//./ })\nMELTANO_VERSION_MAJOR=""${MELTANO_VERSION_ARRAY[0]}""\nMELTANO_VERSION_MAJOR_MINOR=""${MELTANO_VERSION_ARRAY[0]}.${MELTANO_VERSION_ARRAY[1]}""\nMELTANO_VERSION_MAJOR_MINOR_PATCH=""${MELTANO_VERSION_ARRAY[0]}.${MELTANO_VERSION_ARRAY[1]}.${MELTANO_VERSION_ARRAY[2]}""\n\n# To save space, only publish the `latest` tag for each images to the GitHub registry\nif [[ ""${{ env.registry }}"" != ""ghcr.io"" ]]; then\n  echo ""v${MELTANO_VERSION_MAJOR}-python${{ matrix.python-version }}"" >> tags\n  echo ""v${MELTANO_VERSION_MAJOR_MINOR}-python${{ matrix.python-version }}"" >> tags\n  echo ""v${MELTANO_VERSION_MAJOR_MINOR_PATCH}-python${{ matrix.python-version }}"" >> tags\n  [[ ""${{ matrix.is-default-python }}"" == ""true"" ]] && echo ""SHA-${{ github.sha }}"" >> tags\n  [[ ""${{ matrix.is-default-python }}"" == ""true"" ]] && echo ""v${MELTANO_VERSION_MAJOR}"" >> tags\n  [[ ""${{ matrix.is-default-python }}"" == ""true"" ]] && echo ""v${MELTANO_VERSION_MAJOR_MINOR}"" >> tags\n  [[ ""${{ matrix.is-default-python }}"" == ""true"" ]] && echo ""v${MELTANO_VERSION_MAJOR_MINOR_PATCH}"" >> tags\nfi\necho ""latest-python${{ matrix.python-version }}"" >> tags\n[[ ""${{ matrix.is-default-python }}"" == ""true"" ]] && echo ""latest"" >> tags\n\necho ""If this is not a dry run, the image will be published with the following tags:""\ncat tags\n\necho \'IMAGE_TAGS<<EOF\' >> $GITHUB_ENV\necho ""$(cat tags)"" >> $GITHUB_ENV\necho \'EOF\' >> $GITHUB_ENV\n', 'if [[ ""${{ env.registry }}"" == ""ghcr.io"" ]]; then\n  echo ""username=${{ github.actor }}"" >> $GITHUB_OUTPUT\n  echo ""password=${{ secrets.GITHUB_TOKEN }}"" >> $GITHUB_OUTPUT\nfi\nif [[ ""${{ env.registry }}"" == ""docker.io"" ]]; then\n  echo ""username=meltano"" >> $GITHUB_OUTPUT\n  echo ""password=${{ secrets.DOCKERHUB_TOKEN }}"" >> $GITHUB_OUTPUT\nfi\n', 'pipx install poetry\npoetry --version\n', 'pip install pip\npip --version\n', 'poetry env use ""3.10""\npoetry install --extras ""s3""\n', 'docker run -d -p ""5432:5432"" -e ""POSTGRES_PASSWORD=postgres"" -e ""POSTGRES_DB=warehouse"" --name postgres --health-cmd ""pg_isready -d postgres -U postgres"" --health-interval 10s --health-timeout 5s --health-retries 5 postgres:11\n', 'poetry env use ""3.10""\npoetry run bash integration/validate.sh ${{ matrix.integration_test }}\n', 'pipx install poetry\npoetry --version\n', 'docker run -d -p ""5432:5432"" -e ""POSTGRES_PASSWORD=postgres"" --name postgres --health-cmd ""pg_isready -d postgres -U postgres"" --health-interval 10s --health-timeout 5s --health-retries 5 postgres:11\n', 'docker run -d -p ""1433:1433"" -e ""SA_PASSWORD=Meltan0admin"" -e ""ACCEPT_EULA=Y"" --name mssql --health-cmd ""/opt/mssql-tools/bin/sqlcmd -U sa -P Meltan0admin -Q \'select 1\' -b -o /dev/null"" --health-interval 10s --health-timeout 5s --health-retries 5 mcr.microsoft.com/mssql/server:2019-latest\n', 'docker ps -a\n', 'pipx install nox\npipx inject nox nox-poetry\n', 'nox -rs tests --python=${{ matrix.python-version }} -- -m ""${{ env.PYTEST_MARKERS }}"" 2>&1 | \\\n  tee >( \\\n    tail | sed -r ""s/[[:cntrl:]]\\[([0-9]{1,3};)*[0-9]{1,3}m//g"" | grep -E \'^=+ [0-9].+ =+$\' | \\\n    { read pytest_results; echo ""pytest_results=\'${pytest_results}\'"" >> ${GITHUB_ENV}; } )\n', 'pytest_results_count () { pattern=""([0-9]+) $1"" && [[ ${{ env.pytest_results }} =~ $pattern ]] && echo ""${BASH_REMATCH[1]}"" || echo 0; }\necho ""pytest-results-row-${{ matrix.id }}=| \\\n${{ matrix.python-version }} | \\\n${RUNNER_OS} ${RUNNER_ARCH,,} | \\\n${{ matrix.backend-db }} | \\\n$( pytest_results_count passed ) | \\\n$( pytest_results_count failed ) | \\\n$( pytest_results_count xpassed ) | \\\n$( pytest_results_count xfailed ) | \\\n$( pytest_results_count skipped ) | \\\n$( pytest_results_count deselected ) | \\\n$( pytest_results_count warning ) | \\\n$( pytest_results_count error ) | \\\n$( pattern=\'in ([0-9]+)\\.[0-9]+s\' && [[ ${{ env.pytest_results }} =~ $pattern ]] && echo ""${BASH_REMATCH[1]}"" )s |"" >> $GITHUB_OUTPUT\n', 'echo \'## Test results\' >> ${GITHUB_STEP_SUMMARY}\necho \'\' >> ${GITHUB_STEP_SUMMARY}\necho \'| PYTHON | OS     | DB     | PASSED | FAILED | XPASSED | XFAILED | SKIPPED | DESELECTED | WARNINGS | ERRORS | DURATION |\' >> ${GITHUB_STEP_SUMMARY}\necho \'|--------|--------|--------|--------|--------|---------|---------|---------|------------|----------|--------|----------|\' >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-01 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-02 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-03 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-04 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-05 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-06 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-07 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-08 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-09 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-10 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-11 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-12 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-13 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-14 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-15 }}"" >> ${GITHUB_STEP_SUMMARY}\n# echo ""${{ needs.tests.outputs.pytest-results-row-16 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-17 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-18 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-19 }}"" >> ${GITHUB_STEP_SUMMARY}\necho ""${{ needs.tests.outputs.pytest-results-row-20 }}"" >> ${GITHUB_STEP_SUMMARY}\necho \'\' >> ${GITHUB_STEP_SUMMARY}\necho \'Please address any tests which have errored, failed, or xpassed, and any warnings emitted.\' >> ${GITHUB_STEP_SUMMARY}\n', 'pipx install poetry\npoetry --version\n', 'pipx install nox\npipx inject nox nox-poetry\n', 'nox -rs coverage -- combine\nnox -rs coverage -- report --show-missing --ignore-errors\n', 'nox -rs coverage -- xml --ignore-errors\n', 'pipx install poetry\npoetry --version\n', 'pipx install nox\npipx inject nox nox-poetry\n', 'nox -rs mypy\n']"
"['python -m pip install --upgrade pip setuptools wheel build pytest\npython -m pip install torch --index-url https://download.pytorch.org/whl/cpu\npython -m pip install -r requirements.txt\n', 'pytest\n', 'RAVE_VERSION=${{  github.ref_name }} python -m build']"
"['echo ""${{toJSON(github.event)}}""\n', 'echo ""RELEASE_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', 'echo ""${DOCKER_PASSWORD}"" | docker login --username ${DOCKER_USERNAME} --password-stdin\n', 'docker buildx build \\\n--platform=linux/amd64,linux/arm/v7,linux/arm64 \\\n--output ""type=image,push=true"" \\\n--file ./Dockerfile . \\\n--tag benchao/arpt:${{ env.RELEASE_VERSION }}', 'echo ""${{toJSON(github.event)}}""\n']"
"['python -m pip install --upgrade pip wheel\npython -m pip install --upgrade black\n', 'git pull || true', 'python -m black -v .\n', 'if [ $(git diff HEAD | wc -l) -gt 30 ]\nthen\ngit config user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config user.name ""GitHub Actions""\ngit commit -m ""Run formatting"" -a || true\ngit push || true\nfi', 'python -m pip install -r ${{ github.workspace }}/.github/scripts/supported_models/requirements.txt\n', 'git pull || true', 'python3 ${{ github.workspace }}/.github/scripts/supported_models/generate_file.py\n', 'echo ${{ steps.version.outputs.version }}:docker > utils/measure/.VERSION', 'python3 ${{ github.workspace }}/.github/scripts/update_hacs_manifest.py --version ${{ steps.version.outputs.version }}\n', 'echo ${{ steps.version.outputs.version }} > utils/measure/.VERSION', 'cd custom_components/powercalc\nzip powercalc.zip -r ./\n', 'bash tests/setup.sh', 'pip install pytest-cov\npip install coverage\npytest \\\n  -qq \\\n  --timeout=9 \\\n  --durations=10 \\\n  -n auto \\\n  --cov custom_components.powercalc \\\n  --cov-report xml \\\n  -o console_output_style=count \\\n  -p no:sugar \\\n  tests\ncoverage lcov\n', 'cd .github/scripts/lut_validator\nnpm install\n', 'node .github/scripts/lut_validator/validate.js']"
"['cd docs && pip install --requirement requirements.txt\n', 'pip install -e .\n', 'cd docs && make clean && make html SPHINXOPTS=""-W""\n', 'python -m pip install build --user\n', 'python -m build --sdist --wheel --outdir dist/ .\n']"
"['echo ::set-output name=version::$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"")', 'curl -sL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py \\\n  | python - -y \n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry config virtualenvs.in-project true', 'timeout 10s poetry run pip --version || rm -rf .venv', 'poetry install', 'poetry run pre-commit run --all-files\n', 'poetry run pytest --cov=mev_inspect tests']"
"['bash docker/build_dev_docker_image.sh debian', './setup_venv.sh', './test_runner.sh ""/home/runner/work/kic-reference-architectures/kic-reference-architectures""', './setup_venv.sh', './test_runner.sh ""/Users/runner/work/kic-reference-architectures/kic-reference-architectures""']"
""
""
['pip install --upgrade pip\npip install -U pre-commit\npre-commit install --install-hooks\npre-commit run -a\n']
"['pip install pip --upgrade', 'pip install torch==1.7.0+cpu torchvision==0.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install basicsr\npip install facexlib\npip install gfpgan\npip install -r requirements.txt\n', 'rm -rf .eggs && pip install -e .', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade pip\npip install codespell flake8 isort yapf\n', 'codespell\nflake8 .\nisort --check-only --diff realesrgan/ scripts/ inference_realesrgan.py setup.py\nyapf -r -d realesrgan/ scripts/ inference_realesrgan.py setup.py\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'pip install "".[dev]""', 'git push https://boris:$HF_TOKEN@huggingface.co/spaces/dalle-mini/dalle-mini main', 'git push --force https://boris:$HF_TOKEN@huggingface.co/spaces/dalle-mini/dalle-mini-debug +HEAD:main']"
"['python -m pip install --upgrade setuptools\npython -m pip install --upgrade pip\npython -m pip install flake8 pytest safety stix2 pytest-mock\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -m pip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'safety check', 'pytest', 'pip install ruff\n', 'ruff check .\n', 'python3 .github/workflows/scripts/update-ios-releases.py']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install isort==4.3.21\npip install flake8==3.8.3\npip install ""importlib-metadata<5.0""\n', './.github/workflows/format_check.sh']"
"['git config --global user.name ""ermaozi"" \ngit config --global user.email admin@ermao.net \ngit checkout --orphan new_branch\ngit commit -m ""åˆå§‹åŒ–ä»“åº“""\ngit branch -D main\ngit branch -m main\ngit push -f origin main\n', ""sudo timedatectl set-timezone 'Asia/Shanghai'"", 'pip install -r requirements.txt\n', 'python get_projaec_info.py --user ermaozi --project get_subscribe --save_path mail/project_info.svg --theme dark --token ${{ secrets.TOKEN }}\n', 'git config core.ignorecase false\ngit config --local user.email ""admin@ermao.net""\ngit config --local user.name ""ermaozi""\ngit add .\ngit commit -m ""æ›´æ–°é¡¹ç›®ä¿¡æ¯""\ngit push\n', ""sudo timedatectl set-timezone 'Asia/Shanghai'"", 'pip install -r requirements.txt\n', 'python main.py\n', 'git config core.ignorecase false\ngit config --local user.email ""admin@ermao.net""\ngit config --local user.name ""ermaozi""\ngit add .\ngit commit -m ""$(date \'+%Y-%m-%d %H:%M:%S\')æ›´æ–°è®¢é˜…é“¾æŽ¥""\ngit push\n']"
"['python3 -m venv .env\nsource .env/bin/activate\nmake install\nmake install-test\nmake install-training\n', 'source .env/bin/activate\npython -m pytest \\\n  --quiet --co \\\n  --splitting-algorithm least_duration \\\n  --splits ${{ matrix.job_num }} \\\n  --group ${{ matrix.job }} \\\n  -m regression_test \\\n  tests \\\n  | head -n -2 | grep -Po \'test_inference_with_data\\[\\K[^]]*(?=-False]|-True])\' \\\n  > models_gh_runner.txt\nif [ -n ""${{ inputs.manual_revision_reference }}"" ]; then\n  REVISION_REFERENCE=${{ inputs.manual_revision_reference }}\nfi\npython tests/util_test.py \\\n  --save_model_list models_gh_runner.txt \\\n  --model_list models_gh_runner.txt \\\n  --git_revision $REVISION_REFERENCE\n', 'source .env/bin/activate\ntouch .test_durations\ncp .test_durations durations_1\nmv .test_durations durations_2\npython -m pytest \\\n  -x -s -v \\\n  --splitting-algorithm least_duration \\\n  --splits ${{ matrix.job_num }} \\\n  --group ${{ matrix.job }} \\\n  --store-durations \\\n  --durations-path durations_1 \\\n  --clean-durations \\\n  -m ""not regression_test"" \\\n  tests\nOPEN_CLIP_TEST_REG_MODELS=models_gh_runner.txt python -m pytest \\\n  -x -s -v \\\n  --store-durations \\\n  --durations-path durations_2 \\\n  --clean-durations \\\n  -m ""regression_test"" \\\n  tests\njq -s -S \'add\' durations_* > .test_durations\n', ""jq -n -S \\\n  'reduce (inputs | to_entries[]) as {$key, $value} ({}; .[$key] += $value)' \\\n  artifacts/pytest_durations_*/.test_durations > .test_durations\n"", 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get update -y\nsudo apt-get install openjdk-11-jre-headless pandoc --fix-missing\npython -m pip install --upgrade pip setuptools wheel\n', 'docs/build_docs.sh\n', 'python -m pip install --upgrade pip setuptools build\n', 'python -m build\n', 'python -m pip install --upgrade pip setuptools wheel build\npython -m pip install pytest coverage\npython -m build\nfor f in dist/*.whl; do python -m pip install $f[all]; done\npython -m pip install -e ts_datasets/\n', 'python -m pip install --upgrade pip setuptools wheel build pytest\npython -m pip install "".[all]""\npython -m pip install -e ts_datasets/\n', 'pytest -v -s --ignore=tests/spark']"
"['python -m pip install .\npython -m pip install "".[test,k8s]""\n./ding/scripts/install-k8s-tools.sh\nmake algotest\n', 'sudo apt-get update -y\nsudo apt-get install -y cloc\n', 'cloc .\necho ""CODE_LINES=$(./cloc.sh --loc)"" >> $GITHUB_ENV\necho ""COMMENT_LINES=$(./cloc.sh --percentage)%"" >> $GITHUB_ENV\n', 'echo ""::set-output name=date::$(date +\'%Y-%m-%d\')""', 'DOCKER_IMAGE=$DOCKERIO_ORG/$TARGET\nVERSION=$DATE\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/}\nfi\nTAGS=""${DOCKER_IMAGE}:${VERSION}""\nNIGHTLY_TAGS=""${DOCKER_IMAGE}:nightly""\nif [[ $VERSION =~ ^v[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n  TAGS=""$TAGS,${DOCKER_IMAGE}:latest""\nfi\necho ::set-output name=tags::${TAGS}\necho ::set-output name=nightlytags::${NIGHTLY_TAGS}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'echo ""::set-output name=date::$(date +\'%Y-%m-%d\')""', 'DOCKER_IMAGE=$DOCKERIO_ORG/$TARGET\nVERSION=nightly-doc\nDOC_TAGS=""${DOCKER_IMAGE}:${VERSION}""\necho ::set-output name=doctags::${DOC_TAGS}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'DOCKER_IMAGE=$DOCKERIO_ORG/$TARGET\nVERSION=nightly-atari\nATARI_TAGS=""${DOCKER_IMAGE}:${VERSION}""\necho ::set-output name=ataritags::${ATARI_TAGS}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'DOCKER_IMAGE=$DOCKERIO_ORG/$TARGET\nVERSION=nightly-mujoco\nMUJOCO_TAGS=""${DOCKER_IMAGE}:${VERSION}""\necho ::set-output name=mujocotags::${MUJOCO_TAGS}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'DOCKER_IMAGE=$DOCKERIO_ORG/$TARGET\nVERSION=nightly-metaworld\nMETAWORLD_TAGS=""${DOCKER_IMAGE}:${VERSION}""\necho ::set-output name=metaworldtags::${METAWORLD_TAGS}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'docker buildx build -f ./docker/Dockerfile.env . -t opendilab/ding:nightly-smac --target=smac\ndocker push opendilab/ding:nightly-smac\n', 'docker buildx build -f ./docker/Dockerfile.env . -t opendilab/ding:nightly-grf --target=grf\ndocker push opendilab/ding:nightly-grf\n', 'docker buildx build -f ./docker/Dockerfile.env . -t opendilab/ding:nightly-dmc2gym --target=dmc2gym\ndocker push opendilab/ding:nightly-dmc2gym\n', 'docker buildx build -f ./docker/Dockerfile.rpc . -t opendilab/ding:nightly-rpc-base --target=base\ndocker push opendilab/ding:nightly-rpc-base\n', 'docker buildx build -f ./docker/Dockerfile.env . -t opendilab/ding:nightly-evogym --target=evogym\ndocker push opendilab/ding:nightly-evogym\n', 'docker buildx build -f ./docker/Dockerfile.env . -t opendilab/ding:nightly-d4rl --target=d4rl\ndocker push opendilab/ding:nightly-d4rl\n', 'python -m pip install .\ngit clone -b main https://github.com/opendilab/DI-engine-docs.git\ncd DI-engine-docs\npython -m pip install -r requirements.txt\nmake html\nmv build/html ../public\nrm -rf build\n', 'python -m pip install .\npython -m pip install "".[test,k8s]""\npython -m pip install "".[envpool]""\n./ding/scripts/install-k8s-tools.sh\nmake envpooltest\n', 'python -m pip install .\npython -m pip install "".[test,k8s]""\npython -m pip uninstall pytest-timeouts -y\nmake platformtest\n', 'pip install --upgrade pip\npip install --upgrade flake8 setuptools wheel twine\npip install .\npip install --upgrade build\n', 'python -m build --sdist --wheel --outdir dist/\n', 'python -m pip install ""yapf==0.29.0""\npython -m pip install ""flake8<=3.9.2""\npython -m pip install ""importlib-metadata<5.0.0""\nmake format_test flake_check\n', 'python -m pip install box2d-py\npython -m pip install .\npython -m pip install "".[test,k8s]""\n./ding/scripts/install-k8s-tools.sh\nmake unittest\n', 'python -m pip install .\npython -m pip install "".[test,k8s]""\n./ding/scripts/install-k8s-tools.sh\nmake benchmark\n']"
"['git config --global --add safe.directory /__w/towhee/towhee\nrm -rf .git/hooks/post-checkout\n', 'export TOWHEE_WORKER=True\nrm -rf ./coverage.xml\npip install coverage pytest pytest-cov pytest-xdist\npip install -r test_requirements.txt\napt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends -y ffmpeg libsm6 libxext6\ncoverage erase\ncoverage run -m pytest\ncoverage xml\n', 'git config --global --add safe.directory /__w/towhee/towhee', 'cd  /__w/towhee/towhee/tests/testcases\napt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends -y ffmpeg libsm6 libxext6\npython3 -m pip install -U pip\npython3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npython3 -m pip install --no-cache-dir -r requirements.text\nexport TOWHEE_WORKER=True\npython3 -W ignore test_main.py\n', 'cd  tests/testcases\npython3 -m pip install -U pip\npython3 -m pip install --no-cache-dir -r requirements.text\nexport TOWHEE_WORKER=True\npython3 -W ignore test_main.py\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'cd  tests/testcases\npython3 -m pip install -U pip\npython3 -m pip install --no-cache-dir -r requirements.text\nexport TOWHEE_WORKER=True\npython3 -W ignore test_main.py\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'cd  tests/testcases\npython3 -m pip install -U pip\npython3 -m pip install --no-cache-dir -r requirements.text\nexport TOWHEE_WORKER=True\npython3 -W ignore test_main.py\n', 'python -m pip install build --user', 'python setup.py --models bdist_wheel', 'echo ""::set-output name=version::$(date +%Y%m%d)""\necho ""::set-output name=sha_short::$(git rev-parse --short HEAD)""\n', 'export IMAGE_TAG=${{ steps.extracter.outputs.version }}-${{ steps.extracter.outputs.sha_short }}\nDOCKER_BUILDKIT=1 docker build -t towhee/towhee-ut:${IMAGE_TAG} -t towhee/towhee-ut:latest -f Dockerfile .\nDOCKER_BUILDKIT=1 docker build -t towhee/towhee-ut:gpu-latest -f GPUDockerfile .\n', 'docker login -u ${{ secrets.DOCKERHUB_USER }} \\\n             -p ${{ secrets.DOCKERHUB_TOKEN }}\nexport IMAGE_TAG=${{ steps.extracter.outputs.version }}-${{ steps.extracter.outputs.sha_short }}\ndocker push towhee/towhee-ut:${IMAGE_TAG}\ndocker push towhee/towhee-ut:latest\ndocker push towhee/towhee-ut:gpu-latest\necho ""Push towhee-ut image Succeeded""\n', 'pip install pylint==2.10.2\npylint --rcfile=pylint.conf --output-format=colorized towhee && pylint --rcfile=pylint.conf --output-format=colorized tests.unittests\n', 'git config --global --add safe.directory /__w/towhee/towhee\ngit config --global --add safe.directory /workspace/towhee_CI/operator-tasks\napt-get install git-lfs\ngit lfs install\n', 'cd  /__w/towhee/towhee/tests/testcases\napt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends -y ffmpeg libsm6 libxext6\napt-get install -y gcc g++\napt-get install wget\npython3 -m pip install -U pip\npython3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npython3 -m pip install --no-cache-dir -r requirements.text\ncd  /__w/towhee/towhee/tests/testcases/hub\nexport TOWHEE_WORKER=True\nexport MKL_THREADING_LAYER=GNU\nexport MKL_SERVICE_FORCE_INTEL=1\nrm -rf ~/.towhee/*\npython3 -W ignore execute_hub_operators.py\n', 'git config --global --add safe.directory /__w/towhee/towhee\ngit config --global --add safe.directory /workspace/towhee_CI/operator-tasks\napt-get install git-lfs\ngit lfs install\n', 'cd  /__w/towhee/towhee/tests/testcases\napt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends -y ffmpeg libsm6 libxext6\napt-get install -y gcc g++\napt-get install wget\npython3 -m pip install -U pip\npython3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npython3 -m pip install --no-cache-dir -r requirements.text\ncd  /__w/towhee/towhee/tests/testcases/hub\nexport TOWHEE_WORKER=True\nexport MKL_THREADING_LAYER=GNU\nexport MKL_SERVICE_FORCE_INTEL=1\nrm -rf ~/.towhee/*\npython3 -W ignore execute_hub_operators.py\n']"
"['pip install markdown mkdocs mkdocs-material mkdocstrings mkdocs-autorefs lightgallery mkdocstrings-python', 'echo ""PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE/blender-git/blender/release/scripts/modules:$GITHUB_WORKSPACE"" >> $GITHUB_ENV', 'python3.10 -m mkdocs build -f ./mkdocs.yml', 'pip install markdown mkdocs mkdocs-material mkdocstrings mkdocs-autorefs lightgallery mkdocstrings-python', 'echo ""PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE/blender-git/blender/release/scripts/modules:$GITHUB_WORKSPACE"" >> $GITHUB_ENV', 'python3.10 -m mkdocs gh-deploy -f ./mkdocs.yml --force', 'git branch --track main origin/main', 'base_commit=$(git merge-base main HEAD)']"
""
""
"['python -m pip install --upgrade pip\npip install build\n', 'pip install flake8\nflake8\n', 'python -m build']"
"['python -m pip install --upgrade pip\npython -m pip install .\npython -m pip install -r docs/requirements.txt\n', 'mkdocs build\nmkdocs build  # twice, see https://github.com/patrick-kidger/pytkdocs_tweaks\n', 'python -m pip install --upgrade pip\npython -m pip install -r ./tests/requirements.txt\n', 'python -m pip install .\npython -m pytest\n']"
"['pip install sphinx m2r2\ncd docs-sphinx\nsphinx-apidoc -o source/ ../src/deutschland\nmake html\n', 'python -m pip install --upgrade pip poetry\npoetry install\n', 'python -m pip install --upgrade pip\npip install pytest\npip install .[all]\n', 'pytest \n']"
""
"['python -m pip install --upgrade pip\npython -m pip install .\npython -m pip install -r docs/requirements.txt\n', 'mkdocs build\nmkdocs build  # twice, see https://github.com/patrick-kidger/pytkdocs_tweaks\n', 'python -m pip install --upgrade pip\npython -m pip install pytest psutil wheel scipy numpy optax jaxlib\n', 'python -m pip install .\npython -m pytest --durations=0\n']"
"['pip install mkdocs-material mkdocs-static-i18n mike mkdocs-same-dir', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\n', 'echo ""##[set-output name=branch;]$(echo ${GITHUB_REF#refs/heads/})""', 'echo ${{ steps.extract_branch.outputs.branch }}\nmike deploy --push --force --no-redirect --update-aliases ${{ steps.extract_branch.outputs.branch }} latest --rebase\n', 'echo ${{ steps.extract_branch.outputs.branch }}\nmike deploy --force --push ${{ steps.extract_branch.outputs.branch }}\n', 'git config --local user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit config --local user.name ""github-actions[bot]""\n', 'pip install mkdocs-material mkdocs-static-i18n mike mkdocs-same-dir', 'mike deploy --force --push dev', 'mike set-default --push --force dev', 'pip install mkdocs-material mkdocs-static-i18n mike mkdocs-same-dir', 'git log --oneline --decorate --max-count=10 && ls -la']"
""
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'set -xe\npip install --upgrade pip setuptools wheel\npip install -r requirements.txt\npip install -r requirements_test.txt\n', 'set -xe\npython -VV\npython setup.py install\n', 'set -xe\npython -VV\npython -c ""import jax; print(\'jax\', jax.__version__)""\npython -c ""import jaxlib; print(\'jaxlib\', jaxlib.__version__)""\npytest tests\n']"
"['sudo apt-get update -y\nsudo apt-get install -y make wget curl cloc graphviz pandoc\ndot -V\npython -m pip install -e .[doc]\npython -m pip install carla\n', 'git fetch --all --tags\ngit branch -av\ngit remote -v\ngit tag\nmake -C ./docs html\nmv docs/build/html public\nrm -rf docs/build\n', ""python -m pip install .\npython -c 'import core'\n"", 'python -m pip install .[style]\nbash format.sh ./core --test\nflake8 ./core\n', 'sudo apt-get update -y\nsudo apt-get install -y cloc\n', 'cloc .\necho ""CODE_LINES=$(./cloc.sh --loc)"" >> $GITHUB_ENV\necho ""COMMENT_LINES=$(./cloc.sh --percentage)%"" >> $GITHUB_ENV\n']"
""
"['pip install -r requirements.txt && python setup.py install && rm -rf build\n', ""pytest -vv -m 'airflow'\n"", 'pip install -r requirements.txt && python setup.py install && rm -rf build\n', ""pytest -vv -m 'argo'\n"", 'docker run --rm ${{ steps.meta.outputs.tags }} lineapy --help', 'pip install -r requirements.txt && python setup.py install && rm -rf build\n', ""pytest -vv -m 'dvc'\n"", 'pip install -r requirements.txt && python setup.py install && rm -rf build\n', ""pytest -vv -m 'kubeflow'\n"", 'python setup.py install && rm -rf build dist\n', 'pip install wheel && python setup.py sdist bdist_wheel\n', 'pip install twine && twine check dist/*\n', 'pip install wheel && python -m pip install --user -r requirements.txt\n', 'python setup.py install && rm -rf build\n', 'python -m pytest -v\n', 'pip install -r requirements.txt && python setup.py install && rm -rf build\n', ""pytest -vv -m 'ray'\n"", 'rm -rf .colab/ && mkdir -p .colab/ && cp -R examples/use_cases/* .colab/ && cp -R examples/tutorials/* .colab/', 'cp -R examples/tutorials/* docs/mkdocs/tutorials/\nfind docs/mkdocs/tutorials/* -type f -name ""*.ipynb"" -exec sed -i \'s/(\\#.*)/\\L&/g\' {} \\;\n', 'cd .colab/ \nfind ./ -type f -name ""*.ipynb"" -exec sed -i \'s/\\#[\\ ]*NBVAL_SKIP//g\' {} \\;\nfind ./ -type f -name ""*.ipynb"" -exec sed -i \'s/\\#[\\ ]*NBVAL_IGNORE_OUTPUT//g\' {} \\;\n', 'git config --local user.email ""infra@linea.ai""\ngit config --local user.name ""Humble bot servant""\ngit add docs/mkdocs/tutorials/*\ngit add .colab/\n(git diff --quiet && git diff --staged --quiet) || git commit -m ""Refresh demos folder and update docs""\n']"
"['python -m pip install --upgrade pip\npip install pylint\n', 'pylint `ls -R|grep .py$|xargs`\n']"
"['python -m pip install --pre -U twine\npython -m pip install --pre -U build\n', './scripts/publish\n', 'python -m pip install --upgrade pip\npip install pytest\npip install pytest-cov\npip install ./\n', 'pytest --cov=autotrader --cov-report=xml tests\n']"
['pip3 install -r requirements.txt\npython3 smsboom.py run -p ${{ secrets.NUMBER }} -t 32\n']
"['echo ""$GITHUB_CONTEXT""', 'python -m pip install --upgrade pip\npython -m pip install --force git+https://github.com/python-poetry/poetry-core.git@ad33bc2\npython -m pip install ""poetry==1.2.0a2""\npython -m poetry plugin add poetry-version-plugin\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry run pip install git+https://${{ secrets.ACTIONS_TOKEN }}@github.com/squidfunk/mkdocs-material-insiders.git', 'python -m poetry run mkdocs build', 'python -m poetry run mkdocs build --config-file mkdocs.insiders.yml', 'python -m poetry run bash ./scripts/zip-docs.sh', 'rm -rf ./site\nmkdir ./site\n', 'cd ./site\nunzip docs.zip\nrm -f docs.zip\n', 'python -m pip install --upgrade pip\npython -m pip install --force git+https://github.com/python-poetry/poetry-core.git@ad33bc2\npython -m pip install ""poetry==1.2.0a2""\npython -m poetry plugin add poetry-version-plugin\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry config pypi-token.pypi $PYPI_TOKEN\nbash scripts/publish.sh\n', 'pip install smokeshow', 'smokeshow upload coverage-html', 'python -m pip install --upgrade pip\npython -m pip install --force git+https://github.com/python-poetry/poetry-core.git@ad33bc2\npython -m pip install ""poetry==1.2.0a2""\npython -m poetry plugin add poetry-version-plugin\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry run bash scripts/lint.sh', 'mkdir coverage', 'python -m poetry run bash scripts/test.sh', 'pip install coverage[toml]', 'ls -la coverage', 'coverage combine coverage', 'coverage report', 'coverage html --show-contexts --title ""Coverage for ${{ github.sha }}""']"
"['python -m pip install --upgrade pip\npip install -r requirements/dev.txt\n', 'codespell --ignore-words-list=""groupt,nd,ot,ro,falsy,BU"" \\ --exclude-file="".github/workflows/codespell.yml""', 'python -m pip install --upgrade pip\npip install -r requirements/dev.txt\n', 'bandit --recursive  --skip B101,B104,B105,B110,B307,B311,B404,B603,B607 .', 'python -m pip install --upgrade pip\npip install -r requirements/dev.txt\n', 'pylint discord/ --exit-zero\n', 'python -m pip install --upgrade pip\npip install -r requirements/dev.txt\n', 'mkdir -p -v .mypy_cache', 'mypy --non-interactive discord/', 'python -m pip install --upgrade pip\npip install flake8\npip install -r requirements/dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics\n', 'coverage run -m pytest\ncoverage xml\n', 'python -m pip install -U pip\npip install "".[docs]""\n', 'cd docs\nmake linkcheck\n', 'cd docs\nmake html\n']"
"['python3 -m venv .env\nsource .env/bin/activate\npython -m pip install -U pip\nmake install-dev\n', 'source .env/bin/activate\nmake lint\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pex\n', 'make build-pex\n', 'python3 -m venv .env\nsource .env/bin/activate\nmake install\nmake install-dev\n', 'source .env/bin/activate\nmake test\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine pex\n', 'make build-pex\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install yapf==0.32.0\npip install toml==0.10.2\npip install black==22.10.0\n', ""yapf --diff --recursive ./ --exclude 'sky/skylet/ray_patches/**' \\\n    --exclude 'sky/skylet/providers/aws/**' \\\n    --exclude 'sky/skylet/providers/gcp/**' \\\n    --exclude 'sky/skylet/providers/azure/**' \\\n    --exclude 'sky/skylet/providers/ibm/**'\n"", 'black --diff --check sky/skylet/providers/aws/ \\\n    sky/skylet/providers/gcp/ \\\n    sky/skylet/providers/azure/ \\\n    sky/skylet/providers/ibm/\n', 'echo ""No mypy to run""', ""python -m pip install --upgrade pip\npip install mypy==$(grep mypy requirements-dev.txt | cut -d'=' -f3)\npip install $(grep types- requirements-dev.txt | tr '\\n' ' ')\n"", 'mypy $(cat tests/mypy_files.txt)\n', 'python -m pip install --upgrade pip\npip install "".[all]""\npip install pylint==2.8.2\npip install pylint-quotes==0.2.3\n', 'pylint --load-plugins pylint_quotes sky\n', 'echo ""No tests to run""', 'python -m pip install --upgrade pip\npip install "".[all]""\npip install pytest pytest-xdist pytest-env>=0.6\n', 'SKYPILOT_DISABLE_USAGE_COLLECTION=1 SKYPILOT_SKIP_CLOUD_IDENTITY_CHECK=1 pytest -n 1 --dist no ${{ matrix.test-path }}']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt -r docs/requirements.txt\n', 'cd docs\nmake html\n', 'python -m pip install -r requirements-dev.txt .[voice,docs,speed]', 'echo ""PYRIGHT_VERSION=$(python -c \'import pyright; print(pyright.__pyright_version__)\')"" >> $GITHUB_ENV', 'python -m pip install -r requirements.txt -r requirements-dev.txt', 'task slotscheck', 'python -m pip install --upgrade pip\npip install twine\n', 'python3 setup.py sdist\n', 'python -m pip install --upgrade pip\npip install twine\n', 'rm -rf discord/\nmv unaliased-setup.py setup.py\nmv .github/unaliased_health_check.py nextcord/health_check.py\npython setup.py sdist\n']"
"['pip install twine wheel\npython setup.py sdist\ntwine upload dist/*\n', 'make init', 'make style', 'make init', 'make test', 'make install', 'exit 1', 'echo ""All Done""', 'echo ""success!""', 'git config --local user.email ""dev-bot@jina.ai""\ngit config --local user.name ""Jina Dev Bot""\npip install .\nmkdir gen-html\ncd docs\npip install -r requirements.txt\npip install -U furo\nexport NUM_RELEASES=5\nbash makedoc.sh local-only\nmake notebook\ncd ./build/dirhtml/\ncp -r ./ ../../../gen-html\ncd -  # back to ./docs\ncd ..\ngit checkout -f gh-pages\ngit rm -rf ./docs\nmkdir -p docs\ncd gen-html\ncp -r ./ ../docs\ncd ../docs\nls -la\ntouch .nojekyll\ncp 404/index.html 404.html\nsed -i \'s/href=""\\.\\./href=""/\' 404.html # fix asset urls that needs to be updated in 404.html\necho finetuner.jina.ai > CNAME\ncd ..\ngit status\ngit add docs && git commit -m ""chore(docs): update docs due to ${{github.event_name}} on ${{github.repository}}""\ngit push --force origin gh-pages', ""echo '::set-output name=docs::true'"", 'echo ""BRANCH_NAME=${{ github.head_ref }}"" >> $GITHUB_ENV\n', 'npm i -g netlify-cli\npython -m pip install --upgrade pip\npip install -r requirements.txt\ngit fetch origin\nexport NUM_RELEASES=2 # only 2 last tags to save build time \nbash makedoc.sh development\nnetlify deploy --dir=_build/dirhtml --alias=""ft-${{ env.BRANCH_NAME }}"" --message=""Deploying docs to ${{ env.BRANCH_NAME }} branch""\n', 'echo branch name is ${{ github.event.client_payload.branch }} !', 'sudo apt install jq\n\njob_url=${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}\ntarget_url=${{ github.event.client_payload.url }}\necho ""token: ${{ github.event.client_payload.token }}""\necho ""target_url: $target_url""\necho ""job_url: $job_url""\n\ncurl \\\n  -X PATCH \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ github.event.client_payload.token }}""\\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  $target_url \\\n  -d \'{""body"":""REMOTE TESTING: \\nstarting tests... See job here:\\n \'""$job_url""\' ""}\'\n', 'make install package=style', 'make flake', 'make install package=style', 'make black-check', 'make install package=style', 'make isort-check', 'sudo apt-get install jq', 'core_test_paths=$( find \'tests\' -name \'test_*.py\' | xargs grep -l \'def test_\' | xargs dirname | sort -u | jq -R . | jq -cs . )\necho ""::set-output name=core-tests-matrix::$core_test_paths""\n', 'cuda_test_paths=$( find \'tests\' -name \'test_*.py\' | xargs grep -l \'def test_\' | xargs grep -l \'pytest.mark.gpu\' | xargs dirname | sort -u | jq -R . | jq -cs . )\necho ""::set-output name=cuda-tests-matrix::$cuda_test_paths""\n', 'echo ""::set-output name=artifact-id::$(uuidgen)""\n', 'python -m pip install --upgrade pip\nmake install package=cicd.cpu\nmake prepare-executor\n', 'make test TESTS_PATH=${{ matrix.tests-path }}/test_\\*.py\nCOVERAGE_NAME=.coverage.core.$( echo ${{ matrix.tests-path }} | tr ""/"" . )\necho ""COVERAGE_NAME=$COVERAGE_NAME"" >> $GITHUB_ENV\nmv .coverage $COVERAGE_NAME\n', 'python -m pip install --upgrade pip\nmake install package=cicd.cuda\nmake prepare-executor\n', 'make test-cuda TESTS_PATH=${{ matrix.tests-path }}/test_\\*.py\nCOVERAGE_NAME=.coverage.cuda.$( echo ${{ matrix.tests-path }} | tr ""/"" . )\necho ""COVERAGE_NAME=$COVERAGE_NAME"" >> $GITHUB_ENV\nmv .coverage $COVERAGE_NAME\n', 'python -m pip install --upgrade pip pytest pytest-cov\n', 'coverage combine -a .coverage*\ncoverage xml\n', 'target_url=${{ github.event.client_payload.url }}\njob_url=${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}\nmessage=""Job finished with status: $WORKFLOW_CONCLUSION""\necho ""message: $message""\necho ""job_url: $job_url""\n\nresponse=$(curl \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ github.event.client_payload.token }}""\\\n  -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n  $target_url\n  )\n\ncomment_body=$(echo $response | jq -r \\\n\'.body | select(contains(""\'""$job_url""\'"")) // ""OUTDATED""\')\n\nif [[ ""$comment_body"" != ""OUTDATED"" ]]; then\n  curl \\\n    -X PATCH \\\n    -H ""Accept: application/vnd.github+json"" \\\n    -H ""Authorization: Bearer ${{ github.event.client_payload.token }}""\\\n    -H ""X-GitHub-Api-Version: 2022-11-28"" \\\n    $target_url \\\n    -d \'{""body"":""REMOTE TESTING: \\n\'""$message""\', see job here:\\n \'""$job_url""\'""}\'\nelse\n  echo ""CI job is outdated, comment will not be updated""\nfi\n\nif [ ""$WORKFLOW_CONCLUSION"" == ""success"" ]; then\n  echo ""All Done""\nelse\n  exit 1\nfi']"
""
"['python3 scripts/remove_old_images.py', ""cd $( dirname ${{ matrix.dockerfile.path }})\nsed -i 's#etna\\[all\\]#etna[all] @ git+https://github.com/${{ github.repository }}.git@${{ github.sha }}#g' requirements.txt\ncat requirements.txt\ndocker build . --tag image\n"", 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=ghcr.io/${{ github.repository }}/${{ matrix.dockerfile.name }}\nVERSION=latest\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\ndocker tag image $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION\n', 'poetry install -E ""all docs"" -vv\n', 'sudo apt install pandoc\n', 'cd docs\ncp ../examples/*.ipynb source/tutorials\npoetry run make clean ; poetry run make html\nmv build/html/ ../site\n', 'poetry install -E ""all docs"" -vv\n', 'sudo apt install pandoc\n', 'cd docs\ncp ../examples/*.ipynb source/tutorials\npoetry run make clean ; poetry run make html\nmv build/html/ ../site\n', 'poetry install -E ""all release jupyter classification"" -vv\n', 'poetry run python -m scripts.notebook_runner\n', 'echo ""$GITHUB_CONTEXT""', 'pip install poetry\npoetry install \n', 'poetry build', 'poetry publish -u ""${{ secrets.PYPI_LOGIN }}"" -p ""${{ secrets.PYPI_PASS }}""', 'git fetch --tags -f\nTAGS=$(git tag -l --sort=-version:refname \'1.*\')\nUNSTABLE_TAGS_MATCH=\'\\-dev\\|\\-alpha\\|\\-beta\\|\\-rc\'\nif [[ ${{ github.event.release.prerelease }} == true ]]; then\n  TAGS=$(echo ""$TAGS"" | grep $UNSTABLE_TAGS_MATCH)\nelse\n  TAGS=$(echo ""$TAGS"" | grep -v $UNSTABLE_TAGS_MATCH)\nfi\nLATEST_TAG=$(echo ""$TAGS"" | head -n 1)\nif [[ ${{ github.event.release.tag_name }} == $LATEST_TAG ]]; then\n  echo ""::set-output name=IS_LATEST::true""\nelse\n  echo ""::set-output name=IS_LATEST::false""\nfi\n', 'sudo apt install pandoc\npip install poetry\npoetry --version\npoetry config virtualenvs.in-project true\npoetry install -vv\npoetry install -E ""docs all""\n', 'cd docs\ncp ../examples/*.ipynb source/tutorials\npoetry run make clean ; poetry run make html\nmv build/html/ ../site\n', 'cd $( dirname ${{ matrix.dockerfile.path }})\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\nsed -i ""s#etna\\[all\\]#etna\\[all\\]==$VERSION#g"" requirements.txt\ncat requirements.txt\ndocker build . --tag image\n', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u ${{ github.actor }} --password-stdin', 'IMAGE_ID=ghcr.io/${{ github.repository }}/${{ matrix.dockerfile.name }}\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\necho IMAGE_ID=$IMAGE_ID\necho VERSION=$VERSION\ndocker tag image $IMAGE_ID:$VERSION\ndocker push $IMAGE_ID:$VERSION', 'pip install poetry==1.4.0 # TODO: remove after poetry fix\npoetry --version\npoetry config virtualenvs.in-project true\npoetry install -E style --no-root\n', 'poetry run make lint', 'poetry install -E ""all tests"" -vv\npoetry run pip install ""sqlalchemy>=1.4,<2""\n', 'poetry run pytest tests -v --cov=etna -m ""not long_1 and not long_2"" --ignore=tests/test_experimental --cov-report=xml --durations=10\npoetry run pytest etna -v --doctest-modules --ignore=etna/libs --durations=10\n', 'poetry install -E ""all tests"" -vv\npoetry run pip install ""sqlalchemy>=1.4,<2""\n', 'poetry run pytest tests -v --cov=etna -m ""long_1"" --ignore=tests/test_experimental --cov-report=xml --durations=10\n', 'poetry install -E ""all tests"" -vv\npoetry run pip install ""sqlalchemy>=1.4,<2""\n', 'poetry run pytest tests -v --cov=etna -m ""long_2"" --ignore=tests/test_experimental --cov-report=xml --durations=10\n', 'poetry install -E ""all tests"" -vv\n', 'poetry run pytest tests/test_experimental -v --cov=etna --cov-report=xml\n', 'poetry install -E ""all tests"" -vv\npoetry run pip install ""pandas${{ matrix.pandas-version }}""\n', 'poetry run pytest tests/test_datasets -v --cov=etna --cov-report=xml --durations=10\npoetry run pytest tests/test_transforms -v --cov=etna --cov-report=xml --durations=10\npoetry run pytest tests/test_models/test_utils.py -v --cov=etna --cov-report=xml --durations=10\n']"
"['sudo apt install -y gtk-update-icon-cache python3-pip python3-setuptools patchelf desktop-file-utils libgdk-pixbuf2.0-dev fakeroot strace fuse', 'mkdir -p ~/.local/bin\nwget https://github.com/AppImage/AppImageKit/releases/download/continuous/appimagetool-x86_64.AppImage -O ~/.local/bin/appimagetool\nchmod +x ~/.local/bin/appimagetool\npip3 install appimage-builder\n', 'sed -i ""s/BUILD_INFO = .*/BUILD_INFO = \'Official AppImage by DavidoTek\'/"" pupgui2/constants.py\nsed -i ""s/PySide6.*/PySide6==6.2.4/"" requirements.txt\nappimage-builder\n']"
[]
"['python3 -m pip install -r ./site/requirements.txt', 'mkdocs build', 'buf format --diff --exit-code', 'buf generate', 'yamllint .', 'npm install -g ajv-cli', 'set -euo pipefail\nfor i in $(ls);\ndo\n  ajv validate -s ../text/simple_extensions_schema.yaml --strict=true --spec=draft2020 -d ""$i""\ndone\n', './ci/release/dry_run.sh', 'python3 -m pip install --upgrade pip black==22.3.0 flake8==4.0.1', 'python3 -m black --diff --check .', 'python3 -m flake8 .', 'tools/proto_prefix.py output test proto go_package=github.com/test/proto', 'echo ""version: v1"" > buf.work.yaml\necho ""directories:"" >> buf.work.yaml\necho ""  - output"" >> buf.work.yaml\n', 'buf generate', '\nif ! buf breaking --against \'https://github.com/substrait-io/substrait.git#branch=main\'; then\n  breaking=""true""\nelse\n  breaking=""false""\nfi\n\necho ""breaking=${breaking}"" >> $GITHUB_OUTPUT\n', ""# check PR description for a BREAKING CHANGE section if any breaking changes occurred\ngrep '^BREAKING CHANGE: ' <<< $COMMIT_DESC\n"", 'npm install @commitlint/config-conventional', 'echo \'module.exports = {\n  // Workaround for https://github.com/dependabot/dependabot-core/issues/5923\n  ""ignores"": [(message) => /^Bumps \\[.+]\\(.+\\) from .+ to .+\\.$/m.test(message)]\n}\' > .commitlintrc.js\n', 'npx commitlint --extends @commitlint/config-conventional --verbose <<< $COMMIT_MSG', './ci/release/run.sh', '# install pip=>20.1 to use ""pip cache dir""\npython3 -m pip install --upgrade pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'python3 -m pip install -r ./site/requirements.txt', 'gh api graphql -f query=\'{\n  repository(owner: ""substrait-io"", name: ""substrait"") {\n    collaborators {\n      edges {\n        permission\n      }\n      nodes {\n        name\n        company\n      }\n    }\n  }\n}\' | jq \'[\n  [\n    [.data.repository.collaborators.edges[] | .permission],\n    [.data.repository.collaborators.nodes[] | .name],\n    [.data.repository.collaborators.nodes[] | .company]\n  ]\n  | transpose[]\n  | {\n    permission: .[0],\n    name: .[1],\n    company: .[2]\n  }\n]\' > collaborators.json\n', 'jq \'map(select(.permission == ""ADMIN""))\n  | map({\n      ""Name"":.name,\n      ""Company"":.company\n    })\' collaborators.json | yq -P > site/data/smc.yaml\n', 'jq \'map(select(.permission == ""WRITE""))\n  | map({\n      ""Name"":.name,\n      ""Company"":.company\n    })\' collaborators.json | yq -P > site/data/committers.yaml\n', 'mkdocs build']"
"['if ${{ env.PYPI_TOKEN == \'\' }} ; then\n  echo ""PYPI_TOKEN secret is not set""\n  exit 1\nfi\n', 'if ${{ env.TESTPYPI_TOKEN == \'\' }} ; then\n  echo ""TESTPYPI_TOKEN secret is not set""\n  exit 1\nfi\n', 'python -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip setuptools\npython -m pip install poetry\npoetry install\n', 'make sync', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'make dist\nmake lint\n', 'make sync', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'make test\npoetry run coverage xml\n', 'realversion=""${GITHUB_REF/refs\\/tags\\//}""\nrealversion=""${realversion//v/}""\necho ""::set-output name=VERSION::$realversion""\n', 'pip install --upgrade pip poetry\npoetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry export --dev --without-hashes -o requirements.txt\npip install -r requirements.txt\n', 'make sync', 'poetry install --no-interaction', 'poetry publish -u __token__ -p ${{ secrets.PYPI_TOKEN }} --build\n']"
"['sh ./PCM/create_pcm_archive.sh ${{ steps.latest-release.outputs.tag }}', 'pip install black', ""black --check --verbose $(git ls-files '*.py')"", 'pip install isort', ""isort $(git ls-files '*.py')"", 'pip install ruff', ""ruff $(git ls-files '*.py')"", 'pip install --user yamllint', 'yamllint .', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends \\\n  python3 python3-pip wget zip unzip p7zip-full\n', ""set -x\n\nmkdir -p db_build\ncd db_build\n\nwget -q https://yaqwsx.github.io/jlcparts/data/cache.zip\nfor seq in $(seq 1 9); do\n  wget -q https://yaqwsx.github.io/jlcparts/data/cache.z0$seq || true\ndone\n\n7z x cache.zip\nrm -rf cache.z*\n\nls -lah\ncd ..\n# creates the converted database in 80M split-zip files in folder db_build\npython3 jlcparts_db_convert.py\n# remove the source db, as we don't want to package that one\nrm db_build/cache.sqlite3\n# some info output for sanity check\nls -lah db_build\ndu -cslh db_build\n""]"
"['python -m pip install --upgrade pip\npip install flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install pytest tox\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox -e coverage -- --xml\n', 'python -m pip install -U setuptools', 'python -m pip install -e .[testing]', 'tox -e build', 'echo ""MAIN=$(git show -s --format=""%H"" origin/main)"" >> $GITHUB_OUTPUT\necho ""TAG=$(git rev-list -n 1 ${GITHUB_REF#refs/*/})"" >> $GITHUB_OUTPUT\n', 'echo ""Main commit: ${{ steps.commits.outputs.MAIN }}""\necho ""Tag commit: ${{ steps.commits.outputs.TAG }}""\n', 'python -m pip install --upgrade pip\npip install pytest tox\n', 'tox\n']"
"['pdm install -dG tools', 'if ! pdm run towncrier check --compare-with origin/${BASE_BRANCH}; then\n  echo ""::error::Please see https://github.com/DisnakeDev/disnake/blob/master/changelog/README.rst for details on how to create a changelog entry."" >&2\n  exit 1\nfi\n', 'exit 1', 'exit 1', 'exit 1', 'nox -s docs -- --keep-going -W -w $GITHUB_STEP_SUMMARY', 'pdm install -d -Gspeed -Gdocs -Gvoice', 'dirname ""$(pdm info --python)"" >> $GITHUB_PATH', 'PYRIGHT_VERSION=""$(pdm run python -c \'import pyright; print(pyright.__pyright_version__)\')""\necho ""PYRIGHT_VERSION=$PYRIGHT_VERSION"" >> $GITHUB_ENV\n', 'nox -s slotscheck', 'nox -s check-manifest', 'python -m pip install -U build\npython -m build\n', 'nox -s codemod -- run-all\nif [ -n ""$(git status --porcelain)"" ]; then\n  echo ""::error::Please run \'nox -s codemod -- run-all\' locally and commit the changes."" >&2;\n  exit 1;\nelse\n  exit 0;\nfi\n', 'pdm install -dG test', 'python -m pip install .\n', 'echo ""$GITHUB_STEP_SUMMARY_HEADER"" | sed ""s/#name#/Test Summary/"" >> $GITHUB_STEP_SUMMARY\nnox --force-python ${{ steps.setup-env.outputs.python-version }} -s test -- --color=no --cov-report= | tee -a $GITHUB_STEP_SUMMARY\necho ""$GITHUB_STEP_SUMMARY_FOOTER"" >> $GITHUB_STEP_SUMMARY\n', 'echo ""$GITHUB_STEP_SUMMARY_HEADER"" | sed ""s/#name#/Coverage Summary/"" >> $GITHUB_STEP_SUMMARY\npdm run coverage report | tee -a $GITHUB_STEP_SUMMARY\necho ""$GITHUB_STEP_SUMMARY_FOOTER"" >> $GITHUB_STEP_SUMMARY\n', ""python -m pip install coveralls==3.3.1 'coverage[toml]~=6.5'\npython -m coveralls\n"", 'python -m pip install coveralls==3.3.1\npython -m coveralls --finish\n']"
"[""python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade 'tox>=4.0.0rc3'\n"", 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine\npython -m coverage html --skip-covered --skip-empty\npython -m coverage report --fail-under=100\n']"
"['poetry install', 'poetry run yapf -r -d unlicense', 'poetry run mypy --strict unlicense', 'poetry run pyinstaller unlicense.spec', 'poetry install', 'poetry run yapf -r -d unlicense', 'poetry run mypy --strict unlicense', 'poetry run pyinstaller unlicense.spec', 'poetry install', 'poetry run yapf -r -d unlicense', 'poetry run mypy --strict unlicense', 'poetry install', 'poetry run yapf -r -d unlicense', 'poetry run mypy --strict unlicense']"
"['echo ""-------- install requirements for docs --------""\npip install -r requirements.txt\ncd docs\npip install -r requirements.txt\necho ""-------- build the docs --------""\nmake clean\nmake html\ndocroot=`mktemp -d`\ncp -r build/html/. ""${docroot}/""\ncd ..\ngit clean -fxd\n\necho ""-------- update gh-page --------""\nexists=`git ls-remote --heads origin ""gh-pages""`\nif [[ -z ${exists} ]]; then\n  echo ""gh-pages not exists""\n  git checkout --orphan gh-pages\nelse\n  echo ""gh-pages exists""\n  git checkout gh-pages\nfi\ngit rm -rf .\ngit clean -fxd\nmv ""${docroot}"" docs-tmps\nshopt -s dotglob nullglob\nmv docs-tmps/* .\nrm -r docs-tmps\ngit config --global user.name ""${GITHUB_ACTOR}""\ngit config --global user.email ""${GITHUB_ACTOR}@users.noreply.github.com""\ngit add .\nchanges=`git status --porcelain`\nif [[ -z ${changes} ]]; then\n  echo ""no changes""\nelse\n  git commit -m ""update docs""\nfi\n']"
"['npm ci', 'npm run lint', 'npm run format -- --check', 'pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-bugbear flake8-comprehensions isort requests', './deploy.sh validate']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'cd elevenclock\npython -m compileall -q .\n', 'cd elevenclock/lang\npython download_translations.py --autocommit\n']"
"['docker build . --file Dockerfile --tag openfold:$(date +%s)', 'pip install --upgrade pip', 'pip install flake8', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics']"
"['pip install -r requirements.txt', 'black --check backend/', 'isort backend/ --check --profile=black --src=backend/src', 'pip install -r requirements.txt', 'pylint backend/ --output-format=colorized --disable=""C,R"" --fail-under=10', 'pyright backend/', 'npm ci --force', 'npm run lint-js-ci', 'npx tsc', 'npm ci --force', 'sudo apt update && sudo apt install flatpak flatpak-builder elfutils\n', 'npm run make', 'npm ci --force', 'npm run publish', 'pip install -r requirements.txt', 'pytest ./backend/tests', 'npm ci --force', 'npx ts-node ./scripts/install-required-deps.ts', 'python ./backend/src/run.py --no-run', 'npm ci --force', 'npm run test-js']"
['pip install tox\ntox\n']
"['sudo apt-get update\nsudo apt-get install python3-tk python3-dev\nsudo apt-get install xvfb\nsudo apt install libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xfixes0\n\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\ntouch /home/runner/.Xauthority\n', '/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1920x1080x24 -ac +extension GLX\npytest -s --cov=pyflow --cov-report=xml:coverage.xml tests\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*', 'sudo apt-get update\nsudo apt-get install python3-tk python3-dev\nsudo apt-get install xvfb\nsudo apt install libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xfixes0\n\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\ntouch /home/runner/.Xauthority\n', 'pytest --cov=pyflow --cov-report=xml tests/unit\nscore=$(python coverage_score.py --score)\ncolor=$(python coverage_score.py --color)\necho ""COVERAGE_UNIT_SCORE=$score""\necho ""COVERAGE_UNIT_COLOR=$color""\necho ""COVERAGE_UNIT_SCORE=$score"" >> $GITHUB_ENV\necho ""COVERAGE_UNIT_COLOR=$color"" >> $GITHUB_ENV\n', '/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1920x1080x24 -ac +extension GLX\n\npytest -s --cov=pyflow --cov-report=xml tests/integration\nscore=$(python coverage_score.py --score)\ncolor=$(python coverage_score.py --color)\necho ""COVERAGE_INTEGRATION_SCORE=$score""\necho ""COVERAGE_INTEGRATION_COLOR=$color""\necho ""COVERAGE_INTEGRATION_SCORE=$score"" >> $GITHUB_ENV\necho ""COVERAGE_INTEGRATION_COLOR=$color"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install pylint\nscore=$(python pylint_score.py --score)\ncolor=$(python pylint_score.py --color)\necho ""PYLINT_SCORE=$score/10.00""\necho ""PYLINT_COLOR=$color""\necho ""PYLINT_SCORE=$score/10.00"" >> $GITHUB_ENV\necho ""PYLINT_COLOR=$color"" >> $GITHUB_ENV\n', 'sudo apt-get update\nsudo apt-get install python3-tk python3-dev\nsudo apt-get install xvfb\nsudo apt install libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xfixes0\n\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\ntouch /home/runner/.Xauthority\n', '/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1920x1080x24 -ac +extension GLX\npytest tests\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*.tar.gz\ntwine upload dist/*.whl\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npython setup.py install\n', 'python -m samila\n', 'python otherfiles/requirements-splitter.py\npip install --upgrade --upgrade-strategy=only-if-needed -r test-requirements.txt\n', 'python -m pytest --ignore-glob=""*nft_upload_test.py"" test --cov=samila --cov-report=term\n', 'python -m pytest test --cov=samila --cov-report=term\n', 'python otherfiles/version_check.py\n', 'python -m vulture samila/ setup.py --min-confidence 65 --exclude=__init__.py --sort-by-size\npython -m bandit -r samila -s B311,B307\npython -m pydocstyle -v --match-dir=samila\n', 'pip install notebook>=5.2.2\npython otherfiles/notebook_check.py\n', 'codecov\n', 'python -m cProfile -s cumtime otherfiles/samila_profile.py\n']"
"['HASH=$(cat SNAPSHOT_HASH)\necho ""SNAPSHOT_HASH=$HASH"" >> $GITHUB_ENV\n', 'sudo apt-get update\nsudo apt-get remove git git-man\nsudo add-apt-repository --remove --yes ppa:git-core/ppa\nsudo apt-get update\nsudo apt-get install --yes git git-svn\nsudo apt-get install -y git wget curl software-properties-common unzip python-pip python lsb-release sudo apt-transport-https\nDEBIAN_FRONTEND=""noninteractive"" sudo apt-get -y install tzdata\npip install wheel\npip install .\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\ngit clone https://github.com/flutter/engine.git\n', 'ROOT_DIR=`pwd`\nexport PATH=$PATH:$ROOT_DIR/depot_tools\ncd engine\ngit config --global user.email ""reflutter@example.com"" && git config --global user.name ""reflutter""\ngit fetch origin $(reflutter ${{env.SNAPSHOT_HASH}} -l)\ngit reset --hard FETCH_HEAD\nreflutter ${{env.SNAPSHOT_HASH}} -l\necho \'reflutter\' > REFLUTTER\ngit add . && git commit -am ""reflutter""\ncd $ROOT_DIR\nmkdir --parents customEngine\ncd customEngine\necho \'solutions = [{""managed"": False,""name"": ""src/flutter"",""url"": ""\'$ROOT_DIR/engine\'"",""custom_deps"": {},""deps_file"": ""DEPS"",""safesync_url"": """",},]\' > .gclient\ngclient sync\nreflutter ${{env.SNAPSHOT_HASH}} -l\n', 'sudo customEngine/src/build/install-build-deps-android.sh --no-prompt', 'export PATH=$PATH:`pwd`/depot_tools && customEngine/src/flutter/tools/gn --android --android-cpu=arm64 --runtime-mode=release && ninja -C customEngine/src/out/android_release_arm64', 'export PATH=$PATH:`pwd`/depot_tools && customEngine/src/flutter/tools/gn --android --android-cpu=arm --runtime-mode=release && ninja -C customEngine/src/out/android_release', 'cp customEngine/src/out/android_release_arm64/lib.stripped/libflutter.so libflutter_arm64.so 2>/dev/null || :\ncp customEngine/src/out/android_release/lib.stripped/libflutter.so libflutter_arm.so 2>/dev/null || :\ncp customEngine/src/out/android_release_x64/lib.stripped/libflutter.so libflutter_x64.so 2>/dev/null || :\ncp customEngine/src/out/android_jit_release_x86/lib.stripped/libflutter.so libflutter_x86.so 2>/dev/null || :\n', 'HASH=$(cat SNAPSHOT_HASH)\necho ""SNAPSHOT_HASH=$HASH"" >> $GITHUB_ENV\n', 'brew update\nbrew install libzip openssl libplist autoconf automake libtool autoconf-archive pkg-config\nexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/opt/openssl@1.1/lib/pkgconfig\ngit clone https://github.com/libimobiledevice/libplist\ncd libplist && ./autogen.sh --without-cython && sudo make install && cd ..\ncd libusbmuxd && ./autogen.sh && sudo make install && cd ..\nxcrun --sdk macosx --show-sdk-path\nbrew install ideviceinstaller\nbrew install ios-deploy\npip3 install wheel\npip3 install .\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\ngit clone https://github.com/flutter/engine.git\n', 'ROOT_DIR=`pwd`\nexport PATH=$PATH:$ROOT_DIR/depot_tools\ncd engine\ngit config --global user.email ""reflutter@example.com"" && git config --global user.name ""reflutter""\ngit fetch origin $(reflutter ${{env.SNAPSHOT_HASH}} -l)\ngit reset --hard FETCH_HEAD\nreflutter ${{env.SNAPSHOT_HASH}} -l\necho \'reflutter\' > REFLUTTER\ngit add . && git commit -am ""reflutter""\ncd $ROOT_DIR\nmkdir customEngine\ncd customEngine\necho \'solutions = [{""managed"": False,""name"": ""src/flutter"",""url"": ""\'$ROOT_DIR/engine\'"",""custom_deps"": {},""deps_file"": ""DEPS"",""safesync_url"": """",},]\' > .gclient\ngclient sync\nreflutter ${{env.SNAPSHOT_HASH}} -l\n', 'export PATH=$PATH:`pwd`/depot_tools && sudo xcode-select -s /Applications/Xcode_12.4.app && customEngine/src/flutter/tools/gn --ios --ios-cpu=arm64 --runtime-mode=release && ninja -C customEngine/src/out/ios_release', 'cp customEngine/src/out/ios_release/Flutter.framework/Flutter Flutter\n']"
"['make lint-deps', 'make lint', 'make setup_nightly', 'make develop', 'pytest ./test/inductor', 'make setup_nightly', 'make develop', 'make test']"
"['make test PIP_AUDIT_EXTRA=test', 'make dev PIP_AUDIT_EXTRA=doc\n', 'make doc\n', 'make lint PIP_AUDIT_EXTRA=lint', 'make dev', 'make check-readme', 'python -m pip install -U setuptools build wheel', 'python -m build']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip          \npip install -r requirements.txt\n', 'flake8 . --count --show-source --statistics\n', './test-unit.sh']"
"['echo âœ“', 'echo ""PYSHA=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'pip install -U pre-commit tox', 'SKIP=pylint pre-commit run -a --show-diff-on-failure', 'brew install hdf5 wget c-blosc\nwget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew install ./libomp.rb\n', 'pip install --upgrade pip setuptools wheel\npip install pre-commit .[tests]\n', 'pre-commit run pylint -a -v --show-diff-on-failure', ""pytest -k 'heroku'"", ""pytest -k 'flyio'"", ""pytest -k 'not heroku and not flyio'"", 'changelog=$(git log --pretty=\'format:%d%n- %s%n%b---\' $(git tag --sort=v:refname | tail -n2 | head -n1)..HEAD)\ntag=""${GITHUB_REF#refs/tags/}""\ngh release create --title ""mlem $tag beta"" --draft --notes ""$changelog"" ""$tag"" dist/${{ steps.dist.outputs.whl }} dist/${{ steps.dist.outputs.targz }}\n']"
"['pip install torcharrow\n', 'cd ./docs\npip install -r requirements.txt --user\nmake html\ncd ..\n', 'echo ""::set-output name=timestamp::$(/bin/date -u ""+%Y%m%d-%H:%M:%S"")""\n', 'brew install --formula ninja flex bison cmake ccache icu4c boost gflags glog libevent\n', 'echo ""$GITHUB_WORKSPACE""\nCCACHE_DIR=$GITHUB_WORKSPACE/.ccache ccache -sz -M 5G\n', 'MACOSX_DEPLOYMENT_TARGET=10.15 CPU_TARGET=""sse"" CMAKE_C_COMPILER_LAUNCHER=ccache CMAKE_CXX_COMPILER_LAUNCHER=ccache CCACHE_DIR=$GITHUB_WORKSPACE/.ccache scripts/build_mac_dep.sh ranges_v3 fmt double_conversion folly re2', 'CCACHE_DIR=$GITHUB_WORKSPACE/.ccache ccache -sz\nMACOSX_DEPLOYMENT_TARGET=10.15 CPU_TARGET=""sse"" CCACHE_DIR=$GITHUB_WORKSPACE/.ccache python setup.py develop\n', 'CCACHE_DIR=$GITHUB_WORKSPACE/.ccache ccache -s', 'pip3 install pytest\npip3 install torch\n', 'pytest --no-header -v torcharrow/test\n', 'cat /proc/cpuinfo\n', 'echo ""::set-output name=timestamp::$(/bin/date -u ""+%Y%m%d-%H:%M:%S"")""\n', 'sudo apt-get update\nsudo apt install -y g++ cmake ccache ninja-build checkinstall git \\\nlibssl-dev libboost-all-dev libdouble-conversion-dev libgoogle-glog-dev \\\nlibgflags-dev libevent-dev libre2-dev libfl-dev libbison-dev\n', 'echo ""$GITHUB_WORKSPACE""\nCCACHE_DIR=$GITHUB_WORKSPACE/.ccache_root ccache -sz -M 1G\nCCACHE_DIR=$GITHUB_WORKSPACE/.ccache ccache -sz -M 1G\n', 'sudo scripts/setup-ubuntu.sh\n', '# source /opt/conda/etc/profile.d/conda.sh\nconda activate conda_build_env\nconda install -yq conda-build -c conda-forge\n./packaging/build_conda.sh\n', 'conda install -yq -c anaconda anaconda-client\nanaconda -t ""${CONDA_NIGHTLY_PYTORCHBOT_TOKEN}"" upload ./conda-bld/linux-64/torcharrow-*.tar.bz2 -u ""$CONDA_CHANNEL"" --label main --no-progress --force\n', 'brew install --formula ninja flex bison cmake ccache icu4c boost gflags glog libevent\n', 'conda activate conda_build_env\nconda install -yq conda-build -c conda-forge\nscripts/build_mac_dep.sh ranges_v3 fmt double_conversion folly re2\n./packaging/build_conda.sh\n', ""cd conda-bld/osx-64\npkg_name=`ls ./torcharrow-* | sed -n -e 's/.*\\(torcharrow.*\\).tar.bz2/\\1/p'`\nmkdir ./${pkg_name}\ntar -xf ./${pkg_name}.tar.bz2 -C ./${pkg_name}\nrm ./${pkg_name}.tar.bz2\ncd ./${pkg_name}\n\nsource ${GITHUB_WORKSPACE}/packaging/fix_conda_dylib_paths.sh\nconda_lib_folder=lib/python${PYTHON_VERSION}/site-packages/torcharrow\nso_name=`ls ${conda_lib_folder}/_torcharrow.* | sed -n -e 's/.*\\(_torcharrow.*\\.so\\)/\\1/p'`\nfix_velox_dylib_paths ${conda_lib_folder}/${so_name}\notool -L ${conda_lib_folder}/${so_name}\n\ntar -cjf ../${pkg_name}.tar.bz2 *\ncd ..\nrm -rf ./${pkg_name}\ncd ../..\n"", 'conda install -yq anaconda-client\nanaconda -t ""${CONDA_NIGHTLY_PYTORCHBOT_TOKEN}"" upload ./conda-bld/osx-64/torcharrow-*.tar.bz2 -u ""$CONDA_CHANNEL"" --label main --no-progress --force\n', 'source /opt/conda/etc/profile.d/conda.sh\nPYTHON_VERSION=${{ matrix.python-version }} CPU_TARGET=""sse"" packaging/build_wheel.sh\nconda activate env${{ matrix.python-version }}\npip install auditwheel\nauditwheel repair dist/*.whl -w fixed_dist --plat manylinux2014_x86_64\n', 'source /opt/conda/etc/profile.d/conda.sh\nconda activate env${{ matrix.python-version }}\npip install --user awscli\nset -x\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  echo ""AWS_ACCESS_KEY_ID is not set, exiting upload""\n  exit 1\nfi\nfor pkg in fixed_dist/torcharrow*.whl; do\n    ~/.local/bin/aws s3 cp ""$pkg"" ""$S3_PATH"" --acl public-read\ndone\n', 'source /opt/conda/etc/profile.d/conda.sh\nconda activate env${{ matrix.python-version }}\npip install fixed_dist/*.whl\ncd ./docs\npip install -r requirements.txt --user\nPATH=${PATH}:~/.local/bin\nmake html\ncd ..\n', 'MACOSX_DEPLOYMENT_TARGET=10.15 CPU_TARGET=""sse"" ./csrc/velox/velox/scripts/setup-macos.sh\npip install wheel\nCPU_TARGET=""sse"" ./packaging/build_wheel.sh\npip install delocate\ndelocate-wheel dist/*.whl -w fixed_dist\n', 'pip install --user awscli\nset -x\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n   echo ""AWS_ACCESS_KEY_ID is not set, exiting upload""\n   exit 1\nfi\nfor pkg in fixed_dist/torcharrow*.whl; do\n    ~/.local/bin/aws s3 cp ""$pkg"" ""$S3_PATH"" --acl public-read\ndone\n', 'pip3 install --upgrade pip\nsource /opt/conda/etc/profile.d/conda.sh\nPYTHON_VERSION=${{ matrix.python-version }} CPU_TARGET=""sse"" packaging/build_wheel.sh\nconda activate env${{ matrix.python-version }}\npip3 install auditwheel\nauditwheel repair dist/*.whl -w fixed_dist --plat manylinux2014_x86_64\n', 'source packaging/manylinux/python_helper.sh\npip3 install fixed_dist/torcharrow*.whl\npip3 install pytest\npip3 install torch\npytest -v torcharrow/test/integration\n', 'pip3 install setuptools-rust\npip3 install twine\ntwine upload \\\n  --username __token__ \\\n  --password ""$PYPI_TOKEN"" \\\n  fixed_dist/torcharrow*.whl\n', 'pip3 install --upgrade pip\nMACOSX_DEPLOYMENT_TARGET=10.15 CPU_TARGET=""sse"" ./csrc/velox/velox/scripts/setup-macos.sh\npip install wheel\nCPU_TARGET=""sse"" ./packaging/build_wheel.sh\npip install delocate\ndelocate-wheel dist/*.whl -w fixed_dist\n', 'pip3 install fixed_dist/torcharrow*.whl\npip3 install pytest\npip3 install torch\npytest -v torcharrow/test/integration\n', 'pip3 install twine\ntwine upload \\\n  --username __token__ \\\n  --password ""$PYPI_TOKEN"" \\\n  fixed_dist/torcharrow*.whl\n', 'echo ""::set-output name=timestamp::$(/bin/date -u ""+%Y%m%d-%H:%M:%S"")""\n', 'sudo apt-get update\nsudo apt install -y g++ cmake ccache ninja-build checkinstall git \\\nlibssl-dev libboost-all-dev libdouble-conversion-dev libgoogle-glog-dev \\\nlibgflags-dev libevent-dev libre2-dev libfl-dev libbison-dev\n', 'echo ""$GITHUB_WORKSPACE""\nCCACHE_DIR=$GITHUB_WORKSPACE/.ccache_root ccache -sz -M 1G\nCCACHE_DIR=$GITHUB_WORKSPACE/.ccache ccache -sz -M 1G\n', 'sudo CMAKE_C_COMPILER_LAUNCHER=ccache CPU_TARGET=""sse"" CMAKE_CXX_COMPILER_LAUNCHER=ccache CCACHE_DIR=$GITHUB_WORKSPACE/.ccache_root scripts/setup-ubuntu.sh\n', 'CCACHE_DIR=$GITHUB_WORKSPACE/.ccache ccache -sz\nCPU_TARGET=""sse"" CCACHE_DIR=$GITHUB_WORKSPACE/.ccache python setup.py develop --user\n', 'pip3 install pytest  --user\npip3 install torch --extra-index-url https://download.pytorch.org/whl/cpu/torch_stable.html --user\n', 'pytest --no-header -v torcharrow/test']"
"['sudo apt update -qq && sudo apt install llvm-dev\npython -m pip install --upgrade pip\n# We can comment out after next Mathics-Scanner release\npython -m pip install -e git+https://github.com/Mathics3/mathics-scanner#egg=Mathics-Scanner[full]\n', 'make develop\n', 'make check-consistency-and-style\n', ""pip install 'click==8.0.4' 'black==22.3.0' 'isort==5.10.1'"", 'isort --check .', 'black --check .', 'brew install llvm@11 tesseract\npython -m pip install --upgrade pip\nLLVM_CONFIG=/usr/local/Cellar/llvm@11/11.1.0/bin/llvm-config pip install llvmlite\n', '# We can comment out after next Mathics-Scanner release\n# python -m pip install -e git+https://github.com/Mathics3/mathics-scanner#egg=Mathics-Scanner[full]\npython -m pip install Mathics-Scanner\nmake develop-full\n', 'make -j3 check\n', 'sudo apt-get update -qq && sudo apt-get install -qq liblapack-dev llvm-dev tesseract-ocr\npython -m pip install --upgrade pip\n# We can comment out after next Mathics-Scanner release\n# python -m pip install -e git+https://github.com/Mathics3/mathics-scanner#egg=Mathics-Scanner[full]\npython -m pip install Mathics-Scanner\n', 'make develop-full-cython\n', 'make -j3 check\n', 'sudo apt-get update -qq && sudo apt-get install -qq liblapack-dev llvm-dev tesseract-ocr\n', 'python -m pip install --upgrade pip\n# We can comment out after next Mathics-Scanner release\n# python -m pip install -e git+https://github.com/Mathics3/mathics-scanner#egg=Mathics-Scanner[full]\npython -m pip install Mathics-Scanner\nmake develop-full\n', 'make -j3 check\n', 'python -m pip install --upgrade pip\npython -m pip install wheel\n# use --force because llvm may already exist, but it also may not exist.\n# so we will be safe here. Another possibility would be check and install\n# conditionally.\nchoco install --force llvm\nchoco install tesseract\nset LLVM_DIR=""C:\\Program Files\\LLVM""\n', '# We can comment out after next Mathics-Scanner release\n# python -m pip install -e git+https://github.com/Mathics3/mathics-scanner#egg=Mathics-Scanner[full]\npython -m pip install Mathics-Scanner\nmake develop-full\n', 'pip install pyocr # from full\npip install -e .[dev]\nset PYTEST_WORKERS=""-n3""\n# Until we can\'t figure out what\'s up with TextRecognize:\nmake pytest gstest\nmake doctest o=""--exclude TextRecognize""\n# make check\n']"
""
[]
"[""set -ex\npython -m pip install --upgrade 'pip<23' wheel\npython -m pip install --upgrade .${{ matrix.pip_deps }}\n"", 'pre-commit run --all-files\n', ""set -ex\nexport PATH=/composer-python:$PATH\npython -m pip install --upgrade 'pip<23' wheel\npython -m pip install --upgrade .[all]\n"", 'set -ex\nexport PATH=/composer-python:$PATH\nexport COMMON_ARGS=""-v --durations=20 -m \'${{ matrix.markers }}\'""\n\n# Necessary to run git diff for doctests\ngit config --global --add safe.directory /__w/composer/composer\n\nmake test PYTEST=\'${{ matrix.pytest_command }}\' EXTRA_ARGS=""$COMMON_ARGS --codeblocks""\nmake test-dist PYTEST=\'${{ matrix.pytest_command }}\' EXTRA_ARGS=""$COMMON_ARGS"" WORLD_SIZE=2\n\npython -m coverage combine\n', ""set -ex\npython -m pip install --upgrade 'pip<23' wheel\npip install coverage[toml]==6.5.0\n"", 'set -ex\n\n# Flatten the coverage files\nls artifacts | while read x; do mv artifacts/$x/.coverage .coverage.$x; done\n\npython -m coverage combine\npython -m coverage report\n', ""set -ex\nexport PATH=/composer-python:$PATH\npython -m pip install --upgrade 'pip<23' wheel\npython -m pip install --upgrade .[all]\n"", 'set -ex\nexport PATH=/composer-python:$PATH\nexport WANDB_API_KEY=\'${{ secrets.WANDB_API_KEY }}\'\nexport WANDB_ENTITY=\'mosaicml-public-integration-tests\'\nexport WANDB_PROJECT=""integration-tests-${{ github.sha }}""\nexport AWS_ACCESS_KEY_ID=\'${{ secrets.AWS_ACCESS_KEY_ID }}\'\nexport AWS_SECRET_ACCESS_KEY=\'${{ secrets.AWS_SECRET_ACCESS_KEY }}\'\nexport S3_BUCKET=\'mosaicml-internal-integration-testing\'\nexport COMMON_ARGS=""-v --durations=20 -m \'${{ matrix.markers }}\' --s3_bucket \'$S3_BUCKET\'""\n\n# Necessary to run git diff for doctests\ngit config --global --add safe.directory /__w/composer/composer\n\nmake test PYTEST=\'${{ matrix.pytest_command }}\' EXTRA_ARGS=""$COMMON_ARGS --codeblocks""\nmake test-dist PYTEST=\'${{ matrix.pytest_command }}\' EXTRA_ARGS=""$COMMON_ARGS"" WORLD_SIZE=2\n\npython -m coverage combine\n', ""set -ex\npython -m pip install --upgrade 'pip<23' wheel\npip install coverage[toml]==6.5.0\n"", 'set -ex\n\n# Flatten the coverage files\nls artifacts | while read x; do mv artifacts/$x/.coverage .coverage.$x; done\n\npython -m coverage combine\npython -m coverage report\n', 'set -ex\npython -m pip install --upgrade mosaicml-cli\nmcli init --mcloud\nmcli version\n', ""set -ex\npython .github/mcp/mcp_pytest.py --image '${{ matrix.container }}' --git_commit $GITHUB_SHA --pytest_markers '${{ matrix.markers }}' --pytest_command '${{ matrix.pytest_command }}'\n"", 'set -ex\npython -m pip install --upgrade mosaicml-cli\nmcli init --mcloud\nmcli version\n', 'set -ex\nexport PR_NUMBER=$(jq --raw-output .pull_request.number ""$GITHUB_EVENT_PATH"")\npython .github/mcp/mcp_pytest.py --image \'${{ matrix.container }}\' --pr_number $PR_NUMBER --pytest_markers \'${{ matrix.markers }}\' --pytest_command \'${{ matrix.pytest_command }}\' --timeout 1200\n']"
""
"['git config --global user.name ""$(git --no-pager log --format=format:\'%an\' -n 1)""\ngit config --global user.email ""$(git --no-pager log --format=format:\'%ae\' -n 1)""\n', 'make env', 'make freeze', '.github/scripts/benchmark-history.sh\n', 'make env', 'make freeze', '.github/scripts/benchmark.sh\n', 'make env', 'make freeze', 'make test', 'make env', 'make coveralls', 'make env', 'make test-tabular-only', 'make test-win', 'make env requirements dev-requirements && pip install scikit-learn==${{ matrix.scikit-version }}', 'make test args=tests/tabular/suites', 'pip3 install torch==1.10.2+cpu torchvision==0.11.3+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\npip install -r requirements/requirements.txt\npip install -r requirements/vision-requirements.txt\npip install -r requirements/nlp-requirements.txt\npip freeze > requirements-all.txt\n', 'echo ""${{ steps.license_check_report.outputs.report }}""', 'make env', 'make vision-gpu-tests', 'make env', 'make regenerate-examples', 'sudo apt-get update\nsudo apt-get install pandoc\n', 'make docs', 'set -uxo pipefail\n\ncurl \\\n  -L --output lychee-v0.8.2-x86_64-unknown-linux-gnu.tar.gz \\\n  https://github.com/lycheeverse/lychee/releases/download/v0.8.2/lychee-v0.8.2-x86_64-unknown-linux-gnu.tar.gz\n\ntar -xvzf lychee-v0.8.2-x86_64-unknown-linux-gnu.tar.gz\n\n./lychee \\\n  ""./**/*.rst"" ""./docs/build/html/**/*.html"" \\\n  --base ./docs/build/html \\\n  --accept=200,403,429 \\\n  --format markdown \\\n  --output .lychee.output \\\n  --exclude-loopback \\\n  --exclude-mail \\\n  --no-progress \\\n  --exclude-file docs/.lycheeignore\n\nexit_code=$?\n\necho\necho ""===== Lychee Output Report =====""\ncat .lychee.output\necho\n\nexit ${exit_code}\n', 'sudo apt-get update\nsudo apt-get install pandoc dvisvgm texlive texlive-latex-extra\n', 'make docs', 'make validate-examples', 'deepchecks/.github/scripts/push_docs.sh', 'make pylint', 'make docstring', 'make license-check', 'TAG=$(echo -n ""${{ github.event.ref }}"" | cut -d \'/\' -f3)\nmake release version=$TAG\n']"
"['mv docs docs_src\ncd docs_src\npip install -U sphinx karma-sphinx-theme\npip install -U numpy numba tqdm\npip install --upgrade -U pygments\nmake html\ncp -r _build/html ../docs\ncp ../.nojekyll ../docs/.nojekyll\necho docs.ffcv.io > ../docs/CNAME\ngit branch -D ghpages || echo ""branch exists""\ngit checkout -B ghpages\ncd ..\nrm -f .gitmodules\nrm -rf examples/imagenet-example\ngit config --global user.email ""ailyas@mit.edu""\ngit config --global user.name ""Andrew Ilyas""\ngit add --force docs\ngit commit -m generate docs\ngit push --force -u origin ghpages\n']"
""
""
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
""
"['python -m pip install --upgrade pip', 'pip3 install sentence-transformers\npip3 install nvidia-pyindex\n', 'pip3 install "".[CPU]"" --extra-index-url https://download.pytorch.org/whl/cpu --extra-index-url https://pypi.ngc.nvidia.c', 'make test_ci', 'echo ""::set-output name=version::$(cat VERSION)""', 'echo ${{ steps.getversion.outputs.version }} | grep -P ""^\\d+\\.\\d+\\.\\d+$""']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['pip install .\npip install pytest\npip install pytest-cov\npytest --cov=pythae ./ --cov-report=xml --runslow\n', 'pip install .\npip install pytest\npip install pytest-cov\npytest .']"
"['pip install --user feedparser', './.github/workflows/update-angr.sh']"
""
"['python -m pip install py-zabbix xmltodict xmlformatter ruamel.yaml\n./.github/workflows/check.py\n', './.github/workflows/check_templates.py\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""Action Bot""\ngit add --all\ngit commit -m \'Automatic fixed template\' || echo ""No changes to commit""\ngit push origin || echo ""No changes to commit""\n', 'python -m pip install xmltodict', './.github/workflows/update_readme.py', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""Action Bot""\ngit add README.md\ngit commit README.md -m \'Re-build README.md\' || echo ""No changes to commit""\ngit push origin || echo ""No changes to commit""\n']"
"['rm -rf /tmp/.buildx-cache\nmv /tmp/.buildx-cache-new /tmp/.buildx-cache\n', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E aws -E azure -E gcp', 'poetry run skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\npoetry run skyplane config set native_cmd_enabled false\ncat ~/.skyplane/config\npoetry run skyplane init -y --disable-config-cloudflare\npoetry run skyplane config set usage_stats false\n', 'poetry run python tests/integration/cp_local.py ${{ matrix.pairs }} --n-files 1 --file-size-mb 32', 'poetry run python tests/integration/cp_local.py ${{ matrix.pairs }} --n-files 128 --file-size-mb 1', 'poetry run python tests/integration/cp_local.py ${{ matrix.pairs }} --n-files 1 --file-size-mb 2000', 'gcloud iam service-accounts delete ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com', 'pip install skyplane[aws,azure,gcp]', 'skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\nskyplane init -y --disable-config-azure --disable-config-cloudflare\nskyplane config set usage_stats false\n', 'skyplane deprovision', 'gcloud iam service-accounts delete --quiet ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E aws -E azure -E gcp', 'poetry run skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\npoetry run skyplane config set native_cmd_enabled false\ncat ~/.skyplane/config\npoetry run skyplane init -y --disable-config-cloudflare\npoetry run skyplane config set usage_stats false\n', 'poetry run python tests/integration/cp.py ${{ matrix.pairs }} --n-files 1 --file-size-mb 32', 'poetry run python tests/integration/cp.py ${{ matrix.pairs }} --n-files 128 --file-size-mb 1', 'poetry run python tests/integration/cp.py ${{ matrix.pairs }} --n-files 1 --file-size-mb 2000', 'gcloud iam service-accounts delete ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com', 'pip install skyplane[aws,azure,gcp]', 'skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\nskyplane init -y --disable-config-azure --disable-config-cloudflare\nskyplane config set usage_stats false\n', 'skyplane deprovision', 'gcloud iam service-accounts delete --quiet ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E aws -E azure -E gcp', 'poetry run skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\npoetry run skyplane config set native_cmd_enabled false\ncat ~/.skyplane/config\npoetry run skyplane init -y\npoetry run skyplane config set usage_stats false\n', 'poetry run python tests/integration/cp.py ${{ matrix.pairs }}', 'gcloud iam service-accounts delete --quiet ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com', 'pip install skyplane[aws,azure,gcp]', 'skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\nskyplane init -y --disable-config-azure\nskyplane config set usage_stats false\n', 'skyplane deprovision', 'for pattern in ""test-skyplane-"" ""skyplane-integration-us-east-1-"" ""integrationus-east-1-""; do\n  aws s3api list-buckets --query ""Buckets[?starts_with(Name, \\`${pattern}\\`) == \\`true\\`].Name"" --output text | tr \'\\t\' \'\\n\' | while read bucket; do aws s3 rb ""s3://$bucket"" --force; done\ndone\n', 'gcloud iam service-accounts delete --quiet ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com', 'echo ${{ github.sha }}', 'test -z $(git rev-list  --after=""24 hours""  ${{ github.sha }}) && echo ""::set-output name=should_run::false""', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.2.1', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry config repositories.test-pypi https://test.pypi.org/legacy/\npoetry config pypi-token.test-pypi ${{ secrets.TEST_PYPI_API_TOKEN }}\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp\npoetry run pip install -r requirements-dev.txt\n', 'export RELEASEDATE=$(date +%Y%m%d)\necho ""gateway_version = \'nightly-$RELEASEDATE\'"" > skyplane/gateway_version.py\nsed -i \'s/name = ""skyplane""/name = ""skyplane-nightly""/g\' pyproject.toml\nsed -i ""s/version = \\""\\([0-9]\\+\\.[0-9]\\+\\.[0-9]\\+\\)\\(.*\\)\\""/version = \\""\\1.dev$RELEASEDATE\\""/"" pyproject.toml\npoetry build\n', 'poetry publish -r test-pypi', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.2.1', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry config pypi-token.pypi ${{ secrets.PYPI_API_TOKEN }}\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp\npoetry run pip install -r requirements-dev.txt\n', 'export RELEASEDATE=$(date +%Y%m%d)\necho ""gateway_version = \'nightly-$RELEASEDATE\'"" > skyplane/gateway_version.py\nsed -i \'s/name = ""skyplane""/name = ""skyplane-nightly""/g\' pyproject.toml\nsed -i ""s/version = \\""\\([0-9]\\+\\.[0-9]\\+\\.[0-9]\\+\\)\\(.*\\)\\""/version = \\""\\1.dev$RELEASEDATE\\""/"" pyproject.toml\npoetry build\n', 'poetry publish', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry config repositories.test-pypi https://test.pypi.org/legacy/\npoetry config pypi-token.test-pypi ${{ secrets.TEST_PYPI_API_TOKEN }}\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp\npoetry run pip install -r requirements-dev.txt\n', 'export SKYPLANEVERSION=`poetry version | awk \'END {print $NF}\'`\necho ""gateway_version = \'$SKYPLANEVERSION\'"" > skyplane/gateway_version.py\npoetry build\n', 'poetry publish -r test-pypi --dry-run', 'poetry publish -r test-pypi', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry config pypi-token.pypi ${{ secrets.PYPI_API_TOKEN }}\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp\npoetry run pip install -r requirements-dev.txt\n', 'sed -i \'s/name = ""skyplane""/name = ""skyplane-dev""/g\' pyproject.toml\nexport SKYPLANEVERSION=`poetry version | awk \'END {print $NF}\'`\necho ""gateway_version = \'$SKYPLANEVERSION\'"" > skyplane/gateway_version.py\npoetry build\n', 'poetry publish --dry-run', 'poetry publish', 'curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\npoetry config pypi-token.pypi ${{ secrets.PYPI_API_TOKEN }}\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp\npoetry run pip install -r requirements-dev.txt\n', 'export SKYPLANEVERSION=`poetry version | awk \'END {print $NF}\'`\necho ""gateway_version = \'$SKYPLANEVERSION\'"" > skyplane/gateway_version.py\npoetry build\n', 'poetry publish --dry-run', 'poetry publish', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp -E ibm\npoetry run pip install -r requirements-dev.txt\n', 'poetry run black . --check --line-length=140', 'poetry run pytype --config .pytype.cfg -j 2', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp -E ibm\npoetry run pip install -r requirements-dev.txt\n', 'poetry run skyplane init -y --disable-config-aws --disable-config-azure --disable-config-gcp --disable-config-ibm --disable-config-cloudflare\npoetry run skyplane config set usage_stats false\npoetry run pytest -s tests/unit_nocloud\n', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp -E ibm\npoetry run pip install -r requirements-dev.txt\npoetry run sudo apt install default-jdk\npoetry run wget https://archive.apache.org/dist/hadoop/core/hadoop-3.3.0/hadoop-3.3.0.tar.gz -P /tmp && tar -xzf /tmp/hadoop-3.3.0.tar.gz -C /tmp && sudo mv /tmp/hadoop-3.3.0 /usr/local/hadoop && rm /tmp/hadoop-3.3.0.tar.gz\n', 'poetry run skyplane init -y --disable-config-azure --disable-config-gcp --disable-config-ibm --disable-config-cloudflare\npoetry run skyplane config set usage_stats false\npoetry run pytest -s tests/unit_aws\n', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp -E ibm\npoetry run pip install -r requirements-dev.txt\n', 'poetry run pip freeze\npoetry run skyplane init -y --disable-config-aws --disable-config-gcp --disable-config-ibm --disable-config-cloudflare\npoetry run skyplane config set usage_stats false\npoetry run pytest -s tests/unit_azure\n', 'pipx install poetry', 'poetry config virtualenvs.in-project false\npoetry config virtualenvs.path ~/.virtualenvs\n', 'poetry install -E gateway -E solver -E aws -E azure -E gcp -E ibm\npoetry run pip install -r requirements-dev.txt\npoetry run sudo apt install default-jdk\npoetry run wget https://archive.apache.org/dist/hadoop/core/hadoop-3.3.0/hadoop-3.3.0.tar.gz -P /tmp && tar -xzf /tmp/hadoop-3.3.0.tar.gz -C /tmp && sudo mv /tmp/hadoop-3.3.0 /usr/local/hadoop && rm /tmp/hadoop-3.3.0.tar.gz\n', 'poetry run skyplane config set gcp_service_account_name ${{ env.STRATEGY_UUID }}\npoetry run skyplane init -y --disable-config-aws --disable-config-azure --disable-config-ibm --disable-config-cloudflare\npoetry run skyplane config set usage_stats false\npoetry run pytest -s tests/unit_gcs\n', 'gcloud iam service-accounts delete ${{ env.STRATEGY_UUID }}@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com']"
""
['pip install tox\ntox -- --cov matplotx --cov-report xml --cov-report term\n']
"['npm install', 'npm run build', 'pip install pyyaml', 'python sort.py\n', 'python create_yamls.py\n', 'git config --local user.name ${{ github.actor }}\ngit add --all\ngit commit -m ""Generating new YAMLs from the PR merge""\n', 'pip install pyyaml', 'python build_readme.py\n', 'git config --local user.name ${{ github.actor }}\ngit add --all\ngit commit -m ""Rebuild README after merging PR with a new YAML""\n', 'pip install pyyaml', 'python sort.py\n', 'python create_yamls.py\n', 'git config --local user.name ${{ github.actor }}\ngit add --all\ngit commit -m ""Generating new YAMLs from the direct push to README""\n', 'pip install pyyaml', 'python build_readme.py\n', 'git config --local user.name ${{ github.actor }}\ngit add --all\ngit commit -m ""Rebuild README after adding new YAML""\n']"
""
"['pip install nbdev', 'echo ""Check we are starting with clean git checkout""\nif [ -n ""$(git status -uno -s)"" ]; then echo ""git status is not clean""; false; fi\necho ""Trying to strip out notebooks""\nnbdev_clean\necho ""Check that strip out was unnecessary""\ngit status -s # display the status to see which nbs need cleaning up\nif [ -n ""$(git status -uno -s)"" ]; then echo -e ""!!! Detected unstripped out notebooks\\n!!!Remember to run nbdev_install_git_hooks""; false; fi\n', 'pip install ./', ""nbdev_test --skip_file_re '(models|distributed|ets).*.ipynb' --pause 1.0"", ""nbdev_test --skip_file_re '(distributed).*.ipynb' --pause 1.0"", 'pip install "".[dev]"" pytest\npytest action_files\n', 'pip install "".[dev]"" fire', 'python -m src.experiment\npython -m src.evaluation --test\n', 'pip install mypy types-setuptools flake8', 'mypy statsforecast/', 'flake8 --select=F statsforecast/', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
['echo APP_VERSION=`git describe --tags --always` >> $GITHUB_ENV']
"['poetry install', 'poetry run autoflake -ir $(find -iname ""*.py"" ! -name __init__.py) --remove-all-unused-imports', 'poetry run isort --profile=black .', 'poetry run black .', 'poetry install', ""git config user.name 'github-actions[bot]'\ngit config user.email 'github-actions[bot]@users.noreply.github.com'\n"", 'poetry run python -m mkdocs gh-deploy --force', 'poetry install --only main', 'npm install', 'poetry run npx pyright', 'poetry install', 'pip install pytest coverage', 'poetry run pytest -v']"
"['python -m pip install tox', 'tox -e bandit-scan', 'python -m pip install tox', 'tox -vv -e trivy-scan', 'pip install -r requirements/docs.txt\npip install .[full]\n', 'ln -s $ANOMALIB_DATASET_PATH ./datasets', 'cd docs\nmake html\n', 'echo ::set-output name=SOURCE_NAME::${GITHUB_REF#refs/*/}\necho ::set-output name=SOURCE_BRANCH::${GITHUB_REF#refs/heads/}\necho ::set-output name=SOURCE_TAG::${GITHUB_REF#refs/tags/}\n\nexisted_in_remote=$(git ls-remote --heads origin gh-pages)\n\nif [[ -z ${existed_in_remote} ]]; then\n  echo ""Creating gh-pages branch""\n  git config --local user.email ""action@github.com""\n  git config --local user.name ""GitHub Action""\n  git checkout --orphan gh-pages\n  git reset --hard\n  touch .nojekyll\n  git add .nojekyll\n  git commit -m ""Initializing gh-pages branch""\n  git push origin gh-pages\n  git checkout ${{steps.branch_name.outputs.SOURCE_NAME}}\n  echo ""Created gh-pages branch""\nelse\n  echo ""Branch gh-pages already exists""\nfi\n', 'git fetch\ngit checkout gh-pages\nmkdir -p /tmp/docs_build\ncp -r docs/build/html/* /tmp/docs_build/\nrm -rf ./*\ncp -r /tmp/docs_build/* ./\nrm -rf /tmp/docs_build\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""Update documentation"" -a || true\n', 'pip install tox', 'tox -e nightly', 'pip install tox', 'tox -e pre-commit', 'pip install tox', 'ln -s $ANOMALIB_DATASET_PATH ./datasets\nln -s $ANOMALIB_DATASET_PATH ./notebooks/datasets\n', 'tox -e pre-merge-${{ matrix.tox-env }}', '# If the workflow is triggered from PR then it gets the commit id from the PR.\n# else it uses the commit id of the latest commit. This is because the commit\n# of the checked-out branch/commit does not exist in the tree as it is grafted.\nif [ -n ""${{ github.event.pull_request.head.sha }}"" ]\nthen\n  COMMIT_ID=${{ github.event.pull_request.head.sha }}\nelse\n  COMMIT_ID=${{ github.sha }}\nfi\n# Pass token from secrets if available. Otherwise it takes it from the environment variable of the CI\ncurl -Os https://uploader.codecov.io/latest/linux/codecov\nchmod +x codecov\nif [ -n ""${{ secrets.CODECOV_TOKEN }}"" ]\nthen\n  ./codecov -t ${{ secrets.CODECOV_TOKEN }} --sha $COMMIT_ID -U $HTTP_PROXY -f .tox/coverage.xml\nelse\n  ./codecov -t ""${CODECOV_TOKEN}""  --sha $COMMIT_ID -U $HTTP_PROXY -f .tox/coverage.xml\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
""
"['pip install -e .[test] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n', 'export FLAX_LAZY_RNG=no\npytest\n', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
""
"['sudo apt-get -y install libsndfile1-dev\npip install pytest\ngit clone --branch=main https://github.com/google-research/t5x\ncd t5x\npython3 -m pip install -e . -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\ncd ..\npip install -e .\n', 'export FLAX_LAZY_RNG=no\npytest mt3/\n', 'status=""${{ job.status }}""\nlowercase_status=$(echo $status | tr \'[:upper:]\' \'[:lower:]\')\ncurl -sS --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n    ""state"": ""\'$lowercase_status\'"",\n    ""target_url"": ""https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"",\n    ""description"": ""\'$status\'"",\n    ""context"": ""github-actions/build""\n    }\'\n']"
"['pip install -r requirements.txt\npip install basicsr\n', 'python -m unittest']"
""
"['npm install', 'npm install', 'npm run build', 'mvn --batch-mode --update-snapshots -f ./java/pom.xml package', 'npm install', 'npm link', 'npm run build --if-present', 'npm test', 'python -m pip install --upgrade pip\npython -m pip install "".[dev]""\n', 'pytest ./tests', 'pip install jax[cpu]', 'pytest ./scripts/tests', 'pip install --upgrade pip\npip install "".[dev]""\npip install jax[cpu]\n', 'isort --diff --check .\n', 'yapf --diff --recursive budoux tests scripts\n', 'mypy budoux tests scripts\n', 'npm install', 'npm run lint']"
"['cd web-vue && npm install && npm run build\n', 'mkdir -p ~/.ssh\necho ""${{ env.SSH_PRIVATE_KEY }}"" | tr -d \'\\r\' > ~/.ssh/id_rsa\nchmod 700 ~/.ssh/id_rsa\nssh-keyscan -H ""${{ env.REMOTE_HOST }}"" >> ~/.ssh/known_hosts\nssh -o StrictHostKeyChecking=no ""${{ env.REMOTE_USER }}@${{ env.REMOTE_HOST }}"" ""sudo systemctl restart myapp""\n', 'python -m venv venv\nsource venv/bin/activate\n', 'pip install -r requirements.txt', 'python update_cache.py --issue ${{ toJson(github.event.issue.body) }}\npython maintain.py\n', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""github-actions""\ngit add --all\nif [-z ""$(git status --porcelain)""]; then\n   echo ""::set-output name=push::false""\nelse\n   git commit -m ""chore: update cache"" -a\n   echo ""::set-output name=push::true""\nfi\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi\n', 'python maintain.py force']"
"['mkdir mmhuman3d_download\ncd mmhuman3d_download\nwget -O mmhuman3d.7z -q https://openmmlab-share.oss-cn-hangzhou.aliyuncs.com/mmhuman3d/mmhuman3d.7z\n7za x mmhuman3d.7z\nls -l\ncd ..\ncp -r mmhuman3d_download/mmhuman3d/* ./\nrm -rf mmhuman3d_download\n', 'pip install pip --upgrade', 'conda install ffmpeg\nffmpeg -version\n', 'conda install pytorch==${{matrix.torch}} torchvision==${{matrix.torchvision}} torchaudio==${{matrix.torchaudio}} cudatoolkit=10.2 -c pytorch\n', 'conda install -c fvcore -c iopath -c conda-forge fvcore iopath -y\npip install ""git+https://github.com/facebookresearch/pytorch3d.git""\n', 'pip install ""mmcv-full>=1.3.17,<=1.5.3"" -f https://download.openmmlab.com/mmcv/dist/cpu/torch${{matrix.torch}}/index.html\npython -c \'import mmcv; print(mmcv.__version__)\'\n', 'pip install openpifpaf==${{matrix.openpifpaf}}', 'pip install -r requirements.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --source mmhuman3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'sudo apt-add-repository ppa:brightbox/ruby-ng -y\nsudo apt-get update\nsudo apt-get install -y ruby2.7\npip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -vinmMI --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" -f 80 mmhuman3d/\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
""
"['python -m pip install --upgrade pip\npip install torch --index-url https://download.pytorch.org/whl/cpu\npip install torch_geometric\npip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\npip install pytest\npip install coverage\npip install coveralls\n', 'coverage run --source=pygod -m pytest\n', 'coveralls --service=github\n', 'python -m pip install --upgrade pip\npip install torch --index-url https://download.pytorch.org/whl/cpu\npip install torch_geometric\npip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\npip install pytest\npip install coverage\npip install coveralls\n', 'coverage run --source=pygod -m pytest\n', 'coveralls --service=github\n']"
""
"['pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', ""pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'pip install -r requirements/tests.txt -r requirements/optional.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmflow -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender-dev libxext6', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', ""python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'python -m pip install -r requirements/tests.txt -r requirements/optional.txt', 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n', 'coverage run --branch --source mmflow -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender-dev libxext6 git', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', ""python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'python -m pip install -r requirements/tests.txt -r requirements/optional.txt', 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n', 'coverage run --branch --source mmflow -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install opencv-python>=3', 'pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n', 'pip install -r requirements/tests.txt -r requirements/optional.txt', 'pip install -e .', 'coverage run --branch --source mmflow -m pytest tests/\n', 'coverage xml\ncoverage report -m\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 90 mmflow\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmflow']"
"['python -m pip install --upgrade pip\npip install flake8 isort yapf interrogate\n', 'flake8 .', 'isort --recursive --check-only --diff mmdet3d/ tests/ examples/', 'yapf -r -d mmdet3d/ tests/ examples/', 'interrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --exclude mmdet3d/ops --ignore-regex ""__repr__"" --fail-under 95 mmdet3d', 'apt-get update && apt-get install -y ffmpeg libsm6 git ninja-build libglib2.0-0 libsm6 libxrender-dev python${{matrix.python-version}}-dev\napt-get clean\nrm -rf /var/lib/apt/lists/*\n', 'python -m pip install Pillow==6.2.2', 'python -m pip install numpy==1.19.5 torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', 'python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\npython -m pip install mmdet==2.14.0\npython -m pip install mmsegmentation==0.14.1\npython -m pip install -r requirements.txt\n', 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=${CUDA_ARCH} python setup.py build_ext --inplace\n', 'coverage run --branch --source mmdet3d -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install torch', 'python setup.py sdist', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n']"
"['sudo apt-get install python3-sphinx', 'poetry --version', 'poetry install', 'rm -rf docs/plotly_resampler && cd docs/sphinx && poetry run sphinx-autogen -o _autosummary *.rst && poetry run make clean html && mv -f _build/html ../plotly_resampler && rm -rf _build _autosummary && cd ../../', '.github/deploy-gh-pages.sh', 'poetry --version', 'poetry install --all-extras', 'make lint', 'poetry --version', 'poetry install --all-extras', 'poetry run pytest --cov=plotly_resampler --junitxml=junit/test-results-${{ matrix.python-version }}.xml --cov-report=xml tests\n']"
""
['cd docs\nmkdir build\npip install -r requirements.txt\nmake html\ntouch build/html/.nojekyll\n']
"['output=$(python3 .github/release_utils.py --release-type ${{ github.event.inputs.name }})\necho $output\nnew_version=$(echo $output | awk \'{print $1}\')\nnew_tag=$(echo $output | awk \'{print $2}\')\necho ""new version is $new_version""\necho ""new tag is $new_tag""\necho ::set-output name=version::$new_version\necho ::set-output name=tag::$new_tag\n', 'python -m pip install wheel', 'echo ""current folder = $PWD""\necho ""current branch = $(git branch --show-current)""\noutput=$(python3 .github/release_utils.py --release-type ${{ github.event.inputs.name }} --update-version)\n', 'python setup.py sdist bdist_wheel', 'sudo apt-get install -y \\\n    libgl1-mesa-dev \\\n    libgl1-mesa-glx \\\n    libglew-dev \\\n    libosmesa6-dev \\\n    software-properties-common\nsudo apt-get install -y patchelf\n', 'brew install gcc@9', 'echo ""Test MyoSuite PyPI wheel""\npip install -U myosuite\npython -m myosuite.tests.test_myo\n', 'sudo apt-get install -y \\\n    libgl1-mesa-dev \\\n    libgl1-mesa-glx \\\n    libglew-dev \\\n    libosmesa6-dev \\\n    software-properties-common\nsudo apt-get install -y patchelf\n', 'conda info\nCONDA_DEFAULT_ENV=test_myosuite\nconda create --name $CONDA_DEFAULT_ENV python=3.7.1 -y\nconda activate $CONDA_DEFAULT_ENV\n', 'python -m pip install --upgrade pip\npip install -e .\n', 'python -m myosuite.tests.test_myo\n', 'sudo apt-get install --no-install-recommends ffmpeg && pip install ffmpeg scikit-video\n', 'python -m myosuite.utils.examine_env -e myoElbowPose1D6MRandom-v0 -r offscreen -n 1', 'source myosuite/tests/test_tutorials.sh\n']"
"['pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', ""pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${{matrix.mmcv}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'pip install mmcls mmdet', 'pip install -r requirements/tests.txt -r requirements/optional.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmfewshot -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender-dev libxext6', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', ""python -V\npython -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch${{matrix.mmcv}}/index.html\npython -m pip install -r requirements.txt\npython -c 'import mmcv; print(mmcv.__version__)'\npython -m pip install mmcls mmdet\n"", 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 pip install .\n', 'coverage run --branch --source mmfewshot -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\n', 'apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends python${{matrix.python-version}}-dev', 'apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6\napt-get clean\nrm -rf /var/lib/apt/lists/*\n', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', 'python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake', ""python -V\npython -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/torch${{matrix.mmcv}}/index.html\npython -m pip install -r requirements.txt\npython -c 'import mmcv; print(mmcv.__version__)'\npython -m pip install mmcls mmdet\n"", 'rm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 pip install .\n', 'coverage run --branch --source mmdet -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install pip --upgrade --user', 'pip install opencv-python>=3', 'pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n', 'pip install mmcls mmdet', 'pip install -r requirements/tests.txt -r requirements/optional.txt', 'pip install -e .', 'python -m pip install timm\ncoverage run --branch --source mmfewshot -m pytest tests/\n', 'coverage xml\ncoverage report -m\n', '# markdownlint requires ruby >= 2.7\nsudo apt-add-repository ppa:brightbox/ruby-ng -y\nsudo apt-get update\nsudo apt-get install -y ruby2.7\npip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 80 mmfewshot\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmfewshot']"
"['conda create --name testenv python=3.8\nconda activate testenv\nconda --version\npython --version\n', 'conda install -c conda-forge -n testenv mljar-mercury', 'mercury run', 'sudo apt-get update\npython -m pip install --upgrade pip\n./scripts/pack_mercury.sh\npip install --upgrade setuptools\ncd mercury\npip install -U -r requirements.txt\n', 'cd mercury\npython manage.py test apps\n']"
"['for i in ${{ env.list }};\ndo\n  curl https://raw.githubusercontent.com/NCSC-NL/log4shell/main/software/software_list_${i}.md -o software_list_main_${i}.md\ndone\n', 'pwd && ls -la', 'for i in ${{ env.list }};\ndo\n  (git diff --no-index software_list_main_${i}.md software/software_list_${i}.md || true ) > diff_${i}.log\ndone\n', 'ls -la', 'cat diff_*.log', 'python3 .github/check_valid.py `ls diff_*.log`', 'pip install -r ${{ github.workspace }}/tools/log4shell_softwarelist/requirements.txt', 'python ${{ github.workspace }}/tools/log4shell_softwarelist/softwarelist_parser.py --path ${{ github.workspace }}/software/ csv software_list.csv', 'python ${{ github.workspace }}/tools/log4shell_softwarelist/softwarelist_parser.py --path ${{ github.workspace }}/software/ json software_list.json', 'echo ""::set-output name=date::$(date +\'%Y%m%d\')""', 'pip install -r ${{ github.workspace }}/tools/log4shell_softwarelist/requirements.txt', 'python ${{ github.workspace }}/tools/log4shell_softwarelist/softwarelist_parser.py --path ${{ github.workspace }}/software/ csv software_list.csv', 'python ${{ github.workspace }}/tools/log4shell_softwarelist/softwarelist_parser.py --path ${{ github.workspace }}/software/ json software_list.json']"
"['rm -rf ./dist', 'python ./src/generate.py', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add -A\nif git commit -m ""data: Generate dist by GitHub Actions""; then\n  git push\nfi\n']"
""
""
"['sudo apt update', 'sudo apt update\nsudo apt install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev libc++1-9 libc++abi1-9\nsudo apt install libopencv-dev\n', 'mkdir -p $GITHUB_WORKSPACE/Ascend\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%205.1.RC2/Ascend-cann-toolkit_5.1.RC2_linux-x86_64.run\nsh Ascend-cann-toolkit_5.1.RC2_linux-x86_64.run --install --install-path=$GITHUB_WORKSPACE/Ascend --quiet --chip=Ascend310 --blacklist=devtools\n', 'mkdir -p build && pushd build\nsource $GITHUB_WORKSPACE/Ascend/ascend-toolkit/set_env.sh\nexport LD_LIBRARY_PATH=$GITHUB_WORKSPACE/Ascend/ascend-toolkit/latest/runtime/lib64/stub:$LD_LIBRARY_PATH\ncmake .. -DMMDEPLOY_BUILD_SDK=ON -DMMDEPLOY_BUILD_EXAMPLES=ON -DMMDEPLOY_TARGET_BACKENDS=acl\nmake install -j4\n', 'wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/opencv/opencv-osx-arm64-4.6.0.tar.gz\nmkdir $GITHUB_WORKSPACE/opencv-install\ntar xf opencv-osx-arm64-4.6.0.tar.gz -C $GITHUB_WORKSPACE/opencv-install\n', 'wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/libtorch/libtorch-osx-arm64-1.8.0.tar.gz\nmkdir $GITHUB_WORKSPACE/libtorch-install\ntar xf libtorch-osx-arm64-1.8.0.tar.gz -C $GITHUB_WORKSPACE/libtorch-install\n', 'mkdir build && cd build\ncmake .. -DCMAKE_OSX_ARCHITECTURES=""arm64"" \\\n  -DCMAKE_SYSTEM_PROCESSOR=""arm64"" \\\n  -DMMDEPLOY_BUILD_SDK=ON \\\n  -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n  -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n  -DMMDEPLOY_TARGET_BACKENDS=""coreml"" \\\n  -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n  -DMMDEPLOY_BUILD_SDK_MONOLITHIC=OFF \\\n  -DMMDEPLOY_SHARED_LIBS=OFF\ncmake --build . -j 3\ncmake --build . --target install\n', 'mkdir build-shared && cd build-shared\ncmake .. -DCMAKE_OSX_ARCHITECTURES=""arm64"" \\\n  -DCMAKE_SYSTEM_PROCESSOR=""arm64"" \\\n  -DMMDEPLOY_BUILD_SDK=ON \\\n  -DMMDEPLOY_TARGET_DEVICES=""cpu"" \\\n  -DMMDEPLOY_CODEBASES=all \\\n  -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n  -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n  -DMMDEPLOY_TARGET_BACKENDS=""coreml"" \\\n  -DMMDEPLOY_BUILD_SDK_MONOLITHIC=ON \\\n  -DMMDEPLOY_SHARED_LIBS=OFF \\\n  -DMMDEPLOY_BUILD_EXAMPLES=ON\ncmake --build . -j 3\ncmake --build . --target install\n', 'sudo apt update\nsudo apt install wget gcc-multilib g++-multilib wget libprotobuf-dev protobuf-compiler\npython -m pip install cmake onnx\n', 'wget https://github.com/Tencent/ncnn/archive/refs/tags/20220420.tar.gz\ntar xf 20220420.tar.gz\npushd ncnn-20220420\nmkdir build && pushd build\ncmake -DCMAKE_INSTALL_PREFIX=$(pwd)/../install -DNCNN_BUILD_TESTS=OFF -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF ..\ncmake --build . -j2\nmake install\npopd && popd\n', 'mkdir -p build && pushd build\nexport LD_LIBRARY_PATH=/home/runner/work/mmdeploy/mmdeploy/ncnn-20220420/install/lib/:$LD_LIBRARY_PATH\ncmake -DMMDEPLOY_TARGET_BACKENDS=ncnn -Dncnn_DIR=/home/runner/work/mmdeploy/mmdeploy/ncnn-20220420/install/lib/cmake/ncnn/ ..\nmake mmdeploy_onnx2ncnn -j2\npopd\n', 'echo $(pwd)\nln -s build/bin/mmdeploy_onnx2ncnn ./\npython .github/scripts/test_onnx2ncnn.py --run 1\n', ""python -m pip install torch==1.8.2 torchvision==0.9.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cpu\npython -m pip install mmcv-lite\npython tools/scripts/build_ubuntu_x64_ncnn.py 8\npython -c 'import mmdeploy.apis.ncnn as ncnn_api; assert ncnn_api.is_available(with_custom_ops=True)'\n"", 'apt-get update\napt-get install ninja-build -y\npython3 -V\npython3 -m pip install openmim\npython3 -m pip install -r requirements.txt\npython3 -m mim install $(cat requirements/codebases.txt | grep mmpretrain)\npython3 -m pip install numpy==1.22.0\npython3 -m pip list\n', 'rm -rf .eggs && python3 -m pip install -e .\npython3 tools/check_env.py\n', 'git clone -b v0.6.6 --depth 1 https://github.com/openppl-public/ppq\ncd ppq\npython3 -m pip install -r requirements.txt\npython3 setup.py install\n', 'export PYTHONPATH=${PWD}/ppq:${PYTHONPATH}\nexport LD_LIBRARY_PATH=""/root/workspace/mmdeploy/build/lib:${LD_LIBRARY_PATH}""\nexport LD_LIBRARY_PATH=""/root/workspace/mmdeploy/mmdeploy/lib:${LD_LIBRARY_PATH}""\nexport work_dir=./work_dir\nmkdir -p $work_dir\nexport model_cfg=$work_dir/resnet18_8xb32_in1k.py\nexport deploy_cfg=configs/mmpretrain/classification_ncnn-int8_static.py\nexport checkpoint=$work_dir/resnet18_8xb32_in1k_20210831-fbbb1da6.pth\nexport input_img=tests/data/tiger.jpeg\npython3 -m mim download mmpretrain --config resnet18_8xb32_in1k --dest $work_dir\npython3 tools/torch2onnx.py $deploy_cfg $model_cfg $checkpoint $input_img --work-dir $work_dir\nwget https://media.githubusercontent.com/media/tpoisonooo/mmdeploy-onnx2ncnn-testdata/main/dataset.tar\ntar xvf dataset.tar\npython3 tools/onnx2ncnn_quant_table.py \\\n  --onnx $work_dir/end2end.onnx \\\n  --deploy-cfg $deploy_cfg \\\n  --model-cfg $model_cfg \\\n  --out-onnx $work_dir/quant.onnx \\\n  --out-table $work_dir/ncnn.table \\\n  --image-dir ./dataset\nls -sha $work_dir/quant.onnx\ncat $work_dir/ncnn.table\n', ""python -m pip install torch==1.8.2 torchvision==0.9.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cpu\npython -m pip install mmcv-lite openmim\npython tools/scripts/build_ubuntu_x64_ort.py 8\npython -c 'import mmdeploy.apis.onnxruntime as ort_api; assert ort_api.is_available(with_custom_ops=True)'\n"", 'python -m mim install $(cat requirements/codebases.txt | grep mmpretrain)\nexport MMDEPLOY_DIR=$(pwd)\nexport ONNXRUNTIME_DIR=$MMDEPLOY_DIR/../mmdeploy-dep/onnxruntime-linux-x64-1.8.1\nexport LD_LIBRARY_PATH=$ONNXRUNTIME_DIR/lib:$MMDEPLOY_DIR/build/install/lib:$LD_LIBRARY_PATH\nbash .github/scripts/linux/test_full_pipeline.sh ort cpu\n', 'apt update && apt install unzip wget\npython3 -V\npython3 -m pip install openmim numpy\npython3 -m pip install -r requirements.txt\npython3 -m mim install $(cat requirements/codebases.txt | grep mmpretrain)\npython3 -m pip list\n', 'bash .github/scripts/linux/build.sh ""cuda"" ""pplnn"" \\\n    -Dpplcv_DIR=${pplcv_DIR} \\\n    -Dpplnn_DIR=${pplnn_DIR}\nls build/lib\n', 'rm -rf .eggs && python3 -m pip install -e .\nexport LD_LIBRARY_PATH=""/root/workspace/mmdeploy/build/lib:${LD_LIBRARY_PATH}""\npython3 tools/check_env.py\npython3 -c \'import mmdeploy.apis.pplnn as pplnn_api; assert pplnn_api.is_available()\'\n', ""python -m pip install torch==1.8.2 torchvision==0.9.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cpu\npython -m pip install mmcv-lite protobuf==3.20.2\npython tools/scripts/build_ubuntu_x64_pplnn.py 8\npython -c 'import mmdeploy.apis.pplnn as pplnn_api; assert pplnn_api.is_available()'\n"", 'sudo apt update', 'sh -xe tools/scripts/ubuntu_cross_build_rknn.sh rk3588\n', 'sudo apt update', 'sh -xe tools/scripts/ubuntu_cross_build_rknn.sh rv1126\n', 'sudo apt update', 'sudo apt install wget libprotobuf-dev protobuf-compiler\nsudo apt update\nsudo apt install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev libc++1-9 libc++abi1-9\nsudo apt install libopencv-dev\n', 'wget https://media.githubusercontent.com/media/tpoisonooo/mmdeploy_snpe_testdata/main/snpe-1.59.tar.gz\ntar xf snpe-1.59.tar.gz\npushd snpe-1.59.0.3230\npwd\npopd\n', 'mkdir -p build && pushd build\nexport SNPE_ROOT=/home/runner/work/mmdeploy/mmdeploy/snpe-1.59.0.3230\nexport LD_LIBRARY_PATH=${SNPE_ROOT}/lib/x86_64-linux-clang:${LD_LIBRARY_PATH}\nexport MMDEPLOY_SNPE_X86_CI=1\ncmake .. -DMMDEPLOY_BUILD_SDK=ON -DMMDEPLOY_TARGET_BACKENDS=snpe\nmake -j2\nmake install\npushd install/example\nmkdir build && pushd build\ncmake ../cpp -DMMDeploy_DIR=${PWD}/../../lib/cmake/MMDeploy\nmake -j2\nls ./*\npopd\npopd\npopd\n', ""python -m pip install torch==1.11.0 torchvision==0.12.0 --extra-index-url https://download.pytorch.org/whl/lts/1.11/cpu\npython -m pip install mmcv-lite\npython tools/scripts/build_ubuntu_x64_torchscript.py 8\npython -c 'from mmdeploy.backend.torchscript import is_available; assert is_available()'\n"", 'python -m pip install --upgrade pip\npython -V\npython -m pip show pip\npython -m pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -m pip install openmim\npython -m pip install -r requirements.txt\npython -m pip install -r requirements/backends.txt\npython -m mim install ""mmcv>=2.0.0""\npython -m mim install -r requirements/codebases.txt\npython -m pip install clip numba transformers numpy==1.23\npython -m pip list\n', 'git clone -b dev --depth 1 https://github.com/open-mmlab/mmyolo.git /home/runner/work/mmyolo\npython -m pip install -v -e /home/runner/work/mmyolo\n', 'rm -rf .eggs && python -m pip install -e .\npython tools/check_env.py\n', 'coverage run --branch --source mmdeploy -m pytest -rsE tests\ncoverage xml\ncoverage report -m\n', 'python -m pip install xdoctest\ncd /home/runner/work/mmyolo\npytest tests/test_deploy\n', 'sudo apt update', 'sudo apt install gcc-multilib g++-multilib wget libprotobuf-dev protobuf-compiler\nsudo apt update\nsudo apt install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev libc++1-9 libc++abi1-9\nsudo apt install libopencv-dev lcov wget\n', 'mkdir -p build && pushd build\ncmake .. -DCMAKE_CXX_COMPILER=g++  -DMMDEPLOY_CODEBASES=all    -DMMDEPLOY_BUILD_SDK=ON     -DMMDEPLOY_BUILD_SDK_PYTHON_API=OFF     -DMMDEPLOY_TARGET_DEVICES=cpu   -DMMDEPLOY_COVERAGE=ON  -DMMDEPLOY_BUILD_TEST=ON\nmake -j2\nmkdir -p mmdeploy_test_resources/transform\ncp ../tests/data/tiger.jpeg mmdeploy_test_resources/transform/\n./bin/mmdeploy_tests\nlcov --capture --directory . --output-file coverage.info\nls -lah coverage.info\ncp coverage.info ../\n', 'sudo apt update', 'sh -ex tools/scripts/ubuntu_cross_build_aarch64.sh\n', 'apt-key adv --keyserver keyserver.ubuntu.com --recv-keys A4B469963BF863CC\napt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev\napt-get clean\nrm -rf /var/lib/apt/lists/*\n', 'python -V\npython -m pip install --upgrade pip\npython -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -V\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\npython -m pip install openmim\npython -m pip install -r requirements.txt\npython -m pip install -r requirements/backends.txt\npython -m mim install ""mmcv>=2.0.0rc1""\nCFLAGS=$CFLAGS python -m mim install -r requirements/codebases.txt\npython -m pip install -U pycuda numpy clip numba transformers\npython -m pip list\n', 'rm -rf .eggs && python -m pip install -e .\npython tools/check_env.py\n', 'coverage run --branch --source mmdeploy -m pytest -rsE tests\ncoverage xml\ncoverage report -m\n', 'apt-key adv --keyserver keyserver.ubuntu.com --recv-keys A4B469963BF863CC\napt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libxrender-dev\napt-get clean\nrm -rf /var/lib/apt/lists/*\n', 'python -V\npython -m pip install --upgrade pip\npython -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n', 'python -V\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\npython -m pip install openmim\npython -m pip install -r requirements.txt\npython -m pip install -r requirements/backends.txt\npython -m mim install ""mmcv>=2.0.0rc1""\npython -m mim install -r requirements/codebases.txt\npython -m pip install -U pycuda numpy clip numba transformers\npython -m pip list\n', 'rm -rf .eggs && python -m pip install -e .\npython tools/check_env.py\n', 'coverage run --branch --source mmdeploy -m pytest -rsE tests\ncoverage xml\ncoverage report -m\n', 'apt update && apt install unzip\npython3 -V\npython3 -m pip install opencv-python==4.5.4.60 opencv-python-headless==4.5.4.60 opencv-contrib-python==4.5.4.60\npython3 -m pip install openmim numpy pycuda clip transformers\npython3 -m pip install -r requirements.txt\npython3 -m mim install $(cat requirements/codebases.txt | grep mmpretrain)\npython3 -m pip list\n', 'export Torch_DIR=$(python3 -c ""import torch;print(torch.utils.cmake_prefix_path + \'/Torch\')"")\nbash .github/scripts/linux/build.sh ""cpu;cuda"" ""ort;trt;ncnn;torchscript"" \\\n    -Dpplcv_DIR=${pplcv_DIR} \\\n    -DTENSORRT_DIR=${TENSORRT_DIR} \\\n    -DONNXRUNTIME_DIR=${ONNXRUNTIME_DIR} \\\n    -Dncnn_DIR=${ncnn_DIR} \\\n    -DTorch_DIR=${Torch_DIR}\nls build/lib\n', 'rm -rf .eggs && python3 -m pip install -e .\nexport LD_LIBRARY_PATH=""/root/workspace/mmdeploy/build/lib:${LD_LIBRARY_PATH}""\npython3 tools/check_env.py\n', 'export LD_LIBRARY_PATH=""/root/workspace/mmdeploy/build/lib:${LD_LIBRARY_PATH}""\nexport LD_LIBRARY_PATH=""/root/workspace/mmdeploy/mmdeploy/lib:${LD_LIBRARY_PATH}""\nbash .github/scripts/linux/test_full_pipeline.sh trt cuda\n', 'echo ""============================== Info ==============================""\necho ""env:path= $env:path""\necho ""============================== Info ==============================""\nconda info\nconda info -e\n$env:TEMP_ENV = ""$pwd/../temp_envs/$env:GITHUB_RUN_ID""\nNew-Item -Path ""$env:TEMP_ENV"" -ItemType Directory -Force\necho ""TEMP_ENV=$env:TEMP_ENV"" >> $env:GITHUB_ENV\nconda create -p $env:TEMP_ENV --clone $env:BASE_ENV -y\nconda activate $env:TEMP_ENV\npython -V\npython -m pip install openmim\npython -m pip install -r requirements.txt\npython -m pip install -r requirements/backends.txt\npython -m mim install ""mmpretrain>=1.0.0rc7""\npython -m pip list\n', 'conda activate $env:TEMP_ENV\npython -V\nmkdir build\ncd build\ncmake ..  -A x64 -T v142 `\n  -DMMDEPLOY_BUILD_TEST=ON `\n  -DMMDEPLOY_BUILD_SDK_CSHARP_API=ON `\n  -DMMDEPLOY_BUILD_SDK_PYTHON_API=ON `\n  -DMMDEPLOY_BUILD_SDK=ON `\n  -DMMDEPLOY_TARGET_DEVICES=\'cuda\' `\n  -DMMDEPLOY_TARGET_BACKENDS=\'ort;trt\' `\n  -DMMDEPLOY_CODEBASES=\'all\' `\n  -Dpplcv_DIR=""$env:PPLCV_DIR\\pplcv-build\\install\\lib\\cmake\\ppl"" `\n  -DOpenCV_DIR=""$env:OPENCV_DIR\\build\\x64\\vc15\\lib"" `\n  -DTENSORRT_DIR=""$env:TENSORRT_DIR"" `\n  -DONNXRUNTIME_DIR=""$env:ONNXRUNTIME_DIR"" `\n  -DMMDEPLOY_BUILD_EXAMPLES=ON `\n  -DCUDNN_DIR=""$env:CUDNN_DIR""\ncmake --build . --config Release -- /m\ncmake --install . --config Release\nls $pwd\\bin\\Release\n', 'conda activate $env:TEMP_ENV\npython -m pip install -e .\npython .\\tools\\check_env.py\n', 'conda activate $env:TEMP_ENV\n$env:path = ""$pwd\\build\\bin\\Release;"" + $env:path\n.github\\scripts\\windows\\test_full_pipeline.ps1 -Backend trt -Device cuda\n', 'conda env remove --prefix ""$env:TEMP_ENV""\n', 'echo ""MMDEPLOY_VERSION=main""  >> $GITHUB_ENV\necho ""TAG=$TAG_PREFIX""  >> $GITHUB_ENV\n', 'export MMDEPLOY_VERSION=$(python3 -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $MMDEPLOY_VERSION\necho ""TAG=${TAG_PREFIX}${MMDEPLOY_VERSION}""  >> $GITHUB_ENV\necho ""MMDEPLOY_VERSION=v$MMDEPLOY_VERSION""  >> $GITHUB_ENV\n', 'echo $MMDEPLOY_VERSION\necho $TAG\ndocker build docker/Release/ -t ${TAG} --build-arg MMDEPLOY_VERSION=${MMDEPLOY_VERSION}\n', 'echo $MMDEPLOY_VERSION\necho $TAG\ndocker push $TAG\n', 'python -m pip install cmake onnx\n', 'sudo apt update', 'wget https://download.java.net/java/GA/jdk18/43f95e8614114aeaa8e8a5fcf20a682d/36/GPL/openjdk-18_linux-x64_bin.tar.gz\ntar xvf openjdk-18_linux-x64_bin.tar.gz\nexport JAVA_HOME=${PWD}/jdk-18\nexport PATH=${JAVA_HOME}/bin:${PATH}\n', 'wget https://github.com/microsoft/onnxruntime/releases/download/v1.8.1/onnxruntime-linux-x64-1.8.1.tgz\ntar -zxvf onnxruntime-linux-x64-1.8.1.tgz\nexport ONNXRUNTIME_DIR=${PWD}/onnxruntime-linux-x64-1.8.1\nexport LD_LIBRARY_PATH=$ONNXRUNTIME_DIR/lib:$LD_LIBRARY_PATH\n', 'sudo apt-get install libopencv-dev\nsudo apt-get install ffmpeg libgstreamer1.0-0 libgstreamer-plugins-base1.0-0 libavcodec-dev libavformat-dev libswscale-dev\n', 'sudo apt-get install ant\n', 'pushd csrc/mmdeploy/apis/java\njavac mmdeploy/*.java\npopd\n', 'mkdir -p build && pushd build\nexport LD_LIBRARY_PATH=$GITHUB_WORKSPACE/onnxruntime-linux-x64-1.8.1/lib/:$LD_LIBRARY_PATH\ncmake -DMMDEPLOY_BUILD_SDK=ON -DMMDEPLOY_BUILD_SDK_JAVA_API=ON -DMMDEPLOY_TARGET_BACKENDS=ort -DMMDEPLOY_CODEBASES=all -DONNXRUNTIME_DIR=$GITHUB_WORKSPACE/onnxruntime-linux-x64-1.8.1  ..\nmake -j$(nproc) && make install\npopd\n', 'export JAVA_AWT_INCLUDE_PATH=${JAVA_HOME}\nexport JAVA_AWT_LIBRARY=${JAVA_HOME}\nexport JAVA_INCLUDE_PATH=${JAVA_HOME}/include\nexport JAVA_INCLUDE_PATH2=${JAVA_HOME}/include/darwin\nexport JAVA_JVM_LIBRARY=${JAVA_HOME}\n\nmkdir -p opencv/build/bin\nwget https://media.githubusercontent.com/media/hanrui1sensetime/mmdeploy-javaapi-testdata/master/opencv-470.jar -P opencv/build/bin\nwget https://media.githubusercontent.com/media/hanrui1sensetime/mmdeploy-javaapi-testdata/master/opencv470-lib.tar -P opencv/build\ntar -xf opencv/build/opencv470-lib.tar -C opencv/build\nexport OPENCV_DIR=$GITHUB_WORKSPACE/opencv\nexport LD_LIBRARY_PATH=$GITHUB_WORKSPACE/build/lib:${OPENCV_DIR}/build/lib:$LD_LIBRARY_PATH\npython .github/scripts/test_java_demo.py\n', 'python -m pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'python .github/scripts/check_index_rst.py docs/en/index.rst\npython .github/scripts/check_index_rst.py docs/zh_cn/index.rst\n', 'python .github/scripts/doc_link_checker.py --target docs/zh_cn\npython .github/scripts/doc_link_checker.py --target README_zh-CN.md\npython .github/scripts/doc_link_checker.py --target docs/en\npython .github/scripts/doc_link_checker.py --target README.md\n', 'python -m pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-private --ignore-nested-functions --ignore-nested-classes --fail-under 80 mmdeploy\n', 'python -m pip install pylint\npylint mmdeploy\n', 'sudo apt-get update\nsudo apt-get install g++-riscv64-linux-gnu\n', 'mkdir $GITHUB_WORKSPACE/opencv-install\nwget https://github.com/irexyc/mmdeploy-ci-resource/raw/opencv/opencv_4.6.0_linux_riscv64.tar.gz\ntar xf opencv_4.6.0_linux_riscv64.tar.gz -C $GITHUB_WORKSPACE/opencv-install\n', 'mkdir $GITHUB_WORKSPACE/ncnn-install\nwget https://github.com/irexyc/mmdeploy-ci-resource/raw/ncnn/ncnn_20220729_linux_riscv64.tar.gz\ntar xf ncnn_20220729_linux_riscv64.tar.gz -C $GITHUB_WORKSPACE/ncnn-install\n', 'mkdir build && cd build\ncmake .. \\\n  -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/riscv64-linux-gnu.cmake \\\n  -DMMDEPLOY_BUILD_SDK=ON \\\n  -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n  -DMMDEPLOY_TARGET_BACKENDS=""ncnn"" \\\n  -Dncnn_DIR=$GITHUB_WORKSPACE/ncnn-install/lib/cmake/ncnn/ \\\n  -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4\nmake -j$(nproc)\nmake install\n', 'export MMDEPLOY_VERSION=$(python3 -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $MMDEPLOY_VERSION\necho ""MMDEPLOY_VERSION=$MMDEPLOY_VERSION""  >> $GITHUB_ENV\necho ""OUTPUT_DIR=/__w/mmdeploy/prebuild/$MMDEPLOY_VERSION"" >> $GITHUB_ENV\n', ""source activate mmdeploy-3.6\npip install pyyaml packaging setuptools wheel\nmkdir pack; cd pack\npython ../tools/package_tools/generate_build_config.py --backend 'trt;ort' \\\n  --system linux --output config.yml --build-mmdeploy\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", ""source activate mmdeploy-3.6\ncd pack\npython ../tools/package_tools/generate_build_config.py --backend 'ort' \\\n  --system linux --output config.yml --device cpu --build-sdk --build-sdk-monolithic \\\n  --build-sdk-python --sdk-dynamic-net\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", ""source activate mmdeploy-3.6\ncd pack\npython ../tools/package_tools/generate_build_config.py --backend 'ort;trt' \\\n  --system linux --output config.yml --device cuda --build-sdk --build-sdk-monolithic \\\n  --build-sdk-python --sdk-dynamic-net --onnxruntime-dir=$ONNXRUNTIME_GPU_DIR\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", 'cd pack/sdk\nfor folder in *\ndo\n  tar czf $folder.tar.gz $folder\ndone\n', 'mkdir -p $OUTPUT_DIR/mmdeploy  $OUTPUT_DIR/mmdeploy_runtime $OUTPUT_DIR/sdk\nchmod -R 777 $OUTPUT_DIR\nmv -vf pack/mmdeploy/* $OUTPUT_DIR/mmdeploy/\nmv -vf pack/mmdeploy_runtime/* $OUTPUT_DIR/mmdeploy_runtime/\nmv -vf pack/sdk/* $OUTPUT_DIR/sdk/\n', 'export MMDEPLOY_VERSION=$(python3 -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $MMDEPLOY_VERSION\necho ""MMDEPLOY_VERSION=$MMDEPLOY_VERSION""  >> $GITHUB_ENV\necho ""OUTPUT_DIR=/__w/mmdeploy/prebuild/$MMDEPLOY_VERSION"" >> $GITHUB_ENV\n', ""mkdir pack; cd pack\npython ../tools/package_tools/generate_build_config.py --backend 'ort' \\\n  --system linux --output config.yml --device cpu --build-sdk --build-sdk-monolithic \\\n  --sdk-dynamic-net --cxx11abi\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", ""cd pack\npython ../tools/package_tools/generate_build_config.py --backend 'ort;trt' \\\n  --system linux --output config.yml --device cuda --build-sdk --build-sdk-monolithic \\\n  --sdk-dynamic-net --cxx11abi --onnxruntime-dir=$ONNXRUNTIME_GPU_DIR --cudnn-dir /usr\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", 'cd pack/sdk\nfor folder in *\ndo\n  tar czf $folder.tar.gz $folder\ndone\n', 'mkdir -p $OUTPUT_DIR/sdk\nchmod -R 777 $OUTPUT_DIR\nmv -vf pack/sdk/* $OUTPUT_DIR/sdk/\n', 'export MMDEPLOY_VERSION=$(python3 -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $MMDEPLOY_VERSION\necho ""MMDEPLOY_VERSION=$MMDEPLOY_VERSION""  >> $GITHUB_ENV\necho ""OUTPUT_DIR=/__w/mmdeploy/prebuild/$MMDEPLOY_VERSION"" >> $GITHUB_ENV\n', 'cd $OUTPUT_DIR\nbash $GITHUB_WORKSPACE/tools/package_tools/test/test_sdk_python.sh\n', 'cd $OUTPUT_DIR\nbash $GITHUB_WORKSPACE/tools/package_tools/test/test_sdk.sh\n', 'conda activate mmdeploy-3.8\n$env:MMDEPLOY_VERSION=(python -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $env:MMDEPLOY_VERSION\necho ""MMDEPLOY_VERSION=$env:MMDEPLOY_VERSION""  >> $env:GITHUB_ENV\necho ""OUTPUT_DIR=$env:PREBUILD_DIR\\$env:MMDEPLOY_VERSION"" >> $env:GITHUB_ENV\n', "". D:\\DEPS\\cienv\\prebuild_gpu_env.ps1\nconda activate mmdeploy-3.6\nNew-Item -Path pack -ItemType Directory -Force\ncd pack\npython ../tools/package_tools/generate_build_config.py --backend 'trt;ort' `\n  --system windows --output config.yml --build-mmdeploy\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", "". D:\\DEPS\\cienv\\prebuild_cpu_env.ps1\nconda activate mmdeploy-3.6\ncd pack\npython ../tools/package_tools/generate_build_config.py --backend 'ort' `\n  --system windows --output config.yml --device cpu --build-sdk --build-sdk-monolithic `\n  --build-sdk-python --sdk-dynamic-net\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", "". D:\\DEPS\\cienv\\prebuild_gpu_env.ps1\nconda activate mmdeploy-3.6\ncd pack\npython ../tools/package_tools/generate_build_config.py --backend 'ort;trt' `\n  --system windows --output config.yml --device cuda --build-sdk --build-sdk-monolithic `\n  --build-sdk-python --sdk-dynamic-net\npython ../tools/package_tools/mmdeploy_builder.py --config config.yml\n"", 'cd pack/sdk\n$folders = $(ls).Name\nforeach ($folder in $folders) {\n  Compress-Archive -Path $folder -DestinationPath ""$folder.zip""\n}\n', 'New-Item ""$env:OUTPUT_DIR\\mmdeploy"" -ItemType Directory -Force\nNew-Item ""$env:OUTPUT_DIR\\mmdeploy_runtime"" -ItemType Directory -Force\nNew-Item ""$env:OUTPUT_DIR\\sdk"" -ItemType Directory -Force\nMove-Item pack/mmdeploy/* -Force ""$env:OUTPUT_DIR\\mmdeploy\\""\nMove-Item pack/mmdeploy_runtime/* -Force ""$env:OUTPUT_DIR\\mmdeploy_runtime\\""\nMove-Item pack/sdk/* -Force ""$env:OUTPUT_DIR\\sdk\\""\n', 'conda activate mmdeploy-3.8\n$env:MMDEPLOY_VERSION=(python -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $env:MMDEPLOY_VERSION\necho ""MMDEPLOY_VERSION=$env:MMDEPLOY_VERSION""  >> $env:GITHUB_ENV\necho ""OUTPUT_DIR=$env:PREBUILD_DIR\\$env:MMDEPLOY_VERSION"" >> $env:GITHUB_ENV\n', 'cd ""$env:OUTPUT_DIR""\n. D:\\DEPS\\cienv\\prebuild_cpu_env.ps1\nconda activate ci-test\n& ""$env:GITHUB_WORKSPACE/tools/package_tools/test/test_sdk_python.ps1""\n', 'cd ""$env:OUTPUT_DIR""\n. D:\\DEPS\\cienv\\prebuild_cpu_env.ps1\n& ""$env:GITHUB_WORKSPACE/tools/package_tools/test/test_sdk.ps1""\n', 'export MMDEPLOY_VERSION=$(python3 -c ""import sys; sys.path.append(\'mmdeploy\');from version import __version__;print(__version__)"")\necho $MMDEPLOY_VERSION\necho ""MMDEPLOY_VERSION=$MMDEPLOY_VERSION""  >> $GITHUB_ENV\necho ""OUTPUT_DIR=$PREBUILD_DIR/$MMDEPLOY_VERSION"" >> $GITHUB_ENV\npip install twine\n', 'cd $OUTPUT_DIR/mmdeploy\nls -sha *.whl\ntwine upload *.whl -u __token__ -p ${{ secrets.pypi_password }}\n', 'cd $OUTPUT_DIR/mmdeploy_runtime\nls -sha *.whl\ntwine upload *.whl -u __token__ -p ${{ secrets.pypi_password }}\n', 'ls -sha $OUTPUT_DIR/sdk\n', 'gh release upload v${MMDEPLOY_VERSION} ${OUTPUT_DIR}/sdk/*.zip ${OUTPUT_DIR}/sdk/*.tar.gz --clobber', 'echo ""============================== Inputs ==============================""\necho ""${{github.event.inputs.test_linux}}""\necho ""${{github.event.inputs.test_windows}}""\necho ""${{github.event.inputs.models}}""\necho ""${{github.event.inputs.backends}}""\necho ""${{github.event.inputs.run_performance}}""\necho ""$GITHUB_RUN_ID""\necho ""${{matrix.codebase}}""\necho ""${{matrix.torch_version}}""\necho """"\nexport REGRESSION_DIR=/__w/mmdeploy/convert_log/${GITHUB_RUN_ID}-linux\nexport MMDEPLOY_DIR=$(pwd)\nmkdir -p $REGRESSION_DIR\necho ""REGRESSION_DIR=$REGRESSION_DIR""  >> $GITHUB_ENV\necho ""MMDEPLOY_DIR=$MMDEPLOY_DIR""  >> $GITHUB_ENV\necho ""BACKENDS=${{github.event.inputs.backends}}"" >> $GITHUB_ENV\nln -sf /usr/bin/python3 /usr/bin/python\nln -sf /__w/mmdeploy/data $MMDEPLOY_DIR/data\n', 'apt update && apt install unzip\npython -V\npython -m pip install --upgrade pip\npython -m pip install openmim numpy pycuda xlsxwriter packaging prettytable\npython -m pip install opencv-python==4.5.4.60 opencv-python-headless==4.5.4.60 opencv-contrib-python==4.5.4.60\npython .github/scripts/prepare_reg_test.py --torch-version ${{ matrix.torch_version }} --codebases ${{ matrix.codebase}}\npython -m pip install -r requirements.txt\npython -m pip list\n', 'export Torch_DIR=$(python -c ""import torch;print(torch.utils.cmake_prefix_path + \'/Torch\')"")\nexport LD_LIBRARY_PATH=""$MMDEPLOY_DIR/build/lib:${LD_LIBRARY_PATH}""\necho ""LD_LIBRARY_PATH=$LD_LIBRARY_PATH""  >> $GITHUB_ENV\nexport BACKENDS=${BACKENDS//\' \'/;}\nexport BACKENDS=${BACKENDS//tensorrt/trt}\nexport BACKENDS=${BACKENDS//onnxruntime/ort}\necho ""BACKENDS = $BACKENDS""\nmkdir -p build && cd build\ncmake .. \\\n    -DMMDEPLOY_BUILD_SDK=ON \\\n    -DMMDEPLOY_BUILD_SDK_PYTHON_API=ON \\\n    -DMMDEPLOY_TARGET_DEVICES=""cpu;cuda"" \\\n    -DMMDEPLOY_TARGET_BACKENDS=""$BACKENDS"" \\\n    -DMMDEPLOY_CODEBASES=""all"" \\\n    -Dpplcv_DIR=${pplcv_DIR} \\\n    -DTENSORRT_DIR=${TENSORRT_DIR} \\\n    -DONNXRUNTIME_DIR=${ONNXRUNTIME_DIR} \\\n    -Dncnn_DIR=${ncnn_DIR} \\\n    -DTorch_DIR=${Torch_DIR}\n\nmake -j$(nproc) && make install\ncd -\nls build/lib\nrm -rf .eggs && python -m pip install -e .\npython tools/check_env.py\n', 'export work_dir=${REGRESSION_DIR}/${{matrix.codebase}}/torch${{matrix.torch_version}}\nexport log_path=${work_dir}/convert_log.txt\nmkdir -p $work_dir\nln -sf $work_dir ./mmdeploy_regression_dir\npython tools/check_env.py 2>&1 | tee ${log_dir}/check_env_log.txt\npython tools/regression_test.py \\\n    --codebase ${{matrix.codebase}} \\\n    --work-dir mmdeploy_regression_dir \\\n    --device cuda:0 \\\n    --models ${{github.event.inputs.models}} \\\n    --backends ${{github.event.inputs.backends}} \\\n    ${{github.event.inputs.run_performance}} 2>&1 | tee ${log_path}\necho ""Saved to $REGRESSION_DIR""\n', 'export url_prefix=${{github.event.inputs.mapped_domain}}/convert_log/${GITHUB_RUN_ID}-linux/${{matrix.codebase}}/torch${{matrix.torch_version}}\necho ""## ${{matrix.codebase}} + [torch${{matrix.torch_version}}](${url_prefix})"" >> $GITHUB_STEP_SUMMARY\nexport report_prefix=""${REGRESSION_DIR}/${{matrix.codebase}}/torch${{matrix.torch_version}}/${{matrix.codebase}}_report""\npython .github/scripts/gen_reg_summary.py ${report_prefix}.xlsx ${url_prefix}\ncat ${report_prefix}.md >> $GITHUB_STEP_SUMMARY\n', 'echo ""============================== Inputs ==============================""\necho ""${{github.event.inputs.test_linux}}""\necho ""${{github.event.inputs.test_windows}}""\necho ""${{github.event.inputs.models}}""\necho ""${{github.event.inputs.backends}}""\necho ""${{github.event.inputs.run_performance}}""\necho ""$env:GITHUB_RUN_ID""\necho ""${{matrix.codebase}}""\necho ""${{matrix.torch_version}}""\necho """"\necho ""============================== Prepare dataset ==============================""\nNew-Item -Path $pwd/data -ItemType SymbolicLink -Target $env:DATASET_DIR -Force\nNew-Item -Path $pwd/../mmdeploy_checkpoints -ItemType SymbolicLink -Target $env:CHECKPOINT_DIR -Force\nls data\nls ../mmdeploy_checkpoints\necho ""============================== Prepare env ==============================""\n$env:TEMP_ENV = ""$pwd/../temp_envs/$env:GITHUB_RUN_ID""\nNew-Item -Path ""$env:TEMP_ENV"" -ItemType Directory -Force\necho ""TEMP_ENV=$env:TEMP_ENV"" >> $env:GITHUB_ENV\n', 'echo ""============================== Info ==============================""\necho ""env:path= $env:path""\necho ""============================== Info ==============================""\nconda info\nconda info -e\nconda create -p $env:TEMP_ENV --clone $env:BASE_ENV -y\nconda activate $env:TEMP_ENV\npython -V\npython -m pip install --upgrade pip\npython -m pip install openmim numpy pycuda xlsxwriter packaging prettytable\npython -m pip install opencv-python==4.5.4.60 opencv-python-headless==4.5.4.60 opencv-contrib-python==4.5.4.60\npython .github/scripts/prepare_reg_test.py --torch-version ${{ matrix.torch_version }} --codebases ${{ matrix.codebase}}\npython -m pip install -r requirements.txt\npython -m pip list\n', 'conda activate $env:TEMP_ENV\npython -V\n$env:BACKENDS = ""${{github.event.inputs.backends}}""\n$env:BACKENDS=% {$env:BACKENDS -replace "" "", "";""}\n$env:BACKENDS=% {$env:BACKENDS -replace ""tensorrt"", ""trt""}\n$env:BACKENDS=% {$env:BACKENDS -replace ""onnxruntime"", ""ort""}\necho ""env:BACKENDS = $env:BACKENDS""\n$env:Torch_DIR=% {python -c ""import torch;print(torch.utils.cmake_prefix_path + \'\\Torch\')""}\necho ""Torch_DIR=$env:Torch_DIR"" >> $env:GITHUB_ENV\necho ""BACKENDS=$env:BACKENDS"" >> $env:GITHUB_ENV\nNew-Item -Path build -ItemType Directory -Force\ncd build\ncmake ..  -A x64 -T v142 `\n  -DMMDEPLOY_BUILD_TEST=OFF `\n  -DMMDEPLOY_BUILD_SDK_CSHARP_API=ON `\n  -DMMDEPLOY_BUILD_SDK_PYTHON_API=ON `\n  -DMMDEPLOY_BUILD_SDK=ON `\n  -DMMDEPLOY_TARGET_DEVICES=""cpu;cuda"" `\n  -DMMDEPLOY_TARGET_BACKENDS=""$env:BACKENDS"" `\n  -DMMDEPLOY_CODEBASES=""all"" `\n  -Dpplcv_DIR=""$env:PPLCV_DIR\\pplcv-build\\install\\lib\\cmake\\ppl"" `\n  -DOpenCV_DIR=""$env:OPENCV_DIR\\build\\x64\\vc15\\lib"" `\n  -DTENSORRT_DIR=""$env:TENSORRT_DIR"" `\n  -DTorch_DIR=""$env:Torch_DIR"" `\n  -DONNXRUNTIME_DIR=""$env:ONNXRUNTIME_DIR"" `\n  -DMMDEPLOY_BUILD_EXAMPLES=OFF `\n  -DCUDNN_DIR=""$env:CUDNN_DIR""\n\ncmake --build . --config Release -- /m\ncmake --install . --config Release\nls $pwd\\bin\\Release\nls $pwd\\lib\\Release\n', 'conda activate $env:TEMP_ENV\n$env:path = ""$pwd\\build\\bin\\Release;"" + $env:path\npython -m pip install -e .\npython .\\tools\\check_env.py\n', 'conda activate $env:TEMP_ENV\n$env:path = ""$pwd\\build\\bin\\Release;"" + $env:path\npython -V\n$env:CONVERT_DIR = ""$env:REGRESSION_DIR\\convert_log\\$env:GITHUB_RUN_ID-windows\\${{matrix.codebase}}\\torch${{matrix.torch_version}}""\necho ""env:CONVERT_DIR = $env:CONVERT_DIR""\nNew-Item -Path ""$env:CONVERT_DIR"" -ItemType Directory -Force\nNew-Item -Path mmdeploy_regression_dir -ItemType SymbolicLink -Target $env:CONVERT_DIR -Force\npython tools/regression_test.py `\n    --codebase ${{matrix.codebase}} `\n    --work-dir mmdeploy_regression_dir `\n    --device cuda:0 `\n    --models ${{github.event.inputs.models}} `\n    --backends ${{github.event.inputs.backends}} `\n    ${{github.event.inputs.run_performance}}\necho ""Saved to $env:CONVERT_DIR""\n', 'conda activate $env:TEMP_ENV\npython -V\n$env:URL_PREFIX = ""${{github.event.inputs.mapped_domain}}/convert_log/$env:GITHUB_RUN_ID-windows/${{matrix.codebase}}/torch${{matrix.torch_version}}""\necho ""URL_PREFIX = $env:URL_PREFIX""\n$env:REPORT_PREFIX = ""$env:REGRESSION_DIR\\convert_log\\$env:GITHUB_RUN_ID-windows\\${{matrix.codebase}}\\torch${{matrix.torch_version}}\\${{matrix.codebase}}_report""\necho ""REPORT_PREFIX = $env:REPORT_PREFIX""\necho ""## ${{matrix.codebase}} + [torch${{matrix.torch_version}}]($env:URL_PREFIX)"" >> $env:GITHUB_STEP_SUMMARY\npython .github/scripts/gen_reg_summary.py ""$env:REPORT_PREFIX.xlsx"" $env:URL_PREFIX\ncat ""$env:REPORT_PREFIX.md"" >> $env:GITHUB_STEP_SUMMARY\n', 'conda env remove --prefix ""$env:TEMP_ENV""\nRemove-Item -Path ""$pwd\\data"" -Force -Recurse\nRemove-Item -Path ""$pwd\\build"" -Force -Recurse\nRemove-Item -Path ""$pwd\\mmdeploy_regression_dir"" -Force -Recurse\n', 'sudo apt update', 'sudo apt install llvm-dev libclang-dev clang git-lfs', 'wget https://github.com/microsoft/onnxruntime/releases/download/v1.8.1/onnxruntime-linux-x64-1.8.1.tgz\ntar -zxvf onnxruntime-linux-x64-1.8.1.tgz\npushd onnxruntime-linux-x64-1.8.1\npopd\n', 'sudo apt-get install libopencv-dev', 'wget https://github.com/open-mmlab/mmdeploy/releases/download/v0.9.0/mmdeploy-0.9.0-linux-x86_64-onnxruntime1.8.1.tar.gz\ntar -zxvf mmdeploy-0.9.0-linux-x86_64-onnxruntime1.8.1.tar.gz\n', 'git clone https://github.com/liu-mengyang/rust-mmdeploy', 'git clone https://github.com/liu-mengyang/mmdeploy-converted-models --depth=1', 'pushd rust-mmdeploy\nexport MMDEPLOY_DIR=/home/runner/work/mmdeploy/mmdeploy/mmdeploy-0.9.0-linux-x86_64-onnxruntime1.8.1/sdk\nexport LD_LIBRARY_PATH=$MMDEPLOY_DIR/lib:$LD_LIBRARY_PATH\nexport ONNXRUNTIME_DIR=/home/runner/work/mmdeploy/mmdeploy/onnxruntime-linux-x64-1.8.1\nexport LD_LIBRARY_PATH=$ONNXRUNTIME_DIR/lib:$LD_LIBRARY_PATH\ncargo build\nsh ci_test.sh\n']"
"['cd doc\npwd\nyarn --frozen-lockfile\n', 'cd doc\npwd\nrm -rf old\nyarn build\ncp -af ../media dist\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'git init\ngit remote add origin https://github.com/${GITHUB_REPOSITORY}.git\ngit fetch --all\nfor branch in `git branch -a | grep remotes | grep -v HEAD`; do\n  git branch --track ${branch##*/} $branch\ndone\n', 'remote_repo=""https://${TGIT_USERNAME}:${TGIT_PASSWORD}@git.code.tencent.com/${TGIT_REPOSITORY}.git""\ngit remote add tencent ""${remote_repo}""\ngit show-ref\ngit branch --verbose\ngit push --all --force tencent\ngit push --tags --force tencent\n', 'remote_repo=""https://${GITEE_USERNAME}:${GITEE_PASSWORD}@gitee.com/${GITEE_REPOSITORY}.git""\ngit remote add gitee ""${remote_repo}""\ngit show-ref\ngit branch --verbose\ngit push --all --force gitee\ngit push --tags --force gitee\n', 'git config --global core.quotepath false && git diff ${{ github.sha }}^ ${{ github.sha }} --name-only > changed.txt  && cat changed.txt', '/tca_action/entrypoint.sh']"
[]
"['if [ -z ""${{ secrets.SECRET_ID }}"" ] || [ -z ""${{ secrets.SECRET_KEY }}"" ]; then\n  echo ""è¯·é…ç½® SECRET_ID å’Œ SECRET_KEY ä¸¤ä¸ª secrets""\n  echo -e ""\\033[1;31méƒ¨ç½²å¤±è´¥ \\033[0m""\n  exit 1\nfi   \n', 'pip install tencentcloud-sdk-python json5', 'bash ./serverless/deploy.sh\nif [ $? -ne 0 ]; then\n  exit 1;\nfi\n', 'mv config.example.json config.json', 'sed -i ""s/commitId/${{ github.sha }}/g"" config.json', 'ver=$(cat config.json | grep version | grep -o ""[\\.0-9]*"")\necho ""VERSION=$ver"" >> $GITHUB_ENV\n']"
"['sudo apt-get install graphviz\npython -m pip install --upgrade pip\npip install flake8 pytest pep8-naming\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 --ignore ""N801, E203, E266, E501, W503, F812, F401, F841, E741, N803, N802, N806"" minitorch/ tests/ project/\n', 'echo ""Module 0""\npytest tests -x -m task0_1\npytest tests -x -m task0_2\npytest tests -x -m task0_3\npytest tests -x -m task0_4\necho ""Module 1""\npytest tests -x -m task1_1\npytest tests -x -m task1_2\npytest tests -x -m task1_3\npytest tests -x -m task1_4\necho ""Module 2""\npytest tests -x -m task2_1\npytest tests -x -m task2_2\npytest tests -x -m task2_3\npytest tests -x -m task2_4\necho ""Module 3""\npytest tests -x -m task3_1\npytest tests -x -m task3_2\npytest tests -x -m task3_3\npytest tests -x -m task3_4\necho ""Module 4""\npytest tests -x -m task4_1\npytest tests -x -m task4_2\npytest tests -x -m task4_3\npytest tests -x -m task4_4\n']"
""
"['pip install pip --upgrade\npip install wheel\n', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install git+https://github.com/open-mmlab/mmengine.git@main', ""pip install -U openmim\nmim install 'mmcv >= 2.0.0rc1'\n"", ""pip install 'mmcls>=1.0.0rc0'"", 'pip install git+https://github.com/open-mmlab/mmdetection.git@main', 'pip install git+https://github.com/open-mmlab/mmsegmentation.git@main', 'pip install -r requirements.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmrazor -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 80 mmrazor\n']"
"['pip install hatch', 'hatch build\nhatch run docs:build\n', 'hatch publish --no-prompt\nhatch run docs:deploy\n', 'pip install hatch', 'hatch run lint:check', 'hatch build\nhatch run docs:build\n', 'hatch run test:test', 'hatch run test:test', 'hatch run test:cov']"
"['python -m pip install --upgrade pip\npip install pipenv==2021.5.29\npipenv lock\npipenv install --dev\n', 'pipenv run pytest\n', 'mv ""dist/windows/log4shell-detector.exe"" ""dist/windows/log4shell-detector-win.exe""\nmv ""dist/linux/log4shell-detector"" ""dist/linux/log4shell-detector-lin""\n', 'python -m pip install --upgrade pip\npip install pipenv==2021.5.29\npipenv lock\npipenv install --dev\n', 'pipenv run pytest\n']"
"['pwd&&ls\npip install pre-commit\npre-commit --version\n', 'git fetch\ngit branch -a\ngit diff --name-status remotes/origin/master HEAD\nupdated_files=`git diff --name-status remotes/origin/master HEAD|awk \'{print$ 2}\'` \necho $updated_files\nfor file in $updated_files\ndo\n  if [ ""${file##*.}"" = ""py"" ]; then\n    echo $file\n    pre-commit run --files $file --show-diff-on-failure\n  fi\ndone\n', 'echo ""Create Checkout Directory: ${{ github.run_id }}.""\n[ -z ""${{ github.run_id }}"" ] || rm -rf ${{ github.run_id }}\nmkdir ${{ github.run_id }}\n', 'cd ../../ && ./test_ppq.sh ${{ github.run_id }}\n']"
"['poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run pytest']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*']"
"['pip install --upgrade pip setuptools wheel\npip install tox\n', 'tox -e manifest', 'tox -e pyroma', 'tox -e flake8', 'tox -e mypy', 'pip install --upgrade pip setuptools wheel\npip install tox\nsudo apt-get install graphviz\n', 'tox -e doc8', 'tox -e docstr-coverage', 'tox -e docs', 'which python', 'pip install tox\n', 'tox -e py\n', 'tox -e coverage\n']"
"['python3 -m pip install --upgrade setuptools wheel', 'python3 .github/workflows/get_trzsz_version.py', 'python3 setup.py sdist bdist_wheel', 'python3 setup.py sdist bdist_wheel', 'python3 setup.py sdist bdist_wheel', 'python3 setup.py sdist bdist_wheel', 'sleep 600', 'pip install trzsz==${{ needs.build-and-publish.outputs.trzsz_version }}', 'trz -v', 'tsz -v', 'sleep 600', 'python3 -m pip install --upgrade trzsz==${{ needs.build-and-publish.outputs.trzsz_version }}', 'trz -v', 'tsz -v', 'sleep 600', 'python3 -m pip install --upgrade trzsz[iterm2]==${{ needs.build-and-publish.outputs.trzsz_version }}', 'trz -v', 'tsz -v', 'trzsz-iterm2 -v', 'python3 -m pip install --upgrade setuptools wheel', 'python3 .github/workflows/update_test_version.py', 'python3 setup.py sdist bdist_wheel', 'python3 setup.py sdist bdist_wheel', 'python3 setup.py sdist bdist_wheel', 'python3 setup.py sdist bdist_wheel', 'sleep 600', 'pip install -i https://test.pypi.org/simple/ trzsz==${{ needs.build-and-publish.outputs.test_version }}', 'trz -v', 'tsz -v', 'sleep 600', 'python3 -m pip install --upgrade -i https://test.pypi.org/simple/ trzsz==${{ needs.build-and-publish.outputs.test_version }}', 'trz -v', 'tsz -v', 'sleep 600', 'python3 -m pip install --upgrade -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ trzsz[iterm2]==${{ needs.build-and-publish.outputs.test_version }}', 'trz -v', 'tsz -v', 'trzsz-iterm2 -v', 'cd trzsz-libs && python setup.py install && cd -\n', 'python -m unittest discover trzsz-libs', 'python -m unittest discover trzsz-svr', 'cd trzsz-libs && python setup.py install && cd -\n', 'python -m unittest discover trzsz-libs', 'python -m unittest discover trzsz-svr', 'cd trzsz-libs && python setup.py install && cd -\npython -m pip install --upgrade iterm2\n', 'python -m unittest discover trzsz-libs', 'python -m unittest discover trzsz-svr', 'python -m unittest discover trzsz-iterm2']"
"['python -m pip install --upgrade pip\npip install -r $PWD/requirements.txt\n', 'python $PWD/raudi.py --all --push --remote 2>&1 | tee /tmp/log.txt\nchmod +x $PWD/.github/workflows/autocommit.sh && $PWD/.github/workflows/autocommit.sh\n', 'echo ""::set-output name=date::$(date +\'%Y-%m-%d\')""']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
[]
""
"['python -m pip install --upgrade pip\npip install poetry tox tox-gh-actions\n', 'tox', 'ls -l .', 'python -m pip install --upgrade pip\npip install poetry tox tox-gh-actions\n', 'echo $NML_DISABLE_USER_ANALYTICS', 'tox', 'poetry version $(poetry version --short)-dev.$GITHUB_RUN_NUMBER\npoetry version --short\npoetry build\n', 'echo ::set-output name=current_version::${GITHUB_REF#refs/tags/v}\n', 'python -m pip install --upgrade pip\npip install poetry\n', 'poetry build', 'ls -l']"
"['echo ""$GITHUB_CONTEXT""', 'python -m pip install --upgrade pip\npython -m pip install --force git+https://github.com/python-poetry/poetry-core.git@ad33bc2\npython -m pip install ""poetry==1.2.0a2""\npython -m poetry plugin add poetry-version-plugin\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry run pip install git+https://${{ secrets.ACTIONS_TOKEN }}@github.com/squidfunk/mkdocs-material-insiders.git', 'python -m poetry run mkdocs build', 'python -m poetry run mkdocs build --config-file mkdocs.insiders.yml', 'python -m poetry run bash ./scripts/zip-docs.sh', 'rm -rf ./site\nmkdir ./site\n', 'cd ./site\nunzip docs.zip\nrm -f docs.zip\n', 'python -m pip install --upgrade pip\npython -m pip install --force git+https://github.com/python-poetry/poetry-core.git@ad33bc2\npython -m pip install ""poetry==1.2.0a2""\npython -m poetry plugin add poetry-version-plugin\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry config pypi-token.pypi $PYPI_TOKEN\nbash scripts/publish.sh\n', 'pip install smokeshow', 'smokeshow upload coverage-html', 'python -m pip install --upgrade pip\npython -m pip install --force git+https://github.com/python-poetry/poetry-core.git@ad33bc2\npython -m pip install ""poetry==1.2.0a2""\npython -m poetry plugin add poetry-version-plugin\n', 'python -m poetry config virtualenvs.create false', 'python -m poetry install', 'python -m poetry run bash scripts/lint.sh', 'mkdir coverage', 'python -m poetry run bash scripts/test.sh', 'pip install coverage[toml]', 'ls -la coverage', 'coverage combine coverage', 'coverage report', 'coverage html --show-contexts --title ""Coverage for ${{ github.sha }}""']"
""
"['cd ./web\nnpm ci\nnpm run build\nmv ./dist $GITHUB_WORKSPACE\n', 'python -m pip install --upgrade pip\npip install flake8\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pyinstaller\n', 'Move-Item win7\\mdt_control.py mdt_control.py -force\nMove-Item win7\\mdt_deck_reader.py mdt_deck_reader.py -force\npyinstaller -F mdt_gui.py -p mdt_service.py -p mdt.py -p mdt_deck_reader.py -p mdt_control.py -p mdt_cv.py -n mdt_cv -n mdt -i controller.ico --distpath mdt --noconsole --uac-admin\nMove-Item LICENSE mdt\nMove-Item data mdt\nMove-Item locales mdt\nMove-Item README.md mdt\nMove-Item config.ini mdt\n', 'python -m pip install --upgrade pip\npip install flake8\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pip install pyinstaller\n', 'pyinstaller -F mdt_gui.py -p mdt_service.py -p mdt.py -p mdt_deck_reader.py -p mdt_control.py -p mdt_cv.py -n mdt_cv -n mdt -i controller.ico --distpath mdt --noconsole --uac-admin\nMove-Item LICENSE mdt\nMove-Item data mdt\nMove-Item locales mdt\nMove-Item README.md mdt\nMove-Item config.ini mdt\n']"
"['python -m pip install --upgrade pip', 'python -m pip install pre-commit\npre-commit install-hooks\n', 'pre-commit run --files ${{ steps.file_changes.outputs.files }}', 'conda install pytorch torchvision torchtext cpuonly -c pytorch-nightly\npip install -e "".[dev]""\n', 'python -m mypy --install-types --non-interactive --config-file mypy.ini', 'conda info', 'python --version', 'python -m pip install --upgrade pip', 'python -m pip install --upgrade setuptools wheel twine', 'python -m pip install --pre torch torchvision torchtext --extra-index-url \\\n  https://download.pytorch.org/whl/nightly/cpu\npython -m pip install -r requirements.txt\n', 'python -c ""import torch""\necho ""Import torch succeeded""\npython -c ""import torchtext""\necho ""Import torchtext succeeded""\npython -c ""import torchvision""\necho ""Import torchvision succeeded""\n', 'rm -r dist || true\npython setup.py bdist_wheel --package_name torchmultimodal-nightly \\\n  --python-tag=${{ matrix.python-tag }}\n', 'python -m pip install dist/torchmultimodal*.whl', 'pushd ../\npython -c ""import torchmultimodal""\npopd\n', 'python -m pip install pytest pytest-mock pytest-cov\npytest tests --import-mode=append --junitxml=junit/test-results.xml \\\n  --cov-report=xml --cov-report=html --cov=. --durations=20 -vv\n', 'python -m twine upload dist/torchmultimodal_nightly-*.whl \\\n  --username __token__ --password ""$PYPI_API_TOKEN"" \\\n  --skip-existing \\\n  --verbose\n', 'python -m pip install --upgrade pip', 'conda install pytorch torchvision torchtext cpuonly -c pytorch-nightly\npython -m pip install -e "".[dev]""\n', 'pytest tests --cov=. --cov-report=xml --durations=20 -vv']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
""
"['docker run -v ${{ github.workspace }}:/firmwire firmwire:latest ./firmwire.py --help', 'docker run -v ${{ github.workspace }}:/firmwire firmwire:latest make -C modkit/']"
"['python -m pip install --upgrade pip\npip install flake8 pytest flit\nflit install\n', 'pytest\n']"
""
"['sudo apt-get update\nsudo apt-get install ttfautohint\nsudo snap install yq\n', 'make .init.stamp', 'echo ""ZIP_NAME=$(echo \'${{ github.repository }}\' | awk -F \'/\' \'{print $2}\')-fonts"" >> $GITHUB_ENV', 'make build', 'make test', 'make proof', 'cp scripts/index.html out/index.html', 'zip -r ${{ env.ZIP_NAME }}.zip ${{ env.ZIP_NAME }}']"
[]
"['make setup', 'make build', 'make test-ts', 'make test-py', 'make test-integration-parallel', 'make test-examples', 'npm install', 'npx eslint src -c .eslintrc.js', 'aws s3 sync --quiet ./build/ s3://pyscript.net/unstable/', 'cd docs/\nmake html\n', 'aws s3 cp --quiet ./docs/_build/html/_static/redirect.html s3://docs.pyscript.net/index.html', 'aws s3 sync --quiet ./docs/_build/html/ s3://docs.pyscript.net/${{ github.ref_name }}/', 'aws s3 rm --recursive s3://docs.pyscript.net/latest/', 'aws s3 sync --quiet ./docs/_build/html/ s3://docs.pyscript.net/latest/', 'cd docs/\nmake html\n', 'cd docs/\nmake html\n', 'aws s3 cp --quiet ./docs/_static/s3_error.html s3://docs.pyscript.net/error.html', 'aws s3 sync --quiet ./docs/_build/html/ s3://docs.pyscript.net/unstable/', 'make setup', 'make test', 'zip -r -q ./build.zip ./build', 'make setup', 'make test', 'aws s3 sync --quiet ./build/ s3://pyscript.net/latest/\naws s3 sync --quiet ./build/ s3://pyscript.net/releases/${{ github.ref_name }}/\n', 'aws s3 sync s3://pyscript.net/unstable/ s3://pyscript.net/snapshots/${{ inputs.snapshot_version }}/\n', 'aws s3 sync --quiet --delete . s3://pyscript.net/examples/']"
"['pip install --user pipenv==${DEFAULT_PIP_ENV_VERSION}\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} sync --dev\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} run pip list\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} run mkdocs build --config-file ./mkdocs.yml\n', 'docker compose --file ${GITHUB_WORKSPACE}/docker/compose/docker-compose.ci-test.yml pull --quiet\ndocker compose --file ${GITHUB_WORKSPACE}/docker/compose/docker-compose.ci-test.yml up --detach\n', 'pip install --user pipenv==${DEFAULT_PIP_ENV_VERSION}\n', 'sudo apt-get update -qq\nsudo apt-get install -qq --no-install-recommends unpaper tesseract-ocr imagemagick ghostscript libzbar0 poppler-utils\n', 'sudo cp docker/imagemagick-policy.xml /etc/ImageMagick-6/policy.xml\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} run python --version\npipenv --python ${{ steps.setup-python.outputs.python-version }} sync --dev\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} run pip list\n', 'cd src/\npipenv --python ${{ steps.setup-python.outputs.python-version }} run pytest -ra\n', 'docker compose --file ${GITHUB_WORKSPACE}/docker/compose/docker-compose.ci-test.yml logs\ndocker compose --file ${GITHUB_WORKSPACE}/docker/compose/docker-compose.ci-test.yml down\n', 'cd src-ui && npm ci', 'npx playwright install --with-deps', 'cd src-ui && npm run lint', 'cd src-ui && npx playwright test', 'if [[ ${{ github.repository_owner }} == ""paperless-ngx"" && ( ${{ github.ref_name }} == ""main"" || ${{ github.ref_name }} == ""dev"" || ${{ github.ref_name }} == ""beta"" || ${{ startsWith(github.ref, \'refs/tags/v\') }} == ""true"" ) ]] ; then\n  echo ""Enabling DockerHub image push""\n  echo ""enable=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""Not pushing to DockerHub""\n  echo ""enable=false"" >> $GITHUB_OUTPUT\nfi\n', 'ghcr_name=$(echo ""${{ github.repository }}"" | awk \'{ print tolower($0) }\')\necho ""Name is ${ghcr_name}""\necho ""ghcr-repository=${ghcr_name}"" >> $GITHUB_OUTPUT\n', 'docker buildx imagetools inspect ${{ fromJSON(steps.docker-meta.outputs.json).tags[0] }}\n', 'docker create --name frontend-extract ${{ fromJSON(steps.docker-meta.outputs.json).tags[0] }}\ndocker cp frontend-extract:/usr/src/paperless/src/documents/static/frontend src/documents/static/frontend/\n', 'pip install --upgrade --user pipenv==${DEFAULT_PIP_ENV_VERSION} setuptools wheel\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} sync --dev\n', 'sudo apt-get update -qq\nsudo apt-get install -qq --no-install-recommends gettext liblept5\n', 'pipenv --python ${{ steps.setup-python.outputs.python-version }} requirements > requirements.txt\n', 'cd src/\npipenv --python ${{ steps.setup-python.outputs.python-version }} run python3 manage.py compilemessages\n', 'cd src/\npipenv --python ${{ steps.setup-python.outputs.python-version }} run python3 manage.py collectstatic --no-input\n', 'echo ""Making dist folders""\nfor directory in dist \\\n                dist/paperless-ngx \\\n                dist/paperless-ngx/scripts;\ndo\n  mkdir --verbose --parents ${directory}\ndone\n\necho ""Copying basic files""\nfor file_name in .dockerignore \\\n                .env \\\n                Dockerfile \\\n                Pipfile \\\n                Pipfile.lock \\\n                requirements.txt \\\n                LICENSE \\\n                README.md \\\n                paperless.conf.example \\\n                gunicorn.conf.py\ndo\n  cp --verbose ${file_name} dist/paperless-ngx/\ndone\nmv --verbose dist/paperless-ngx/paperless.conf.example dist/paperless-ngx/paperless.conf\n\necho ""Copying Docker related files""\ncp --recursive docker/ dist/paperless-ngx/docker\n\necho ""Copying startup scripts""\ncp --verbose scripts/*.service scripts/*.sh scripts/*.socket dist/paperless-ngx/scripts/\n\necho ""Copying source files""\ncp --recursive src/ dist/paperless-ngx/src\necho ""Copying documentation""\ncp --recursive docs/_build/html/ dist/paperless-ngx/docs\n\nmv --verbose static dist/paperless-ngx\n', 'echo ""Creating release archive""\ncd dist\nsudo chown -R 1000:1000 paperless-ngx/\ntar -cJf paperless-ngx.tar.xz paperless-ngx/\n', 'echo ""version=${{ github.ref_name }}"" >> $GITHUB_OUTPUT\nif [[ ${{ contains(github.ref_name, \'-beta.rc\') }} == \'true\' ]]; then\n  echo ""prerelease=true"" >> $GITHUB_OUTPUT\nelse\n  echo ""prerelease=false"" >> $GITHUB_OUTPUT\nfi\n', 'pip install --upgrade --user pipenv==${DEFAULT_PIP_ENV_VERSION} setuptools wheel\n', 'git branch ${{ needs.publish-release.outputs.version }}-changelog\ngit checkout ${{ needs.publish-release.outputs.version }}-changelog\necho -e ""# Changelog\\n\\n${{ needs.publish-release.outputs.changelog }}\\n"" > changelog-new.md\necho ""Manually linking usernames""\nsed -i -r \'s|@(.+?) \\(\\[#|[@\\1](https://github.com/\\1) ([#|ig\' changelog-new.md\nCURRENT_CHANGELOG=`tail --lines +2 changelog.md`\necho -e ""$CURRENT_CHANGELOG"" >> changelog-new.md\nmv changelog-new.md changelog.md\npipenv run pre-commit run --files changelog.md || true\ngit config --global user.name ""github-actions""\ngit config --global user.email ""41898282+github-actions[bot]@users.noreply.github.com""\ngit commit -am ""Changelog ${{ needs.publish-release.outputs.version }} - GHA""\ngit push origin ${{ needs.publish-release.outputs.version }}-changelog\n']"
""
"['docker run --rm curl-impersonate-tests --log-cli-level DEBUG', 'rm -rf /tmp/.buildx-cache-ff\nmv /tmp/.buildx-cache-ff-new /tmp/.buildx-cache-ff\nrm -rf /tmp/.buildx-cache-ch\nmv /tmp/.buildx-cache-ch-new /tmp/.buildx-cache-ch\n', ""sudo apt-get update\nsudo apt-get install build-essential pkg-config cmake ninja-build curl autoconf automake libtool\n# Chrome version dependencies\nsudo apt-get install golang-go\n# Needed to compile 'minicurl'\nsudo apt-get install libcurl4-openssl-dev\n# More dependencies for the tests\nsudo apt-get install tcpdump nghttp2-server libnss3\n"", 'sudo apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu\n', ""brew install pkg-config make cmake ninja autoconf automake libtool\n# Chrome version dependencies\n# (Go is already installed)\n# brew install go\n# Needed to compile 'minicurl'\nbrew install curl\n# More dependencies for the tests\nbrew install tcpdump nghttp2 nss\n"", '# Firefox version dependencies\npip3 install gyp-next\n', 'pip3 install -r tests/requirements.txt\n', 'curl -LO https://zlib.net/zlib-1.2.13.tar.gz\ntar xf zlib-1.2.13.tar.gz\ncd zlib-1.2.13\nCHOST=${{ matrix.host }} ./configure --prefix=${{ runner.temp }}/zlib\nmake\nmake install\n# Make sure curl will link with libz.so.1 and not libz.so\nrm -f ${{ runner.temp }}/zlib/lib/libz.so\n', 'mkdir ${{ runner.temp }}/install\n./configure --prefix=${{ runner.temp }}/install\n', 'mkdir ${{ runner.temp }}/install\n./configure --prefix=${{ runner.temp }}/install \\\n            --host=${{ matrix.host }} \\\n            --with-zlib=${{ runner.temp }}/zlib \\\n            --with-ca-path=/etc/ssl/certs \\\n            --with-ca-bundle=/etc/ssl/certs/ca-certificates.crt \\\n            --with-libnssckbi=/usr/lib/${{ matrix.host }}/nss\n', 'touch boringssl.zip\ntouch boringssl/.patched\nfind boringssl/build -type f | xargs touch\n', '${{ matrix.make }} chrome-build\n${{ matrix.make }} chrome-checkbuild\n${{ matrix.make }} chrome-install\n', 'touch ${{ env.NSS_VERSION }}.tar.gz\nfind ${{ env.NSS_VERSION }}/dist -type f | xargs touch\n', '${{ matrix.make }} firefox-build\n${{ matrix.make }} firefox-checkbuild\n${{ matrix.make }} firefox-install\n', ""# Compile 'minicurl' which is used by the tests\ngcc -Wall -Werror -o ${{ runner.temp }}/install/bin/minicurl tests/minicurl.c `curl-config --libs`\n"", 'cd tests\n# sudo is needed for capturing packets\npython_bin=$(which python3)\nsudo $python_bin -m pytest . --log-cli-level DEBUG --install-dir ${{ runner.temp }}/install --capture-interface ${{ matrix.capture_interface }}\n', 'cd ${{ runner.temp }}/install/lib\ntar -c -z -f ${{ runner.temp }}/libcurl-impersonate-${{ github.ref_name }}.${{ matrix.host }}.tar.gz libcurl-impersonate*\necho ""release_file_lib=${{ runner.temp }}/libcurl-impersonate-${{ github.ref_name }}.${{ matrix.host }}.tar.gz"" >> $GITHUB_ENV\n', '${{ matrix.make }} chrome-clean\n${{ matrix.make }} firefox-clean\nrm -Rf ${{ runner.temp }}/install\nmkdir ${{ runner.temp }}/install\n', './configure --prefix=${{ runner.temp }}/install --enable-static\n', './configure --prefix=${{ runner.temp }}/install \\\n            --enable-static \\\n            --host=${{ matrix.host }} \\\n            --with-zlib=${{ runner.temp }}/zlib \\\n            --with-ca-path=/etc/ssl/certs \\\n            --with-ca-bundle=/etc/ssl/certs/ca-certificates.crt \\\n            --with-libnssckbi=/usr/lib/${{ matrix.host }}/nss\n', '${{ matrix.make }} chrome-build\n${{ matrix.make }} chrome-checkbuild\n${{ matrix.make }} chrome-install-strip\n${{ matrix.make }} firefox-build\n${{ matrix.make }} firefox-checkbuild\n${{ matrix.make }} firefox-install-strip\n', 'cd ${{ runner.temp }}/install/bin\ntar -c -z -f ${{ runner.temp }}/curl-impersonate-${{ github.ref_name }}.${{ matrix.host }}.tar.gz curl-impersonate-ff curl-impersonate-chrome curl_*\necho ""release_file_bin=${{ runner.temp }}/curl-impersonate-${{ github.ref_name }}.${{ matrix.host }}.tar.gz"" >> $GITHUB_ENV\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', ""pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${{matrix.torch}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'pip install -r requirements/tests.txt -r requirements/optional.txt', 'rm -rf .eggs && pip install -e .', 'coverage run --branch --source mmrotate -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender-dev libxext6 git', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', ""python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch${{matrix.torch_version}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'python -m pip install -r requirements/tests.txt -r requirements/optional.txt', '# Some dependencies may be required for the build of pycocotools\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\nrm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n', 'coverage run --branch --source mmrotate -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\napt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n', 'apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender-dev libxext6 git', 'apt-get update && apt-get install -y python${{matrix.python-version}}-dev', 'python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html', ""python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/torch${{matrix.torch_version}}/index.html\npython -c 'import mmcv; print(mmcv.__version__)'\n"", 'python -m pip install -r requirements/tests.txt -r requirements/optional.txt', '# Some dependencies may be required for the build of pycocotools\nexport CFLAGS=`python -c \'import sysconfig;print(""-I""+sysconfig.get_paths()[""include""])\'`\nrm -rf .eggs\npython setup.py check -m -s\nTORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n', 'coverage run --branch --source mmrotate -m pytest tests/\ncoverage xml\ncoverage report -m\n', 'python -m pip install pip --upgrade --user', 'pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html', 'pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full', 'python -m pip install -r requirements/tests.txt -r requirements/optional.txt', 'pip install -e .', 'coverage run --branch --source mmrotate -m pytest tests', 'coverage xml\ncoverage report -m\n', 'pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files', 'pip install interrogate\ninterrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex ""__repr__"" --fail-under 95 mmrotate\n', 'pip install torch', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install twine\ntwine upload dist/* -u __token__ -p ${{ secrets.pypi_password }}\n', 'pip install pip --upgrade', 'pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html', 'pip install openmim', 'rm -rf .eggs && mim install -e .', 'mim search mmrotate']"
[]
"['python -m pip install --upgrade pip\npip install -r requirements_dev.txt\n', 'make lint\n', 'make coverage\n', 'make test\n']"
""
"['sudo apt-get install -y libprotobuf-dev\npython -m pip install --upgrade pip setuptools\npip install wheel\npip install grpcio-tools==1.48.2\npip install pytest pytest-xdist\npip wheel -e .\npip install -e .\npip install -r requirements-test.txt\n', 'pip list', 'bash run_tests.sh ${{ matrix.suffix }}', 'python -m pip install --upgrade pip\npip install -r docs/requirements-docs.txt\n', 'pip freeze\n', 'cd docs\nbash ./generate_docs.sh html\n', 'cd docs\nbash ./generate_docs.sh doctest\n', 'sudo apt-get install -y libprotobuf-dev\npython -m pip install --upgrade pip\npip install setuptools wheel twine grpcio-tools==1.48.2\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller\n', 'python -c ""with open(\'termtyper.py\', \'w\') as f: f.write(\'from termtyper import main\\nmain()\\n\')""\npyinstaller -F termtyper.py -i pic.ico\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller\n', 'python -c ""with open(\'termtyper.py\', \'w\') as f: f.write(\'from termtyper import main\\nmain()\\n\')""\npyinstaller -F termtyper.py -i pic.ico\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller\n', 'python -c ""with open(\'termtyper.py\', \'w\') as f: f.write(\'from termtyper import main\\nmain()\\n\')""\npyinstaller -F termtyper.py -i pic.ico\n', 'python -m pip install --upgrade pip\npip install flake8\n', ""flake8 $(git ls-files '*.py')\n""]"
""
"['python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel\n', 'python3 setup.py sdist bdist_wheel']"
['mkdir dist && cd dist\nmkdir win32 && cp -r ../plugins/* ./win32/ && cp -r ../artifact/keystone_win32/* ./win32/patching/keystone && cd ./win32 && zip -r ../patching_win32.zip ./* && cd ..\nmkdir linux && cp -r ../plugins/* ./linux/ && cp -r ../artifact/keystone_linux/* ./linux/patching/keystone && cd ./linux && zip -r ../patching_linux.zip ./* && cd ..\nmkdir darwin && cp -r ../plugins/* ./darwin/ && cp -r ../artifact/keystone_darwin/* ./darwin/patching/keystone && cd ./darwin && zip -r ../patching_macos.zip ./* && cd ..\n']
"['pip install git+https://github.com/dhruvkb/pre-commit@main\npre-commit run --all-files\n', 'git config --global user.email ""user@example.com""\ngit config --global user.name ""User""\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'just test']"
""
"['docker-compose up -d mysql postgres presto trino clickhouse vertica', 'pip install poetry', 'poetry install', 'poetry add google-cloud-bigquery', 'chmod +x tests/waiting_for_stack_up.sh\n./tests/waiting_for_stack_up.sh && TEST_ACROSS_ALL_DBS=0 poetry run unittest-parallel -j 16\n', 'docker-compose up -d mysql postgres presto trino clickhouse vertica', 'pip install poetry', 'poetry install', 'chmod +x tests/waiting_for_stack_up.sh\n./tests/waiting_for_stack_up.sh && poetry run unittest-parallel -j 16\n']"
"['pip install flake8\n', 'flake8 --select F,E722 --ignore F403,F405,F541 --per-file-ignores=""*/__init__.py:F401,F403""\n', 'pip install poetry\npoetry install\n', 'poetry run pytest --exitfirst --reruns 2 --disable-warnings --log-cli-level=DEBUG --cov-report xml:cov.xml --cov=bbot .\n', 'python -m pip install --upgrade pip\npip install poetry build\npoetry self add ""poetry-dynamic-versioning[plugin]""\n', 'python -m build', 'echo ""::set-output name=BBOT_VERSION::$(poetry version | cut -d\' \' -f2 | tr -d v)""\n']"
""
"['curl --request POST \\\n  --url https://circleci.com/api/v2/project/gh/cider-security-research/cicd-goat/pipeline \\\n  -u ""${{ secrets.CCI_TOKEN }}:"" \\\n  --header \'content-type: application/json\' \\\n  --data \'{""parameters"":{""VERSION"":""${{ github.event.release.tag_name }}""}}\'']"
"[""pip install -e '.[test]'\n"", 'python -m playwright install-deps\nshot-scraper install\n', 'pytest\n', 'pip install setuptools wheel twine build\n', 'python -m build\ntwine upload dist/*\n', ""pip install -e '.[test]'\n"", 'python -m playwright install-deps\nshot-scraper install\nshot-scraper install -b firefox\nshot-scraper install -b webkit\n', 'pytest\n', 'cog --check docs/*.md\n', 'tests/run_examples.sh\n']"
"['pip install pre-commit\npre-commit install\n', 'pre-commit run --all-files\n[ $? -eq 1 ] && exit 1 || echo ""Passed""\n', 'pip install numpy typing-extensions dataclasses\npip install torch==${{ matrix.torch-version}}+cpu torchvision==${{matrix.torchvision-version}}+cpu torchaudio==${{matrix.torchaudio-version}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'pip install -e .[test]\n', 'pip install nltk\n', 'pip install transformers==4.21.0\n', 'git clone https://github.com/bheinzerling/pyrouge\ncd pyrouge\npip install -e .\ngit clone https://github.com/andersjo/pyrouge.git rouge\npyrouge_set_rouge_path $(realpath rouge/tools/ROUGE-1.5.5/)\nsudo apt-get install libxml-parser-perl --fix-missing\ncd rouge/tools/ROUGE-1.5.5/data\nrm WordNet-2.0.exc.db\n./WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db\npython -m pyrouge.test\n', 'wget -c http://www.cs.cmu.edu/~alavie/METEOR/download/meteor-1.5.tar.gz\ntar -zxvf meteor-1.5.tar.gz\nmkdir federatedscope/nlp/metric/meteor/data/\nmv meteor-1.5/data/paraphrase-en.gz federatedscope/nlp/metric/meteor/data/\nmv meteor-1.5/meteor-1.5.jar federatedscope/nlp/metric/meteor/\n', 'python federatedscope/main.py \\\n  --cfg federatedscope/nlp/hetero_tasks/baseline/config_isolated.yaml \\\n  --client_cfg federatedscope/nlp/hetero_tasks/baseline/config_client_isolated.yaml \\\n  outdir exp/isolated/ \\\n  use_gpu False \\\n  data.is_debug True \\\n  data.root test_data/ \\\n\n[ $? -eq 1 ] && exit 1\n\npython federatedscope/main.py \\\n  --cfg federatedscope/nlp/hetero_tasks/baseline/config_fedavg.yaml \\\n  --client_cfg federatedscope/nlp/hetero_tasks/baseline/config_client_fedavg.yaml \\\n  outdir exp/fedavg/ \\\n  use_gpu False \\\n  data.is_debug True \\\n  data.root test_data/ \\\n\n[ $? -eq 1 ] && exit 1\n\npython federatedscope/main.py \\\n  --cfg federatedscope/nlp/hetero_tasks/baseline/config_pretrain.yaml \\\n  outdir exp/atc/pretrain/ \\\n  use_gpu False \\\n  data.is_debug True \\\n  data.root test_data/ \\\n\n[ $? -eq 1 ] && exit 1\n\npython federatedscope/main.py \\\n  --cfg federatedscope/nlp/hetero_tasks/baseline/config_atc.yaml \\\n  --client_cfg federatedscope/nlp/hetero_tasks/baseline/config_client_atc.yaml \\\n  outdir exp/atc/train/ \\\n  use_gpu False \\\n  data.is_debug True \\\n  data.root test_data/ \\\n\n[ $? -eq 1 ] && exit 1 || echo ""Passed""\n', 'pip install numpy typing-extensions dataclasses\npip install torch==${{ matrix.torch-version}}+cpu torchvision==${{matrix.torchvision-version}}+cpu torchaudio==${{matrix.torchaudio-version}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'pip install -e .[test,hpo]\n', 'python federatedscope/hpo.py --cfg federatedscope/autotune/baseline/fedhpo_vfl.yaml\n[ $? -eq 1 ] && exit 1 || echo ""Passed""', 'pip install numpy typing-extensions dataclasses\npip install torch==${{ matrix.torch-version}}+cpu torchvision==${{matrix.torchvision-version}}+cpu torchaudio==${{matrix.torchaudio-version}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n', 'pip install -e .[test]\n', 'python scripts/distributed_scripts/gen_data.py\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_server_no_data.yaml distribute.grpc_compression gzip &\nsleep 2\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_1.yaml distribute.grpc_compression gzip &\nsleep 2\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_2.yaml distribute.grpc_compression gzip &\nsleep 2\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_3.yaml distribute.grpc_compression gzip\n[ $? -eq 1 ] && exit 1 || echo ""Passed""\n', ""python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_server.yaml data.file_path 'toy_data/server_data' distribute.data_idx -1 &\nsleep 2\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_1.yaml data.file_path 'toy_data/client_1_data' distribute.data_idx -1 &\nsleep 2\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_2.yaml data.file_path 'toy_data/client_2_data' distribute.data_idx -1 &\nsleep 2\npython federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_3.yaml data.file_path 'toy_data/client_3_data' distribute.data_idx -1\n""]"
"['python -m pip install --upgrade build', 'python -m build', 'pip install --upgrade --pre hatch', 'hatch run tests']"
"['bash test.sh', 'RELEASE_VER=${GITHUB_REF#refs/*/}\nPACKAGE_VER=""v`python setup.py --version`""\nif [ $RELEASE_VER != $PACKAGE_VER ]\nthen\n  echo ""package ver. ($PACKAGE_VER) != release ver. ($RELEASE_VER)""; exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get update\nsudo apt-get install -qy npm git\n', 'make build-js', 'git add -u\nchanges=$(git status --porcelain)\n# Check for changes in regenerated files\nif test -n ""$changes""; then\n  echo ""Generated files not up to date.""\n  echo ""$changes""\n  echo """"\n  git diff --staged || true\n  exit 1\nfi\n', 'sudo apt-get update\nsudo apt-get install -qy libunwind-dev liblz4-dev pkg-config npm gdb lldb\n', 'python3 -m venv venv\n', './venv/bin/python -m pip install --upgrade pip\n', 'echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n', 'make dev-install pycoverage\n', 'apk add --update build-base libunwind-dev lz4-dev musl-dev python3-dev python3-dbg gdb lldb\n', 'python3 -m venv /venv\n', '/venv/bin/python -m pip install --upgrade pip\n', 'make dev-install pycoverage\n', 'sudo apt-get update\nsudo apt-get install -qy clang-format npm libunwind-dev liblz4-dev pkg-config\n', 'python3 -m pip install -r requirements-extra.txt\n', 'python3 -m pip install -e .\n', 'make lint\npython3 -m pre_commit run --all-files --hook-stage pre-push\n', 'towncrier build --version 99.99 --name memray --keep\nmake docs\n', 'sudo apt-get update\nsudo apt-get install -qy libunwind-dev liblz4-dev pkg-config npm valgrind\n', 'python3 -m pip install --upgrade pip\npython3 -m pip install -r requirements-test.txt\npython3 -m pip install -e .\n', 'make valgrind', 'make helgrind', 'echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n', 'echo ""CFLAGS=-target arm64-apple-macos11"" >> $GITHUB_ENV\necho ""MEMRAY_LIBBACKTRACE_TARGET=arm64-apple-macos11"" >> $GITHUB_ENV\n', 'pipx run build --sdist', 'sudo apt-get update\nsudo apt-get install -qy \\\n  libunwind-dev \\\n  liblz4-dev \\\n  gdb \\\n  lcov \\\n  libdw-dev \\\n  libelf-dev \\\n  python3.10-dev \\\n  python3.10-dbg\n', 'python3 -m pip install --upgrade pip cython\nmake test-install\n', 'echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n', 'export PATH=""./node_modules/.bin:$PATH""\n', 'make ccoverage\n']"
"['python3 -m venv .env\nsource .env/bin/activate\nmake install\n', 'source .env/bin/activate\nmake test\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['echo Hello, world!', 'pwd\nls\n\nwhich python\npython --version\n\npip install -r requirements.txt\npython setup.py install --user\ncd tests/\n# sh run_all_tests.sh\nsh run_all_tests_cpu.sh\n']"
"['git lfs checkout', 'set -e\nUNITTEST_OSS_CONFIG=~/.ossutilconfig.unittest\nif [ ! -e $UNITTEST_OSS_CONFIG ]; then\n    echo ""$UNITTEST_OSS_CONFIG does not exists""\n    exit\nfi\n\nexport OSS_CONFIG_FILE=$UNITTEST_OSS_CONFIG\nexport TEST_DIR=""/tmp/easycv_test_${USER}_`date +%s`""\n\n# do not uncomments, casue faild in Online UT, install requirements by yourself on UT machine\n# pip install -r requirements.txt\n#run test\nexport CUDA_VISIBLE_DEVICES=7\nsource ~/workspace/anaconda2/etc/profile.d/conda.sh\nconda activate easycv_torch1.8.0\n\n# pip install pai-easycv\npip uninstall -y pai-easycv\n\npython setup.py install\npython setup.py sdist bdist_wheel\n# move source code, ensure import easycv from site-package\nmv ./easycv ./easycv_src\n\nPYTHONPATH=. python tests/run.py --skip_dir tests/test_toolkit/modelscope\n\nconda activate easycv_torch1.8.0_py37\nPYTHONPATH=. python tests/run.py --test_dir tests/test_toolkit/modelscope\n', 'set -e\nUNITTEST_OSS_CONFIG=~/.ossutilconfig.unittest\nif [ ! -e $UNITTEST_OSS_CONFIG ]; then\n    echo ""$UNITTEST_OSS_CONFIG does not exists""\n    exit\nfi\n\nexport OSS_CONFIG_FILE=$UNITTEST_OSS_CONFIG\n\nexport PYTHONPATH=.\nexport CUDA_HOME=/apsarapangu/disk6/xinyi.zxy/cuda-10.2\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64\nexport PATH=${CUDA_HOME}/bin:${PATH}\nexport TEST_DIR=""/tmp/easycv_test_${USER}_`date +%s`""\n\n# do not uncomments, casue faild in Online UT, install requirements by yourself on UT machine\n# pip install -r requirements.txt\n#run test\nexport CUDA_VISIBLE_DEVICES=6\nsource ~/workspace/anaconda2/etc/profile.d/conda.sh\nconda activate torch1.8.1_blade\nPYTHONPATH=. python tests/test_predictors/test_detector_blade.py\nPYTHONPATH=. python tests/test_apis/test_export_blade.py\n', 'pip install pre-commit\ncp .github/hooks/pre-commit .git/hooks/\n', 'pre-commit run --all-files', 'pip install wheel', 'python setup.py sdist bdist_wheel', 'pip install twine\ntwine upload package/dist/* --skip-existing -u __token__ -p ${{ secrets.PYPI_API_TOKEN }}\n']"
"['python -m pip install --upgrade pip\npip install ruff\n', ""ruff $(git ls-files '*.py')\n"", 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
[]
"['npm ci', 'npm run docs:build', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install -e .\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['HASH=$(cat SNAPSHOT_HASH)\necho ""SNAPSHOT_HASH=$HASH"" >> $GITHUB_ENV\n', 'brew update\nbrew install libzip openssl libplist autoconf automake libtool autoconf-archive pkg-config ninja\nexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/opt/openssl@1.1/lib/pkgconfig\ngit clone https://github.com/libimobiledevice/libplist\ncd libplist && ./autogen.sh --without-cython && sudo make install && cd ..\ncd libusbmuxd && ./autogen.sh && sudo make install && cd ..\nxcrun --sdk macosx --show-sdk-path\nbrew install ideviceinstaller\nbrew install ios-deploy\npip3 install wheel\npip3 install .\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\ngit clone https://github.com/flutter/engine.git\n', 'ROOT_DIR=`pwd`\nexport PATH=$PATH:$ROOT_DIR/depot_tools\ncd engine\ngit config --global user.email ""reflutter@example.com"" && git config --global user.name ""reflutter""\ngit fetch origin $(reflutter ${{env.SNAPSHOT_HASH}} -l)\ngit reset --hard FETCH_HEAD\nreflutter ${{env.SNAPSHOT_HASH}} -l\necho \'reflutter\' > REFLUTTER\ngit add . && git commit -am ""reflutter""\ncd $ROOT_DIR\nmkdir customEngine\ncd customEngine\necho \'solutions = [{""managed"": False,""name"": ""src/flutter"",""url"": ""\'$ROOT_DIR/engine\'"",""custom_deps"": {},""deps_file"": ""DEPS"",""safesync_url"": """",},]\' > .gclient\ngclient sync\nreflutter ${{env.SNAPSHOT_HASH}} -l\n', 'export PATH=$PATH:`pwd`/depot_tools && sudo xcode-select -s /Applications/Xcode.app && customEngine/src/flutter/tools/gn --no-goma --ios --ios-cpu=arm64 --runtime-mode=release && ninja -C customEngine/src/out/ios_release', 'export PATH=$PATH:`pwd`/depot_tools && customEngine/src/flutter/tools/gn --no-goma --android --android-cpu=arm64 --runtime-mode=release && ninja -C customEngine/src/out/android_release_arm64', 'export PATH=$PATH:`pwd`/depot_tools && customEngine/src/flutter/tools/gn --no-goma --android --android-cpu=arm --runtime-mode=release && ninja -C customEngine/src/out/android_release', 'cp customEngine/src/out/ios_release/Flutter.framework/Flutter Flutter\ncp customEngine/src/out/android_release_arm64/lib.stripped/libflutter.so libflutter_arm64.so 2>/dev/null || :\ncp customEngine/src/out/android_release/lib.stripped/libflutter.so libflutter_arm.so 2>/dev/null || :\n', 'HASH=$(cat SNAPSHOT_HASH)\necho ""SNAPSHOT_HASH=$HASH"" >> $GITHUB_ENV\n', 'brew update\nbrew install libzip openssl libplist autoconf automake libtool autoconf-archive pkg-config ninja\nexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/opt/openssl@1.1/lib/pkgconfig\ngit clone https://github.com/libimobiledevice/libplist\ncd libplist && ./autogen.sh --without-cython && sudo make install && cd ..\ncd libusbmuxd && ./autogen.sh && sudo make install && cd ..\nxcrun --sdk macosx --show-sdk-path\nbrew install ideviceinstaller\nbrew install ios-deploy\npip3 install wheel\npip3 install .\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\ngit clone https://github.com/flutter/engine.git\n', 'ROOT_DIR=`pwd`\nexport PATH=$PATH:$ROOT_DIR/depot_tools\ncd engine\ngit config --global user.email ""reflutter@example.com"" && git config --global user.name ""reflutter""\ngit fetch origin $(reflutter ${{env.SNAPSHOT_HASH}} -l)\ngit reset --hard FETCH_HEAD\nreflutter ${{env.SNAPSHOT_HASH}} -l patchDump\necho \'reflutter\' > REFLUTTER\ngit add . && git commit -am ""reflutter""\ncd $ROOT_DIR\nmkdir customEngine\ncd customEngine\necho \'solutions = [{""managed"": False,""name"": ""src/flutter"",""url"": ""\'$ROOT_DIR/engine\'"",""custom_deps"": {},""deps_file"": ""DEPS"",""safesync_url"": """",},]\' > .gclient\ngclient sync\nreflutter ${{env.SNAPSHOT_HASH}} -l patchDump\n', 'export PATH=$PATH:`pwd`/depot_tools && sudo xcode-select -s /Applications/Xcode.app && customEngine/src/flutter/tools/gn --no-goma --ios --ios-cpu=arm64 --runtime-mode=release && ninja -C customEngine/src/out/ios_release', 'export PATH=$PATH:`pwd`/depot_tools && customEngine/src/flutter/tools/gn --no-goma --android --android-cpu=arm64 --runtime-mode=release && ninja -C customEngine/src/out/android_release_arm64', 'export PATH=$PATH:`pwd`/depot_tools && customEngine/src/flutter/tools/gn --no-goma --android --android-cpu=arm --runtime-mode=release && ninja -C customEngine/src/out/android_release', 'cp customEngine/src/out/ios_release/Flutter.framework/Flutter Flutter\ncp customEngine/src/out/android_release_arm64/lib.stripped/libflutter.so libflutter_arm64.so 2>/dev/null || :\ncp customEngine/src/out/android_release/lib.stripped/libflutter.so libflutter_arm.so 2>/dev/null || :\n']"
['nox']
""
"['cargo install rustfilt coverage-prepare', 'rustup component add llvm-tools-preview', 'pip install -r tests/requirements.txt', 'rustc --version --verbose', 'pip install -e .', 'pip freeze', 'coverage run -m pytest', 'ls -lha', 'coverage xml', 'coverage-prepare lcov pydantic_core/*.so', 'pip install -r tests/requirements.txt', 'pip install -e .', 'pip freeze', 'pytest', 'pip install -r tests/requirements.txt', 'pip install -e .', 'pip freeze', 'pytest', 'cargo test', 'pip install -r tests/requirements.txt', 'pip install ""maturin>=0.15,<0.16"" typing_extensions', 'make build-dev', 'pip freeze', 'pytest', 'pip install -r tests/requirements-linting.txt', 'pip install .', 'pip freeze', 'make lint', 'make pyright', 'npm install', 'npm run lint', 'pip install typing_extensions', 'cargo bench', ""pip install 'maturin>=0.15,<0.16' 'black>=22.3.0,<23' typing_extensions"", 'make build-wasm', 'npm install', 'npm run test', 'ls -lh dist/\nls -l dist/\n', ""pip install -U twine 'black>=22.3.0,<23' typing_extensions"", 'python generate_self_schema.py', ""${{ matrix.ls || 'ls -lh' }} dist/"", 'twine check --strict dist/*', 'ls -lh dist/\nls -l dist/\necho ""`ls dist | wc -l` files""\n', 'mkdir sdist-files\ntar -xvf dist/*.tar.gz -C sdist-files\ntree -a sdist-files\n', 'ls dist/*cp310-manylinux*x86_64.whl | head -n 1\npython -m zipfile --list `ls dist/*cp310-manylinux*x86_64.whl | head -n 1`\n', 'rm -r pydantic_core', 'pip install typing-extensions', 'pip install -r tests/requirements.txt', 'pip install pydantic-core --no-index --no-deps --find-links dist --force-reinstall', 'pytest --ignore=tests/test_docstrings.py', 'pip install -U twine', 'python .github/check_version.py', 'twine check --strict dist/*', 'twine upload dist/*', 'pip install -r tests/requirements.txt', 'pip uninstall pytest-speed -y', 'pip install pytest-codspeed~=1.2.0 pytest-benchmark==4.0.0', ""pip install -e .\nmake build-prod\npython -c 'import pydantic_core'\n"", 'rm tests/__init__.py']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python -m tests.main -a']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
""
"['python -m pip install --upgrade pip\npip install twine\n', 'twine upload wheelhouse/*-manylinux*.whl\n', 'python -m pip install --upgrade pip\npip install twine\n', 'twine upload dist/*-manylinux*.whl\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine numpy\n', 'python setup.py bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine numpy\n', 'python setup.py bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine numpy\n', 'python setup.py bdist_wheel\ntwine upload dist/*\n', 'micromamba install --file tests-requirements.txt --name mahotas_test_env\n', 'make debug\n', 'python -m pytest -v\n']"
"['python -m pip install --upgrade pip setuptools setuptools_scm twine wheel\n', 'python setup.py sdist bdist_wheel', 'twine check dist/*', 'twine upload --username __token__ --non-interactive -r testpypi dist/*', 'twine upload --username __token__ --non-interactive -r pypi dist/*', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['pip install -U tox\ntox\n', 'pip install --upgrade build twine\npython -m build\n', 'pip install -U tox\ntox -e py\n', 'bash <(curl -s https://codecov.io/bash)', 'sudo apt-get update\nsudo apt-get install libxml2-dev libxslt-dev\n', 'pip install -U tox\ntox\n', 'bash <(curl -s https://codecov.io/bash)', 'pip install -U tox\ntox\n']"
""
"['pip install tox\npython setup.py install_egg_info\n', 'tox -e ${{ matrix.versions.toxenv }}', 'pip install codecov\ncodecov\n']"
"['python -m pip install --upgrade pip wheel\npip install -r test_requirements.txt\n', 'pip install -e .', 'coverage run -m unittest discover -s urwid -v\ncoverage report\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install -U pip setuptools>=18.5\npython -m pip install -r requirements-dev.txt\n', 'tox']"
"['python -m pip install --upgrade pip', 'pip install coverage>=5.3', 'pip install ""Django>=3.2,<4.0""', 'pip install ""Django>=4.0,<4.1""', 'pip install mysqlclient>=2.0.1 django-mysql>=3.9.0', 'pip install psycopg2-binary>=2.8.6', 'pip install -U django-rest-framework rest-framework-generic-relations drf-spectacular', 'pip install -e .', 'coverage run runtests/manage.py test -v3 --noinput actstream testapp testapp_nested']"
"['python -m pip install --upgrade pip\npip install -r dev-requirements.txt\n', 'tox']"
""
"['git remote set-url --push origin ""https://:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}""', 'make install', 'make docs', 'make install', 'make release', 'make install', 'make lint', 'make -e DOCS_OPTS=""${{ matrix.opts }}"" docs']"
"['git checkout HEAD^2', 'sudo apt update\nsudo apt install python3-dbus python3-lxml python3-pip libaugeas-dev augeas-tools\n', 'sudo npm install -g npm\nsudo npm install -g bower babel-cli babel-preset-es2015 babel-plugin-external-helpers less coffee-script\nsudo npm install babel-cli babel-preset-es2015 babel-plugin-external-helpers\n', 'sudo python3 -m pip install --upgrade pip\nsudo python3 -m pip install wheel setuptools\n', 'sudo python3 -m pip install -r ajenti-core/requirements.txt\nsudo python3 -m pip install -r plugins/core/requirements.txt\nsudo python3 -m pip install -r plugins/augeas/requirements.txt\nsudo python3 -m pip install -r plugins/auth_users/requirements.txt\nsudo python3 -m pip install -r plugins/datetime/requirements.txt\nsudo python3 -m pip install -r plugins/services/requirements.txt\nsudo python3 -m pip install -r plugins/terminal/requirements.txt\nsudo python3 -m pip install ajenti-dev-multitool\nsudo python3 -m pip install pytest\nsudo python3 -m pip uninstall -y gevent-socketio-hartwork python-socketio\nsudo python3 -m pip install python-socketio==5.2.1\nsudo python3 -m pip install cryptography==39.0.0\nsudo python3 -m pip install pyOpenSSL==23.0.0\n', 'sudo ajenti-dev-multitool --bower install || exit 1\nsudo ajenti-dev-multitool --build || exit 1\n', 'cd tests-pytest\nsudo pytest -s || exit 1\ncd ..\n']"
"['python3 -m pip install --upgrade pip\npython3 -m pip install tox tox-gh-actions\n', 'tox -e lint', 'python3 -m pip install --upgrade pip\npython3 -m pip install tox tox-gh-actions\n', 'tox -e rst', 'python3 -m pip install --upgrade pip setuptools\npython3 -m pip install tox tox-gh-actions\n', 'GEOPY_TOX_TARGET=test-local tox', 'python3 -m pip install --upgrade pip setuptools\npython3 -m pip install tox tox-gh-actions\n', 'tox']"
"['sudo apt install -y binutils libproj-dev gdal-bin libsqlite3-mod-spatialite\n', 'python -m pip install --upgrade pip wheel virtualenv tox coveralls\n', 'coverage erase\n', 'tox -e py${{matrix.python-version}}-flake8\n', 'tox -e py${{matrix.python-version}}-docs\n', 'tox -e py${{matrix.python-version}}-dj${{matrix.django-version}}\n', 'coveralls --service=github']"
""
"['sudo DEBIAN_FRONTEND=noninteractive apt-get update\nsudo DEBIAN_FRONTEND=noninteractive apt-get install $DIST_PREREQ\n', '(cd build/pkgs/${{ env.SPKG }}/src && python3 setup.py sdist ) \\\n&& mkdir -p upstream && cp build/pkgs/${{ env.SPKG }}/src/dist/*.tar.gz upstream/${{ env.SPKG }}-git.tar.gz \\\n&& echo ""sage-package create ${{ env.SPKG }} --version git --tarball ${{ env.SPKG }}-git.tar.gz --type=standard"" > upstream/update-pkgs.sh \\\n&& if [ -n ""${{ env.REMOVE_PATCHES }}"" ]; then echo ""(cd ../build/pkgs/${{ env.SPKG }}/patches && rm -f ${{ env.REMOVE_PATCHES }}; :)"" >> upstream/update-pkgs.sh; fi \\\n&& ls -l upstream/\n', 'git config --global user.email ""ci-sage@example.com""\ngit config --global user.name ""ci-sage workflow""\nif [ ! -d .git ]; then git init; fi; git remote add trac ${{ env.SAGE_TRAC_GIT }} && x=1 && while [ $x -le 5 ]; do x=$(( $x + 1 )); sleep $(( $RANDOM % 60 + 1 )); if git-trac-command/git-trac fetch $SAGE_TICKET; then git merge FETCH_HEAD || echo ""(ignored)""; exit 0; fi; sleep 40; done; exit 1\n', 'sudo DEBIAN_FRONTEND=noninteractive apt-get update\nsudo DEBIAN_FRONTEND=noninteractive apt-get install tox python3-setuptools\n', '(export PATH=$(pwd)/build/bin:$PATH; (cd upstream && bash -x update-pkgs.sh) && sed -i.bak \'/upstream/d\' .dockerignore && echo ""/:toolchain:/i ADD upstream upstream"" | sed -i.bak -f - build/bin/write-dockerfile.sh && git diff)\n', 'set -o pipefail; EXTRA_DOCKER_BUILD_ARGS=""--build-arg USE_MAKEFLAGS=\\""-k V=0 SAGE_NUM_THREADS=3\\"""" tox -e $TOX_ENV -- $TARGETS 2>&1 | sed ""/^configure: notice:/s|^|::warning file=artifacts/$LOGS_ARTIFACT_NAME/config.log::|;/^configure: warning:/s|^|::warning file=artifacts/$LOGS_ARTIFACT_NAME/config.log::|;/^configure: error:/s|^|::error file=artifacts/$LOGS_ARTIFACT_NAME/config.log::|;""\n', 'mkdir -p ""artifacts/$LOGS_ARTIFACT_NAME""\ncp -r .tox/$TOX_ENV/Dockerfile .tox/$TOX_ENV/log ""artifacts/$LOGS_ARTIFACT_NAME""\nif [ -f .tox/$TOX_ENV/Dockertags ]; then CONTAINERS=$(docker create $(tail -1 .tox/$TOX_ENV/Dockertags) /bin/bash || true); fi\nif [ -n ""$CONTAINERS"" ]; then for CONTAINER in $CONTAINERS; do for ARTIFACT in /sage/logs; do docker cp $CONTAINER:$ARTIFACT artifacts/$LOGS_ARTIFACT_NAME && HAVE_LOG=1; done; if [ -n ""$HAVE_LOG"" ]; then break; fi; done; fi\n', '.github/workflows/scan-logs.sh ""artifacts/$LOGS_ARTIFACT_NAME""\n', 'if [ -f .tox/$TOX_ENV/Dockertags ]; then\n  TOKEN=""${{ secrets.DOCKER_PKG_GITHUB_TOKEN }}""\n  if [ -z ""$TOKEN"" ]; then\n    TOKEN=""${{ secrets.GITHUB_TOKEN }}""\n  fi\n  echo ""$TOKEN"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin\n  for a in $(cat .tox/$TOX_ENV/Dockertags); do\n    FULL_TAG=docker.pkg.github.com/$(echo ${{ github.repository }}|tr \'A-Z\' \'a-z\')/$a\n    docker tag $a $FULL_TAG\n    echo Pushing $FULL_TAG\n    docker push $FULL_TAG\n  done || echo ""(Ignoring errors)""\nfi\n', 'curl --fail ${{ steps.step1.outputs.url }} | grep $GITHUB_SHA\n', 'release/ci_release_script.sh ${{ env.release_version }} ${{ env.previous_version }}', 'ls -R', 'pip install sympy-${{ env.release_version }}-py3-none-any.whl', ""python -c 'import sympy; sympy.test()'"", 'python -m pip install --upgrade pip', 'pip install mpmath flake8 flake8-comprehensions ruff pytest', 'bin/test quality', 'flake8 sympy', 'ruff check .', 'python -We:invalid -We::SyntaxWarning -m compileall -f -q sympy/', 'bin/test_setup.py', 'python -m pip install --upgrade pip', 'pip install mpmath mypy', 'mypy sympy', 'python -m pip install --upgrade pip', 'pip install sphinx-lint', 'sphinx-lint doc/', 'python -m pip install --upgrade pip', 'pip install mpmath', 'bin/mailmap_check.py', 'python -m pip install --upgrade pip', 'pip install mpmath', 'bin/doctest --force-colors', 'examples/all.py -q', 'python -m pip install --upgrade pip', 'pip install mpmath pytest pytest-split', 'bin/test --split ${{ matrix.group }}/4', 'python -m pip install --upgrade pip', 'pip install build', 'wget -qO- https://github.com/pyodide/pyodide/releases/download/0.22.0/pyodide-core-0.22.0.tar.bz2 | tar xjf -', 'python -m build --wheel', 'node bin/test_pyodide.mjs --group=${{ matrix.group }} --splits=4 2>/dev/null', 'sudo apt-get update', 'sudo apt-get install antlr4 libgfortran5 gfortran libmpfr-dev libmpc-dev libopenblas-dev clang', 'python -m pip install --upgrade pip wheel setuptools', ""pip install mpmath numpy numexpr matplotlib ipython cython scipy \\ aesara wurlitzer autowrap lxml pytest                \\ 'antlr4-python3-runtime==4.11.*'"", 'pip install llvmlite numba', 'pip install gmpy2 jax jaxlib symengine pymc libclang', 'bin/test_external_imports.py', 'bin/test_submodule_imports.py', 'bin/test_executable.py', 'bin/test_optional_dependencies.py', 'python -m pip install --upgrade pip', 'pip install mpmath numpy scipy tensorflow pytest', 'bin/test_tensorflow.py', 'python -m pip install --upgrade pip', 'pip install mpmath numpy symengine pytest', 'bin/test_symengine.py', 'python -m pip install --upgrade pip', 'pip install mpmath pytest pytest-split pytest-timeout', 'bin/test --slow --timeout 595 --split ${{ matrix.group }}/4', 'python -m pip install --upgrade pip', 'pip install mpmath pytest pytest-split', 'bin/test --split ${{ matrix.group }}/4', 'python -m pip install --upgrade pip', 'pip install mpmath', 'bin/doctest --force-colors', 'examples/all.py -q', 'doc/aptinstall.sh', 'pip install -r doc/requirements.txt', 'bin/test_sphinx.sh', 'bin/test_py2_import.py', 'python -m pip install --upgrade pip build', 'python -m build --sdist', 'release/compare_tar_against_git.py dist/*.tar.gz .', 'pip install asv virtualenv', 'git submodule add https://github.com/sympy/sympy_benchmarks.git', 'git remote add upstream https://github.com/sympy/sympy.git', 'git fetch upstream master', 'git fetch upstream 1.12', 'asv machine --yes --config asv.conf.actions.json', 'asv run --config asv.conf.actions.json', 'asv compare upstream/master HEAD --config asv.conf.actions.json --factor 1.5 | tee pr_vs_master.txt', 'asv compare upstream/master HEAD --config asv.conf.actions.json --factor 1.5 --only-changed | tee pr_vs_master_changed.txt', 'asv compare upstream/1.12 upstream/master --config asv.conf.actions.json --factor 1.5 | tee master_vs_release.txt', 'asv compare upstream/1.12 upstream/master --config asv.conf.actions.json --factor 1.5 --only-changed | tee master_vs_release_changed.txt', 'echo -n ${{ github.event.number }} > pr_number.txt', 'echo -n > pr_number.txt']"
"['python3.10 -m pip install --upgrade pip\npython3.10 -m pip install --upgrade tox\n', 'python3.10 -m tox -q']"
"['python -m pip install --upgrade pip\npython -m pip install build --user\n', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e ""py-dj${{ matrix.django-version }}-sqlite""\ntox -e ""py-dj${{ matrix.django-version }}-postgres""\ntox -e ""py-dj${{ matrix.django-version }}-mysql""\n']"
"['ls /etc/yum.repos.d/\ncat /etc/yum.repos.d/CentOS-Base.repo\nrm /etc/yum.repos.d/CentOS-Base.repo\ncat > /etc/yum.repos.d/CentOS-Base.repo <<EOF\n[base]\nname=CentOS-$releasever - Base\nbaseurl=https://vault.centos.org/6.10/os/x86_64/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n[updates]\nname=CentOS-$releasever - Updates\nbaseurl=https://vault.centos.org/6.10/updates/x86_64/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n[extras]\nname=CentOS-$releasever - Extras\nbaseurl=https://vault.centos.org/6.10/extras/x86_64/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\nEOF\necho installing\nyum clean all\nyum repolist all\nyum install -y git make python curl gcc libffi-devel python-devel glibc-devel openssl-devel wget\n', 'apt-get update\napt-get install -y git make python-is-python3 python3 curl wget python3-distutils python3-pip\n', 'git config --global --add safe.directory /__w/python-ecdsa/python-ecdsa\n', 'git status\ngit remote -v\n', 'git fetch origin master:refs/remotes/origin/master', 'python -c ""import sys; print(sys.version)""', 'pip list || :\n', 'curl -o get-pip.py https://bootstrap.pypa.io/pip/3.3/get-pip.py\npython get-pip.py\n', '# pypi deprecated SNI-less access to the CDN, so we have to download\n# the packages manually\ncurl -o get-pip.py https://bootstrap.pypa.io/pip/2.6/get-pip.py\nwget https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/27/f6/fabfc9c71c9b1b99d2ec4768a6e1f73b2e924f51c89d436302b8c2a25459/setuptools-36.8.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/8a/e9/8468cd68b582b06ef554be0b96b59f59779627131aad48f8a5bce4b13450/wheel-0.29.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\npython get-pip.py pip-9.0.3-py2.py3-none-any.whl setuptools-36.8.0-py2.py3-none-any.whl wheel-0.29.0-py2.py3-none-any.whl argparse-1.4.0-py2.py3-none-any.whl\npip list\nwget https://files.pythonhosted.org/packages/3b/7e/293d19ccd106119e35db4bf3e111b1895098f618b455b758aa636496cf03/setuptools-28.8.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/83/53/e120833aa2350db333df89a40dea3b310dd9dabf6f29eaa18934a597dc79/wheel-0.30.0a0-py2.py3-none-any.whl\npip install setuptools-28.8.0-py2.py3-none-any.whl wheel-0.30.0a0-py2.py3-none-any.whl\n', 'pip install instrumental', 'pip install gmpy', 'sudo apt-get install -y libmpfr-dev libmpc-dev', 'pip install gmpy2', 'wget https://files.pythonhosted.org/packages/1d/4e/20c679f8c5948f7c48591fde33d442e716af66a31a88f5791850a75041eb/tox-2.9.1-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/d9/9d/077582a4c6d771e3b742631e6c1d3688f48210626de488e032776242b3f2/inflect-0.3.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/79/db/7c0cfe4aa8341a5fab4638952520d8db6ab85ff84505e12c00ea311c3516/pyOpenSSL-17.5.0-py2.py3-none-any.whl \nwget https://files.pythonhosted.org/packages/2d/bf/960e5a422db3ac1a5e612cb35ca436c3fc985ed4b7ed13a1b4879006f450/cffi-1.13.2.tar.gz\nwget https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/72/20/7f0f433060a962200b7272b8c12ba90ef5b903e218174301d0abfd523813/unittest2-1.1.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/a8/5a/5cf074e1c6681dcbb4e640113f58bed16955e7da9a6c8090b518031775e7/hypothesis-2.0.0.tar.gz\nwget https://files.pythonhosted.org/packages/85/d5/818d0e603685c4a613d56f065a721013e942088047ff1027a632948bdae6/coverage-4.5.4.tar.gz\nwget https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/ed/ea/e20b5cbebf45d3096e8138ab74eda139595d827677f38e9dd543e6015bdf/virtualenv-15.2.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/53/67/9620edf7803ab867b175e4fd23c7b8bd8eba11cb761514dcd2e726ef07da/py-1.4.34-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/4d/d1/e478b8a33230f85f38e35b386376fbd115219de2a2c4c8783610851ad1c3/pluggy-0.5.2-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/78/c5/7188f15a92413096c93053d5304718e1f6ba88b818357d05d19250ebff85/cryptography-2.1.4.tar.gz\nwget https://files.pythonhosted.org/packages/8c/2d/aad7f16146f4197a11f8e91fb81df177adcc2073d36a17b1491fd09df6ed/pycparser-2.18.tar.gz\nwget https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\nwget https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/17/0a/6ac05a3723017a967193456a2efa0aa9ac4b51456891af1e2353bb9de21e/traceback2-1.4.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/31/77/3781f65cafe55480b56914def99022a5d2965a4bb269655c89ef2f1de3cd/importlib-1.0.4.zip\nwget https://files.pythonhosted.org/packages/7d/b0/23d19892f8d91ec9c5b8a2035659bce23587fed419d68fa3d70b6abf8bcd/Counter-1.0.0.tar.gz\nwget https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/b5/a8/56be92dcd4a5bf1998705a9b4028249fe7c9a035b955fe93b6a3e5b829f8/asn1crypto-1.4.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl\nwget https://files.pythonhosted.org/packages/c2/f8/49697181b1651d8347d24c095ce46c7346c37335ddc7d255833e7cde674d/ipaddress-1.0.23-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/c7/a3/c5da2a44c85bfbb6eebcfc1dde24933f8704441b98fdde6528f4831757a6/linecache2-1.0.0-py2.py3-none-any.whl\nwget https://files.pythonhosted.org/packages/53/25/ef88e8e45db141faa9598fbf7ad0062df8f50f881a36ed6a0073e1572126/ordereddict-1.1.tar.gz\nwget https://files.pythonhosted.org/packages/ef/41/d8a61f1b2ba308e96b36106e95024977e30129355fd12087f23e4b9852a1/pytest-3.2.5-py2.py3-none-any.whl\npip install pycparser-2.18.tar.gz importlib-1.0.4.zip Counter-1.0.0.tar.gz tox-2.9.1-py2.py3-none-any.whl inflect-0.3.0-py2.py3-none-any.whl pyOpenSSL-17.5.0-py2.py3-none-any.whl cffi-1.13.2.tar.gz idna-2.7-py2.py3-none-any.whl unittest2-1.1.0-py2.py3-none-any.whl hypothesis-2.0.0.tar.gz coverage-4.5.4.tar.gz mock-2.0.0-py2.py3-none-any.whl virtualenv-15.2.0-py2.py3-none-any.whl py-1.4.34-py2.py3-none-any.whl six-1.13.0-py2.py3-none-any.whl pluggy-0.5.2-py2.py3-none-any.whl cryptography-2.1.4.tar.gz docopt-0.6.2.tar.gz requests-2.19.1-py2.py3-none-any.whl traceback2-1.4.0-py2.py3-none-any.whl funcsigs-1.0.2-py2.py3-none-any.whl pbr-5.5.1-py2.py3-none-any.whl asn1crypto-1.4.0-py2.py3-none-any.whl enum34-1.1.10-py2-none-any.whl ipaddress-1.0.23-py2.py3-none-any.whl chardet-3.0.4-py2.py3-none-any.whl urllib3-1.23-py2.py3-none-any.whl certifi-2020.12.5-py2.py3-none-any.whl linecache2-1.0.0-py2.py3-none-any.whl ordereddict-1.1.tar.gz pytest-3.2.5-py2.py3-none-any.whl git+https://github.com/tomato42/coveralls-python.git@add-py26#egg=coveralls\n', 'PYTHON_VERSION=${{ matrix.python-version }}\nPYTHON_VERSION=${PYTHON_VERSION#pypy-}\nif [[ -e build-requirements-${PYTHON_VERSION}.txt ]]; then\n  pip install -r build-requirements-${PYTHON_VERSION}.txt;\nelse\n  pip install -r build-requirements.txt;\nfi\n', 'pip list', 'tox -e speed', 'tox -e speedgmpy', 'tox -e speedgmpy2', '# because tox uses pip, and pip is broken on py2.6, we need run the\n# tests directly on the system\ncoverage run --branch -m pytest\n', 'tox -e ${{ matrix.tox-env }}', 'git fetch origin $BASE_REF\nMERGE_BASE=$(git merge-base origin/$BASE_REF HEAD)\necho ""MERGE_BASE:"" $MERGE_BASE\ngit checkout $MERGE_BASE\ninstrumental -t ecdsa -i \'.*test_.*|.*_version|.*_compat|.*_sha3\' `which pytest` src/ecdsa/test*.py\ninstrumental -f .instrumental.cov -s\ninstrumental -f .instrumental.cov -s | python diff-instrumental.py --save .diff-instrumental\ngit checkout $GITHUB_SHA\ninstrumental -t ecdsa -i \'.*test_.*|.*_version|.*_compat|.*_sha3\' `which pytest` src/ecdsa/test*.py\ninstrumental -f .instrumental.cov -sr\ninstrumental -f .instrumental.cov -s | python diff-instrumental.py --read .diff-instrumental --fail-under 70 --max-difference -0.1\n', 'instrumental -t ecdsa -i \'.*test_.*|.*_version|.*_compat|.*_sha3\' `which pytest` src/ecdsa\ninstrumental -f .instrumental.cov -s\n# just log the values when merging\ninstrumental -f .instrumental.cov -s | python diff-instrumental.py\necho ""COND_COV=$(instrumental -f .instrumental.cov -s | python diff-instrumental.py --raw)"" >> $GITHUB_ENV\n', 'coveralls', 'pip3 install --upgrade coveralls\n', 'coveralls --finish\n']"
"['sudo apt-get update\nsudo apt-get install graphviz\n', 'python -m pip install --upgrade pip setuptools coverage rstvalidator\npip install -r docs/requirements.txt\n', 'python -m rstvalidator long_description.rst\npython tools/fixup_whats_new_pr.py\nmake -C docs/ html SPHINXOPTS=""-W"" \\\n  PYTHON=""coverage run -a"" \\\n  SPHINXBUILD=""coverage run -a -m sphinx.cmd.build""\n', 'coverage combine `find . -name .coverage\\*` && coverage xml\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'cd .. \ngit clone https://github.com/ipython/ipykernel\ncd ipykernel\npip install -e .[test] \ncd ..\n', ""python -m pip install --upgrade -e file://$PWD#egg=ipython[test]\n# we must install IPython after ipykernel to get the right versions.\npython -m pip install --upgrade --upgrade-strategy eager flaky ipyparallel\npython -m pip install --upgrade 'pytest<7'  'pytest_asyncio<0.21'\n"", 'cd ../ipykernel\npytest\n', 'python -m pip install --upgrade pip\npip install mypy pyflakes flake8 types-decorator\n', 'set -e\nmypy IPython\n', 'set -e\nflake8  IPython/core/magics/script.py\nflake8  IPython/core/magics/packaging.py\n', 'python -m pip install build\npython -m build\n', ""python -m pip install --upgrade pip\npip install --only-binary ':all:' darker==1.5.1 black==22.10.0\n"", 'darker -r 60625f241f298b5039cb2debc365db38aa7bb522 --check --diff . || (\necho ""Changes need auto-formatting. Run:""\necho ""    darker -r 60625f241f298b5039cb2debc365db38aa7bb522""\necho ""then commit and push changes to fix.""\nexit 1\n)\n', 'echo ""disable latex for now, issues in mirros""', ""python -m pip install --only-binary ':all:' --upgrade pip setuptools wheel build\npython -m pip install --only-binary ':all:' --no-binary curio --upgrade -e .[${{ matrix.deps }}]\npython -m pip install --only-binary ':all:' --upgrade check-manifest pytest-cov  pytest-json-report\n"", 'python -m pip install --pre --upgrade pip setuptools wheel build\npython -m pip install --pre --extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple --no-binary curio --upgrade -e .[${{ matrix.deps }}]\npython -m pip install --pre --extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple --upgrade check-manifest pytest-cov  pytest-json-report\n', 'python -m build\nshasum -a 256 dist/*\n', 'check-manifest', ""pytest --color=yes -raXxs ${{ startsWith(matrix.python-version, 'pypy') && ' ' || '--cov --cov-report=xml' }} --json-report --json-report-file=./report-${{ matrix.python-version }}-${{runner.os}}.json\n""]"
"['pip install -U pip tox', 'tox -e ${{ matrix.tox-env }}', 'pip install tox -U pip', 'tox -e package -e lint -e pyupgrade', 'pip install -e . && pip install sphinx', 'echo ""RELEASE_TAG=${{ steps.get-latest-tag.outputs.tag }}"" >> $GITHUB_ENV', 'sphinx-build -b html docs docs/build']"
"['python -m pip install --upgrade pip setuptools wheel build\n', 'python -m build\n', 'python -m pip install --upgrade pip setuptools\npython -m pip install -r doc-requirements.txt\n', 'python -m mkdocs build --clean --verbose\n', 'python -m pip install --upgrade pip setuptools\npython -m pip install -r doc-requirements.txt\n', 'python -m mkdocs build --clean --verbose\n', 'sudo apt-get install libtidy-dev\npython -m pip install --upgrade pip tox coverage codecov\n', 'python -m tox', 'python -m pip install --upgrade pip tox\nif [[ ""$TOXENV"" == \'checkspelling\' ]]; then sudo apt-get install aspell aspell-en; fi\n', 'python -m tox']"
"['pip install --upgrade pip wheel setuptools\npip install -r requirements.txt\npip list\n', 'pre-commit run --all-files --show-diff-on-failure --color always', 'python -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements.txt\npython -m pip list\n', 'make test']"
"[""python -m pip install --upgrade pip\npython -m pip install --pre django'${{ matrix.django-version }}'\npython -m pip install flake8 coverage sphinx sphinx_rtd_theme psycopg2 mysqlclient -e .\n"", 'flake8\n', 'coverage run tests/manage.py test tests\ncoverage report\n', '(cd docs && sphinx-build -n -W . _build)\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
[]
"['pip install -r requirements.txt', 'tox', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install --upgrade -r requirements.txt\npip install .\n', 'make ci\n']"
""
""
"['sudo apt-get update && sudo apt-get install libcurl4-openssl-dev libssl-dev', 'pip install --upgrade pip wheel tox tox-docker', 'tox -v -e ${{ matrix.python-version }}-linux-unit -- -v', 'sudo apt-get update && sudo apt-get install libcurl4-openssl-dev libssl-dev', 'pip install --upgrade pip wheel tox tox-docker', 'tox -v -e ${{ matrix.python-version }}-linux-integration-py-amqp -- -v', 'tox -v -e ${{ matrix.python-version }}-linux-integration-py-redis -- -v', 'tox -v -e ${{ matrix.python-version }}-linux-integration-py-mongodb -- -v', 'tox -v -e ${{ matrix.python-version }}-linux-integration-py-kafka -- -v', 'sudo apt-get update && sudo apt-get install libcurl4-openssl-dev libssl-dev', 'pip install --upgrade pip wheel tox tox-docker', 'tox -v -e flake8 -- -v', 'tox -v -e pydocstyle -- -v', 'tox -v -e apicheck -- -v', 'tox -v -e mypy -- -v', 'semgrep ci']"
"['gh extension install actions/gh-actions-cache\n\nREPO=${{ github.repository }}\nBRANCH=""refs/pull/${{ github.event.pull_request.number }}/merge""\n\necho ""Fetching list of cache key""\ncacheKeysForPR=$(gh actions-cache list -R $REPO -B $BRANCH | cut -f 1 )\n\n## Setting this to not fail the workflow while deleting cache keys. \nset +e\necho ""Deleting caches...""\nfor cacheKey in $cacheKeysForPR\ndo\n    gh actions-cache delete $cacheKey -R $REPO -B $BRANCH --confirm\ndone\necho ""Done""\n', 'npm ci', './tools/ci/check_for_commit_message.sh', 'npm run check-pinned-versions', 'npm run check-license', 'npm run check-absolute-paths', 'npm run lint', 'npm run style-lint', '# https://jestjs.io/docs/en/troubleshooting.html#tests-are-extremely-slow-on-docker-andor-continuous-integration-ci-server\nsed -i \'s/""test"": ""jest""/""test"": ""jest --runInBand""/g\' package.json\n\nnpm run test', './tools/ci/check_for_commit_message.sh\n', 'sudo apt-get update\nsudo apt-get install -y gcc g++ build-essential python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv python${{ matrix.python-version }}-distutils asciidoc rsync curl sudo libkrb5-dev libldap2-dev libsasl2-dev libxml2-dev libxslt-dev  libsasl2-modules-gssapi-mit libsnappy-dev libffi-dev # This should not be needed as some point\nsudo curl -sL https://deb.nodesource.com/setup_14.x | sudo bash - && sudo apt-get install -y nodejs\nsudo curl -sL https://bootstrap.pypa.io/get-pip.py | sudo python${{ matrix.python-version }}\nsudo apt-get install -y python3-setuptools\nsudo apt-get install -y libncursesw5-dev libgdbm-dev libc6-dev libssl-dev openssl\n\nexport PYTHON_VER=python${{ matrix.python-version }}\nexport ROOT=$PWD\nmake apps\n', './build/env/bin/pip install pylint==2.5.3 pylint-django==2.3.0 configparser==5.3.0\n./tools/ci/check_for_python_lint.sh\n', 'PYTHONWARNINGS=always ./build/env/bin/hue test unit --with-xunit --with-cover\n']"
"['python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox -e release']"
"['pip install -r development.txt', 'pip install -e .', 'pytest -vv']"
""
""
"['sudo apt-get install aspell aspell-en enchant iamerican ispell', 'pip install -U pip wheel', 'pip install -r requirements-ci.txt -r requirements-cidb.txt', 'coverage run --rcfile=.coveragerc $(which trial) --reporter=text --rterrors buildbot.test buildbot_worker.test', 'curl -Os https://uploader.codecov.io/v0.4.1/linux/codecov\necho ""32cb14b5f3aaacd67f4c1ff55d82f037d3cd10c8e7b69c051f27391d2e66e15c  codecov"" | sha256sum --check\nchmod +x ./codecov\nCODECOV_TOKEN=""b80c80d7-689d-46d7-b1aa-59168bb4c9a9"" ./codecov\n', 'python -c ""import sys; print(sys.prefix)""\npython -c ""import sys; print(sys.exec_prefix)""\npython -c ""import sys; print(sys.executable)""\npython -V -V\npython -m pip install -U pip setuptools wheel\npython -m pip install -r requirements-ci.txt\npython -m pip list\n# Check that pywin32 is properly installed\npython -c ""import win32api""\n', 'python -m twisted.trial --reporter=text --rterrors buildbot.test buildbot_worker.test']"
"['grep -v plugins .coveragerc > .coveragerc-save\nmv .coveragerc-save .coveragerc\n', 'echo ""PYZMQ_CYTHON_COVERAGE=1"" >> ""$GITHUB_ENV""\n', 'pip install --upgrade pip wheel\npip install -r test-requirements.txt\n', 'pip uninstall -y tornado\n', 'pip install https://github.com/tornadoweb/tornado/archive/HEAD.zip\n', 'pip install tornado==""${{ matrix.tornado }}""\n', 'pip freeze\n', 'brew install libsodium zeromq\n', 'sudo apt-get update\nsudo apt-get -y remove libzmq5 || true # workaround https://github.com/actions/virtual-environments/issues/3317\nsudo apt-get -y install libzmq3-dev libsodium-dev\n', 'echo ""ZMQ_PREFIX=${{ matrix.zmq }}"" >> ""$GITHUB_ENV""\n', 'wget https://github.com/zeromq/libzmq/archive/HEAD.zip -O libzmq.zip\nunzip libzmq.zip\npushd libzmq-*\n./autogen.sh\n./configure --enable-drafts\nmake -j4\nsudo make install\nsudo ldconfig\npopd\necho ""ZMQ_PREFIX=/usr/local"" >> ""$GITHUB_ENV""\necho ZMQ_DRAFT_API=1 >> ""$GITHUB_ENV""\n', 'pip install -v -e .\n', 'python -c ""import zmq""\n', 'python -m pytest --maxfail 2 --cov zmq -m ""not wheel and not new_console"" -v zmq/tests\n', 'codecov', 'python -m buildutils.bundle checksums', 'pip install --upgrade pip build pytest\npip install -r tools/wheel-requirements.txt\n', 'python setup.py fetch_libzmq\npython setup.py cython\npython -m build --sdist .\n', 'pytest -v tools/test_sdist.py', 'pip install twine\ntwine upload --skip-existing dist/*.tar.gz\n', 'echo \'MACOSX_DEPLOYMENT_TARGET=10.15\' >> ""$GITHUB_ENV""\n', 'pip install --upgrade setuptools pip wheel\npip install -r tools/wheel-requirements.txt\n', 'pip freeze\n', 'python -m cibuildwheel . --print-build-identifiers\n', 'python -m cibuildwheel .\n', 'pip install twine\ntwine upload --skip-existing wheelhouse/*\n']"
"['cargo test\n', 'just venv pytest\n']"
['pip install tox\ntox -e ${{ matrix.session }}\n']
"['python -c ""import sys; print(sys.version)""', 'tools/ci/create_venv.sh', 'source tools/ci/build_archive.sh\necho ""ARCHIVE=$ARCHIVE"" >> $GITHUB_ENV\n', 'tools/ci/install_deb_dependencies.sh', 'tools/ci/install_dependencies.sh', 'tools/ci/install.sh', 'tools/ci/check.sh', 'pip install --upgrade build twine', 'python -m build', 'twine check dist/*', 'python -c ""import sys; print(sys.version)""', 'pip install --upgrade pip', 'pip install dist/nipype-*.whl', 'pip install dist/nipype-*.tar.gz', ""python -c 'import nipype; print(nipype.__version__)'"", 'pip install nipype[tests]', 'pytest --doctest-modules -v --pyargs nipype', 'python -c ""import sys; print(sys.version)""', 'tools/ci/create_venv.sh', 'source tools/ci/build_archive.sh\necho ""ARCHIVE=$ARCHIVE"" >> $GITHUB_ENV\n', 'tools/ci/install_deb_dependencies.sh', 'tools/ci/install_dependencies.sh', 'tools/ci/install.sh', 'tools/ci/check.sh', 'echo ""::set-output name=start_time::$(date +\'%Y-%m-%dT%H:%M:%S%z\')""', 'set -x\ncurl -X POST \\\n-H ""Authorization: Bearer ${{ secrets.TUTORIAL_ACCESS_TOKEN }}"" \\\n-H ""Accept: application/vnd.github+json"" \\\nhttps://api.github.com/repos/miykael/nipype_tutorial/actions/workflows/testing.yml/dispatches \\\n-d \'{""ref"": ""master"", ""inputs"": {""nipype_branch"": ""\'${BRANCH_NAME}\'""}}\'\nsleep 10\n', 'START=${{ steps.start.outputs.start_time }}\nRUN_ID=$(curl -s -H ""Accept: application/vnd.github+json"" \\\n  \'https://api.github.com/repos/miykael/nipype_tutorial/actions/runs?created=>\'${START}\'&per_page=1\' \\\n  | jq -r \'.workflow_runs[0].id\')\n\n# fail if not extracted\n[[ -n $RUN_ID ]] || exit 1\necho ""::set-output name=run_id::$RUN_ID""\n', 'RUN_ID=${{ steps.dispatched.outputs.run_id }}\nwhile :\ndo\n  TIMESTAMP=$(date +\'%Y-%m-%dT%H:%M:%S%z\')\n  # check status every 5 minutes\n  STATUS=$(curl -s -H ""Accept: application/vnd.github+json"" \\\n  https://api.github.com/repos/miykael/nipype_tutorial/actions/runs/${RUN_ID} \\\n  | jq -r \'.conclusion\')\n  case $STATUS in\n  success)\n    echo ""[$TIMESTAMP] Tutorial run $RUN_ID completed successfully.""\n    exit 0\n    ;;\n  failure)\n    echo ""[$TIMESTAMP] Tutorial run $RUN_ID failed.""\n    exit 1\n    ;;\n  *)\n    echo ""[$TIMESTAMP] Conclusion ($STATUS) is not yet complete""\n    sleep 300\n  esac\ndone\n', 'set -x\nRUN_ID=${{ steps.dispatched.outputs.run_id }}\necho ""Something went wrong, cancelling dispatched run""\ncurl -s -X POST \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.TUTORIAL_ACCESS_TOKEN }}"" \\\n  https://api.github.com/repos/miykael/nipype_tutorial/actions/runs/${RUN_ID}/cancel\n']"
"['pip install --upgrade pip\npip install flake8 pytest coverage wheel\n', '# stop the build if there are Python syntax errors or undefined names\n# flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --select=E9,F63,F7 --show-source --statistics\n# exit-zero treats all errors as warnings. The Sfepy style should be 79 chars \nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=79 --statistics\n', 'pip install .\n', '# We create a special directory for running coverage, to ensure that\n# the installed package is used, rather than the source in current directory.\n# coverage results will be left in this directory.\nmkdir -p coverage\ncd coverage\ncoverage run --source=../sfepy/ -m pytest ../sfepy/tests -v -x --durations=10\n']"
"['python -c ""import sys; print(sys.version)""', 'tools/ci/create_venv.sh', 'source tools/ci/build_archive.sh\necho ""ARCHIVE=$ARCHIVE"" >> $GITHUB_ENV\n', 'tools/ci/install_dependencies.sh', 'tools/ci/install.sh', 'tools/ci/check.sh', 'python -c ""import sys; print(sys.version)""', 'tools/ci/create_venv.sh', 'source tools/ci/build_archive.sh\necho ""ARCHIVE=$ARCHIVE"" >> $GITHUB_ENV\n', 'tools/ci/install_dependencies.sh', 'tools/ci/install.sh', 'tools/ci/check.sh', 'pip install --upgrade build twine', 'python -m build', 'twine check dist/*', 'mkdir archive && git archive -v -o archive/nibabel-archive.tgz HEAD', 'python -c ""import sys; print(sys.version)""', 'pip install --upgrade pip', 'pip install dist/nibabel-*.whl', 'pip install dist/nibabel-*.tar.gz', 'pip install archive/nibabel-archive.tgz', ""python -c 'import nibabel; print(nibabel.__version__)'"", 'pip install nibabel[test]', 'pytest --doctest-modules --doctest-plus -v --pyargs nibabel', 'python -c ""import sys; print(sys.version)""', 'tools/ci/create_venv.sh', 'source tools/ci/build_archive.sh\necho ""ARCHIVE=$ARCHIVE"" >> $GITHUB_ENV\n', 'tools/ci/install_dependencies.sh', 'tools/ci/install.sh', 'tools/ci/check.sh']"
"['make install', 'make lint', 'make test', 'T_FLAGS=""-n 10 --junitxml=report/junit.xml --tb=native""\n\n# json report does not support 2.7\nif [[ ""${{ matrix.python-version }}"" != ""2.7"" ]]; then\n  T_FLAGS=""$T_FLAGS --json-report --json-report-file=report/report.json""\nfi\n\n# allowed to fail for test runs\nmake test-update T_FLAGS+=""$T_FLAGS"" || true\n', './utils/report.py report/report.json >> $GITHUB_STEP_SUMMARY\n']"
"['gh extension install actions/gh-actions-cache\n\nREPO=${{ github.repository }}\nBRANCH=""refs/pull/${{ github.event.pull_request.number }}/merge""\n\necho ""Fetching list of cache key""\ncacheKeysForPR=$(gh actions-cache list -R $REPO -B $BRANCH | cut -f 1 )\n\n## Setting this to not fail the workflow while deleting cache keys.\nset +e\necho ""Deleting caches...""\nfor cacheKey in $cacheKeysForPR\ndo\n    gh actions-cache delete $cacheKey -R $REPO -B $BRANCH --confirm\ndone\necho ""Done""\n', 'cd ci && ./code_checks.sh doctests', 'ci/code_checks.sh single-docs', 'ci/code_checks.sh code', 'ci/code_checks.sh docstrings', 'ci/code_checks.sh notebooks', 'echo $PATH >> $GITHUB_PATH\necho ""PYTHONHOME=$PYTHONHOME"" >> $GITHUB_ENV\necho ""PYTHONPATH=$PYTHONPATH"" >> $GITHUB_ENV\n', 'pytest scripts', 'cd asv_bench\nasv machine --yes\nasv run --quick --dry-run --strict --durations=30 --python=same\n', 'docker image prune -f', 'docker build --pull --no-cache --tag pandas-dev-env .', 'docker run --rm pandas-dev-env python -c ""import pandas as pd; print(pd.show_versions())""', 'pip install -r requirements-dev.txt', 'echo ${{ steps.setup_python.outputs.cache-hit }}', 'echo ""Assigning issue ${{ github.event.issue.number }} to ${{ github.event.comment.user.login }}""\ncurl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" -d \'{""assignees"": [""${{ github.event.comment.user.login }}""]}\' https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/assignees\n', 'if curl --output /dev/null --silent --head --fail ""https://pandas.pydata.org/preview/${{ github.event.issue.number }}/""; then\n  curl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" -d \'{""body"": ""Website preview of this PR available at: https://pandas.pydata.org/preview/${{ github.event.issue.number }}/""}\' https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/comments\nelse\n  curl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" -d \'{""body"": ""No preview found for PR #${{ github.event.issue.number }}. Did the docs build complete?""}\' https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/comments\nfi\n', '# extracting the regex, see https://stackoverflow.com/a/36798723\nREGEX=$(echo ""$COMMENT"" | sed -n ""s/^.*-b\\s*\\(\\S*\\).*$/\\1/p"")\ncd asv_bench\nasv check -E existing\ngit remote add upstream https://github.com/pandas-dev/pandas.git\ngit fetch upstream\nasv machine --yes\nasv continuous -f 1.1 -b $REGEX upstream/main HEAD\necho \'BENCH_OUTPUT<<EOF\' >> $GITHUB_ENV\nasv compare -f 1.1 upstream/main HEAD >> $GITHUB_ENV\necho \'EOF\' >> $GITHUB_ENV\necho ""REGEX=$REGEX"" >> $GITHUB_ENV\n', 'echo ""Adding deprecation PR number to deprecation tracking issue""\nexport PR=${{ github.event.pull_request.number }}\nBODY=$(curl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{ github.repository }}/issues/${DEPRECATION_TRACKER_ISSUE} |\n  python3 -c ""import sys, json, os; x = {\'body\': json.load(sys.stdin)[\'body\']}; pr = os.environ[\'PR\']; x[\'body\'] += f\'\\n- [ ] #{pr}\'; print(json.dumps(x))"")\necho ${BODY}\ncurl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" -X PATCH -d ""${BODY}"" https://api.github.com/repos/${{ github.repository }}/issues/${DEPRECATION_TRACKER_ISSUE}\n', 'python web/pandas_web.py web/pandas --target-path=web/build', 'doc/make.py --warnings-are-errors', 'doc/make.py zip_html', 'mkdir -m 700 -p ~/.ssh\necho ""${{ secrets.server_ssh_key }}"" > ~/.ssh/id_rsa\nchmod 600 ~/.ssh/id_rsa\necho ""${{ secrets.server_ip }} ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFjYkJBk7sos+r7yATODogQc3jUdW1aascGpyOD4bohj8dWjzwLJv/OJ/fyOQ5lmj81WKDk67tGtqNJYGL9acII="" > ~/.ssh/known_hosts\n', 'cp doc/cheatsheet/Pandas_Cheat_Sheet* web/build/', ""rsync -az --delete --exclude='pandas-docs' --exclude='docs' web/build/ web@${{ secrets.server_ip }}:/var/www/html"", 'rsync -az --delete doc/build/html/ web@${{ secrets.server_ip }}:/var/www/html/pandas-docs/dev', 'rsync -az --delete doc/build/html/ web@${{ secrets.server_ip }}:/var/www/html/pandas-docs/version/${GITHUB_REF_NAME:1}', 'mv doc/build/html web/build/docs', 'curl -X POST https://pandas.pydata.org/preview/submit/$RUN_ID/$PR_ID/', 'python -m pip install .[${{ matrix.extra }}] -v\n', 'conda mambabuild ci --no-anaconda-upload --verify --strict-verify --output --output-folder .', 'sudo apt-get update && sudo apt-get install -y xsel ${{ env.EXTRA_APT }}', 'sudo locale-gen ${{ matrix.extra_loc }}\n', 'git config --global --add safe.directory $PWD\n\nif [ $GITHUB_EVENT_NAME != pull_request ]; then\n    git clone --recursive --branch=$GITHUB_REF_NAME https://github.com/${GITHUB_REPOSITORY}.git $GITHUB_WORKSPACE\n    git reset --hard $GITHUB_SHA\nelse\n    git clone --recursive https://github.com/${GITHUB_REPOSITORY}.git $GITHUB_WORKSPACE\n    git fetch origin $GITHUB_REF:my_ref_name\n    git checkout $GITHUB_BASE_REF\n    git -c user.email=""you@example.com"" merge --no-commit my_ref_name\nfi\n', ""/opt/python/cp39-cp39/bin/python -m venv ~/virtualenvs/pandas-dev\n. ~/virtualenvs/pandas-dev/bin/activate\npython -m pip install -U pip wheel setuptools meson[ninja]==1.0.1 meson-python==0.13.1\npython -m pip install --no-cache-dir versioneer[toml] cython numpy python-dateutil pytz pytest>=7.0.0 pytest-xdist>=2.2.0 pytest-asyncio>=0.17 hypothesis>=6.46.1\npython -m pip install --no-cache-dir --no-build-isolation -e .\npython -m pip list --no-cache-dir\nexport PANDAS_CI=1\npython -m pytest -m 'not slow and not network and not clipboard and not single_cpu' pandas --junitxml=test-data.xml\n"", 'git config --global --add safe.directory $PWD\n\nif [ $GITHUB_EVENT_NAME != pull_request ]; then\n    git clone --recursive --branch=$GITHUB_REF_NAME https://github.com/${GITHUB_REPOSITORY}.git $GITHUB_WORKSPACE\n    git reset --hard $GITHUB_SHA\nelse\n    git clone --recursive https://github.com/${GITHUB_REPOSITORY}.git $GITHUB_WORKSPACE\n    git fetch origin $GITHUB_REF:my_ref_name\n    git checkout $GITHUB_BASE_REF\n    git -c user.email=""you@example.com"" merge --no-commit my_ref_name\nfi\n', 'apk update\napk add musl-locales\n', '/opt/python/cp39-cp39/bin/python -m venv ~/virtualenvs/pandas-dev\n. ~/virtualenvs/pandas-dev/bin/activate\npython -m pip install -U pip wheel setuptools meson-python==0.13.1 meson[ninja]==1.0.1\npython -m pip install --no-cache-dir versioneer[toml] cython numpy python-dateutil pytz pytest>=7.0.0 pytest-xdist>=2.2.0 pytest-asyncio>=0.17 hypothesis>=6.46.1\npython -m pip install --no-cache-dir --no-build-isolation -e .\npython -m pip list --no-cache-dir\n', "". ~/virtualenvs/pandas-dev/bin/activate\nexport PANDAS_CI=1\npython -m pytest -m 'not slow and not network and not clipboard and not single_cpu' pandas --junitxml=test-data.xml\n"", 'python --version\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --pre --extra-index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy\npython -m pip install git+https://github.com/nedbat/coveragepy.git\npython -m pip install versioneer[toml]\npython -m pip install python-dateutil pytz cython hypothesis>=6.46.1 pytest>=7.0.0 pytest-xdist>=2.2.0 pytest-cov pytest-asyncio>=0.17\npython -m pip list\n', 'python -m pip install -e . --no-build-isolation --no-index\n', 'python -c ""import pandas; pandas.show_versions();""\n', 'python -m pip install build\npython -m build --sdist\n', 'echo ""sdist_name=$(ls ./dist)"" >> ""$GITHUB_OUTPUT""', 'for whl in $(ls wheelhouse); do wheel unpack wheelhouse/$whl -d /tmp; done', '$TST_CMD = @""\npython -m pip install pytz six numpy python-dateutil tzdata>=2022.1 hypothesis>=6.46.1 pytest>=7.0.0 pytest-xdist>=2.2.0 pytest-asyncio>=0.17;\npython -m pip install --find-links=pandas\\wheelhouse --no-index pandas;\npython -c `\'import pandas as pd; pd.test()`\';\n""@\ndocker pull python:${{ matrix.python[1] }}-windowsservercore\ndocker run --env PANDAS_CI=\'1\' -v ${PWD}:C:\\pandas python:${{ matrix.python[1] }}-windowsservercore powershell -Command $TST_CMD\n', 'source ci/upload_wheels.sh\nset_upload_vars\nupload_wheels\n']"
"['echo ""webpack-path=.webpack_cache"" >> ""$GITHUB_OUTPUT""', 'yarn install --frozen-lockfile', 'yarn build-acceptance\n', 'make build-chartcuterie-config\n', 'mkdir -p ${{ steps.setup.outputs.acceptance-dir }}\nmkdir -p ${{ steps.setup.outputs.acceptance-dir }}-mobile\nmkdir -p ${{ steps.setup.outputs.acceptance-dir }}-tooltips\nmake run-acceptance\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'yarn add ts-node && make test-api-docs\n', 'make test-python-ci\n', 'MIGRATIONS_TEST_MIGRATE=1 PYTEST_ADDOPTS=""$PYTEST_ADDOPTS -m migrations"" make test-python-ci\n', 'make test-cli\n', ""python -S -m tools.freeze_requirements\nif ! git diff --exit-code; then\n  echo $'\\n\\nrun `make freeze-requirements` locally to update requirements'\n  exit 1\nfi\n"", 'pre-commit install-hooks\n', ""# Run pre-commit to lint and format check files that were changed (but not deleted) compared to master.\n# XXX: there is a very small chance that it'll expand to exceed Linux's limits\n#      `getconf ARG_MAX` - max # bytes of args + environ for exec()\npre-commit run --files ${{ steps.files.outputs.all_files }}\n"", './.github/workflows/scripts/migration-check.sh\n', 'make test-plugins\n', ""# pull relay we'll run and kill it for each test\ndocker pull us.gcr.io/sentryio/relay:nightly\ndocker ps -a\n"", 'make test-relay-integration\n', 'make test-snuba\n', 'echo $PWD\ndocker run \\\n  -d \\\n  -v $PWD/config/symbolicator/:/etc/symbolicator \\\n  --network host \\\n  --name symbolicator \\\n  us.gcr.io/sentryio/symbolicator:nightly \\\n  run -c /etc/symbolicator/config.yml\ndocker ps -a\n', 'make test-symbolicator\n', 'SENTRY_LIGHT_BUILD=1 pip install --no-deps -e .\nsentry init\n', 'make backend-typing', 'python3 -m tools.mypy_helpers.make_module_ignores\ngit diff --exit-code\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'bash <(curl -s https://raw.githubusercontent.com/getsentry/bootstrap-sentry/master/bootstrap.sh)\n', ""cd getsentry\n\npython -S -m bin.bump_sentry ${{ github.sha }}\n\n# If getsentry is pushed to by any other means while we were here,\n# we won't be able to push.\nfor i in 1 2 3 4 5; do\n    git push origin master && exit 0\n    # There's a little bit of network delay here that suffices\n    # as a small sleep.\n    git \\\n      -c user.name=getsentry-bot \\\n      -c user.email=bot@sentry.io \\\n      pull --rebase origin master\ndone\n\n# 5th and final attempt.\ngit push origin master\n"", 'set -euxo pipefail\n\nif [ ""$VERSION"" = latest ]; then\n  VERSION=""$(curl -sL https://pypi.org/pypi/$PACKAGE/json | jq -r .info.version)""\nfi\n\ngit checkout -b ""bot/bump-version/$PACKAGE/$VERSION""\n\nre=""$(sed \'s/[_-]/[_-]/g\' <<< ""$PACKAGE"")""\nsed -i ""s/^$re==.*/$PACKAGE==$VERSION/g"" -- requirements*.txt\nsed -i ""s/^$re>=.*/$PACKAGE>=$VERSION/g"" -- requirements*.txt\n\nif git diff --exit-code; then\n  exit 0\nfi\n\ngit \\\n    -c user.name=getsentry-bot \\\n    -c user.email=\'10587625+getsentry-bot@users.noreply.github.com\' \\\n    commit \\\n    --all \\\n    --message ""ref: bump $PACKAGE to $VERSION"" \\\n    --message ""Co-Authored-By: $SENDER <$SENDER_ID+$SENDER@users.noreply.github.com>""\n\ngit push origin HEAD --quiet\n\ngh pr create --fill ${{ inputs.pr_options }}\n', 'echo ""yarn-cache-dir=$(yarn cache dir)"" >> ""$GITHUB_OUTPUT""\n', '# d4m 4.11+ never starts: https://github.com/docker/for-mac/issues/6450\ncurl -o /tmp/docker.rb https://raw.githubusercontent.com/Homebrew/homebrew-cask/fe866ec0765de141599745f03e215452db7f511b/Casks/docker.rb\nHOMEBREW_NO_AUTO_UPDATE=1 brew install -v --HEAD -s /tmp/docker.rb\nsudo /Applications/Docker.app/Contents/MacOS/Docker --unattended --install-privileged-components\nopen -a /Applications/Docker.app --args --unattended --accept-license\n', 'make bootstrap', 'HOMEBREW_NO_AUTO_UPDATE=1 brew install pyenv\n', 'make setup-pyenv\n[[ $(which python) != ""${HOME}/.pyenv/shims/python"" ]]\nsource ~/.zprofile\n[[ $(which python) == ""${HOME}/.pyenv/shims/python"" ]]\n[[ $(python -V) == ""Python $(cat .python-version)"" ]]\npython -m venv .venv\nsource .venv/bin/activate\n[[ $(python -V) == ""Python $(cat .python-version)"" ]]\n', 'make test-tools', 'bash <(curl -s https://raw.githubusercontent.com/getsentry/bootstrap-sentry/main/bootstrap.sh)\n', 'curl \\\n    --silent \\\n    -X POST \\\n    -H \'Authorization: token ${{ secrets.BUMP_SENTRY_TOKEN }}\' \\\n    -d\'{""body"": ""revert failed (conflict? already reverted?) -- [check the logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})""}\' \\\n    https://api.github.com/repositories/${{ github.event.repository.id }}/issues/${{ github.event.number || github.event.inputs.pr }}/comments\n', 'yarn install --frozen-lockfile', 'echo ""::remove-matcher owner=masters::""\necho ""::add-matcher::.github/tsc.json""\necho ""::add-matcher::.github/eslint-stylish.json""\n', 'echo ""all-files=true"" >> ""$GITHUB_OUTPUT""', 'yarn lint\nyarn lint:css\n', 'yarn eslint --fix ${{ needs.files-changed.outputs.frontend_modified_lintable_files }}\n', 'yarn stylelint ${{ needs.files-changed.outputs.frontend_components_modified_lintable_files }}\n', ""git diff --quiet || (echo '::error ::lint produced file changes, run linter locally and try again' && exit 1)\n"", 'yarn tsc -p config/tsconfig.ci.json', 'yarn install --frozen-lockfile', 'NODE_ENV=production yarn build-css', 'JEST_TESTS=$(yarn -s jest --listTests --json) yarn test-ci --forceExit\n', 'echo ""One of the dependent jobs have failed. You may need to re-run it."" && exit 1\n', 'python3 -uS .github/workflows/scripts/getsentry-dispatch-setup \\\n    --repo-id ${{ github.event.repository.id }} \\\n    --pr ${{ github.event.number }} \\\n    --event ${{ github.event.action }} \\\n    --username ""$ARG_USERNAME"" \\\n    --label-names ""$ARG_LABEL_NAMES""\n', 'yarn install --frozen-lockfile', 'NODE_ENV=production yarn build-css', 'JEST_TEST_BALANCER=1 yarn test-ci', 'echo ""If you have a valid reason to modify a migration please get approval""\necho ""from @getsentry/owners-migrations."" && exit 1\n', 'sentry upgrade --noinput\n', 'echo \'added<<EOF\' >> ""$GITHUB_OUTPUT""\ngit diff --diff-filter=A --name-only origin/master HEAD | grep \'src/sentry/migrations/\' >> ""$GITHUB_OUTPUT""\necho \'EOF\' >> ""$GITHUB_OUTPUT""\n', 'yarn add ts-node && make build-api-docs\n', 'npx json-diff@0.5.4 --color sentry-api-schema/openapi-derefed.json tests/apidocs/openapi-derefed.json\n', 'yarn add ts-node && make build-api-docs\n', 'cp tests/apidocs/openapi-derefed.json sentry-api-schema\n', './bin/react-to-product-owners-yml-changes.sh', 'make test-python-ci\n', 'yarn global add github-label-sync@2.2.0', ""github-label-sync \\\n  --access-token ${{ secrets.GITHUB_TOKEN }} \\\n  --labels .github/labels.yml \\\n  ${{ github.event_name == 'pull_request' && '--dry-run' || '' }} \\\n  ${{ github.repository }} \\\n  2>&1 | tee sync-report.txt\n"", 'echo ""webpack-path=.webpack_cache"" >> ""$GITHUB_OUTPUT""', 'yarn install --frozen-lockfile', 'yarn build-acceptance\n', 'make build-chartcuterie-config\n', 'mkdir -p ${{ steps.setup.outputs.acceptance-dir }}\nmkdir -p ${{ steps.setup.outputs.acceptance-dir }}-mobile\nmkdir -p ${{ steps.setup.outputs.acceptance-dir }}-tooltips\nmake run-acceptance\n', 'echo ""webpack-path=.webpack_cache"" >> ""$GITHUB_OUTPUT""', 'yarn install --frozen-lockfile', 'yarn build-acceptance\n', 'make build-chartcuterie-config\n', 'mkdir -p ${{ steps.setup.outputs.acceptance-dir }}\nmkdir -p ${{ steps.setup.outputs.acceptance-dir }}-mobile\nmkdir -p ${{ steps.setup.outputs.acceptance-dir }}-tooltips\nmake run-acceptance\n', 'yarn install --frozen-lockfile', 'NODE_ENV=production yarn build-css', 'SENTRY_PROFILER_LOGGING_MODE=eager JEST_TESTS=$(yarn -s jest --listTests --json) yarn test-ci --forceExit\n', 'yarn install --frozen-lockfile', 'NODE_ENV=production yarn build-css', 'SENTRY_PROFILER_LOGGING_MODE=eager JEST_TESTS=$(yarn -s jest --listTests --json) yarn test-ci --forceExit\n']"
"['python -m pip install --upgrade pip\npython -m pip install tox==3.8.3 sphinx\n', 'python -m pip install --upgrade pip\npython -m pip install tox sphinx\n', 'sudo apt update\nsudo apt install ffmpeg  # For replaygain\n', 'tox -e py-test\n', 'tox -vv -e py-cov\n', 'tox -vv -e py-mypy\n', 'tox -e py-test\n', 'pip install codecov || true\ncodecov || true\n', 'python -m pip install --upgrade pip\npython -m pip install tox sphinx\n', 'echo ""::add-matcher::.github/sphinx-problem-matcher.json""', 'tox -e docs', 'python -m pip install --upgrade pip\npython -m pip install tox sphinx\n', 'echo ""::add-matcher::.github/flake8-problem-matcher.json""', 'tox -e py-lint', 'python -m pip install --upgrade pip\npython -m pip install tox sphinx\n', 'tox -e int\n', 'tox -e links\n', 'if [ -z ""${ZULIP_BOT_CREDENTIALS}"" ]; then\n  echo ""Skipping notify, ZULIP_BOT_CREDENTIALS is unset""\n  exit 0\nfi\n\ncurl -X POST https://beets.zulipchat.com/api/v1/messages \\\n  -u ""${ZULIP_BOT_CREDENTIALS}"" \\\n  -d ""type=stream"" \\\n  -d ""to=github"" \\\n  -d ""subject=${GITHUB_WORKFLOW} - $(date -u +%Y-%m-%d)"" \\\n  -d ""content=[${GITHUB_WORKFLOW}#${GITHUB_RUN_NUMBER}](${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}) failed.""\n']"
"['curl --request POST \\\n--silent --output /dev/null \\\n--url https://circleci.com/api/v2/project/gh/${{ secrets.CORPORATE_REPO }}/pipeline \\\n--user \'${{ secrets.CIRCLECI_USER_TOKEN }}:\' \\\n--header \'content-type: application/json\' \\\n--data \'{""branch"": ""main"", ""parameters"":{""CIRCLE_JOB"": ""${{ secrets.CIRCLECI_JOB }}""}}\'\n', 'python -m pip install -U pip-tools', 'pip-compile --upgrade --resolver=backtracking --output-file=requirements/pip.txt requirements/pip.in', 'pip-compile --upgrade --resolver=backtracking --output-file=requirements/docker.txt requirements/docker.in', 'pip-compile --upgrade --resolver=backtracking --output-file=requirements/testing.txt requirements/testing.in', 'pip-compile --upgrade --resolver=backtracking --output-file=requirements/docs.txt requirements/docs.in', 'sudo apt-get install libpq-dev', 'pip-compile --upgrade --resolver=backtracking --output-file=requirements/deploy.txt requirements/deploy.in']"
""
""
"['python -m pip install --upgrade pip wheel setuptools tox\n', 'ENV_PREFIX=$(tr -C -d ""0-9"" <<< ""${{ matrix.python-version }}"")\nTOXENV=$(tox --listenvs | grep ""^py$ENV_PREFIX"" | tr \'\\n\' \',\') python -m tox\n', 'python -m pip install --upgrade pip tox\n', 'tox -e style']"
"['pip install -U pip tox', 'tox -e ${{ matrix.tox-env }}', 'pip install tox -U pip', 'tox -e package -e lint -e pyupgrade', 'pip install -e . && pip install sphinx', 'echo ""RELEASE_TAG=${{ steps.get-latest-tag.outputs.tag }}"" >> $GITHUB_ENV', 'sphinx-build -b html docs docs/build']"
"['python -m pip install -r linter_requirements.txt', 'python tools/linter.py --branch origin/${{ github.base_ref }}', 'sudo apt update\n# for add-apt-repository\nsudo apt install software-properties-common -y\nsudo add-apt-repository ppa:deadsnakes/ppa -y\nsudo apt install python3.9-dev -y\nsudo ln -s /usr/bin/python3.9 /usr/bin/pythonx\npythonx -m pip install --upgrade pip setuptools wheel\npythonx -m pip install -r test_requirements.txt\n', 'sudo apt install g++-7 g++-8 -y', 'export CC=/usr/bin/gcc-7\nexport CXX=/usr/bin/g++-7\nrm -rf build && pythonx setup.py install --user\n', 'pythonx runtests.py -n', 'export CC=/usr/bin/gcc-8\nexport CXX=/usr/bin/g++-8\nrm -rf build && pythonx setup.py install --user\n', 'pythonx runtests.py -n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', '# use x86_64 cross-compiler to speed up the build\nsudo apt update\nsudo apt install -y gcc-arm-linux-gnueabihf g++-arm-linux-gnueabihf gfortran-arm-linux-gnueabihf\n\n# Keep the `test_requirements.txt` dependency-subset synced\ndocker run --name the_container --interactive -v /:/host arm32v7/ubuntu:22.04 /bin/bash -c ""\n  apt update &&\n  apt install -y git python3 python3-dev python3-pip  &&\n  python3 -m pip install cython==0.29.34 setuptools\\<49.2.0 hypothesis==6.23.3 pytest==6.2.5 \'typing_extensions>=4.2.0\' &&\n  ln -s /host/lib64 /lib64 &&\n  ln -s /host/lib/x86_64-linux-gnu /lib/x86_64-linux-gnu &&\n  ln -s /host/usr/arm-linux-gnueabihf /usr/arm-linux-gnueabihf &&\n  rm -rf /usr/lib/gcc/arm-linux-gnueabihf && ln -s /host/usr/lib/gcc-cross/arm-linux-gnueabihf /usr/lib/gcc/arm-linux-gnueabihf &&\n  rm -f /usr/bin/arm-linux-gnueabihf-gcc && ln -s /host/usr/bin/arm-linux-gnueabihf-gcc /usr/bin/arm-linux-gnueabihf-gcc &&\n  rm -f /usr/bin/arm-linux-gnueabihf-g++ && ln -s /host/usr/bin/arm-linux-gnueabihf-g++ /usr/bin/arm-linux-gnueabihf-g++ &&\n  rm -f /usr/bin/arm-linux-gnueabihf-gfortran && ln -s /host/usr/bin/arm-linux-gnueabihf-gfortran /usr/bin/arm-linux-gnueabihf-gfortran &&\n  rm -f /usr/bin/arm-linux-gnueabihf-ar && ln -s /host/usr/bin/arm-linux-gnueabihf-ar /usr/bin/arm-linux-gnueabihf-ar &&\n  rm -f /usr/bin/arm-linux-gnueabihf-as && ln -s /host/usr/bin/arm-linux-gnueabihf-as /usr/bin/arm-linux-gnueabihf-as &&\n  rm -f /usr/bin/arm-linux-gnueabihf-ld && ln -s /host/usr/bin/arm-linux-gnueabihf-ld /usr/bin/arm-linux-gnueabihf-ld &&\n  rm -f /usr/bin/arm-linux-gnueabihf-ld.bfd && ln -s /host/usr/bin/arm-linux-gnueabihf-ld.bfd /usr/bin/arm-linux-gnueabihf-ld.bfd\n""\ndocker commit the_container the_container\n', 'sudo docker run --name the_build --interactive -v $(pwd):/numpy -v /:/host the_container /bin/bash -c ""\n  uname -a &&\n  gcc --version &&\n  g++ --version &&\n  arm-linux-gnueabihf-gfortran --version &&\n  python3 --version &&\n  git config --global --add safe.directory /numpy\n  cd /numpy &&\n  python3 setup.py install\n""\ndocker commit the_build the_build\n', 'docker run --rm --interactive -v $(pwd):/numpy the_build /bin/bash -c ""\n  cd /numpy && F90=arm-linux-gnueabihf-gfortran python3 runtests.py -n -v -- -k \'test_simd or test_kind\'\n""\n', 'curl -o /tmp/sde.tar.xz https://downloadmirror.intel.com/732268/sde-external-9.7.0-2022-05-09-lin.tar.xz\nmkdir /tmp/sde && tar -xvf /tmp/sde.tar.xz -C /tmp/sde/\nsudo mv /tmp/sde/* /opt/sde && sudo ln -s /opt/sde/sde64 /usr/bin/sde\n', 'python -m pip install -r test_requirements.txt', 'python setup.py build --simd-test=""\\$werror AVX512F AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL"" install', 'sde -knm -- python runtests.py -n -v -- -k test_simd', 'sde -icl -- python runtests.py -n -v -- -k test_simd', 'curl -o /tmp/sde.tar.xz https://downloadmirror.intel.com/751535/sde-external-9.14.0-2022-10-25-lin.tar.xz\nmkdir /tmp/sde && tar -xvf /tmp/sde.tar.xz -C /tmp/sde/\nsudo mv /tmp/sde/* /opt/sde && sudo ln -s /opt/sde/sde64 /usr/bin/sde\n', 'python -m pip install -r test_requirements.txt\nsudo apt install gcc-12 g++-12\n', 'export CC=/usr/bin/gcc-12\nexport CXX=/usr/bin/g++-12\npython -m pip install -e .\n', 'python -c ""import numpy as np; np.show_config()""\n', 'python -m pytest numpy/core/tests/test_umath* numpy/core/tests/test_ufunc.py numpy/linalg/tests/test_*\n#sde -spr -- python -m pytest numpy/core/tests/test_umath* numpy/core/tests/test_ufunc.py numpy/linalg/tests/test_*\n', 'command bash\nbash -c ""uname -svrmo""\n', 'dash -c ""which git; /usr/bin/git config --system --add safe.directory /cygdrive/d/a/numpy/numpy""\n', 'dash -c ""which python3.9; /usr/bin/python3.9 --version -V""\n', 'dash -c ""/usr/bin/python3.9 -m pip install \'setuptools<49.2.0\' pytest pytz cffi pickle5 importlib_metadata typing_extensions""\ndash -c ""/usr/bin/python3.9 -m pip install -r test_requirements.txt""\ndash -c ""/usr/bin/python3.9 setup.py bdist_wheel""\n', 'bash -c ""/usr/bin/python3.9 -m pip install dist/numpy-*cp39*.whl""\n', 'dash ""tools/rebase_installed_dlls_cygwin.sh"" 3.9\n', '/usr/bin/python3.9 runtests.py -n\n', 'dash -c ""/usr/bin/python3.9 -m pip show numpy""\ndash -c ""/usr/bin/python3.9 -m pip show -f numpy | grep .dll""\ndash -c ""/bin/tr -d \'\\r\' <tools/list_installed_dll_dependencies_cygwin.sh >list_dlls_unix.sh""\ndash ""list_dlls_unix.sh"" 3.9\n', 'cygcheck -c\n', 'pip install pyodide-build==$PYODIDE_VERSION', 'CFLAGS=-g2 LDFLAGS=-g2 pyodide build', 'pyodide venv .venv-pyodide\nsource .venv-pyodide/bin/activate\npip install dist/*.whl\npip install -r test_requirements.txt\n', 'source .venv-pyodide/bin/activate\ncd ..\npython numpy/runtests.py -n -vv\n', 'pip install -r build_requirements.txt\nsudo apt-get install -y libopenblas-serial-dev\n', 'spin build -- --werror', 'ninja -C build -t missingdeps', ""python tools/check_installed_files.py $(find ./build-install -path '*/site-packages/numpy')"", 'pip install pytest hypothesis typing_extensions\nspin test\n', 'apk update --quiet\n\n# using git commands to clone because versioneer doesn\'t work when\n# actions/checkout is used for the clone step in a container\n\ngit config --global --add safe.directory $PWD \n\nif [ $GITHUB_EVENT_NAME != pull_request ]; then\n    git clone --recursive --branch=$GITHUB_REF_NAME https://github.com/${GITHUB_REPOSITORY}.git $GITHUB_WORKSPACE\n    git reset --hard $GITHUB_SHA\nelse        \n    git clone --recursive https://github.com/${GITHUB_REPOSITORY}.git $GITHUB_WORKSPACE\n    git fetch origin $GITHUB_REF:my_ref_name\n    git checkout $GITHUB_BASE_REF\n    git -c user.email=""you@example.com"" merge --no-commit my_ref_name\nfi\n\nln -s /usr/local/bin/python3.10 /usr/local/bin/python\n', 'python -m venv test_env\nsource test_env/bin/activate\n\n# required for figuring out the system tags in openblas_support\npip install packaging       \n\n# install openblas by co-opting the CIBW setup script\nRUNNER_OS=Linux sh tools/wheels/cibw_before_build.sh .\n\npip install -r build_requirements.txt\npip install pytest hypothesis typing_extensions\n\n# use meson to build and test \nspin build\nspin test\n', 'set -xe\nCOMMIT_MSG=$(git log --no-merges -1 --oneline)\necho ""message=$COMMIT_MSG"" >> $GITHUB_OUTPUT\necho github.ref ${{ github.ref }}\n', 'echo ""PLAT=i686"" >> $env:GITHUB_ENV\necho ""PATH=c:\\rtools40\\mingw32\\bin;$env:PATH"" >> $env:GITHUB_ENV\ngfortran --version\n', 'conda install -y anaconda-client\nsource tools/wheels/upload_wheels.sh\nset_upload_vars\n# trigger an upload to\n# https://anaconda.org/scientific-python-nightly-wheels/numpy\n# for cron jobs or ""Run workflow"" (restricted to main branch).\n# Tags will upload to\n# https://anaconda.org/multibuild-wheels-staging/numpy\n# The tokens were originally generated at anaconda.org\nupload_wheels\n', 'python setup.py sdist\n', '# TODO: Don\'t run test suite, and instead build wheels from sdist\n# Depends on pypa/cibuildwheel#1020\npython -m pip install dist/*.gz\npip install -r test_requirements.txt\ncd .. # Can\'t import numpy within numpy src directory\npython -c ""import numpy, sys; print(numpy.__version__); sys.exit(numpy.test() is False)""\n', 'python -mpip install twine\ntwine check dist/*\n', 'conda install -y anaconda-client\nsource tools/wheels/upload_wheels.sh\nset_upload_vars\n# trigger an upload to\n# https://anaconda.org/scientific-python-nightly-wheels/numpy\n# for cron jobs or ""Run workflow"" (restricted to main branch).\n# Tags will upload to\n# https://anaconda.org/multibuild-wheels-staging/numpy\n# The tokens were originally generated at anaconda.org\nupload_wheels\n', 'pip install -r build_requirements.txt\n', '# Download and install pre-built OpenBLAS library\n# with 32-bit interfaces\n# Unpack it in the pkg-config hardcoded path\nchoco install unzip -y\nchoco install wget -y\nchoco install -y --checksum 6004DF17818F5A6DBF19CB335CC92702 pkgconfiglite\nwget https://anaconda.org/multibuild-wheels-staging/openblas-libs/v0.3.21/download/openblas-v0.3.21-win_amd64-gcc_10_3_0.zip\nunzip -d c:\\opt openblas-v0.3.21-win_amd64-gcc_10_3_0.zip\necho ""PKG_CONFIG_PATH=c:\\opt\\64\\lib\\pkgconfig;"" >> $env:GITHUB_ENV\n', 'meson setup build --prefix=$PWD\\build-install -Ddebug=false --optimization 2 --vsenv \n', 'meson compile -C build -v\n', 'cd build\nmeson install --no-rebuild\n', 'echo ""installed_path=$PWD\\build-install\\Lib\\site-packages"" >> $env:GITHUB_ENV\n', '$numpy_path = ""${env:installed_path}\\numpy""\n$libs_path = ""${numpy_path}\\.libs""\nmkdir ${libs_path}\n$ob_path = ""C:/opt/64/bin/""\ncp $ob_path/*.dll $libs_path\n# Write _distributor_init.py to load .libs DLLs.\npython -c ""from tools import openblas_support; openblas_support.make_init(r\'${numpy_path}\')""\n', 'echo ""PYTHONPATH=${env:installed_path}"" >> $env:GITHUB_ENV\npython -m pip install -r test_requirements.txt\npython -m pip install threadpoolctl\n', 'mkdir tmp\ncd tmp\necho ""============================================""\npython -c ""import numpy; print(numpy.show_runtime())""\necho ""============================================""\necho ""LASTEXITCODE is \'$LASTEXITCODE\'""\npython -c ""import numpy, sys; sys.exit(numpy.test(verbose=3) is False)""\necho ""LASTEXITCODE is \'$LASTEXITCODE\'""\n']"
"['sudo apt-get update\nsudo apt-get install graphviz graphviz-dev\n', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/default.txt -r requirements/test.txt\npip install -r requirements/extra.txt\npip install .\npip list\n', 'pytest --cov=networkx --runslow --doctest-modules --durations=20 --pyargs networkx\n', 'sudo apt-get update\nsudo apt-get install graphviz graphviz-dev\nsudo apt-get install libspatialindex-dev\n', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/default.txt -r requirements/test.txt\npip install -r requirements/extra.txt\npip install -r requirements/example.txt\npip install -U -r requirements/doc.txt\npip install .\npip list\n', 'export DISPLAY=:99\nmake -C doc/ html\n', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/developer.txt\npip list\n', 'pre-commit run --all-files --show-diff-on-failure --color always', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/developer.txt\npip install -e .\npip list\n', 'mypy -p networkx', 'pip install --upgrade pip wheel setuptools\npip install -U --pre --extra-index-url https://pypi.org/simple -i https://pypi.anaconda.org/scipy-wheels-nightly/simple -r requirements/default.txt\npip install --pre -r requirements/test.txt\npip install .\npip list\n', 'pytest --doctest-modules --durations=10 --pyargs networkx\n', 'python -m pip install -r requirements/release.txt\npython -m build --sdist --wheel\n', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/default.txt -r requirements/test.txt\npip install pytest-randomly\npip install .\npip list\n', 'pytest --doctest-modules --durations=10 --pyargs networkx', 'python -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements/test.txt\npython -m pip install .\npython -m pip list\n', 'pytest --durations=10 --pyargs networkx\n', 'python -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements/default.txt -r requirements/test.txt\npython -m pip install .\npython -m pip list\n', 'pytest --doctest-modules --durations=10 --pyargs networkx\n', 'NETWORKX_GRAPH_CONVERT=nx-loopback pytest --doctest-modules --durations=10 --pyargs networkx\n', 'sudo apt-get update && sudo apt-get install graphviz graphviz-dev', 'brew install graphviz', 'choco install graphviz', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/default.txt -r requirements/test.txt\npip install -r requirements/extra.txt\npip install .\npip list\n', 'pip install --upgrade pip wheel setuptools\npip install -r requirements/default.txt -r requirements/test.txt\npip install --global-option=build_ext --global-option=""-I/usr/local/include/"" --global-option=""-L/usr/local/lib/"" pygraphviz\npip install -r requirements/extra.txt\npip install .\npip list\n', 'echo ""C:\\Program Files\\Graphviz\\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\npython -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements/default.txt -r requirements/test.txt\npython -m pip install --global-option=build_ext `\n                      --global-option=""-IC:\\Program Files\\Graphviz\\include"" `\n                      --global-option=""-LC:\\Program Files\\Graphviz\\lib"" `\n                      pygraphviz\npython -m pip install -r requirements/extra.txt\npython -m pip install .\npython -m pip list\n', 'pytest --doctest-modules --durations=10 --pyargs networkx\n', 'pip install --upgrade pip wheel setuptools\npip install --pre -r requirements/default.txt -r requirements/test.txt\npip install scipy==1.10.1\npip install .\npip list\n', 'pytest --doctest-modules --durations=10 --pyargs networkx\n']"
"['pip install tox', 'tox -e lint', 'tox -e tests -- -n 4 tests/test_browser.py tests/test_element_list.py tests/test_request_handler.py tests/test_xpath_concat.py\n', 'tox -e tests -- -n 4 tests/test_flaskclient.py tests/test_zopetestbrowser.py tests/test_djangoclient.py tests/test_is_element_present_nojs.py\n', 'pip install tox', 'echo ""DISPLAY=:99.0"" >> $GITHUB_ENV', 'wget https://github.com/SeleniumHQ/selenium/releases/download/selenium-4.3.0/selenium-server-4.3.0.jar -O selenium-server.jar\n', 'echo ""Start Selenium Server""\nxvfb-run java -jar selenium-server.jar standalone > selenium-server.log 2>&1 &\ntimeout 60 bash -c \'while ! wget -O /dev/null -T 1 http://localhost:4444/readyz; do echo waiting for selenium server; sleep 1; done\' || (cat selenium-server.log && exit 2)\necho ""Selenium server is ready, running tests""\ntox -e tests_selenium -- tests/test_webdriver_remote.py\n', 'pip install tox', 'tox -e tests_selenium -- -n 4 tests/test_element_is_visible.py tests/test_screenshot.py tests/test_shadow_root.py tests/test_mouse_interaction.py tests/test_async_finder.py tests/test_html_snapshot.py tests/test_iframes.py tests/test_popups.py tests/test_webdriver.py tests/test_webdriver_firefox.py tests/test_webdriver_chrome.py;\n', 'pip install twine wheel', 'pip install -e .', 'python setup.py sdist bdist_wheel', 'twine upload --skip-existing dist/*', 'pip install tox', 'echo ""DISPLAY=:99.0"" >> $GITHUB_ENV', 'tox -e tests_windows_selenium -- -n 4 tests/test_webdriver.py tests/test_popups.py tests/test_webdriver_edge_chromium.py;\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', ""python -m pip install --upgrade pip\npython -m pip install --upgrade 'tox<4' tox-gh-actions\n"", 'tox -v\n']"
"[""python -m pip install --upgrade pip\npip install wheel\npip install six  # for configobj and wxpython\npip install distro  # used by createInitFile\npip install attrdict3  # used by wxpython setup\npython ./building/plat_custom_installs.py\n\npip install pdm\npdm config python.use_venv false  # don't need/want venv in a vm!\npdm install  # fetch dependendices in lock file\npdm install -G tests\npdm build\n# install with pip both the main and [tests] group\npip install . --prefer-binary  # install with pip\npip install '.[tests]' --prefer-binary  # optional dependency group\n"", '# for pocketsphinx we need this adapted package:\nbrew install swig vlc portaudio portmidi liblo sdl2\n# install optional components\npip install psychopy-sounddevice psychopy-pyo psychopy-legacy-mic psychopy-connect psychopy-crs psychopy-emotiv psychopy-gammasci psychopy-mri-emulator psychopy-visionscience\npip install moviepy\n', '# quick test of the gui toolkits (without pytest swallowing errors)\npython psychopy/tests/test_gui/test_DlgFromDictQt.py\n', 'pytest --ignore=""psychopy/tests/test_app"" --ignore=""psychopy/tests/test_preferences"" --ignore=""psychopy/tests/test_experiment/needs_wx"" -m ""not needs_sound and not needs_wx and not emulator""  --cov=psychopy -v psychopy\n', 'echo ""::set-output name=month::$(date +\'%Y-%m\')""', ""# mostly for wxPython:\nsudo apt-get update\nsudo apt-get install -y -qq python3-dev libgtk-3-dev\nsudo apt-get install -y -qq libgstreamer1.0-0 gstreamer1.0-plugins-base\nsudo apt-get install -y -qq libwebkit2gtk-4.0-dev\nsudo apt-get install -y -qq libpng-dev libjpeg-dev libtiff-dev libnotify-dev libsm-dev\nsudo apt-get install -y -qq libsdl2-dev libsdl2-mixer-2.0-0 libsdl2-image-2.0-0 libsdl2-2.0-0\nsudo apt-get install -y -qq libportaudio2\n# virtual frame buffer\nsudo apt install llvm-13-dev  # for xvfb\nsudo apt install xvfb xauth libgl1-mesa-dri\n\n# set up fake sound device?\n# sudo apt-get install -y -qq libasound2-dev alsa-utils alsa-oss\n# sudo apt-get install -y linux-modules-extra-$(uname -r)  # needed for modprobe snd-dummy\n# sudo sh .github/workflows/prep_dummy_soundcard.sh  # couldn't find that file\n\n# for PyQt:\nsudo apt-get install -y -qq libdbus-1-3\nsudo apt-get install -y -qq '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\nexport LIBGL_ALWAYS_INDIRECT=0\nexport QT_DEBUG_PLUGINS=1 # let us know about missing dependencies?\n"", 'python -m pip install --upgrade pip\npip install pytest pytest-cov flake8 xmlschema\npip install wheel\npip install six  # for configobj and wxpython\npip install distro  # used by createInitFile\npip install pdm\n', '# these are needed to build wxPython\npip install attrdict3  # used by wxpython setup\npip install pip install -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-20.04/ wxPython\npdm remove wxpython  # no longer needed in the pdm\n', 'pdm install  # fetch dependendices in lock file\npdm build  # test that we can build the sdist.tar.gz and whl files\npip install .\n', '# install optional components\npip install psychopy-sounddevice psychopy-pyo psychopy-legacy-mic psychopy-connect psychopy-crs psychopy-emotiv psychopy-gammasci psychopy-mri-emulator psychopy-visionscience\npip install moviepy\n', '# sudo setcap cap_sys_nice=eip ${{ env.pythonLocation }}/python\nexport PYTHON_EXECUTABLE=`python -c ""import os; import sys; print(os.path.realpath(sys.executable))""`\nsudo setcap cap_sys_nice+ep ${PYTHON_EXECUTABLE}\ngetcap ${PYTHON_EXECUTABLE}  # did it work?\n', '# quick test of the gui toolkits (without pytest swallowing errors)\nxvfb-run -a --server-args=""-screen 0 1024x768x24"" python psychopy/tests/test_gui/test_DlgFromDictQt.py\n', '# quick test of the gui toolkits (without pytest swallowing errors)\nxvfb-run -a --server-args=""-screen 0 1024x768x24"" python psychopy/tests/test_gui/test_DlgFromDictWx.py\n', 'xvfb-run -a --server-args=""-screen 0 1024x768x24"" pytest -m ""not needs_sound and not needs_wx and not needs_qt and not needs_pygame and not emulator"" --cov=psychopy -v psychopy\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['python -m pip install --upgrade pip wheel\npython -m pip install flake8\npython --version\npip list\n', 'flake8 -v src tests docs', 'python -m pip install --upgrade pip wheel\npython -m pip install bandit\n', 'bandit -r src docs\nbandit -r tests --skip B101\n', 'pip install build\npip list\n', 'python -m build', 'python -m pip install --upgrade pip wheel\npip install django~=${{ matrix.django }}.0 coverage[toml]~=6.0 coveralls~=3.0 -r tests-requirements.txt .\npython --version\npip list\n', 'coverage run -m pytest -v', 'coverage report', 'coveralls --service=github']"
"['wget https://repo1.maven.org/maven2/org/python/jython-installer/2.7.1/jython-installer-2.7.1.jar -O jython-installer.jar\njava -jar jython-installer.jar -s -d ""$HOME/jython""\necho ""$HOME/jython/bin"" >> $GITHUB_PATH\n', 'pip install nose', 'wget https://files.pythonhosted.org/packages/99/4f/13fb671119e65c4dce97c60e67d3fd9e6f7f809f2b307e2611f4701205cb/nose-1.3.7-py2-none-any.whl\npip install nose-1.3.7-py2-none-any.whl\n', './devscripts/run_tests.${{ matrix.run-tests-ext }}', 'pip install flake8', 'flake8 .']"
"[""pip install psycopg2-binary cython pymysql 'apsw<3.38' mysql-connector\npython setup.py build_ext -i\npsql peewee_test -c 'CREATE EXTENSION hstore;'\n"", 'pip install sqlcipher3-binary', ""wget -qO- https://binaries.cockroachdb.com/cockroach-v22.2.6.linux-amd64.tgz | tar xz\n./cockroach-v22.2.6.linux-amd64/cockroach start-single-node --insecure --background\n./cockroach-v22.2.6.linux-amd64/cockroach sql --insecure -e 'create database peewee_test;'\n"", 'python runtests.py --mysql-user=root --mysql-password=peewee -s']"
""
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions coveralls\n', 'tox', 'coveralls --service=github', 'python -m pip install --upgrade pip\npip install tox\n', 'tox']"
""
"['python -m pip install cibuildwheel', 'python -m cibuildwheel', 'python -m pip install -U setuptools wheel', ""for platform in 'manylinux2014_x86_64' 'manylinux2014_i686' 'manylinux2014_aarch64' 'manylinux2014_armv7l' 'manylinux2014_ppc64' 'manylinux2014_ppc64le' 'manylinux2014_s390x' 'win32' 'win_amd64' 'win_ia64'\ndo\n  python setup.py bdist_wheel --plat-name $platform\ndone\n"", 'python setup.py sdist', 'ls -l dist', 'python -m pip install tox\n', 'python -m tox -e ${{ matrix.tox.environment }}\n']"
"['python -m pip install --upgrade pip tox', 'echo ""::set-output name=dir::$(pip cache dir)""', 'echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'sudo apt-get -y update && sudo apt-get install -y tesseract-ocr tesseract-ocr-fra', 'echo $(tesseract --version)', 'tox -e ${{ matrix.tox }}', ""pip install -U git+${{ github.server_url }}/${{ github.repository }} && pip show pytesseract && python -c 'import pytesseract'""]"
"['pip install tox', 'tox -e py -- ${{ matrix.pytest-args }}', 'pip install tox', 'tox -e py38-cover,coverage', 'pip install tox', 'tox -e docs', 'pip install tox', 'tox -e lint']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt -r requirements-testing.txt -c constraints-Django${{ matrix.django-version }}.txt\n', 'pip install autopep8\nautopep8 --exit-code --global-config .flake8 helpdesk\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 helpdesk --count --show-source --statistics --exit-zero --max-complexity=20\n', 'isort --line-length=120 --src helpdesk . --check\n', 'pip install pytest\ncd ${GITHUB_WORKSPACE} && python quicktest.py\n', 'python -m pip install --upgrade build twine\npython -m build\ntwine check --strict dist/*\n', 'export PKG=$(ls dist/ | grep tar)\nset -- $PKG\necho ""name=$1"" >> $GITHUB_ENV\n']"
"['python -m pip install --upgrade build twine\npython -m build\ntwine check --strict dist/*\n', 'sudo pip install coveralls', 'sudo PATH=$PATH coverage run setup.py test', 'python -m pip install --upgrade build twine\npython -m build\ntwine check --strict dist/*\nrm -f dist/*.whl\n', 'export PKG=$(ls dist/ | grep tar)\nset -- $PKG\necho ""name=$1"" >> $GITHUB_ENV\n']"
"['source .ci/ubuntu_ci.sh\ninstall_kivy_test_wheel_run_pip_deps\n', 'source .ci/ubuntu_ci.sh\ncreate_kivy_examples_wheel\n', ""docker run --rm -v `pwd`:/root:rw --workdir=/root \\\n   quay.io/pypa/manylinux2014_${{ matrix.cibw_archs }} \\\n   bash -ec 'source .ci/ubuntu_ci.sh && install_manylinux_build_deps && ./tools/build_linux_dependencies.sh'\n"", 'source .ci/ubuntu_ci.sh\nupdate_version_metadata\n', 'python -m pip install cibuildwheel==2.11.2\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'source .ci/ubuntu_ci.sh\nrename_wheels\n', 'source .ci/ubuntu_ci.sh\nupload_file_to_server ""$SERVER_IP"" ""linux/kivy/""\n', 'source .ci/ubuntu_ci.sh\nupload_artifacts_to_pypi\n', 'source .ci/ubuntu_ci.sh\nprepare_env_for_unittest\n', 'source .ci/ubuntu_ci.sh\ninstall_kivy_wheel dev\n', 'source .ci/ubuntu_ci.sh\ninstall_kivy_examples_wheel dev\n', 'source .ci/ubuntu_ci.sh\ntest_kivy_install\n', 'source .ci/ubuntu_ci.sh\ntest_kivy_benchmark\n', 'source .ci/ubuntu_ci.sh\ngenerate_sdist\n', 'source .ci/ubuntu_ci.sh\ninstall_ubuntu_build_deps\n./tools/build_linux_dependencies.sh\ninstall_kivy_test_wheel_run_pip_deps\n', 'source .ci/ubuntu_ci.sh\nprepare_env_for_unittest\n', 'source .ci/ubuntu_ci.sh\nexport KIVY_DEPS_ROOT=$(pwd)/kivy-dependencies\ninstall_kivy_sdist\n', 'source .ci/ubuntu_ci.sh\ntest_kivy_install\n', 'source .ci/ubuntu_ci.sh\ninstall_kivy_test_wheel_run_pip_deps dev\n', 'source .ci/ubuntu_ci.sh\ncreate_kivy_examples_wheel\n', 'source .ci/ubuntu_ci.sh\nupdate_version_metadata\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\n./tools/build_macos_dependencies.sh\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython -m pip install cibuildwheel==2.11.2\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nexport KIVY_DEPS_ROOT=$(pwd)/kivy-dependencies\nexport REPAIR_LIBRARY_PATH=$KIVY_DEPS_ROOT/dist/Frameworks:$KIVY_DEPS_ROOT/dist/Frameworks/SDL2_mixer.framework/Frameworks\npython -m cibuildwheel --output-dir wheelhouse\n', 'source .ci/ubuntu_ci.sh\nrename_wheels\n', 'source .ci/ubuntu_ci.sh\nupload_file_to_server ""$SERVER_IP"" ""osx/kivy/""\n', 'source .ci/ubuntu_ci.sh\nupload_artifacts_to_pypi\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ninstall_kivy_test_wheel_run_pip_deps dev\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ninstall_kivy_wheel dev\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ninstall_kivy_examples_wheel dev\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ntest_kivy_install\n', 'source .ci/ubuntu_ci.sh\nupdate_version_metadata\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\ninstall_platypus\ninstall_kivy_test_run_pip_deps dev\n', 'py_version=$(python3 -c ""import platform; print(platform.python_version())"")\nsource .ci/osx_ci.sh\ngenerate_osx_app_bundle ""$py_version""\n', 'source .ci/osx_ci.sh\ngenerate_osx_app_dmg_from_bundle\n', 'py_version=$(python3 -c ""import platform; print(platform.python_version())"")\nsource .ci/osx_ci.sh\nrename_osx_app ""$py_version""\n', 'source .ci/ubuntu_ci.sh\nupload_file_to_server ""$SERVER_IP"" ""osx/app/"" ""*.dmg"" ""app""\n', 'source .ci/osx_ci.sh\nmount_osx_app\n', 'source .ci/osx_ci.sh\nsource .ci/ubuntu_ci.sh\nactivate_osx_app_venv\ninstall_kivy_test_wheel_run_pip_deps dev\ninstall_kivy_examples_wheel dev\n', 'source .ci/osx_ci.sh\nsource .ci/ubuntu_ci.sh\nactivate_osx_app_venv\ntest_kivy_install\n', 'source .ci/ubuntu_ci.sh\nupdate_version_metadata\n', 'source .ci/ubuntu_ci.sh\ngenerate_rpi_wheels ${{ matrix.docker_images }}\n', 'source .ci/ubuntu_ci.sh\nrename_wheels\n', 'source .ci/ubuntu_ci.sh\nupload_file_to_server ""$SERVER_IP"" ""raspberrypi/kivy/""\n', 'echo ""This is run to prevent the workflow from showing an error if the wheels job is not run and no jobs run is an error.""\necho ""See https://github.community/t5/GitHub-Actions/Workflow-is-failing-if-no-job-can-be-ran-due-to-condition/m-p/38085""\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\n./tools/build_macos_dependencies.sh\ninstall_kivy_test_run_pip_deps\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nexport KIVY_DEPS_ROOT=$(pwd)/kivy-dependencies\ninstall_kivy\n', 'source .ci/ubuntu_ci.sh\nsource .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ntest_kivy\n', 'source .ci/ubuntu_ci.sh\nvalidate_pep8\n', 'source .ci/ubuntu_ci.sh\ninstall_ubuntu_build_deps\n./tools/build_linux_dependencies.sh\ninstall_kivy_test_run_pip_deps\n', 'source .ci/ubuntu_ci.sh\nprepare_env_for_unittest\n', 'source .ci/ubuntu_ci.sh\nexport KIVY_DEPS_ROOT=$(pwd)/kivy-dependencies\ninstall_kivy\n', 'source .ci/ubuntu_ci.sh\ntest_kivy\n', 'source .ci/ubuntu_ci.sh\nupload_coveralls\n', 'source .ci/ubuntu_ci.sh\ntest_kivy_benchmark\n', 'source .ci/ubuntu_ci.sh\ninstall_ubuntu_build_deps\n./tools/build_linux_dependencies.sh\ninstall_kivy_test_run_pip_deps\n', 'source .ci/ubuntu_ci.sh\nprepare_env_for_unittest\n', 'source .ci/ubuntu_ci.sh\ninstall_kivy\n', 'source .ci/ubuntu_ci.sh\ngenerate_docs\n', 'branch_name=$(python3 -c ""print(\'$REF_NAME\'.split(\'/\')[-1])"")\nsource .ci/ubuntu_ci.sh\nupload_docs_to_server ""$branch_name"" ""$GITHUB_SHA""\n', '. .\\.ci\\windows_ci.ps1\nInstall-kivy-test-run-win-deps\nInstall-kivy-test-run-pip-deps\n', '. .\\.ci\\windows_ci.ps1\nInstall-kivy\n', '. .\\.ci\\windows_ci.ps1\nTest-kivy\n', '. .\\.ci\\windows_ci.ps1\nTest-kivy-benchmark\n', '. .\\.ci\\windows_ci.ps1\nUpdate-version-metadata\n', '. .\\.ci\\windows_ci.ps1\nInstall-kivy-test-run-win-deps\nInstall-kivy-test-run-pip-deps\n', '. .\\.ci\\windows_ci.ps1\nGenerate-sdist\n', '. .\\.ci\\windows_ci.ps1\nInstall-kivy\n', '. .\\.ci\\windows_ci.ps1\nGenerate-windows-wheels\n', '. .\\.ci\\windows_ci.ps1\nRename-windows-wheels\n', 'choco install msys2', '. .\\.ci\\windows_ci.ps1\nUpload-windows-wheels-to-server -ip ""$env:SERVER_IP""\n', '. .\\.ci\\windows_ci.ps1\nUpload-artifacts-to-pypi\n', '. .\\.ci\\windows_ci.ps1\nInstall-kivy-wheel\n', '. .\\.ci\\windows_ci.ps1\nTest-kivy-installed\n', '. .\\.ci\\windows_ci.ps1\nTest-kivy-benchmark\n', '. .\\.ci\\windows_ci.ps1\nInstall-kivy-sdist\n', '. .\\.ci\\windows_ci.ps1\nTest-kivy-installed\n', 'echo ""This is run to prevent the workflow from showing an error if the wheels job is not run and no jobs run is an error.""\necho ""See https://github.community/t5/GitHub-Actions/Workflow-is-failing-if-no-job-can-be-ran-due-to-condition/m-p/38085""\n', 'echo ""$GITHUB_CONTEXT""']"
"['bash ./Tools/ci-run.sh', 'bash ./Tools/ci-run.sh', 'bash ./Tools/ci-run.sh', 'pip install --upgrade wheel setuptools\npython setup.py sdist\npython setup.py bdist_wheel --no-cython-compile --universal\n']"
"['git config --global core.autocrlf input', '/usr/bin/git config --global --add safe.directory $(pwd)\n/usr/bin/git config --global protocol.file.allow always\n', 'set -x\n/usr/bin/python -m pip install --upgrade pip setuptools wheel\n/usr/bin/python --version; /usr/bin/git --version\n/usr/bin/git submodule update --init --recursive\n/usr/bin/git fetch --tags\n/usr/bin/python -m pip install -r requirements.txt\n/usr/bin/python -m pip install -r test-requirements.txt\nTRAVIS=yes ./init-tests-after-clone.sh\n/usr/bin/git config --global user.email ""travis@ci.com""\n/usr/bin/git config --global user.name ""Travis Runner""\n# If we rewrite the user\'s config by accident, we will mess it up\n# and cause subsequent tests to fail\ncat test/fixtures/.gitconfig >> ~/.gitconfig\n', '/usr/bin/python -m pytest\n', 'set -x\n\npython -m pip install --upgrade pip setuptools wheel\npython --version; git --version\ngit submodule update --init --recursive\ngit fetch --tags\n\npip install -r requirements.txt\npip install -r test-requirements.txt\nTRAVIS=yes ./init-tests-after-clone.sh\n\ngit config --global user.email ""travis@ci.com""\ngit config --global user.name ""Travis Runner""\n# If we rewrite the user\'s config by accident, we will mess it up\n# and cause subsequent tests to fail\ncat test/fixtures/.gitconfig >> ~/.gitconfig\n', 'set -x\nmypy -p git\n', '/usr/bin/git config --global --add safe.directory $(pwd)\n/usr/bin/git config --global protocol.file.allow always\n', 'set -x\npytest\n', 'set -x\npip install -r doc/requirements.txt\nmake -C doc html\n']"
""
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'sudo apt-get install libgraphicsmagick1-dev graphicsmagick libjpeg62 zlib1g-dev', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
""
"['python -m pip install poetry==1.5.1 tox==4.5.2', 'python -m tox -e ${{ matrix.tox }}', 'python -m tox -e ${{ matrix.tox }} -- --cov-report=xml', 'python -m pip install poetry==1.5.1 tox==4.5.2', 'tox --skip-missing-interpreters true', 'poetry build', 'poetry publish --username=__token__ --password=${{ secrets.PYPI_TOKEN }}']"
"['pip install --upgrade -r requirements.txt -r requirements-dev.txt\npip install .\n', 'py.test -vv --cov=injector --cov-branch --cov-report html --cov-report term\nif which mypy; then mypy injector ; fi\nif which black; then black --check . ; fi\ncheck-manifest\n']"
"['pip install .\n', 'pip install pytest\npytest --pyargs bottleneck\n', 'echo ""Done""', 'pipx run build --sdist']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'sudo apt-get -q -y update\nsudo apt-get -q -y install binutils gdal-bin libproj-dev libgeos-c1v5\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['sudo mkdir -p /usr/local/share/keyrings\nsudo wget -q -O /usr/local/share/keyrings/mopidy-archive-keyring.gpg https://apt.mopidy.com/mopidy.gpg\nsudo wget -q -O /etc/apt/sources.list.d/mopidy.list https://apt.mopidy.com/buster.list\nsudo apt-get update\nsudo apt-get install -y libspotify-dev\n', 'python -m pip install tox', 'python -m tox -e ${{ matrix.tox }}', 'python -m tox -e ${{ matrix.tox }} -- --cov-report=xml']"
"['pip install virtualenv tox', 'python -c \'import sys; print(""TOXENV=py%d%d"" % (sys.version_info.major, sys.version_info.minor))\' | tee -a $GITHUB_OUTPUT', 'TOXENV=${{steps.toxenv.outputs.TOXENV}} tox', 'TOXENV=${{steps.toxenv.outputs.TOXENV}} END_TO_END=1 tox', 'pip install virtualenv tox', 'python -c \'import sys; e=""cover"" if sys.version_info.major == 2 else ""cover3""; print(""TOXENV=%s"" % e)\' | tee -a $GITHUB_OUTPUT', 'TOXENV=${{steps.toxenv.outputs.TOXENV}} tox', 'pip install virtualenv tox>=4.0.0', 'TOXENV=docs tox']"
"['python -m pip install --upgrade pip\npip install pytest\npip install -e .[test]\npip install -r requirements.txt\npip install ""django~=${{ matrix.django-version }}.0""\n', 'coverage run --parallel -m pytest -x\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'flake8 src tests setup.py\nisort -c -q --diff src/ tests/\n', 'npm ci\n', 'npm run eslint\n', 'python -m pip install --upgrade pip\nmake sandbox\n', 'make docs\n']"
"['python -m pip install --upgrade pip\npip install .\npip install flake8 pytest pytest-cov pydiff\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'flake8 autopep8.py\npycodestyle autopep8.py\n', 'python -m doctest -v README.rst\n', 'pytest --cov-report xml --cov=autopep8\npython test/acid.py -aaa --experimental test/example.py\npython test/acid.py -aaa --experimental test/example_with_reduce.py\npython test/acid.py --pycodestyle= -aaa --compare-bytecode --experimental test/example.py\npython test/acid.py --pycodestyle= --aggressive --line-range 550 610 test/inspect_example.py\npython test/acid.py --pycodestyle= --line-range 289 925 test/vectors_example.py\npython test/test_suite.py\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip3 install -U websockets asyncio\npython3 websocket/tests/echo-server.py &\n', 'pip3 install -U pip setuptools wheel readme_renderer twine pytest python-socks\npython3 -c ""import setuptools; print(\'Setup tools version\'); print(setuptools.__version__)""\npytest websocket/tests -v -rP\npython3 setup.py sdist\ntwine check dist/*\n', 'pip3 install -U websockets asyncio\npython3 websocket/tests/echo-server.py &\n', 'pip3 install coverage pytest pytest-cov setuptools\npython3 -c ""import setuptools; print(\'Setup tools version\'); print(setuptools.__version__)""\npython3 setup.py install\npytest -vrP --cov=websocket websocket/tests --cov-config=.coveragerc\ncoverage report\n', 'pip3 install wsaccel python-socks\npytest -vrP --cov=websocket websocket/tests --cov-config=.coveragerc --cov-append\ncoverage report\n', 'ls $WEBSOCKET_CLIENT_CA_BUNDLE\npython3 -c ""import ssl; print(ssl.get_default_verify_paths().capath)""\npytest -vrP --cov=websocket websocket/tests --cov-config=.coveragerc --cov-append -k ""testSSLopt""\ncoverage report -m\n', 'python3 -c ""import ssl; print(ssl.get_default_verify_paths().capath)""\npytest -vrP --cov=websocket websocket/tests --cov-config=.coveragerc --cov-append -k ""testSSLopt""\ncoverage xml\n', 'pip3 install pytest \'websocket-client[docs]\' rel\npytest ./docs/source -v --doctest-glob=""*.rst"" --doctest-modules\n', 'pip3 install flake8\nflake8 --version\nflake8 . --show-source --statistics --count --ignore E231,E241,E501,E722,E741,F401,F403,F405,W504\n', 'flake8 . --exit-zero --statistics --count\n']"
"['pip install --upgrade pip\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero  --max-line-length=127 --statistics\n', 'python3 -m pytest .\n', 'docker build . --file Dockerfile --tag dnsrecon:$(date +%s)']"
"['sudo apt-get update -qq\nsudo apt-get install -qq toilet figlet\nsudo mkdir -p /usr/local/share/pyfiglet/\nsudo chmod 777 /usr/local/share/pyfiglet/\npython -m pip install --upgrade pip\npip install -r dev-requirements.txt\nmake full\npython setup.py install\n', './pyfiglet/test.py\n', 'pytest -vv\n']"
"['pip install -e .', 'python -m flask_frozen.tests']"
"['python -VV\npython -msite\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine', 'python -m coverage html --skip-covered --skip-empty --ignore-errors', 'python -m coverage report --ignore-errors --fail-under 95', 'python -m pip install -e .[dev]', ""python -c 'import hamcrest; print(hamcrest.__version__)'""]"
"['sudo apt-get install bedtools', 'brew install bedtools', 'python -m pip install --upgrade pip\npip install -e .\n', 'pip install PyYAML pytest pytest-cov pytest-benchmark\npytest --cov=jcvi tests\n']"
"['docker build . --file Dockerfile --tag theharvester:$(date +%s)', 'pip install --upgrade pip\npip install wheel\npip install -r requirements/dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero  --max-line-length=127 --statistics\n', 'pytest\n', 'python theHarvester.py -d apple.com -b anubis\n', 'python theHarvester.py -d yale.edu -b baidu\n', 'python theHarvester.py -d yale.edu -b bing\n', 'python theHarvester.py -d yale.edu -b certspotter\n', 'python theHarvester.py -d hcl.com -b crtsh\n', 'python theHarvester.py -d yale.edu -b dnsdumpster\n', 'python theHarvester.py -d yale.edu -b duckduckgo\n', 'python theHarvester.py -d yale.edu -b hackertarget\n', 'python theHarvester.py -d yale.edu -b intelx\n', 'python theHarvester.py -d yale.edu -b otx\n', 'python theHarvester.py -d yale.edu -b rapiddns\n', 'python theHarvester.py -d yale.edu -b threatminer\n', 'python theHarvester.py -d yale.edu -b urlscan\n', 'python theHarvester.py -d yale.edu -b yahoo\n', 'python theHarvester.py -d yale.edu -c\n', 'mypy --pretty theHarvester/*/*.py\nmypy --pretty theHarvester/*/*/*.py\n']"
"['python -m pip install --upgrade pip\npython -m pip install setuptools\n', 'python -m pip install -e .\npython -m pip install -r test-requirements.txt\n', ""python -m pip install 'numpy<1.17'\n"", 'python -m nose2 --verbosity=3\n']"
"['python -m pip install --upgrade pip\npip install .[all]\n', 'make testone\n']"
"['./tools/github_actions_env_vars.sh', '# TODO: As of 2023/02/28, notebook tests need a pinned mesalib\nmamba install -c conda-forge ""vtk>=9.2=*osmesa*"" ""mesalib=21.2.5""\nmamba list\n', './tools/github_actions_dependencies.sh', './tools/get_minimal_commands.sh', './tools/github_actions_install.sh', './tools/github_actions_infos.sh', './tools/check_qt_import.sh $MNE_QT_BACKEND', 'MNE_SKIP_TESTING_DATASET_TESTS=true pytest -m ""not (ultraslowtest or pgtest)"" --tb=short --cov=mne --cov-report xml -vv -rfE mne/', './tools/get_testing_version.sh', './tools/github_actions_download.sh', './tools/github_actions_test.sh']"
"['make unit', 'make bugfixes', 'make pyopenssl', 'make functional', 'echo ""https://app.codecov.io/gh/gabrielfalcao/httpretty/""']"
['pip install pytest pytest-md pytest-emoji']
[]
"['pip install --upgrade pip setuptools\npip install codecov\n', '# Install ipdb, which will install the right IPython version for the current python.\npip install -e .\n', 'coverage run setup.py test\ncodecov\n']"
"['sudo apt-get install libaspell-dev aspell-en', 'python --version  # just to check\npip install --upgrade pip wheel # upgrade to latest pip find 3.5 wheels; wheel to avoid errors\npip install --upgrade ""setuptools!=47.2.0"" docutils setuptools_scm[toml] twine\npip install aspell-python-py3\npip install -e "".[dev]"" # install the codespell dev packages\n', 'codespell --help', 'codespell --version', 'make check', 'pip uninstall -yq tomli', 'codespell --check-filenames --skip=""./.git/*,*.pyc,./codespell_lib/tests/test_basic.py,./codespell_lib/data/*,./example/code.c,./build/lib/codespell_lib/tests/test_basic.py,./build/lib/codespell_lib/data/*,README.rst,*.egg-info/*,pyproject-codespell.precommit-toml,./.mypy_cache""', '! codespell codespell_lib/tests/test_basic.py', 'pip install --upgrade pip wheel', 'pip install -e "".[dev]""', 'make check-dictionaries', 'pip install -e "".[dev]""', 'ruff --select=ANN --ignore=ANN101,ANN401 .', 'python --version\npip install -U pip\npip install setuptools\npip install -e .[dev]\n', 'codespell --help', 'codespell --version', 'pytest codespell_lib', 'pip install -e .[types]', 'mypy .', 'python -m pip install --upgrade pip\npip install build twine\n', 'python -m build', 'twine check --strict dist/*', 'echo ""Triggered by: ${{ github.event_name }}""\n', 'pip install --user ruff', 'ruff --format=github .']"
"['python -m pip install --upgrade setuptools pip wheel tox', 'tox -e ${{ matrix.env }}', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox']"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install numpy\npython -m pip install vtk\npython -m pip install pillow\npython -m pip install pytest\npython -m pip install traitsui==7.2.1\n', 'python -m pip install -v .', 'pytest -v --pyargs tvtk', 'pytest -v --pyargs mayavi', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install ${{ matrix.qt-api }}\npython -m pip install numpy\npython -m pip install vtk\npython -m pip install pillow\npython -m pip install pytest\npython -m pip install traitsui==7.2.1\n', 'python -um pip install -ve .[app]', 'pytest -v mayavi', 'pytest -sv tvtk']"
"['python -m pip install --upgrade pip\npython -m pip install tox-gh-actions\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox\n', 'python -m pip install --upgrade pip\npython -m pip install tox-gh-actions\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npython -m pip install tox-gh-actions\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox -e typecheck\n', 'sudo apt-get install -y gettext-base gettext\n', 'python -m pip install --upgrade pip\npython -m pip install django\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox -e i18n\n']"
"['echo ""Assigning issue ${{ github.event.issue.number }} to ${{ github.event.comment.user.login }}""\ncurl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" -d \'{""assignees"": [""${{ github.event.comment.user.login }}""]}\' https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/assignees\ncurl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" -X ""DELETE"" https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/labels/help%20wanted\n', 'python -m pip install --upgrade pip black\n', 'black --check .', 'set -x\nif ! git remote -v | grep upstream; then\n  git remote add upstream https://github.com/nilearn/nilearn.git\nfi\ngit fetch upstream\n', 'set -x\necho $(git log -1 --pretty=%B) | tee gitlog.txt\necho ""gitlog.txt = $(cat gitlog.txt)""\necho $GITHUB_REF_NAME | tee merge.txt\nif [ ""$GITHUB_REF_NAME"" != ""main"" ]; then\n    echo ""Merging $(cat merge.txt)"";\n    git pull --ff-only upstream ""refs/pull/$(cat merge.txt)"";\nfi\n', './build_tools/github/build_docs_apt_dependencies.sh\n', './build_tools/github/build_docs_dependencies.sh\n', 'commit_msg=$(git log -2 --format=oneline);\necho $commit_msg;\nif [[ $commit_msg == *""[force download]""* ]]; then\n  echo ""All datasets will be downloaded as requested (only for full builds)."";\n  echo ""false"" | tee restore.txt;\nelse\n  echo ""Data cache will be used if available."";\n  echo ""true"" | tee restore.txt;\nfi\n', 'if [[ $(cat restore.txt) == ""true"" ]]; then\n    date +%U > week_num;\n    echo ""restore=true"" >> $GITHUB_OUTPUT\nfi\n', './build_tools/github/build_type.sh\n', 'echo ""PATTERN = $(cat pattern.txt)""\necho ""BUILD = $(cat build.txt)""\n', 'Xvfb -ac :99 -screen 0 1280x1024x16 > /dev/null 2>&1 &\n', 'source activate testenv\necho ""Conda active env = $CONDA_DEFAULT_ENV"";\ncd doc;\nset -o pipefail;\nPATTERN=$(cat ../pattern.txt) make $(cat ../build.txt) 2>&1 | tee log.txt;\n', 'if [[ ""$(cat build.txt)"" != ""html-strict"" ]]; then\n    echo ""Partial build : No data is downloaded or cached"";\nelse\n    echo ""Full build : Data is downloaded and cached if not restored from previous cache"";\nfi\n', 'python -c ""import sys; print(sys.version)""', './build_tools/github/dependencies.sh', './build_tools/github/install.sh', 'pip install pytest-random-order', 'python -m pytest --pyargs nilearn --cov=nilearn --random-order', 'python -m pip install --upgrade pip flake8 flake8-docstrings\n', 'flake8 --verbose \\\n  examples/0[0234567]* \\\n  maint_tools \\\n  nilearn/_utils \\\n  nilearn/connectome \\\n  nilearn/datasets \\\n  nilearn/de* \\\n  nilearn/glm \\\n  nilearn/image \\\n  nilearn/interfaces \\\n  nilearn/input_data \\\n  nilearn/maskers \\\n  nilearn/mass_univariate \\\n  nilearn/regions \\\n  nilearn/reporting \\\n  nilearn/surface \\\n  nilearn/*.py\n', 'python -m pip install --upgrade pip isort\n', 'isort --diff --check --settings-path pyproject.toml \\\n  examples/0[0234567]* \\\n  maint_tools \\\n  nilearn/_utils \\\n  nilearn/connectome \\\n  nilearn/datasets \\\n  nilearn/de* \\\n  nilearn/glm \\\n  nilearn/image \\\n  nilearn/interfaces \\\n  nilearn/input_data \\\n  nilearn/maskers \\\n  nilearn/mass_univariate \\\n  nilearn/plotting \\\n  nilearn/regions \\\n  nilearn/rerporting \\\n  nilearn/surface \\\n  nilearn/*.py\n', 'python -c ""import sys; print(sys.version)""', './build_tools/github/dependencies.sh', './build_tools/github/install.sh', './build_tools/github/test.sh', 'python -c ""import sys; print(sys.version)""', './build_tools/github/dependencies.sh', './build_tools/github/install.sh', './build_tools/github/test.sh', 'python -c ""import sys; print(sys.version)""', './build_tools/github/dependencies.sh', './build_tools/github/install.sh', './build_tools/github/test_docs.sh', './build_tools/github/test.sh', 'python -c ""import sys; print(sys.version)""', './build_tools/github/dependencies.sh', './build_tools/github/install.sh', './build_tools/github/test_docs.sh', './build_tools/github/test.sh', 'bash build_tools/github/trigger_hosting.sh', 'pip install pre-commit', 'pre-commit autoupdate']"
"['pip install tox', 'tox', 'poetry publish --build']"
"['python -m pip install --upgrade pip\npip install pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python cleanup.py\n']"
['pipx run build --sdist']
"['git checkout HEAD^2', 'make\n', 'make ci\n']"
"['python -m pip install --upgrade pip\n', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT', 'echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'python -m pip install tox', 'python -m tox -e ${{ matrix.tox }}', 'echo ""TAG=${GITHUB_REF#refs/*/}"" | tee -a $GITHUB_ENV', 'echo ""BRANCH=$(git branch -a --contains ${{ env.TAG }} | grep -v HEAD | cut -d \'/\' -f3)"" | tee -a $GITHUB_ENV', 'python -m pip install --upgrade pip build\n', 'python -m build\necho ""TAG=${GITHUB_REF#refs/*/}"" | tee -a $GITHUB_ENV\n', 'git rev-parse HEAD^2 2>/dev/null >/dev/null || echo NOT_MERGE_COMMIT=1 | tee -a $GITHUB_ENV', 'python -m pip install --upgrade pip build\n', 'python -m build\n']"
"['curl -Lo LICENSE/LICENSE_QHULL https://github.com/qhull/qhull/raw/2020.2/COPYING.txt\n', 'python -m pip install build twine', 'python -m build --sdist\npython ci/export_sdist_name.py\n', 'twine check dist/*', 'python .circleci/fetch_doc_logs.py ""${{ github.event.target_url }}""\n', ""# The 'status' event does not contain information in the way that\n# reviewdog expects, so we unset those so it reads from the\n# environment variables we set above.\nunset GITHUB_ACTIONS GITHUB_EVENT_PATH\ncat logs/sphinx-errors-warnings.log | \\\n  reviewdog \\\n    -efm '%f\\:%l: %tEBUG: %m' \\\n    -efm '%f\\:%l: %tNFO: %m' \\\n    -efm '%f\\:%l: %tARNING: %m' \\\n    -efm '%f\\:%l: %tRROR: %m' \\\n    -efm '%f\\:%l: %tEVERE: %m' \\\n    -efm '%f\\:%s: %tARNING: %m' \\\n    -efm '%f\\:%s: %tRROR: %m' \\\n    -name=sphinx -tee -fail-on-error=false \\\n    -reporter=github-check -filter-mode=nofilter\ncat logs/sphinx-deprecations.log | \\\n  reviewdog \\\n    -efm '%f\\:%l: %m' \\\n    -name=examples -tee -reporter=github-check -filter-mode=nofilter\n"", 'git fetch --quiet origin ""$GITHUB_BASE_REF""\nbase=""$(git merge-base ""origin/$GITHUB_BASE_REF"" \'HEAD^2\')""\nad=""$(git log ""$base..HEAD^2"" --pretty=tformat: --name-status --diff-filter=AD |\n      cut --fields 2 | sort | uniq --repeated)""\nif [[ -n ""$ad"" ]]; then\n  printf \'The following files were both added and deleted in this PR:\\n%s\\n\' ""$ad""\n  exit 1\nfi\n', 'git fetch --quiet origin ""$GITHUB_BASE_REF""\nbase=""$(git merge-base ""origin/$GITHUB_BASE_REF"" \'HEAD^2\')""\nam=""$(git log ""$base..HEAD^2"" --pretty=tformat: --name-status --diff-filter=AM |\n      cut --fields 2 | sort | uniq --repeated |\n      grep -E \'\\.(png|pdf|ps|eps|svg)\' || true)""\nif [[ -n ""$am"" ]]; then\n  printf \'The following images were both added and modified in this PR:\\n%s\\n\' ""$am""\n  exit 1\nfi\n', 'git fetch --quiet origin ""$GITHUB_BASE_REF""\nbase=""$(git merge-base ""origin/$GITHUB_BASE_REF"" \'HEAD^2\')""\nlib=""$(git log ""$base..HEAD^2"" --pretty=tformat: --name-status -- lib src |\n       cut --fields 2 | sort || true)""\nif [[ -n ""$lib"" ]]; then\n  printf \'Changes to the following files have no effect and should not be backported:\\n%s\\n\' ""$lib""\n  exit 1\nfi\n', 'python -m pip install --upgrade pip setuptools wheel\n# TODO: Use pip-tools instead when it supports build-system\n# dependencies so we don\'t need another copy here.\n# https://github.com/jazzband/pip-tools/pull/1681\npython -m pip install --upgrade \\\n  certifi contourpy cycler fonttools kiwisolver importlib_resources \\\n  numpy packaging pillow pyparsing python-dateutil setuptools-scm \\\n  pybind11\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n', 'mkdir ~/.cache/matplotlib\n$CODEQL_PYTHON setup.py build\n', 'git config --global core.autocrlf input', '/bin/mkpasswd.exe -c | sed -e ""s/$(id -u)/0/"" >/etc/passwd\n', 'git.exe config --global --add safe.directory /proc/cygdrive/d/a/matplotlib/matplotlib\ngit config --global --add safe.directory /cygdrive/d/a/matplotlib/matplotlib\nC:/cygwin/bin/git.exe config --global --add safe.directory D:/a/matplotlib/matplotlib\n/usr/bin/git config --global --add safe.directory /cygdrive/d/a/matplotlib/matplotlib\n', 'ls -l /bin/sh.exe /bin/bash.exe /bin/dash.exe\n/bin/rm -f /bin/sh.exe || exit 1\ncp -sf /bin/dash.exe /bin/sh.exe || exit 1\nls -l /bin/sh.exe /bin/bash.exe /bin/dash.exe\n', '/usr/sbin/alternatives --set python /usr/bin/python3.${{ matrix.python-minor-version }}\n/usr/sbin/alternatives --set python3 /usr/bin/python3.${{ matrix.python-minor-version }}\n', 'python -m pip install --upgrade pip \'setuptools<60\' wheel\npython -m pip install kiwisolver \'numpy!=1.21.*\' pillow importlib_resources\ngrep -v -F -e psutil requirements/testing/all.txt >requirements_test.txt\npython -m pip install --upgrade \'contourpy>=1.0.1\' cycler fonttools \\\n    packaging pyparsing python-dateutil setuptools-scm \\\n    -r requirements_test.txt sphinx ipython\npython -m pip install --upgrade pycairo \'cairocffi>=0.8\' PyGObject &&\n   python -c \'import gi; gi.require_version(""Gtk"", ""3.0""); from gi.repository import Gtk\' &&\n   echo \'PyGObject is available\' ||\n   echo \'PyGObject is not available\'\npython -m pip install --upgrade pyqt5  &&\n   python -c \'import PyQt5.QtCore\' &&\n   echo \'PyQt5 is available\' ||\n   echo \'PyQt5 is not available\'\npython -mpip install --upgrade pyside2 &&\n    python -c \'import PySide2.QtCore\' &&\n    echo \'PySide2 is available\' ||\n    echo \'PySide2 is not available\'\npython -m pip uninstall --yes wxpython || echo \'wxPython already uninstalled\'\n', 'ccache -s\ngit describe\ncat <<EOT >> mplsetup.cfg\n[rc_options]\nbackend=Agg\n\n[libs]\nsystem_freetype = False\nsystem_qhull = True\nEOT\ncat mplsetup.cfg\n# All dependencies must have been pre-installed, so that the minver\n# constraints are held.\npython -m pip install --no-deps -ve .\n', 'find {/usr,/usr/local}/{bin,lib/python3.*/site-packages} /usr/lib/lapack . -name \\*.exe -o -name \\*.dll -print >files_to_rebase.txt\n', 'rebase --database --filelist=files_to_rebase.txt', '/usr/bin/python -c ""import matplotlib as mpl; import matplotlib.pyplot as plt""\n', 'oldmplrc=$(python -c ""from matplotlib import matplotlib_fname as mplrc_file; print(mplrc_file())"")\necho ""${oldmplrc}""\nmkdir -p ~/.matplotlib/\nsed -E -e \'s~#animation\\.ffmpeg_path:.+~animation.ffmpeg_path: /usr/bin/ffmpeg.exe~\' ""${oldmplrc}"" >~/.matplotlib/matplotlibrc\n', 'xvfb-run python -mpytest -raR -n auto \\\n  --maxfail=50 --timeout=300 --durations=25 \\\n  --cov-report=xml --cov=lib --log-level=DEBUG --color=yes\n', 'pip3 install -r requirements/testing/mypy.txt \\\n  -r requirements/testing/all.txt\npip3 install -e .\n', 'mkdir -p ""$HOME/bin""\ncurl -sfL \\\n  https://github.com/reviewdog/reviewdog/raw/master/install.sh | \\\n    sh -s -- -b ""$HOME/bin""\necho ""$HOME/bin"" >> $GITHUB_PATH\n', ""set -o pipefail\nMPLBACKEND=agg python -m mypy.stubtest \\\n  --mypy-config-file pyproject.toml \\\n  --allowlist ci/mypy-stubtest-allowlist.txt \\\n  matplotlib | \\\n    reviewdog \\\n      -efm '%Eerror: %m' \\\n      -efm '%CStub: in file %f:%l' \\\n      -efm '%CStub: in file %f' \\\n      -efm '%+CRuntime:%.%#' \\\n      -efm '%+CMISSING' \\\n      -efm '%+Cdef %.%#' \\\n      -efm '%+C<%.%#>' \\\n      -efm '%Z' \\\n      -reporter=github-check -tee -name=mypy-stubtest \\\n      -filter-mode=nofilter\n"", 'PROJECT_REPO=""matplotlib/matplotlib""\nBRANCH=""main""\nWORKFLOW_NAME=""cibuildwheel.yml""\nARTIFACT_NAME=""wheels""\n\ngh run --repo ""${PROJECT_REPO}"" \\\n   list --branch ""${BRANCH}"" \\\n        --workflow ""${WORKFLOW_NAME}"" \\\n        --json event,status,conclusion,databaseId > runs.json\nRUN_ID=$(\n  jq --compact-output \\\n    \'[\n      .[] |\n      # Filter on ""push"" events to main (merged PRs) ...\n      select(.event == ""push"") |\n      # that have completed successfully ...\n      select(.status == ""completed"" and .conclusion == ""success"")\n     ] |\n    # and get ID of latest build of wheels.\n    sort_by(.databaseId) | reverse | .[0].databaseId\' runs.json\n)\ngh run --repo ""${PROJECT_REPO}"" view ""${RUN_ID}""\ngh run --repo ""${PROJECT_REPO}"" \\\n   download ""${RUN_ID}"" --name ""${ARTIFACT_NAME}""\n\nmkdir dist\nmv *.whl dist/\nls -l dist/\n', 'pip3 install -r requirements/testing/flake8.txt', 'mkdir -p ""$HOME/bin""\ncurl -sfL \\\n  https://github.com/reviewdog/reviewdog/raw/master/install.sh | \\\n    sh -s -- -b ""$HOME/bin""\necho ""$HOME/bin"" >> $GITHUB_PATH\n', 'set -o pipefail\nflake8 --docstring-convention=all | \\\n  reviewdog -f=pep8 -name=flake8 \\\n    -tee -reporter=github-check -filter-mode nofilter\n', 'pip3 install -r requirements/testing/mypy.txt -r requirements/testing/all.txt', 'mkdir -p ""$HOME/bin""\ncurl -sfL \\\n  https://github.com/reviewdog/reviewdog/raw/master/install.sh | \\\n    sh -s -- -b ""$HOME/bin""\necho ""$HOME/bin"" >> $GITHUB_PATH\n', 'set -o pipefail\n# The --ignore-missing-imports can be removed when typed cycler is released and used\nmypy --config pyproject.toml lib/matplotlib \\\n   --ignore-missing-imports \\\n   --follow-imports silent | \\\n  reviewdog -f=mypy -name=mypy \\\n    -tee -reporter=github-check -filter-mode nofilter\n', 'case ""${{ runner.os }}"" in\nLinux)\n  echo \'Acquire::Retries ""3"";\' | sudo tee /etc/apt/apt.conf.d/80-retries\n  sudo apt-get update -yy\n  sudo apt-get install -yy \\\n    ccache \\\n    cm-super \\\n    dvipng \\\n    ffmpeg \\\n    fonts-noto-cjk \\\n    gdb \\\n    gir1.2-gtk-3.0 \\\n    graphviz \\\n    inkscape \\\n    lcov \\\n    libcairo2 \\\n    libcairo2-dev \\\n    libffi-dev \\\n    libgeos-dev \\\n    libgirepository1.0-dev \\\n    libsdl2-2.0-0 \\\n    libxkbcommon-x11-0 \\\n    libxcb-cursor0 \\\n    libxcb-icccm4 \\\n    libxcb-image0 \\\n    libxcb-keysyms1 \\\n    libxcb-randr0 \\\n    libxcb-render-util0 \\\n    libxcb-xinerama0 \\\n    lmodern \\\n    fonts-freefont-otf \\\n    texlive-pictures \\\n    pkg-config \\\n    qtbase5-dev \\\n    texlive-fonts-recommended \\\n    texlive-latex-base \\\n    texlive-latex-extra \\\n    texlive-latex-recommended \\\n    texlive-luatex \\\n    texlive-xetex \\\n    ttf-wqy-zenhei\n  if [[ ""${{ matrix.os }}"" = ubuntu-20.04 ]]; then\n    sudo apt-get install -yy libopengl0\n  else  # ubuntu-22.04\n    sudo apt-get install -yy gir1.2-gtk-4.0 libnotify4\n  fi\n  ;;\nmacOS)\n  brew install ccache\n  brew tap homebrew/cask-fonts\n  brew install font-noto-sans-cjk\n  ;;\nesac\n', '# Upgrade pip and setuptools and wheel to get as clean an install as\n# possible.\npython -m pip install --upgrade pip setuptools wheel\n\n# Install dependencies from PyPI.\npython -m pip install --upgrade $PRE \\\n  \'contourpy>=1.0.1\' cycler fonttools kiwisolver importlib_resources \\\n  numpy packaging pillow pyparsing python-dateutil setuptools-scm \\\n  -r requirements/testing/all.txt \\\n  ${{ matrix.extra-requirements }}\n\n# Preinstall pybind11 on no-build-isolation builds.\nif [[ ""${{ matrix.name-suffix }}"" == \'(Minimum Versions)\' ]]; then\n  python -m pip install \'pybind11>=2.6\'\nfi\n\n# Install optional dependencies from PyPI.\n# Sphinx is needed to run sphinxext tests\npython -m pip install --upgrade sphinx!=6.1.2\n\n# GUI toolkits are pip-installable only for some versions of Python\n# so don\'t fail if we can\'t install them.  Make it easier to check\n# whether the install was successful by trying to import the toolkit\n# (sometimes, the install appears to be successful but shared\n# libraries cannot be loaded at runtime, so an actual import is a\n# better check).\n# PyGObject, pycairo, and cariocffi do not install on OSX 10.12.\n python -m pip install --upgrade pycairo \'cairocffi>=0.8\' PyGObject &&\n   python -c \'import gi; gi.require_version(""Gtk"", ""3.0""); from gi.repository import Gtk\' &&\n   echo \'PyGObject is available\' ||\n   echo \'PyGObject is not available\'\n\n# There are no functioning wheels available for OSX 10.12 (as of\n# Sept 2020) for either pyqt5 (there are only wheels for 10.13+) or\n# pyside2 (the latest version (5.13.2) with 10.12 wheels has a\n# fatal to us bug, it was fixed in 5.14.0 which has 10.13 wheels)\npython -mpip install --upgrade pyqt5${{ matrix.pyqt5-ver }} &&\n  python -c \'import PyQt5.QtCore\' &&\n  echo \'PyQt5 is available\' ||\n  echo \'PyQt5 is not available\'\nif [[ ""${{ runner.os }}"" != \'macOS\' ]]; then\n  python -mpip install --upgrade pyside2${{ matrix.pyside2-ver }} &&\n    python -c \'import PySide2.QtCore\' &&\n    echo \'PySide2 is available\' ||\n    echo \'PySide2 is not available\'\nfi\nif [[ ""${{ runner.os }}"" != \'macOS\' ]]; then\n  python -mpip install --upgrade pyqt6${{ matrix.pyqt6-ver }} &&\n    python -c \'import PyQt6.QtCore\' &&\n    echo \'PyQt6 is available\' ||\n    echo \'PyQt6 is not available\'\n  python -mpip install --upgrade pyside6${{ matrix.pyside6-ver }} &&\n    python -c \'import PySide6.QtCore\' &&\n    echo \'PySide6 is available\' ||\n    echo \'PySide6 is not available\'\nfi\n\npython -mpip install --upgrade \\\n  -f ""https://extras.wxpython.org/wxPython4/extras/linux/gtk3/${{ matrix.os }}"" \\\n  wxPython &&\n  python -c \'import wx\' &&\n  echo \'wxPython is available\' ||\n  echo \'wxPython is not available\'\n', 'python -m pip install pytz  # Must be installed for Pandas.\npython -m pip install \\\n  --index-url https://pypi.anaconda.org/scipy-wheels-nightly/simple \\\n  --upgrade --only-binary=:all: numpy pandas\n', 'ccache -s\ngit describe\n\n# Set flag in a delayed manner to avoid issues with installing other\n# packages\nif [[ ""${{ runner.os }}"" != \'macOS\' ]]; then\n  if [[ ""$(lsb_release -r -s)"" == ""20.04"" ]]; then\n    export CPPFLAGS=\'--coverage -fprofile-abs-path\'\n  else\n    export CPPFLAGS=\'--coverage\'\n  fi\nfi\n\ncat <<EOT >> mplsetup.cfg\n[rc_options]\nbackend=Agg\nEOT\n\ncat mplsetup.cfg\n\nif [[ ""${{ matrix.name-suffix }}"" == \'(Minimum Versions)\' ]]; then\n  # Minimum versions run does not use build isolation so that it\n  # builds against the pre-installed minver dependencies.\n  python -m pip install --no-deps --no-build-isolation -ve .\nelse\n  python -m pip install --no-deps -ve .\nfi\n\nif [[ ""${{ runner.os }}"" != \'macOS\' ]]; then\n  unset CPPFLAGS\nfi\n', 'rm -rf ~/.cache/matplotlib\n', 'python -mpytest -raR -n auto \\\n  --maxfail=50 --timeout=300 --durations=25 \\\n  --cov-report=xml --cov=lib --log-level=DEBUG --color=yes\n', ""lcov --capture --directory . --output-file coverage.info\nlcov --output-file coverage.info \\\n  --extract coverage.info $PWD/src/'*' $PWD/lib/'*'\nlcov --list coverage.info\nfind . -name '*.gc*' -delete\n""]"
""
""
"['python -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox', 'set -xe\npython -m pip install coverage[toml]\npython -m coverage combine\npython -m coverage xml\n', 'python -m pip install pep517 twine', 'python -m pep517.build --source --binary .', 'ls -l dist', 'python -m twine check dist/*', 'python -m pip install -e .[dev]', ""python -c 'import jwt; print(jwt.__version__)'"", ""echo ${{ join(steps.stale.outputs.*, ',') }}""]"
"['brew install automake libtool ccache\nln -s /usr/local/bin/glibtoolize /usr/local/bin/libtoolize\n', 'bash -c \'GITHUB_API_TOKEN=""${{ secrets.GITHUB_TOKEN }}""  bash ./tools/ci-run.sh\'', 'make html', 'sudo apt-get update -y -q && sudo apt-get install -y -q ""libxml2=2.9.13*"" ""libxml2-dev=2.9.13*"" libxslt1.1 libxslt1-dev', 'python -m pip install -U pip setuptools && python -m pip install -U docutils pygments sphinx sphinx-rtd-theme -r requirements.txt', 'make html sdist', 'python -m pip install -r requirements.txt', 'make sdist wheel_${{ matrix.image }}', 'brew install automake libtool\nln -s /usr/local/bin/glibtoolize /usr/local/bin/libtoolize\n', 'python -m pip install setuptools wheel -r requirements.txt', 'make sdist wheel']"
"[""pip install 'black==21.6b0'"", 'black --check .', 'black .\ngit config --global user.name \'autoblack\'\ngit config --global user.email \'rocky@users.noreply.github.com\'\ngit remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY\ngit checkout $GITHUB_HEAD_REF\ngit commit -am ""fixup: Format Python code with Black""\ngit push\n', 'brew install llvm@11\npython -m pip install --upgrade pip\nLLVM_CONFIG=/usr/local/Cellar/llvm@11/11.1.0/bin/llvm-config pip install llvmlite\n', 'make develop-full\n', 'pip install -r requirements-dev.txt\nmake -j3 check\n', 'sudo apt-get update -qq && sudo apt-get install -qq liblapack-dev llvm-dev\npython -m pip install --upgrade pip\n', 'make develop-full\n', 'pip install -r requirements-dev.txt\nmake -j3 check\n', 'python -m pip install --upgrade pip\npython -m pip install wheel\nchoco install llvm\nset LLVM_DIR=""C:\\Program Files\\LLVM""\npip install -e .\n', 'python setup.py install\n', 'pip install -e .[full]\npip install -r requirements-dev.txt\nset PYTEST_WORKERS=""-n3""\nmake check\n']"
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox']"
"['docker-compose --version\n', 'docker-compose build', ""docker-compose run -T testing urlwait postgresql://kuma:kuma@postgres:5432/developer_mozilla_org 30\ndocker-compose run -T testing ./manage.py migrate\n# Essentially compares **/models.py with **/migrations/*.py and\n# makes sure the developer didn't forget to create a new migration.\ndocker-compose run -T testing ./manage.py makemigrations --check --dry-run\n"", 'docker-compose run -T testing make coveragetest\n', 'bash <(curl -s --retry 3 --retry-connrefused https://codecov.io/bash)\n', 'python -m pip install --upgrade pip\npip install --disable-pip-version-check -r docs/requirements.txt\n', 'sphinx-build -b html -d doctrees -W docs html\n', 'python -m pip install --upgrade pip\npip install --disable-pip-version-check black flake8 flake8-isort\necho ""Version of black installed:""\nblack --version\necho ""Version of flake8 installed:""\nflake8 --version\n', 'flake8 kuma docs\n', 'black --check --diff kuma docs\n']"
"['python -m pip install --upgrade pip setuptools wheel', 'python setup.py sdist', 'DISABLE_SPEEDUPS=1 python setup.py bdist_wheel', 'mkdir tmp\ncd tmp\ntar zxf ../dist/simplejson-*.tar.gz\ncd simplejson-*\nREQUIRE_SPEEDUPS=1 python setup.py build build_ext -i test\n']"
"['make build', 'make pytest', 'npm ci', 'npm run types -- --pretty', 'npm run build', 'npm install --global npm@8', 'npm ci', 'npm test --coverage', 'python3 -m json.tool < app.json > /dev/null', 'npm ci', 'npm run eslint', 'npm ci', 'npm run check-prettier', 'pip install -U pip\npip install -r requirements/lint.txt\n', 'flake8 pontoon', 'pip install -U pip\npip install -r requirements/lint.txt\n', 'black --check pontoon', 'pip install -U pip\npip install -r requirements/lint.txt\n', 'pyupgrade --py39-plus *.py `find pontoon -name \\*.py`', 'npm install --global npm@8', 'npm ci', 'npm test', 'npm run build']"
""
"['git submodule update --init --recursive', 'python -m pip install -U ${{ contains(matrix.python-version, \'-dev\') && \'pip setuptools\' || \'""pip<21"" ""setuptools<45""\' }}\npython -m pip install -U wheel tox virtualenv -r requirements.txt\n', 'sudo apt-get install lib${{ matrix.lua-version }}-dev', ""python setup.py sdist ${{ contains(matrix.python-version, '3.') && 'build_ext -j6' || '' }} bdist_wheel"", 'python setup.py test', 'git submodule update --init --recursive', 'python -m pip install -r requirements.txt', 'make sdist', 'git submodule update --init --recursive', 'python -m pip install -r requirements.txt', 'make USE_BUNDLE=true sdist wheel_${{ matrix.image }}', 'git submodule update --init --recursive', 'brew install automake libtool\nln -s /usr/local/bin/glibtoolize /usr/local/bin/libtoolize\n', 'python -m pip install setuptools wheel -r requirements.txt', ""set USE_BUNDLE=true\npython setup.py --with-cython sdist ${{ contains(matrix.python-version, '3.') && 'build_ext -j5' || '' }} bdist_wheel\n""]"
"['python -m pip install --upgrade pip\npip install .\n', 'pip install -r test_requirements.txt\n', 'python runtests.py runtests\n']"
""
"['cd ${{ env.PKGDIR }}\npython setup.py sdist\npip wheel -w dist --no-deps dist/*.zip\n', 'pip install ${{ env.PKGDIR }}/dist/*.whl\n', 'python -c ""from mpl_toolkits import basemap_data; print(basemap_data)""\n', 'pip install --prefer-binary twine\n', 'python -m twine check                                               \\\n    ${{ env.PKGDIR }}/dist/*.zip                                    \\\n    ${{ env.PKGDIR }}/dist/*.whl\npython -m twine upload --skip-existing                              \\\n    ${{ env.PKGDIR }}/dist/*.zip                                    \\\n    ${{ env.PKGDIR }}/dist/*.whl\n', 'cd ${{ env.PKGDIR }}\npython setup.py sdist\npip wheel -w dist --no-deps dist/*.zip\n', 'pip install ${{ env.PKGDIR }}/dist/*.whl\n', 'python -c ""from mpl_toolkits import basemap_data; print(basemap_data)""\n', 'pip install --prefer-binary twine\n', 'python -m twine check                                               \\\n    ${{ env.PKGDIR }}/dist/*.zip                                    \\\n    ${{ env.PKGDIR }}/dist/*.whl\npython -m twine upload --skip-existing                              \\\n    ${{ env.PKGDIR }}/dist/*.zip                                    \\\n    ${{ env.PKGDIR }}/dist/*.whl\n', 'cd ${{ env.PKGDIR }}\npip install -r requirements-lint.txt\n', 'cd ${{ env.PKGDIR }}\npip install --prefer-binary -r requirements.txt\n', 'cd ${{ env.PKGDIR }}\nif [ -x ""$(command -v flake8)"" ]; then\n    flake8 src/mpl_toolkits/basemap/cm.py;\nfi\n', 'cd ${{ env.PKGDIR }}\nif [ -x ""$(command -v pylint)"" ]; then\n    pylint src/mpl_toolkits/basemap/cm.py;\nfi\n', 'apt-get update\napt-get install -y libidn11\npkgvers=3.6.2\npkgname=cmake\npkgcode=cmake-${pkgvers}\ncase ""${{ matrix.arch }}"" in\n    x86)  pkgfile=${pkgcode}-Linux-i386.tar.gz;;\n    *)    pkgfile=${pkgcode}-Linux-x86_64.tar.gz;;\nesac\nwget https://github.com/Kitware/CMake/releases/download/v${pkgvers}/${pkgfile} -P /tmp\ntar -xf /tmp/${pkgfile} --strip-components=1 -C /usr\nrm -rf /tmp/${pkgfile}\n', 'apt-get update\napt-get install -y gcc g++ make\n', 'cd ${{ env.PKGDIR }}\npython -c ""import utils; utils.GeosLibrary(\'3.5.1\').build(\'extern\', njobs=16)""\n', 'apt-get update\napt-get install -y gcc g++ make\n', 'case ""${{ matrix.python-version }}"" in\n    2.6|3.[23])      pkgvers=1.11.3;;\n    2.7|3.[456789])  pkgvers=1.16.6;;\n    3.10)            pkgvers=1.21.4;;\n    *)               pkgvers=1.23.3;;\nesac\n# Dirty solution to get NumPy headers for Python 3.11.\nif [ ""${{ matrix.python-version }}"" = ""3.11"" ]; then\n    case ""${{ matrix.arch }}"" in\n        x64)  kwds=""--plat=manylinux_2_17_x86_64"" ;;\n        x86)  kwds=""--plat=manylinux_2_17_i686""   ;;\n    esac\n    pip download --no-deps ${kwds} ""numpy==${pkgvers}""\n    oldpkgfile=$(ls *.whl | head -n1)\n    newpkgfile=$(echo ""${oldpkgfile}"" | sed \'s/manylinux_2_17/linux/\')\n    mv ""${oldpkgfile}"" ""${newpkgfile}""\n    pip install ""${newpkgfile}""\n    rm ""${newpkgfile}""\nelse\n    pip install ""numpy == ${pkgvers}""\nfi\n', 'sitepkgdir=$(pip show numpy 2>/dev/null | grep Location: | cut -d\' \' -f2)\nexport GEOS_DIR=""${GITHUB_WORKSPACE}/${{ env.PKGDIR }}/extern""\nexport NUMPY_INCLUDE_PATH=${sitepkgdir}/numpy/core/include\nif [ ""${{ matrix.python-version }}"" = ""3.11"" ]; then\n    kwds=""--no-build-isolation""\n    pip install setuptools wheel ""cython >= 0.29, < 3.1""\nfi\ncd ${{ env.PKGDIR }}\npython setup.py sdist\npip wheel -w dist --no-deps ${kwds} dist/*.zip\n', ""sed -i 's|deb\\.debian\\.org|archive\\.debian\\.org|g' /etc/apt/sources.list\n"", 'apt-get update\napt-get install -y unzip\npip install patchelf\npip install ""auditwheel < 4.0""\n', 'cd ${{ env.PKGDIR }}\nexport LD_LIBRARY_PATH=""$(readlink -f extern/lib)""\nauditwheel repair -w dist dist/*.whl\n', ""sed -i 's|deb\\.debian\\.org|archive\\.debian\\.org|g' /etc/apt/sources.list\n"", 'apt-get update\napt-get install -y gcc g++ make\npip install ""numpy < 1.24""\n', 'pip install --prefer-binary ${{ env.PKGDIR }}/dist/*-manylinux1*.whl\n', 'python -c ""from mpl_toolkits.basemap import Basemap""\npython -c ""from mpl_toolkits.basemap import cm""\n', 'pip install --prefer-binary twine\n', 'python -m twine check                                               \\\n    ${{ env.PKGDIR }}/dist/*.zip                                    \\\n    ${{ env.PKGDIR }}/dist/*-manylinux1*.whl\npython -m twine upload --skip-existing                              \\\n    ${{ env.PKGDIR }}/dist/*.zip                                    \\\n    ${{ env.PKGDIR }}/dist/*-manylinux1*.whl\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'cd ${{ env.PKGDIR }}\npython -m pip install -r requirements-lint.txt\n', 'cd ${{ env.PKGDIR }}\npip install --prefer-binary -r requirements.txt\n', 'cd ${{ env.PKGDIR }}\nif (Get-Command flake8 -errorAction SilentlyContinue)\n{\n    flake8 src/mpl_toolkits/basemap/cm.py;\n}\n', 'cd ${{ env.PKGDIR }}\nif (Get-Command pylint -errorAction SilentlyContinue)\n{\n    pylint src/mpl_toolkits/basemap/cm.py;\n}\n', 'cd ${{ env.PKGDIR }}\npython -c ""import utils; utils.GeosLibrary(\'3.5.1\').build(\'extern\', njobs=16)""\n', 'if (""${{ matrix.python-version }}"" -eq ""2.7"") {\n    echo ""msvc-toolset=9.0"" >> $env:GITHUB_ENV\n} else {\n    echo ""msvc-toolset=14.0"" >> $env:GITHUB_ENV\n}\n', 'python -m pip install --upgrade pip setuptools wheel\n', 'Switch -regex (""${{ matrix.python-version }}"") {\n    ""^2\\.6|3\\.[123]$""     { Set-Variable -Name ""pkgvers"" -Value ""1.11.3"" }\n    ""^2\\.7|3\\.[456789]$""  { Set-Variable -Name ""pkgvers"" -Value ""1.16.6"" }\n    ""^3\\.10$""             { Set-Variable -Name ""pkgvers"" -Value ""1.21.4"" }\n    default               { Set-Variable -Name ""pkgvers"" -Value ""1.23.3"" }\n}\n$env:SETUPTOOLS_USE_DISTUTILS = ""stdlib""\npython -m pip install ""numpy == ${pkgvers}""\n', 'cd ${{ env.PKGDIR }}\n$env:GEOS_DIR = ""$env:GITHUB_WORKSPACE/${{ env.PKGDIR }}/extern""\npip install -r requirements-setup.txt\npython setup.py sdist\npip wheel -w dist --no-deps (Get-Item dist/*.zip)\n', 'pip install --prefer-binary (Get-Item ${{ env.PKGDIR }}/dist/*-win*.whl)\n', 'python -c ""from mpl_toolkits.basemap import Basemap""\npython -c ""from mpl_toolkits.basemap import cm""\n', 'pip install --prefer-binary twine\n', 'python -m twine check                                               `\n    ${{ env.PKGDIR }}/dist/*.zip                                    `\n    ${{ env.PKGDIR }}/dist/*.whl\npython -m twine upload --skip-existing                              `\n    ${{ env.PKGDIR }}/dist/*.whl\n']"
"['sudo apt-get -y update\nsudo apt-get -y install \\\n  libqt5core5a libqt5dbus5 libqt5gui5 libqt5svg5 libqt5widgets5 libqt5xml5 \\\n  python3-all python3-astropy python3-h5py python3-pyqt5 python3-pyqt5.qtsvg \\\n  python3-setuptools python3-all-dev python3-numpy python3-sip-dev \\\n  pyqt5-dev pyqt5-dev-tools qt5-qmake qtbase5-dev qt5-default\n', 'python3 setup.py build_ext --inplace\n', 'QT_QPA_PLATFORM=minimal VEUSZ_INPLACE_TEST=1 python3 tests/runselftest.py\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py build_ext --inplace\n', 'tests/runselftest.py\n', 'python -V\npython -m pip install --upgrade pip\n', 'pip install -r requirements.txt\n', 'python setup.py build_ext --inplace\n', 'set QT_QPA_PLATFORM=minimal\nset VEUSZ_INPLACE_TEST=1\npython tests/runselftest.py\n']"
"['python -m pip install --upgrade pip\npip install .\npip install python-dateutil httpretty nosexcover python-coveralls\n', 'nosetests --with-xcoverage --cover-package=pyzotero --cover-tests\n']"
"['sudo apt-get update\nsudo apt-get install -y tigervnc-standalone-server xserver-xephyr gnumeric x11-utils\n', 'brew install --cask xquartz\n# https://docs.github.com/en/actions/learn-github-actions/workflow-commands-for-github-actions#adding-a-system-path\necho ""/opt/X11/bin"" >> $GITHUB_PATH  \n# https://github.com/ponty/PyVirtualDisplay/issues/42\n# mkdir /tmp/.X11-unix\n# sudo chmod 1777 /tmp/.X11-unix\n# sudo chown root /tmp/.X11-unix\n', 'Xvfb -help\n', 'python -m pip install .\npip install -r requirements-test.txt\n', 'cd tests\npytest -v .\n']"
"['python -m pip install --upgrade pip setuptools virtualenv wheel', 'python -m pip install --upgrade codecov tox', 'python -m pip install --upgrade tox-py', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'tox --py current', 'tox -e base,dist,docs\n', 'codecov -e TOXENV,DJANGO\n']"
"['python -m pip install ruff cython-lint\n', 'set -euo pipefail\npython tools/lint.py --diff-against origin/$GITHUB_BASE_REF\npython tools/unicode-check.py\npython tools/check_test_name.py\n', ""# NOTE: not the same OpenBLAS version as in upstream CI (I'm being lazy here)\nsudo apt-get update\nsudo apt-get install -y libopenblas-dev libatlas-base-dev liblapack-dev gfortran libgmp-dev libmpfr-dev libsuitesparse-dev ccache libmpc-dev\n"", 'python -m pip install numpy cython pytest pytest-xdist pytest-timeout pybind11 mpmath gmpy2 pythran ninja meson click rich-click doit pydevtool pooch\n', 'python -m pip install git+https://github.com/numpy/numpy.git\npython -m pip install ninja cython pytest pybind11 pytest-xdist pytest-timeout click rich-click doit pydevtool pooch\npython -m pip install git+https://github.com/serge-sans-paille/pythran.git\npython -m pip install git+https://github.com/mesonbuild/meson.git\n', 'mkdir -p ""${CCACHE_DIR}""\necho ""dir=$CCACHE_DIR"" >> $GITHUB_OUTPUT\nNOW=$(date -u +""%F-%T"")\necho ""timestamp=${NOW}"" >> $GITHUB_OUTPUT\n', 'python dev.py build --werror\n', 'ccache -s', 'pushd tools\npython check_installation.py ${{ env.INSTALLDIR }}\n./check_pyext_symbol_hiding.sh ../build\npopd\n', '# Packages that are only needed for their annotations\npython -m pip install -r mypy_requirements.txt\npython -m pip install pybind11 sphinx\n\npython -u dev.py mypy\n', 'export OMP_NUM_THREADS=2\nexport SCIPY_USE_PROPACK=1\npython dev.py --no-build test -j2 -- --durations 10 --timeout=60\n', ""# We're not running the full test suite here, only testing the install\n# into a venv is working, so leave out optional dependencies. That's\n# also why we can get away with an old version of OpenBLAS from Ubuntu\nsudo apt-get update\nsudo apt-get install -y python3-dev libopenblas-dev pkg-config gfortran\n"", ""python -m venv ../venvs/scipy-venv\nsource ../venvs/scipy-venv/bin/activate\n# Note that this uses build isolation. That's why we don't need build\n# dependencies to be installed in the venv itself.\npython -m pip install . -vv\n"", 'source ../venvs/scipy-venv/bin/activate\ncd ..\npython -c ""import scipy""\npython -c ""import scipy.linalg""\npython -m pip install pytest\npython -c ""from scipy import cluster; cluster.test()""\n', 'python -m venv .venv\nsource .venv/bin/activate\n# Install build dependencies. Use meson-python from its main branch,\n# most convenient to test in this job because we\'re using pip without\n# build isolation here.\npython -m pip install numpy pybind11 pythran cython pytest ninja\npython -m pip install git+https://github.com/mesonbuild/meson-python.git\n# Non-isolated build, so we use dependencies installed inside the source tree\npython -m pip install -U pip  # need pip >=23 for `--config-settings`\npython -m pip install . --no-build-isolation --config-settings=compile-args=-j2\n\n# Basic tests\ncd ..\npython -c ""import scipy""\npython -c ""import scipy.linalg""\npython -c ""from scipy import cluster; cluster.test()""\n', 'sudo apt-get update\nsudo apt install python3-dbg python3-dev libatlas-base-dev liblapack-dev gfortran ccache libgmp-dev libmpfr-dev libmpc-dev\npython3-dbg --version # just to check\npython3-dbg -c \'import sys; print(""Python debug build:"", hasattr(sys, ""gettotalrefcount""))\'\n', 'python3-dbg -m pip install build\npython3-dbg -m build -Csetup-args=-Dbuildtype=debugoptimized -Csetup-args=-Dblas=blas-atlas -Csetup-args=-Dlapack=lapack-atlas -Ccompile-args=-j2\npython3-dbg -m pip install dist/scipy*.whl\n', 'cd doc\npython3-dbg -m pip install pytest pytest-xdist pytest-timeout mpmath gmpy2 threadpoolctl pooch\npython3-dbg -m pytest --pyargs scipy -n2 --durations=10 -m ""not slow""\n', 'sudo apt-get -y update\nsudo apt install -y g++-8 gcc-8 gfortran-8\nsudo apt install -y libatlas-base-dev liblapack-dev libgmp-dev \\\n  libmpfr-dev libmpc-dev pkg-config libsuitesparse-dev liblapack-dev\n', 'pip install ""numpy==1.21.6"" &&\n# build deps\npip install build meson-python ninja pythran pybind11 cython wheel\n# test deps\npip install gmpy2 threadpoolctl mpmath pooch pythran pybind11 pytest pytest-xdist==2.5.0 pytest-timeout\n', 'set -euo pipefail\nexport PYTHONOPTIMIZE=2\n\n# specify which compilers to use using environment variables\nCC=gcc-8 CXX=g++-8 FC=gfortran-8 python -m build --wheel --no-isolation -Csetup-args=-Dblas=blas-atlas -Csetup-args=-Dlapack=lapack-atlas -Ccompile-args=-j2\npython -m pip install dist/scipy*.whl\n', ""# can't be in source directory\npushd $RUNNER_TEMP\nexport PYTHONOPTIMIZE=2\npython -m pytest --pyargs scipy -n2 --durations=10\npopd\n"", 'sudo apt-get update\nsudo apt-get install -y libgmp-dev libmpfr-dev libmpc-dev ccache gfortran\n\n# Install OpenBLAS a la cibuildwheel.\nchmod +x tools/wheels/cibw_before_build_linux.sh\nsudo tools/wheels/cibw_before_build_linux.sh .        \n', 'python -m pip install ""Cython>=3.0.0b3""\npython -m pip install ninja meson meson-python wheel click rich_click pydevtool\npython -m pip install --pre --upgrade --timeout=60 -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy\n# Use pytest-xdist<4.0 due to https://github.com/pytest-dev/pytest-cov/issues/557\n# can update once pytest-cov>4.0 and remove warning filtering in pytest.ini\npython -m pip install --pre --upgrade pytest pytest-cov ""pytest-xdist<4.0"" pybind11 mpmath gmpy2 pythran threadpoolctl pooch matplotlib\n', 'mkdir -p ""${CCACHE_DIR}""\necho ""dir=$CCACHE_DIR"" >> $GITHUB_OUTPUT\nNOW=$(date -u +""%F-%T"")\necho ""timestamp=${NOW}"" >> $GITHUB_OUTPUT\n', 'python dev.py build --gcov\n', 'ccache -s', 'export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n\nexport OPENBLAS_NUM_THREADS=1\npython dev.py test -j2 --mode full -- --cov --cov-report term-missing \n', 'set -euo pipefail\ndocker pull quay.io/pypa/manylinux2014_i686\ndocker run -v $(pwd):/scipy --platform=linux/i386 quay.io/pypa/manylinux2014_i686 /bin/bash -c ""cd /scipy && \\\nuname -a && \\\nbasedir=\\$(python3.9 tools/openblas_support.py) && \\\ncp -r \\$basedir/lib/* /usr/local/lib && \\\ncp \\$basedir/include/* /usr/local/include && \\\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig && \\\npython3.9 -m venv test && \\\nsource test/bin/activate && \\\npython -m pip install doit click rich_click pydevtool meson ninja && \\\npython -m pip install numpy==1.21.6 cython==0.29.35 pybind11 pytest pytest-timeout pytest-xdist pytest-env Pillow mpmath pythran pooch meson && \\\npython dev.py build && \\\nLD_LIBRARY_PATH=/usr/local/lib python dev.py test""\n', 'brew install ccache\n', 'mkdir -p ""${CCACHE_DIR}""\necho ""dir=$CCACHE_DIR"" >> $GITHUB_OUTPUT\nNOW=$(date -u +""%F-%T"")\necho ""timestamp=${NOW}"" >> $GITHUB_OUTPUT\n', 'echo ""today=$(/bin/date -u \'+%Y%m%d\')"" >> $GITHUB_OUTPUT', 'mamba env update -n scipy-dev -f environment.yml', 'conda activate scipy-dev\n\n# optional test dependencies\nmamba install scikit-umfpack scikit-sparse\n\n# Python.org installers still use 10.9, so let\'s use that too. Note\n# that scikit-learn already changed to 10.13 in Jan 2021, so increasing\n# this number in the future (if needed) should not be a problem.\n# Conda-forge is still at 10.9, see:\n# https://conda-forge.org/docs/maintainer/knowledge_base.html#requiring-newer-macos-sdks\nexport MACOSX_DEPLOYMENT_TARGET=10.9\nexport MACOSX_SDK_VERSION=10.9\nCC=""ccache $CC"" python dev.py build\n', 'conda activate scipy-dev\nexport OMP_NUM_THREADS=2\nexport SCIPY_USE_PROPACK=1\npython dev.py -n test -j2\n', 'ccache -s\n', 'set -xe\nCOMMIT_MSG=$(git log --no-merges -1)\nRUN=""0""\nif [[ ""$COMMIT_MSG"" == *""[wheel build]""* ]]; then\n    RUN=""1"" \nfi\necho ""message=$RUN"" >> $GITHUB_OUTPUT\necho github.ref ${{ github.ref }}\n', '# mingw-w64\nchoco install rtools -y --no-progress --force --version=4.0.0.20220206\necho ""c:\\rtools40\\ucrt64\\bin;"" >> $env:GITHUB_PATH\n', 'conda install -y anaconda-client\nsource tools/wheels/upload_wheels.sh\nset_upload_vars\n# For cron jobs (restricted to main branch) or ""Run workflow"" trigger\n# an upload to:\n#\n# https://anaconda.org/scientific-python-nightly-wheels/scipy\n# \n# Pushes to a maintenance branch that contain \'[wheel build]\' will\n# cause wheels to be built and uploaded to:\n#\n# https://anaconda.org/multibuild-wheels-staging/scipy\n#\n# The tokens were originally generated at anaconda.org\nupload_wheels\n', 'choco install rtools -y --no-progress --force --version=4.0.0.20220206\necho ""c:\\rtools40\\ucrt64\\bin;"" >> $env:GITHUB_PATH\n', 'gcc --version\ngfortran --version\n', 'pip install numpy==1.22.2 cython pybind11 pythran meson ninja pytest pytest-xdist pytest-timeout pooch rich_click click doit pydevtool\n', '# same OpenBLAS install method as cibuildwheel\nset -xe\nbash tools/wheels/cibw_before_build_win.sh .\necho ""PKG_CONFIG_PATH=c:\\opt\\64\\lib\\pkgconfig;"" >> $GITHUB_ENV\n', 'echo ""SCIPY_USE_PROPACK=1"" >> $env:GITHUB_ENV\npython dev.py build --win-cp-openblas\n# Necessary because GitHub Actions checks out the repo to D:\\ while OpenBLAS\n# got installed to C:\\ higher up. The copying with `--win-cp-openblas` fails\n# when things are split over drives.\ncp C:\\opt\\64\\bin\\*.dll $pwd\\build-install\\Lib\\site-packages\\scipy\\.libs\\\npython tools\\openblas_support.py --write-init $PWD\\build-install\\Lib\\site-packages\\scipy\\\n', 'python dev.py test -j2\n', 'choco install rtools -y --no-progress --force --version=4.0.0.20220206\necho ""c:\\rtools40\\ucrt64\\bin;"" >> $env:GITHUB_PATH\n', 'set -xe\nbash tools/wheels/cibw_before_build_win.sh .\necho ""PKG_CONFIG_PATH=c:\\opt\\64\\lib\\pkgconfig;"" >> $GITHUB_ENV\n', '# 1.22.3 is currently the oldest numpy usable on cp3.9 according\n# to pyproject.toml\npython -m pip install numpy==1.22.3 cython pybind11 pythran meson-python meson ninja pytest pytest-xdist pytest-timeout pooch rich_click click doit pydevtool\n', 'python dev.py build --win-cp-openblas\n# Copy OpenBLAS DLL, write distributor-init (see first job in this file for why)\ncp C:\\opt\\64\\bin\\*.dll $pwd\\build-install\\Lib\\site-packages\\scipy\\.libs\\\npython tools\\openblas_support.py --write-init $PWD\\build-install\\Lib\\site-packages\\scipy\\\n', 'python dev.py test -j2 --mode full\n', '# mingw-w64\nchoco install rtools -y --no-progress --force --version=4.0.0.20220206\necho ""c:\\rtools40\\ucrt64\\bin;"" >> $env:GITHUB_PATH\n', 'set -xe\nbash tools/wheels/cibw_before_build_win.sh .\necho ""PKG_CONFIG_PATH=c:\\opt\\64\\lib\\pkgconfig;"" >> $GITHUB_ENV\n', 'python -m pip install build delvewheel numpy cython pybind11 meson-python meson ninja pytest pytest-xdist pytest-timeout pooch\n', 'python -m build -Csetup-args=""-Duse-pythran=false""\n\n# Vendor openblas.dll and the DLL\'s it depends on into the wheel \n$env:wheel_name=Get-ChildItem -Path dist/* -Include *.whl\ndelvewheel repair --add-path c:\\opt\\openblas\\openblas_dll -w dist $env:wheel_name\n\npython -m pip install $env:wheel_name\n', 'cd $RUNNER_TEMP\n# run full test suite\npytest --pyargs scipy\n', '# mingw-w64\nchoco install rtools -y --no-progress --force --version=4.0.0.20220206\necho ""c:\\rtools40\\ucrt64\\bin;"" >> $env:GITHUB_PATH\n', ""set -xe\nbash tools/wheels/cibw_before_build_win.sh .\n\n# force static linking\npushd /c/opt/64/lib\nrm *.dev.a *.dll.a *.lib\npopd\n\n# site.cfg.template currently distributed with\n# multibuild-wheels-staging/openblas-libs\ncp /c/opt/64/site.cfg.template site.cfg\nsed -i  's/{openblas_root}/c:\\\\opt/g' site.cfg\n"", '# pyproject.toml currently states this numpy minimum version.\npython -m pip install --upgrade pip ""setuptools==59.6.0"" wheel\npython -m pip install cython numpy==1.22.3 pybind11 pythran pytest pooch\n', ""python setup.py bdist_wheel\n\n# Vendor openblas.dll and the DLL's it depends on into the wheel \n$env:wheel_name=Get-ChildItem -Path dist/* -Include *.whl\ndelvewheel repair --add-path c:\\opt\\openblas\\openblas_dll -w dist $env:wheel_name\n\npython -m pip install $env:wheel_name\n"", 'cd $RUNNER_TEMP\npytest --pyargs scipy -m ""not slow""\n']"
"['docker build -t test_image -f TestDockerfile --build-arg PYTHON_VERSION=${{ matrix.image-tag }} .', 'docker run -dt --name thumbor_test -v $(pwd):/app test_image\n', 'docker exec thumbor_test make redis', 'docker exec thumbor_test make setup', 'docker exec thumbor_test make compile_ext', 'docker exec thumbor_test make sequential-unit', 'docker exec -e ASYNC_TEST_TIMEOUT=30 thumbor_test make integration_run', 'docker exec thumbor_test make pylint', 'docker exec thumbor_test coverage lcov', 'make build\n', 'pip install flake8', 'pip install isort', 'isort thumbor tests --profile black', 'pip install -U pip\n', 'python setup.py sdist', 'ls -lah dist', 'echo ""THUMBOR_VERSION=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', 'echo ""THUMBOR_VERSION=$(git describe --tags --abbrev=0)"" >> $GITHUB_ENV', 'echo ""CACHE_TO=type=registry,ref=ghcr.io/thumbor/thumbor:buildcache-${{ matrix.python_version }},mode=max"" >> $GITHUB_ENV']"
"['pip install nox', 'nox -s docs', 'git config --global user.email ""pypa-dev@googlegroups.com""\ngit config --global user.name ""pip""\n', 'pip install nox', 'nox -s prepare-release -- 99.9', 'nox -s build-release -- 99.9', 'pipx run check-manifest', 'pip install nox', 'nox -s vendoring', 'git diff --exit-code', 'sudo apt-get install bzr', 'brew install breezy', 'pip install nox', 'nox -s test-${{ matrix.python }} -- -m unit --verbose --numprocesses auto --showlocals', 'nox -s test-${{ matrix.python }} -- -m integration --verbose --numprocesses auto --showlocals --durations=5', './tools/ci/New-RAMDisk.ps1 -Drive R -Size 1GB', 'mkdir R:\\Temp\n$acl = Get-Acl ""R:\\Temp""\n$rule = New-Object System.Security.AccessControl.FileSystemAccessRule(\n    ""Everyone"", ""FullControl"", ""ContainerInherit,ObjectInherit"", ""None"", ""Allow""\n)\n$acl.AddAccessRule($rule)\nSet-Acl ""R:\\Temp"" $acl\n', 'pip install nox', 'nox -s test-${{ matrix.python }} -- -m unit --verbose --numprocesses auto --showlocals', 'nox -s test-${{ matrix.python }} -- -m integration -k ""not test_install"" --verbose --numprocesses auto --showlocals', 'nox -s test-${{ matrix.python }} -- -m integration -k ""test_install"" --verbose --numprocesses auto --showlocals', 'sudo apt-get install bzr', ""pip install nox 'virtualenv<20' 'setuptools != 60.6.0'"", 'nox -s test-3.10 -- -m integration --verbose --numprocesses auto --showlocals --durations=5 --use-zipapp', 'if ! pipx run towncrier check --compare-with origin/${{ github.base_ref }}; then\n  echo ""Please see https://pip.pypa.io/dev/news-entry-failure for guidance.""\n  false\nfi\n', 'pip install httpx pyyaml rich', 'python tools/update-rtd-redirects.py']"
"['set -x\nfor i in 1 2 3; do\n  echo ""try $i"" && \\\n  ${{ runner.os == \'Linux\' && \'sudo apt-get update -y && sudo apt-get install snapd fish csh -y\' || true }} && \\\n  ${{ runner.os == \'Linux\' && \'cargo binstall -y nu\' || true }} && \\\n  ${{ runner.os == \'macOS\' && \'brew install fish tcsh nushell\' || true }} && \\\n  ${{ runner.os == \'Windows\' && \'choco install nushell\' || true }} && \\\n  exit 0 || true;\n  sleep 1\ndone\nexit 1\n', 'python -m pip install tox pip -U', 'python -m pip install .', 'set -e\nPY=$(echo \'${{ matrix.py }}\' | cut -c 6-)\nbrew upgrade python@$PY || brew install python@$PY\necho ""/usr/local/opt/python@$PY/libexec/bin"" >>""${GITHUB_PATH}""\n', 'import os; import platform; import sys; from pathlib import Path\nenv = f\'TOXENV=py{"""" if platform.python_implementation() == ""CPython"" else ""py""}3{sys.version_info.minor}\'\nprint(f""Picked: {env} for {sys.version} based of {sys.executable}"")\nwith Path(os.environ[""GITHUB_ENV""]).open(""ta"") as file_handler:\n    file_handler.write(env)\n', 'tox -vv --notest', 'tox --skip-pkg-install', 'python -m pip install tox', 'python -m tox -e ${{ matrix.tox_env }}', 'python -m pip install build', 'pyproject-build -s -w . -o dist']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'tox', 'python -m pip install -U pip\npython -m pip install build twine\n', 'python -m build\ntwine check dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\npython -m pip install  -e .\n', 'tox -e py\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements/tox.txt\n', 'tox -v\n']"
"['git config --global user.email ${{ secrets.git_email }}\ngit config --global user.name ${{ secrets.git_user }}\n', 'pip --cache-dir pip_cache install -r dev-requirements.txt\n', 'set -x\npython update-changelog.py wiki/ChangeLog.md\ncd wiki\ngit add ChangeLog.md\nif ! git diff --cached --exit-code; then\n    git commit -m ""Automated ChangeLog update""\n    git push origin main\nfi\n', 'sudo apt-get update\nsudo apt-get install -qy unrar\n', 'poetry install --no-interaction --no-root', 'poetry install --no-interaction', 'poetry run pytest -n auto\n', 'echo ""::error::No PRs should be made against master""\nexit 1\n', 'git config user.email ${{ secrets.git_email }}\ngit config user.name ${{ secrets.git_user }}\n', 'pip install -r requirements-release.txt\n', './release.sh\n', ""echo 'CHANGELOG_BODY<<EOF' >> $GITHUB_ENV\npython dev_tools.py get-changelog ${{ env.release_tag }} >> $GITHUB_ENV\necho 'EOF' >> $GITHUB_ENV\n""]"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade tox\n', 'ENV_PREFIX=$(tr -C -d ""0-9"" <<< ""${{ matrix.python-version }}"")\nTOXENV=$(tox --listenvs | grep ""^py$ENV_PREFIX"" | tr \'\\n\' \',\') tox\n', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine\npython -m coverage html --skip-covered --skip-empty\npython -m coverage report --fail-under=55\n']"
"['pip install tox coverage', 'tox -e ${{ matrix.versions.env }}']"
"['docker compose -f docker-compose.test.yml run sut', 'echo ${{ steps.docker_build.outputs.digest }}', ""sed -i -e 's/>=/==/g; s/~=.*==\\(.*\\)/==\\1/g; s/~=/==/g;' setup.cfg"", 'python -m pip install --upgrade pip\npython -m pip install tox\n', 'tox -e py', 'tox -e py', 'tox -e py', 'tox -e lint_docs']"
""
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'pre-commit run --all-files --show-diff-on-failure', 'pytest -v --cov', 'pip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'make', './svtplay-dl --version', 'python scripts/cibuild.py', 'pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'python setversion.py', 'python setup.py build_exe', 'build\\\\exe.${{ matrix.arch-cx }}-${{ matrix.python }}\\\\svtplay-dl.exe --version', 'mkdir svtplay-dl\nxcopy /s build\\\\exe.${{ matrix.arch-cx }}-${{ matrix.python }} svtplay-dl\n', '7z a -tzip svtplay-dl-${{ matrix.cx_name }}.zip svtplay-dl', 'python scripts/cibuild.py', 'pip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'python setup.py sdist bdist_wheel', 'python scripts/cibuild.py']"
"['pip install twine', 'python setup.py sdist', 'pip install tox', 'rm -rf ./docs/_build\ntox -e docs\n', 'SPLUNK_VERSION=${{matrix.splunk-version}} docker-compose up -d', 'pip install tox', 'tox -e py']"
"['pip install .[test] coveralls', 'pytest --cov csvkit', 'coveralls --service=github', 'pip install --upgrade check-manifest flake8 isort setuptools', 'check-manifest', 'flake8 .', 'isort . --check-only']"
"['PYTHON=`which python` util/install.sh -n\npython -m pip install pylint==2.15.7\n', 'make codecheck', 'sudo apt install ${{ matrix.py }}', 'sudo apt-get update -qq\n# This seems too slow unfortunately:\n# sudo apt-get upgrade -y -qq\nPYTHON=${{ matrix.py }} util/install.sh -nv\n', 'sudo systemctl stop systemd-udevd systemd-udevd-kernel.socket systemd-udevd-control.socket || echo ""couldn\'t disable udevd""', 'export sudo=""sudo env PATH=$PATH""\nexport PYTHON=${{ matrix.py }}\n# Newer OvS tries OpenFlow15 which crashes ovsc on ubuntu-20.04\n$sudo mn --switch ovs,protocols=OpenFlow13 --test pingall\n', 'sudo apt-get install -qq vlan\nexport PYTHON=${{ matrix.py }}\nsudo $PYTHON -m pip install pexpect\nutil/install.sh -fw\n', 'export sudo=""sudo env PATH=$PATH""\nexport PYTHON=${{ matrix.py }}\n$sudo $PYTHON mininet/test/runner.py -v\n', 'export sudo=""sudo env PATH=$PATH""\nexport PYTHON=${{ matrix.py }}\n$sudo $PYTHON examples/test/runner.py -v\n']"
"['mkdir ~/.ssh && echo -e ""Host gitlab.tiker.net\\n\\tStrictHostKeyChecking no\\n"" >> ~/.ssh/config\neval $(ssh-agent) && echo ""$GITLAB_AUTOPUSH_KEY"" | ssh-add -\ngit fetch --unshallow\ngit push ""git@gitlab.tiker.net:inducer/$(basename $GITHUB_REPOSITORY).git""  main\n', 'curl -L -O -k https://gitlab.tiker.net/inducer/ci-support/raw/main/prepare-and-run-flake8.sh\n. ./prepare-and-run-flake8.sh ""$(basename $GITHUB_REPOSITORY)"" test/*.py\n']"
"['sudo apt-get update -q\nsudo apt-get install intltool desktop-file-utils\npip3 install pytest-cov minimock pycodestyle isort requests pytest pytest-httpserver\npip3 install podcastparser mygpoclient\n', 'make lint', 'make releasetest']"
"['mkdir ~/.ssh && echo -e ""Host gitlab.tiker.net\\n\\tStrictHostKeyChecking no\\n"" >> ~/.ssh/config\neval $(ssh-agent) && echo ""$GITLAB_AUTOPUSH_KEY"" | ssh-add -\ngit fetch --unshallow\ngit push ""git@gitlab.tiker.net:inducer/$(basename $GITHUB_REPOSITORY).git""  main\n', 'curl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\nbuild_py_project_in_conda_env\ninstall_and_run_flake8 ""$(get_proj_name)"" examples/*.py test/*.py\n', 'CONDA_ENVIRONMENT=.test-conda-env-py3.yml\necho ""- matplotlib"" >> $CONDA_ENVIRONMENT\necho ""- pyopengl"" >> $CONDA_ENVIRONMENT\necho ""- ipython"" >> $CONDA_ENVIRONMENT\n\ncurl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\nbuild_py_project_in_conda_env\nrun_pylint ""$(get_proj_name)"" test/*.py\n', 'curl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n./configure.py --cl-use-shipped-ext\nbuild_py_project_in_conda_env\npython -m pip install mypy types-setuptools\npython -m mypy --show-error-codes pyopencl test\n', 'curl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n./configure.py --cl-use-shipped-ext\nbuild_py_project_in_conda_env\ntest_py_project\n', 'export CONDA_ENVIRONMENT=.test-conda-env-py3.yml\n.ci/hack-intel-cl-into-conda-env.sh\n\ncurl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n./configure.py --cl-use-shipped-ext\nbuild_py_project_in_conda_env\ntest_py_project\n', 'set -x\nexport CONDA_ENVIRONMENT=.test-conda-env-py3.yml\n\nsed -i \'s/- ocl-icd/- khronos-opencl-icd-loader/g\' ""$CONDA_ENVIRONMENT""\nsed -i \'/- git/d\' ""$CONDA_ENVIRONMENT""\n\n.ci/hack-intel-cl-into-conda-env.sh\n\ncurl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n./configure.py --cl-use-shipped-ext\nbuild_py_project_in_conda_env\ntest_py_project\n', 'export CC=gcc\nCONDA_ENVIRONMENT=.test-conda-env.yml\ngrep -v ocl-icd .test-conda-env-py3.yml > $CONDA_ENVIRONMENT\n\ncurl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n./configure.py --cxxflags= --ldflags= --cl-libname=OpenCL\nbuild_py_project_in_conda_env\ntest_py_project\n', 'CONDA_ENVIRONMENT=.test-conda-env-py3.yml\n\ncurl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n./configure.py --cl-use-shipped-ext\nbuild_py_project_in_conda_env\nbuild_docs\n', 'EXTRA_INSTALL=""pillow cgen mako imageio""\n\ncurl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\nbuild_py_project_in_conda_env\n(cd examples; rm -f gl_*)\nrun_examples --no-require-main\n', 'curl -L -O https://tiker.net/ci-support-v0\n. ci-support-v0\n\nTEST_ENV_ROOT=""$(pwd)/$DOWNSTREAM_PROJECT/.miniforge3/envs/testing""\n./configure.py --cl-inc-dir=""$TEST_ENV_ROOT/include"" --cl-lib-dir=""$TEST_ENV_ROOT/lib""\ngit add -f siteconf.py\n\nprepare_downstream_build ""https://github.com/inducer/$DOWNSTREAM_PROJECT.git""\nsed -i \'s/pyopencl/ocl-icd/\' .test-conda-env-py3.yml\nbuild_py_project_in_conda_env\ntest_py_project\n', 'pipx run build --sdist']"
"['pip install --upgrade pip setuptools wheel\npip install numpy==1.23.1\npip install pyinstaller==4.10\ngit clone https://github.com/CellProfiler/distribution.git\n', ""sed -i '' 's/4.0.0/4.2.4/' Info.plist\nbrew install mysql\nmake\nchmod +x add-osx-certificate.sh && ./add-osx-certificate.sh\nchmod +x osx-codesign.sh && ./osx-codesign.sh\nditto -ck --keepParent --rsrc --sequesterRsrc ./dist/CellProfiler.app ./dist/CellProfiler-macOS-4.2.4.zip\n"", 'python -m pip install --upgrade pip setuptools wheel\npip install cython\npip install mysqlclient==1.4.6\npip install --editable . --no-use-pep517\n', 'pyinstaller distribution/windows/cellprofiler.spec\niscc /dMyAppVersion=""4.2.4"" ""distribution/windows/cellprofiler.iss""\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install pyinstaller\npip install --upgrade pip setuptools wheel\npip install attrdict\npip install numpy>=1.20.1\ngit clone https://github.com/CellProfiler/core.git ~/core\n', 'brew install mysql\nmysql.server start\nmysql --host=$CP_MYSQL_TEST_HOST --user=$CP_MYSQL_TEST_USER --execute=""CREATE DATABASE $CP_MYSQL_TEST_DB;"" --skip-password\n', 'pip install -e ~/core\npip install -e .[test]\n', 'pytest\n']"
"['python3 -m pip install --upgrade pip\n', 'python3 -m pip install --upgrade pip\npip3 install -r requirements.txt\n', 'LIMNORIA_WARN_OLD_PYTHON=0 python3 setup.py install\n', 'supybot-test test -v --plugins-dir=./plugins/ --no-network\n', 'git clone https://github.com/ProgVal/irctest.git\ncd irctest\npip3 install -r requirements.txt\nmake limnoria PYTEST_ARGS=-vs\n']"
"['pip install flake8', 'flake8 statsd', 'python -m pip install --upgrade pip\npip install build twine\n', './run.sh build', './run.sh check']"
""
"['python -m pip install tox', 'tox', 'python -m pip install tox', 'tox -e linkcheck', 'python -m pip install tox-gh-actions', 'tox']"
"['mkdir ~/.ssh && echo -e ""Host gitlab.tiker.net\\n\\tStrictHostKeyChecking no\\n"" >> ~/.ssh/config\neval $(ssh-agent) && echo ""$GITLAB_AUTOPUSH_KEY"" | ssh-add -\ngit fetch --unshallow\ngit push ""git@gitlab.tiker.net:inducer/$(basename $GITHUB_REPOSITORY).git""  main\n', 'curl -L -O -k https://gitlab.tiker.net/inducer/ci-support/raw/main/prepare-and-run-flake8.sh\n. ./prepare-and-run-flake8.sh ./pudb ./test\n', 'curl -L -O -k https://gitlab.tiker.net/inducer/ci-support/raw/master/prepare-and-run-pylint.sh\n. ./prepare-and-run-pylint.sh ""$(basename $GITHUB_REPOSITORY)"" test/test_*.py\n', 'EXTRA_INSTALL=""numpy""\nREQUIREMENTS_TXT=requirements.dev.txt\ncurl -L -O -k https://gitlab.tiker.net/inducer/ci-support/raw/main/build-and-test-py-project.sh\n. ./build-and-test-py-project.sh\n', 'EXTRA_INSTALL=""numpy""\nREQUIREMENTS_TXT=requirements.dev.txt\nPYTEST_FLAGS=""--cov=pudb""\ncurl -L -O -k https://gitlab.tiker.net/inducer/ci-support/raw/main/build-and-test-py-project.sh\n. ./build-and-test-py-project.sh\n', 'EXTRA_INSTALL=""numpy""\ncurl -L -O -k https://gitlab.tiker.net/inducer/ci-support/raw/main/ci-support.sh\n. ci-support.sh\nbuild_py_project_in_venv\nbuild_docs\n']"
"['python -m pip install --upgrade pip pyinstaller setuptools wheel cython cffi -r requirements.txt\n', 'python setup.py build_ext --inplace\n', 'pyi-makespec --hidden-import=""pkg_resources.py2_warn"" -F --add-data images/\\*:images --add-data \\*.png:. --add-data \\*.ico:. -w -i P-face.icns pronterface.py\n# Edit spec file\nexport git_hash=$(git rev-parse --short ""$GITHUB_SHA"")\nsed -i \'\' \'$ s/.$//\' pronterface.spec\ncat >> pronterface.spec <<EOL\ninfo_plist={\n    \'CFBundleShortVersionString\': \'$git_hash\',\n    \'NSPrincipalClass\': \'NSApplication\',\n    \'NSAppleScriptEnabled\': False,\n    \'NSAppSleepDisabled\': True,\n  },\n)\nEOL\n', 'pyinstaller --clean pronterface.spec -y\n# Zip app manually to avoid losing execute permissions for binary file in app\ncd dist\nzip -r -X pronterface-app.zip pronterface.app\n', 'echo ""EXE_NAME=${{ github.ref_name }}"" >> $GITHUB_ENV\n', 'echo ""EXE_NAME=printrun-nightly"" >> $GITHUB_ENV\n', 'echo ""EXE_NAME=printrun-test"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip pyinstaller setuptools wheel simplejson polygon3 cython cffi -r requirements.txt\n', '# python -m pip install -U --pre -f https://wxpython.org/Phoenix/snapshot-builds/ wxPython\npython -m pip install -U wxPython\n', 'python setup.py build_ext --inplace\n', 'pyi-makespec --hidden-import=""pkg_resources.py2_warn"" -F --add-binary ""PrintrunGTK3/GTK3Windows10-64/*.dll;."" --add-data ""VERSION;cairocffi"" --add-data ""VERSION;cairosvg"" --add-data ""images/*;images"" --add-data ""*.png;."" --add-data ""*.ico;."" -w -i pronterface.ico pronterface.py\n', 'pyi-makespec --hidden-import=""pkg_resources.py2_warn"" -F --add-binary ""PrintrunGTK3/GTK3Windows10-32/*.dll;."" --add-data ""VERSION;cairocffi"" --add-data ""VERSION;cairosvg"" --add-data ""images/*;images"" --add-data ""*.png;."" --add-data ""*.ico;."" -w -i pronterface.ico pronterface.py\n', 'pyi-makespec --hidden-import=""pkg_resources.py2_warn"" -F --add-data ""VERSION;cairocffi"" --add-data ""VERSION;cairosvg"" --add-data ""images/*;images"" --add-data ""*.png;."" --add-data ""*.ico;."" -w -i pronterface.ico pronterface.py\n', 'pyinstaller --clean pronterface.spec -y\n', '""EXE_NAME=${{ github.ref_name }}"" >> $env:GITHUB_ENV\n', '""EXE_NAME=printrun-nightly"" >> $env:GITHUB_ENV\n', '""EXE_NAME=printrun-test"" >> $env:GITHUB_ENV\n', 'python -m pip install --upgrade pip setuptools wheel twine cython\n', 'python setup.py bdist_wheel\n', 'twine upload dist/*\n', 'python -m pip install --upgrade pip setuptools wheel twine\n', 'twine upload dist/*-manylinux*.whl\n', 'python -m pip install --upgrade pip setuptools wheel twine\n', 'python setup.py sdist\n', 'twine upload dist/*\n', 'python -m pip install --upgrade pip setuptools wheel twine cython\n', 'python setup.py bdist_wheel\n', 'twine upload dist/*\n']"
"['sudo apt-get update\nsudo apt-get install --yes ghostscript imagemagick\nsudo sed -i \'s#<policy domain=""coder"" rights=""none"" pattern=""PDF"" />#<policy domain=""coder"" rights=""read|write"" pattern=""PDF"" />#\' /etc/ImageMagick-6/policy.xml\npython -m pip install --upgrade pip\npython -m pip install tox\n', 'pip install -r requirements.txt', 'make test-ref', 'tox']"
"['pip install pre-commit', 'pre-commit run --all-files', 'pip install tox', 'tox -e docs', 'pip install tox coverage', 'tox -e ${{ matrix.tox_env }}', 'coverage xml -i', 'python -m pip install tox --user', 'python -m tox -e build']"
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox', 'python -m pip install --upgrade pip\npython -m pip install tox\n', 'tox -e ${{ matrix.tox-arg }}']"
"['pip install flake8', './run.sh flake8\n', 'python -m pip install --upgrade pip\npip install build twine\n', './run.sh build', './run.sh check']"
""
"['python -m pip install --upgrade pip\npip install coverage coveralls\npip install -r requirements.txt\npip install -r test-requirements.txt\n', 'pytest test --verbosity=3 --cov=ncclient\n', 'coveralls --service=github\n', 'pip3 install --upgrade coveralls\ncoveralls --service=github --finish\n']"
"['make dependencies', 'make runtests']"
""
"['echo $EVENT_TYPE', 'python -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip install -r requirements-doc.txt\npython -m pip list\n', 'python -m pip install . -vv', 'pushd docs\nmake html\npopd\n', 'source tools/ci/docbuild-commit.sh\n', 'source tools/ci/docbuild-push.sh\n']"
""
""
"[""echo '${{ steps.file_changes.outputs.files}}'"", 'pip wheel --no-deps -w dist python/core\n', 'pip install --upgrade twine\ntwine upload --skip-existing ltp_core-*\n', 'pip3 install ltp_extension --no-index --find-links dist --force-reinstall\npython3 -c ""import ltp_extension""\n', 'pip3 install ltp_extension --no-index --find-links dist --force-reinstall\npython3 -c ""import ltp_extension""\n', 'pip3 install ltp_extension --no-index --find-links dist --force-reinstall\npython3 -c ""import ltp_extension""\n', 'pip3 install ltp_extension --no-index --find-links dist --force-reinstall\npython3 -c ""import ltp_extension""\n', 'pip3 install --upgrade twine\ntwine upload --skip-existing ltp_extension-*\n', 'pip wheel --no-deps -w dist python/interface\n', 'pip install --upgrade twine\ntwine upload --skip-existing ltp-*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install protobuf==3.20.0\n', 'python -m pip list\n', 'pytest -v python/core\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pytest\npip install pytest-cov[toml]\npip install protobuf==3.20.0\n', 'pytest --cov python/core/ltp']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install .\n', 'mkdir -p ~/cache\ntest ! -e ~/cache/minio && wget -O ~/cache/minio https://dl.minio.io/server/minio/release/linux-amd64/minio || echo ""Minio already in cache""\n', 'export MINIO_ROOT_USER=Q3AM3UQ867SPQQA43P2F\nexport MINIO_ROOT_PASSWORD=zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\nexport export MINIO_BROWSER=off\nchmod +x ~/cache/minio\nmkdir -p ~/minio_tmp\n~/cache/minio server ~/minio_tmp &\nsleep 4 # give minio some time to start\n', 'python ./run-tests.py -c .ci.s3cfg -p baseauto -s minio', 'killall minio']"
""
"['pip install flake8', 'flake8', 'pip install tox tox-gh-actions', ""tox ${{ matrix.experimental && '-e py310-djmain' || '' }}""]"
[]
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'npm install', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n', 'pip install --user ruff', 'ruff --extend-select=C4,C9,I,PLC,PLE,PLR,U --format=github --ignore=C414,I001,UP032 --target-version=py37 .']"
"['make lint', 'make fmtcheck', 'make venv', 'set -x\nsource venv/bin/activate\npython setup.py clean --all sdist bdist_wheel --universal\npython -m twine check dist/*\n', 'pip install --upgrade pip virtualenv', 'make ci-test', 'make coveralls', 'set -ex\necho $GPG_SIGNING_PRIVKEY | base64 --decode | gpg --import --batch --yes --pinentry-mode loopback --passphrase ""$GPG_SIGNING_PASSPHRASE""\n', 'make venv', 'set -ex\nsource venv/bin/activate\nexport VERSION=$(cat VERSION)\ngpg --detach-sign --local-user $GPG_SIGNING_KEYID  --pinentry-mode loopback --passphrase $GPG_SIGNING_PASSPHRASE -a dist/stripe-$VERSION.tar.gz\ngpg --detach-sign --local-user $GPG_SIGNING_KEYID  --pinentry-mode loopback --passphrase $GPG_SIGNING_PASSPHRASE -a dist/stripe-$VERSION-py2.py3-none-any.whl\n\npython -m twine upload --verbose dist/stripe-$VERSION.tar.gz  dist/stripe-$VERSION-py2.py3-none-any.whl dist/stripe-$VERSION.tar.gz.asc dist/stripe-$VERSION-py2.py3-none-any.whl.asc\n']"
"['sudo apt update\nsudo apt install libblosc-dev libbz2-dev libhdf5-dev liblz4-dev liblzo2-dev libsnappy-dev libzstd-dev zlib1g-dev\n', 'pip install build\npython -m build --sdist\n', 'conda install -q setuptools pip wheel build packaging numpy cython bzip2 hdf5 lzo\npython -m pip install ""blosc2!=2.2.1,!=2.2.2""\npython -m pip install -r requirements.txt\n# conda install sphinx sphinx_rtd_theme numpydoc ipython\n', 'python -m pip install -v tables-*.tar.gz\n', 'cd .. && python -m tables.tests.test_all -v\npt2to3 -h\nptrepack -h\nptdump -h\npttree -h\n', ""conda install setuptools pip wheel build packaging py-cpuinfo numpy cython numexpr bzip2 hdf5 lzo 'typing_extensions<4.2' c-blosc2\n"", 'python -m pip install -v tables-*.tar.gz --no-deps\n', ""! conda list | grep -E '^(python-)?blosc2'\n"", 'cd .. && python -m tables.tests.test_all -v\npt2to3 -h\nptrepack -h\nptdump -h\npttree -h\n', 'sudo apt-get update\nsudo apt install libblosc-dev libbz2-dev libhdf5-dev liblz4-dev liblzo2-dev libsnappy-dev libzstd-dev zlib1g-dev\nsudo apt install python3-all-dev python3-setuptools python3-packaging python3-numexpr  # python3-numpy cython3\nsudo apt install python3-pytest python3-pytest-doctestplus\nsudo apt install python3-numpydoc python3-sphinx python3-sphinx-rtd-theme python3-ipython\nsudo apt install latexmk texlive-fonts-recommended texlive-latex-recommended texlive-latex-extra texlive-plain-generic\n', 'python3 -m pip install --user --upgrade setuptools pip wheel build ""blosc2!=2.2.1,!=2.2.2""\npython3 -m pip install --user --upgrade -r requirements.txt\npython3 -c ""import numpy as np; print(\'numpy:\', np.__version__)""\npython3 -c ""import blosc2; print(\'blosc2:\', blosc2.__version__)""\n', 'make build', 'make html', 'make latex', 'make dist', 'make check', 'sudo apt update\nsudo apt install libbz2-dev libhdf5-serial-dev liblzo2-dev\nsudo apt install latexmk texlive-fonts-recommended texlive-latex-recommended texlive-latex-extra texlive-plain-generic\n', 'python -m pip install --upgrade setuptools pip wheel build\npython -m pip install -r requirements.txt\npython -m pip install sphinx>=1.1 sphinx_rtd_theme numpydoc ipython\npython -m pip install ""blosc2!=2.2.1,!=2.2.2""\n', 'make PYTHON=python dist', ""mkdir hdf5_build\nif [[ ${{ runner.os }} = 'Linux' ]]; then\n  docker run --rm -e HDF5_DIR=/io/hdf5_build -e CFLAGS=-g0 -e HDF5_VERSION=${{ env.HDF5_VERSION }} -v `pwd`:/io:rw quay.io/pypa/manylinux2014_${{ matrix.arch }} /io/ci/github/get_hdf5.sh\nelse\n  HDF5_DIR=`pwd`/hdf5_build ci/github/get_hdf5.sh\nfi\n"", 'python -m pip install --upgrade cibuildwheel\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'cp requirements.txt ./wheelhouse/\n', 'python -m pip install --upgrade cibuildwheel\n', 'cibuildwheel --output-dir wheelhouse', 'python -m pip install --pre --find-links wheelhouse/artifact/ tables\n', 'python -m tables.tests.test_all\n', 'python -m pip install twine\n', 'python -m twine check wheelhouse/artifact/tables*.tar.gz wheelhouse/artifact/*.whl\n']"
""
""
"['sudo apt-get update && sudo apt-get install -y universal-ctags', 'brew install universal-ctags', 'git config --global user.email ""you@example.com""\ngit config --global user.name ""Your Name""\nif ${{ matrix.ctags }}; then\n  pip install -r test_requirements.txt\n  pip install -e .\n  bash ./runtests.sh -v tests\nelse\n  grep -v ctags test_requirements.txt > /tmp/test_requirements.txt\n  pip install -r /tmp/test_requirements.txt\n  pip install -e .\n  bash ./runtests.sh -v tests -k ""not ctags""\nfi\n']"
"['make setup', 'make lint', 'make test', 'python -m pip install wheel setuptools build', 'python -m build']"
"['set -xe\npython -m pip install -r requirements_dev.txt\n', 'set -xe\npython -VV\npython setup.py sdist\npip wheel --verbose --no-deps --no-clean dist/pyte*.tar.gz\npip install pyte*.whl\n', 'set -xe\n# Change directory to avoid importing pyte from repo root.\npushd tests\npython -m pytest -vv --pyargs .\npopd\n', 'set -xe\npython -m mypy --strict pyte\n']"
"['pip install black', 'black --check --diff .', 'pip install flake8', 'flake8', 'pip install isort[colors]', 'isort --check --diff .', 'pip install mypy pytest', 'mypy .', 'conda install -c conda-forge numpy libspatialindex=${{ matrix.sidx-version }} -y\n', 'pip install -e .\n', 'pip install pytest\npython -m pytest --doctest-modules rtree tests\n', 'sudo apt install libspatialindex-c6 python3-pip\npython3 -m pip install --upgrade pip\npython3 -m pip install setuptools numpy pytest\n', 'python3 -m pip install --user .\n', 'python3 -m pytest --doctest-modules rtree tests\n', '# provides sha256sum\nbrew install coreutils\npip install cmake\nbash ci/install_libspatialindex.bash\n', 'choco install vcpython27 -f -y\nci\\install_libspatialindex.bat\n', 'sudo apt install libspatialindex-c6 python3-pip\npython3 -m pip install --upgrade pip\npython3 -m pip install setuptools numpy pytest wheel\nexport PATH=$PATH:/home/runner/.local/bin\npython3 setup.py sdist\n', 'ls -R', 'for f in *whl\ndo\n  cd ""$f""\n  cp *.whl ..\n  cd ..\ndone;\nrm -rf *\\-whl\nls -al\n']"
"['sudo systemctl start mysql.service\nmysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -uroot -proot mysql\nmysql -uroot -proot -e ""set global innodb_flush_log_at_trx_commit=0;""\nmysql -uroot -proot -e ""CREATE USER \'scott\'@\'%\' IDENTIFIED BY \'tiger\'; GRANT ALL ON *.* TO scott;""\nmysql -uroot -proot -e ""CREATE DATABASE django_default; CREATE DATABASE django_other;""\n', '#pip install mysqlclient  # Use stable version\npip install .[rsa]\n', 'sudo apt-get install libmemcached-dev\nwget https://github.com/django/django/archive/${{ matrix.django }}.tar.gz\ntar xf ${{ matrix.django }}.tar.gz\ncp ci/test_mysql.py django-${{ matrix.django }}/tests/\ncd django-${{ matrix.django }}\npip install . -r tests/requirements/py3.txt\n', ""cd django-${{ matrix.django }}/tests/\n# test_runner does not using our test_mysql.py\n# We can't run whole django test suite for now.\n# Run olly backends test\nDJANGO_SETTINGS_MODULE=test_mysql python runtests.py backends\n"", 'sudo chown 999:999 /run/mysqld\n/usr/bin/docker ps --all --filter status=exited --no-trunc --format ""{{.ID}}"" | xargs -r /usr/bin/docker start\n', 'pip install --upgrade -r requirements-dev.txt\n', 'while :\ndo\n    sleep 1\n    mysql -h127.0.0.1 -uroot -e \'select version()\' && break\ndone\nmysql -h127.0.0.1 -uroot -e ""SET GLOBAL local_infile=on""\nmysql -h127.0.0.1 -uroot --comments < ci/docker-entrypoint-initdb.d/init.sql\nmysql -h127.0.0.1 -uroot --comments < ci/docker-entrypoint-initdb.d/mysql.sql\nmysql -h127.0.0.1 -uroot --comments < ci/docker-entrypoint-initdb.d/mariadb.sql\ncp ci/docker.json pymysql/tests/databases.json\n', 'pytest -v --cov --cov-config .coveragerc pymysql\npytest -v --cov-append --cov-config .coveragerc --doctest-modules pymysql/converters.py\n', 'docker cp mysqld:/var/lib/mysql/public_key.pem ""${HOME}""\ndocker cp mysqld:/var/lib/mysql/ca.pem ""${HOME}""\ndocker cp mysqld:/var/lib/mysql/server-cert.pem ""${HOME}""\ndocker cp mysqld:/var/lib/mysql/client-key.pem ""${HOME}""\ndocker cp mysqld:/var/lib/mysql/client-cert.pem ""${HOME}""\npytest -v --cov-append --cov-config .coveragerc tests/test_auth.py;\n']"
""
""
""
"['python3 -m pip install --upgrade pip\n', 'pip3 install --upgrade build wheel', 'python3 -m build --wheel --sdist\n', 'echo AUTOBAHN_BUILD_DATE=`date -u +""%Y-%m-%d""` >> $GITHUB_ENV\necho AUTOBAHN_BUILD_ID=$(date --utc +%Y%m%d)-$(git rev-parse --short ${GITHUB_SHA}) >> $GITHUB_ENV\necho AUTOBAHN_VCS_REF=`git rev-parse --short ${GITHUB_SHA}` >> $GITHUB_ENV\necho AUTOBAHN_VERSION=$(grep -E \'^(__version__)\' ./autobahn/_version.py | cut -d \' \' -f3 | sed -e \'s|[u""\'\\\'\']||g\') >> $GITHUB_ENV\n', 'echo """"\necho ""Build environment configured:""\necho """"\necho ""  AUTOBAHN_BUILD_DATE = ${AUTOBAHN_BUILD_DATE}""\necho ""  AUTOBAHN_BUILD_ID   = ${AUTOBAHN_BUILD_ID}""\necho ""  AUTOBAHN_VCS_REF    = ${AUTOBAHN_VCS_REF}""\necho ""  AUTOBAHN_VERSION    = ${AUTOBAHN_VERSION}""\necho """"\necho ""Wheels (source):""\necho ""  AWS_DEFAULT_REGION  = ${AWS_DEFAULT_REGION}""\necho ""  AWS_S3_BUCKET_NAME  = ${AWS_S3_BUCKET_NAME}""\necho """"\necho ""Docker image (publish):""\necho ""  DOCKERHUB_USER      = ${DOCKERHUB_USER}""\necho """"\n', 'cd ./docker && \\\nmake download_wheels && \\\nmake build_cpy_amd64 && \\\nmake test_cpy_amd64\ndocker login -u ${{ secrets.DOCKERHUB_USER }} -p ${{ secrets.DOCKERHUB_PASSWORD }} docker.io && \\\nmake publish_cpy_amd64\n', 'cd ./docker && \\\nmake download_wheels && \\\nmake build_pypy_amd64 && \\\nmake test_pypy_amd64\ndocker login -u ${{ secrets.DOCKERHUB_USER }} -p ${{ secrets.DOCKERHUB_PASSWORD }} docker.io && \\\nmake publish_pypy_amd64\n', 'sudo apt update\nsudo apt install libenchant-2-dev libbz2-dev libsnappy-dev libunwind-dev libgirepository1.0-dev\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'tox -c tox.ini -e flake8', 'sudo apt update\nsudo apt install build-essential libssl-dev libffi-dev libunwind-dev \\\n  libreadline-dev zlib1g-dev libbz2-dev libsqlite3-dev libncurses5-dev \\\n  libsnappy-dev libgirepository1.0-dev\n', 'python -m pip install -U pip\npip install -U -r requirements-dev.txt\n', 'pip install .[all]\n', 'python -c ""import autobahn; print(autobahn.__version__)""\npython -c ""from autobahn import xbr; print(xbr.HAS_XBR)""\npython -c ""from autobahn.testutil import FakeTransport; print(FakeTransport)""\npython -c ""import os; print(\'WEB3_INFURA_PROJECT_ID\', len(os.environ.get(\'WEB3_INFURA_PROJECT_ID\', \'\')))""\npython -c ""import os; print(\'WEB3_INFURA_API_SECRET\', len(os.environ.get(\'WEB3_INFURA_API_SECRET\', \'\')))""\n', 'tox -c tox.ini -e ${{ matrix.framework }}\n', 'sudo apt update\nsudo apt install libenchant-2-dev libbz2-dev libsnappy-dev libunwind-dev libgirepository1.0-dev\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'pip install .[all]\ntox -c tox.ini -e sphinx\n', 'pip3 install -U pip\npip3 show pip\npip3 install -r requirements-dev.txt\n', 'echo AUTOBAHN_BUILD_DATE=`date -u +""%Y-%m-%d""` >> $GITHUB_ENV\necho AUTOBAHN_BUILD_ID=$(date --utc +%Y%m%d)-$(git rev-parse --short ${GITHUB_SHA}) >> $GITHUB_ENV\necho AUTOBAHN_VCS_REF=`git rev-parse --short ${GITHUB_SHA}` >> $GITHUB_ENV\necho AUTOBAHN_VERSION=$(grep -E \'^(__version__)\' ./autobahn/_version.py | cut -d \' \' -f3 | sed -e \'s|[u""\'\\\'\']||g\') >> $GITHUB_ENV\necho XBRNETWORK_EXE_FILENAME=""xbrnetwork-linux-amd64-$(date --utc +%Y%m%d)-$(git rev-parse --short ${GITHUB_SHA})"" >> $GITHUB_ENV\n', 'echo """"\necho ""Build environment configured:""\necho """"\necho ""  AUTOBAHN_BUILD_DATE     = ${AUTOBAHN_BUILD_DATE}""\necho ""  AUTOBAHN_BUILD_ID       = ${AUTOBAHN_BUILD_ID}""\necho ""  AUTOBAHN_VCS_REF        = ${AUTOBAHN_VCS_REF}""\necho ""  AUTOBAHN_VERSION        = ${AUTOBAHN_VERSION}""\necho """"\necho ""  XBRNETWORK_EXE_FILENAME = ${XBRNETWORK_EXE_FILENAME}""\necho """"\necho ""  AWS_DEFAULT_REGION      = ${AWS_DEFAULT_REGION}""\necho ""  AWS_S3_BUCKET_NAME      = ${AWS_S3_BUCKET_NAME}""\necho """"\n', 'pip3 install .[all]\n', 'make build_exe\nmake upload_exe\n', 'curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py ""pip<20""\npip3 show pip\npip3 install -r requirements-dev.txt\n', 'echo AUTOBAHN_BUILD_DATE=`date -u +""%Y-%m-%d""` >> $GITHUB_ENV\necho AUTOBAHN_BUILD_ID=$(date --utc +%Y%m%d)-$(git rev-parse --short ${GITHUB_SHA}) >> $GITHUB_ENV\necho AUTOBAHN_VCS_REF=`git rev-parse --short ${GITHUB_SHA}` >> $GITHUB_ENV\necho AUTOBAHN_VERSION=$(grep -E \'^(__version__)\' ./autobahn/_version.py | cut -d \' \' -f3 | sed -e \'s|[u""\'\\\'\']||g\') >> $GITHUB_ENV\n', 'echo """"\necho ""Build environment configured:""\necho """"\necho ""  AUTOBAHN_BUILD_DATE = ${AUTOBAHN_BUILD_DATE}""\necho ""  AUTOBAHN_BUILD_ID   = ${AUTOBAHN_BUILD_ID}""\necho ""  AUTOBAHN_VCS_REF    = ${AUTOBAHN_VCS_REF}""\necho ""  AUTOBAHN_VERSION    = ${AUTOBAHN_VERSION}""\necho """"\necho ""  AWS_DEFAULT_REGION  = ${AWS_DEFAULT_REGION}""\necho ""  AWS_S3_BUCKET_NAME  = ${AWS_S3_BUCKET_NAME}""\necho """"\n', 'pip install .[all]\n', 'cd wstest\nmake wstest_server_docker_stop\nmake wstest_server_docker_pull\nmake wstest_server_docker_quick\nsleep 5\npython testee_client_tx.py\npython testee_client_aio.py\nmake wstest_server_docker_stop\npwd\nls -la .\nls -la ./reports\naws s3 sync --delete --acl public-read --region=${AWS_DEFAULT_REGION} \\\n  ./reports \\\n  s3://crossbario.com/reports/websocket-testsuite/autobahn-${AUTOBAHN_VERSION}-${{ matrix.python-version }}\n']"
"['pip install --upgrade setuptools\npip install tox==3.27.*\n', 'tox -e py', 'pip install -r ""requirements/dev.pip""\npip install types-pkg_resources # one of mypy required stubs\n', 'mypy docker/test_docker.py pypiserver/config.py  tests/test_init.py', 'black --diff --check .', './bin/check_readme.sh', 'pip install -r ""requirements/test.pip""', 'pip install -r ""requirements/exe.pip""', 'pytest docker/test_docker.py', 'echo true', './bin/package.sh\n', 'echo ""::set-output name=tags::$(bin/ci_helper.py ${{ github.ref }} docker_tags)""', 'echo ""::set-output name=has_tags::$(bin/ci_helper.py ${{ github.ref }} has_tags)""', 'echo ${{ steps.docker_build.outputs.digest }}', 'RC_DATE=$(date +\'%m-%d-%Y\')\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit checkout -b auto-release-candidate-${RC_DATE}\ngit push -u origin auto-release-candidate-${RC_DATE}\n\ngit status\ngit fetch\n\n./bin/update_changelog.sh\n\ngit add CHANGES.rst\ngit commit -m ""chore(rc-changes): update Changes.rst""\ngit push\n\ngh pr create  --title ""chore(auto-release-candidate-${RC_DATE})"" \\\n              --body ""Automated release candidate for ${RC_DATE}."" \\\n              --base master \\\n              --draft\n']"
"['sudo apt-get install libpq-dev', 'python${{ matrix.python }} -m venv .venv\nsource .venv/bin/activate\npip install -U pip setuptools wheel\n', 'echo ""VIRTUAL_ENV=$(pwd)/.venv"" >> $GITHUB_ENV\necho ""$(pwd)/.venv/bin"" >> $GITHUB_PATH\n', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT', 'pip install -r requirements.dev.txt\npip install -r requirements.txt\n', 'npm ci', 'tar cf /tmp/env.tar .venv node_modules', 'tar xf /tmp/env.tar', 'echo ""VIRTUAL_ENV=$(pwd)/.venv"" >> $GITHUB_ENV\necho ""$(pwd)/.venv/bin"" >> $GITHUB_PATH\n', 'pip install -e .', 'isort --diff --check-only indico/', 'python bin/maintenance/update_backrefs.py --ci', 'echo ""::add-matcher::.github/matchers/headers-problem-matcher.json""\npython bin/maintenance/update_header.py --ci\necho ""::remove-matcher owner=headers::""\n', 'echo ""::add-matcher::.github/matchers/flake8-problem-matcher.json""\nflake8 --format \'%(path)s:%(row)d:%(col)d: %(code)s %(text)s\'\necho ""::remove-matcher owner=flake8::""\n', 'pyquotes --check .', 'indico i18n extract-messages', 'echo ""::add-matcher::.github/matchers/react-jsx-i18n-problem-matcher.json""\nFORCE_COLOR=1 npx react-jsx-i18n extract --ext jsx indico/web/client/ indico/modules/ > /dev/null\necho ""::remove-matcher owner=react-jsx-i18n::""\n', 'indico i18n check-format-strings', 'npx eslint --ext .js --ext .jsx indico/modules/rb/ indico/modules/logs/ indico/modules/events/editing/ indico/modules/events/client/js/reviewing/ indico/modules/events/papers/client/js/ indico/modules/events/registration/client/js/ indico/web/client/js/react/ indico/modules/users/ indico/modules/search/', 'echo ""::add-matcher::.github/matchers/stylelint-problem-matcher.json""\nnpx stylelint --formatter unix \'indico/**/*.{scss,css}\'\necho ""::remove-matcher owner=stylelint::""\n', 'echo ""::add-matcher::.github/matchers/stylelint-problem-matcher.json""\njq -rs \'(.[0] + .[1]) | unique | .[]\' ~/files_{created,updated}.json | tr \'\\n\' \'\\0\' | xargs -0 -r \\\n  npx stylelint --formatter unix\necho ""::remove-matcher owner=stylelint::""\n', 'tar xf /tmp/env.tar', 'echo ""VIRTUAL_ENV=$(pwd)/.venv"" >> $GITHUB_ENV\necho ""$(pwd)/.venv/bin"" >> $GITHUB_PATH\n', 'pip install -e .', 'sudo apt-get install redis-server', ""sudo apt-get update\nsudo apt-get install postgresql-client libpq-dev\nexport PGHOST=localhost\nexport PGPORT=${{ job.services.postgres.ports[5432] }}\nexport PGUSER=postgres\ncreateuser indicotest\ncreatedb -O indicotest indicotest\npsql indicotest -c 'CREATE EXTENSION unaccent;'\npsql indicotest -c 'CREATE EXTENSION pg_trgm;'\n"", 'export INDICO_TEST_DATABASE_URI=""postgresql://indicotest@localhost:${{ job.services.postgres.ports[5432] }}/indicotest""\npytest --color=yes\n', 'tar xf /tmp/env.tar', 'echo ""VIRTUAL_ENV=$(pwd)/.venv"" >> $GITHUB_ENV\necho ""$(pwd)/.venv/bin"" >> $GITHUB_PATH\n', 'pip install -e .', 'npm test', 'python bin/maintenance/build-assets.py indico --dev', 'sudo apt-get install libpq-dev\npip install -U pip setuptools wheel\npip install -r requirements.dev.txt\npip install -r requirements.txt\necho ""CODEQL_PYTHON=$(which python)"" >> $GITHUB_ENV\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'RELEASE=${{ steps.get_version.outputs.VERSION }} python setup.py sdist bdist_wheel\ntwine upload dist/*\nREADTHEDOCS_TOKEN=""${{ secrets.READTHEDOCS_TOKEN }}"" bin/publish-docs ${{ steps.get_version.outputs.VERSION }}\n', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls', 'python -m pip install --upgrade pip wheel\npip install https://github.com/bboe/coveralls-python/archive/github_actions.zip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'bin/check-legal', 'bin/check-install', 'bin/test -v -x', 'coveralls']"
['python --version']
"['python -m pip install -U coverage flake8 pip pytest pytest-coverage pytest-benchmark', 'flake8 sqlitedict.py tests', 'python setup.py install', 'rm -f tests/db\nmkdir -p tests/db\n', 'pytest tests --cov=sqlitedict', 'pytest benchmarks', 'python -m doctest README.rst', 'echo ::set-output name=V::$(python sqlitedict.py)\n', 'python -m pip install --upgrade pip\npython -m venv venv\n. venv/bin/activate\npip install twine\n', '. venv/bin/activate\npython setup.py sdist\n', '. venv/bin/activate\ntwine upload dist/sqlitedict-${{ steps.get_version.outputs.V }}.tar.gz -u ${{ env.PYPI_USERNAME }} -p ${{ env.PYPI_PASSWORD }}\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'tox -e py\n']"
"['sudo apt-get update -qq\nsudo apt-get install -qq swig python-dev libxml2-dev libxmlsec1-dev\npip install --force-reinstall --no-binary lxml lxml\npip install .\npip install -e "".[test]""\n', '#pycodestyle tests/src/OneLogin/saml2_tests/*.py demo-flask/*.py demo-django/*.py src/onelogin/saml2/*.py --config=tests/pep8.rc\n#pyflakes src/onelogin/saml2 demo-django demo-flask tests/src/OneLogin/saml2_tests\nflake8 --ignore E226,E302,E41,E731,E501,C901,W504\n', 'coverage run --source=src/onelogin/saml2 --rcfile=tests/coverage.rc setup.py test\n', 'pip install python-coveralls\ncoveralls\n']"
"['pip install -U pip build', 'python -m build', 'pip install ""$(ls dist/*.whl)[dev]"" \'django~=${{ matrix.django-version }}.0\'', 'pytest --cov --cov-report=', 'mv .coverage .coverage.py${{ matrix.python-version }}.dj${{ matrix.django-version }}', 'pip install coverage\ncoverage combine\ncoverage xml\n', 'pip install black', 'black --check .', 'pip install isort', 'isort --check .', 'pip install -U pip build twine', 'python -m build', 'twine upload dist/*']"
"['which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )\neval $(ssh-agent -s)\necho ""$SSH_PRIVATE_KEY"" | tr -d \'\\r\' | ssh-add -\nmkdir -p ~/.ssh\nchmod 700 ~/.ssh\nssh-keyscan -p $PORT $TAKOYAKI >> ~/.ssh/known_hosts\nchmod 644 ~/.ssh/known_hosts\nssh $USER@$TAKOYAKI -p $PORT ""doconfly/doconfly.sh $GITHUB_REPOSITORY $GITHUB_REF $DOCUMENTATION_PATH $DOCUMENTATION_URL""\n', 'python -m pip install --upgrade pip setuptools', 'python -m pip install .', 'git clone https://github.com/CourtBouillon/weasyprint-samples.git', 'mkdir ${{env.REPORTS_FOLDER}}', 'python -m weasyprint weasyprint-samples/book/book.html -s weasyprint-samples/book/book-classical.css ${{env.REPORTS_FOLDER}}/book-classical.pdf', 'python -m weasyprint weasyprint-samples/book/book.html -s weasyprint-samples/book/book.css ${{env.REPORTS_FOLDER}}/book-fancy.pdf', 'python -m weasyprint weasyprint-samples/invoice/invoice.html ${{env.REPORTS_FOLDER}}/invoice.pdf', 'python -m weasyprint weasyprint-samples/letter/letter.html ${{env.REPORTS_FOLDER}}/letter.pdf', 'python -m weasyprint weasyprint-samples/poster/poster.html -s weasyprint-samples/poster/poster.css ${{env.REPORTS_FOLDER}}/poster.pdf', 'python -m weasyprint weasyprint-samples/poster/poster.html -s weasyprint-samples/poster/flyer.css ${{env.REPORTS_FOLDER}}/flyer.pdf', 'python -m weasyprint weasyprint-samples/report/report.html ${{env.REPORTS_FOLDER}}/report.pdf', 'python -m weasyprint weasyprint-samples/ticket/ticket.html ${{env.REPORTS_FOLDER}}/ticket.pdf', 'sudo apt-get update -y && sudo apt-get install fonts-dejavu ghostscript -y', 'rm /usr/local/bin/2to3\nbrew update\nbrew tap homebrew/cask-fonts\nbrew install --cask font-dejavu\nbrew install pango ghostscript\n', 'C:\\msys64\\usr\\bin\\bash -lc \'pacman -S mingw-w64-x86_64-ttf-dejavu mingw-w64-x86_64-pango mingw-w64-x86_64-ghostscript --noconfirm\'\nxcopy ""C:\\msys64\\mingw64\\share\\fonts\\TTF"" ""C:\\Users\\runneradmin\\.fonts"" /e /i\necho ""C:\\msys64\\mingw64\\bin"" | Out-File -FilePath $env:GITHUB_PATH\nrm C:\\msys64\\mingw64\\bin\\python.exe\n', 'python -m pip install --upgrade pip setuptools', 'python -m pip install .[test] pytest-xdist', 'python -m pytest -n auto', 'python -m flake8', 'python -m isort . --check --diff']"
"['pip install -U tox\ntox\n', 'pip install -U tox\ntox\n', 'pip install -U tox\ntox\n']"
"['python dev/deps.py', 'python dev/lint.py', 'python dev/tests.py', 'python dev/deps.py', 'python dev/lint.py', 'python dev/tests.py', 'python dev/pyenv-install.py 3.3 >> $GITHUB_PATH', 'python dev/deps.py', 'python dev/lint.py', 'python dev/tests.py', 'python dev/python-install.py 3.3 ${{ matrix.arch }} | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append', 'python dev/deps.py', 'python dev/lint.py', 'python dev/tests.py']"
"['python2 -m pip install --upgrade setuptools wheel\npython3 -m pip install --upgrade setuptools wheel\npython2 setup.py sdist bdist_wheel\npython3 setup.py sdist bdist_wheel\n', 'python2 -m pip install dist/ROPGadget*py2*.whl\npython3 -m pip install dist/ROPGadget*py3*.whl\n', 'cd test-suite-binaries\n./test.sh\n./test.sh python2\ncd ..\n']"
"['git fetch --depth=1 origin +refs/tags/release-*:refs/tags/release-*', './scripts/package/macos-setup.sh\nPYTHON_BASE_VERSION=$(echo $PYTHON_VERSION | sed -e ""s/\\.[0-9]\\{1,\\}$//"")\necho ""/Library/Frameworks/Python.framework/Versions/$PYTHON_BASE_VERSION/bin"" >> $GITHUB_PATH\necho ""/usr/local/opt/gettext/bin"" >> $GITHUB_PATH\nRELEASE_TAG=$(git describe --match ""release-*"" --abbrev=0 --always HEAD)\nBUILD_NUMBER=$(git rev-list --count $RELEASE_TAG..HEAD)\necho ""BUILD_NUMBER=$BUILD_NUMBER"" >> $GITHUB_ENV\nmkdir artifacts\npython3 -m pip install --upgrade pip setuptools wheel\n', 'python3 setup.py patch_version --platform=$BUILD_NUMBER.$(git rev-parse --short HEAD)\n', 'git clone --depth 1 --branch ""$PYINSTALLER_VERSION"" https://github.com/pyinstaller/pyinstaller.git pyinstaller\ncd pyinstaller/bootloader\npython3 ./waf --verbose all\ncd ..\npip3 install .\n', 'mkdir -p build\ncd build\ngit clone -b release-1.46-picard --single-branch --depth 1 https://github.com/phw/mutagen.git\ncd mutagen\npip3 install .\n', 'pip3 install -r requirements-build.txt\npip3 install -r requirements-macos-${MACOSX_DEPLOYMENT_TARGET}.txt\n', 'python3 setup.py test\n', 'if [ -n ""$CODESIGN_MACOS_PFX_URL"" ] && [ -n ""$AWS_ACCESS_KEY_ID"" ]; then\n  pip3 install awscli\n  aws s3 cp ""$CODESIGN_MACOS_PFX_URL"" ./scripts/package/appledev.p12\nelse\n  echo ""::warning::No code signing certificate available, skipping code signing.""\nfi\n', './scripts/package/macos-package-app.sh\nrm -f ./scripts/package/appledev.p12\nmv dist/*.dmg artifacts/\n', 'git fetch --depth=1 origin +refs/tags/release-*:refs/tags/release-*', '& .\\scripts\\package\\win-setup.ps1 `\n  -DiscidVersion $Env:DISCID_VERSION -DiscidSha256Sum $Env:DISCID_SHA256SUM `\n  -FpcalcVersion $Env:FPCALC_VERSION -FpcalcSha256Sum $Env:FPCALC_SHA256SUM\nWrite-Output ""C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8\n$ReleaseTag = $(git describe --match ""release-*"" --abbrev=0 --always HEAD)\n$BuildNumber = $(git rev-list --count ""$ReleaseTag..HEAD"")\nWrite-Output ""BUILD_NUMBER=$BuildNumber"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8\nNew-Item -Name .\\artifacts -ItemType Directory\n', 'New-Item -Name .\\build -ItemType Directory -Force\ncd build\ngit clone -b release-1.46-picard --single-branch --depth 1 https://github.com/phw/mutagen.git\ncd mutagen\npip install .\n', 'python -m pip install --upgrade pip\npip install -r requirements-build.txt\npip install -r requirements-win.txt\n', 'python setup.py patch_version --platform=$Env:BUILD_NUMBER.$(git rev-parse --short HEAD)\n', 'python setup.py test', 'If ($Env:CODESIGN_PFX_URL -And $Env:AWS_ACCESS_KEY_ID) {\n  pip install awscli\n  aws s3 cp ""$Env:CODESIGN_PFX_URL"" .\\codesign.pfx\n  Write-Output ""CODESIGN=1"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8\n} Else {\n  Write-Output ""::warning::No code signing certificate available, skipping code signing.""\n}\n', '& .\\scripts\\package\\win-package-appx.ps1 -BuildNumber $Env:BUILD_NUMBER\nMove-Item .\\dist\\*.msix .\\artifacts\n', '$CertPassword = ConvertTo-SecureString -String $Env:CODESIGN_PFX_PASSWORD -Force -AsPlainText\n& .\\scripts\\package\\win-package-appx.ps1 -BuildNumber $Env:BUILD_NUMBER -CertificateFile .\\codesign.pfx -CertificatePassword $CertPassword\nMove-Item .\\dist\\*.msix .\\artifacts\n', '# choco install nsis\nIf ($Env:CODESIGN) {\n  $CertPassword = ConvertTo-SecureString -String $Env:CODESIGN_PFX_PASSWORD -Force -AsPlainText\n  $Certificate = Get-PfxCertificate -FilePath .\\codesign.pfx -Password $CertPassword\n} Else {\n  $Certificate = $null\n}\n& .\\scripts\\package\\win-package-installer.ps1 -BuildNumber $Env:BUILD_NUMBER -Certificate $Certificate\nMove-Item .\\installer\\*.exe .\\artifacts\ndist\\picard\\fpcalc -version\n', 'If ($Env:CODESIGN) {\n  $CertPassword = ConvertTo-SecureString -String $Env:CODESIGN_PFX_PASSWORD -Force -AsPlainText\n  $Certificate = Get-PfxCertificate -FilePath .\\codesign.pfx -Password $CertPassword\n} Else {\n  $Certificate = $null\n}\n& .\\scripts\\package\\win-package-portable.ps1 -BuildNumber $Env:BUILD_NUMBER -Certificate $Certificate\nMove-Item .\\dist\\*.exe .\\artifacts\n', 'Remove-Item .\\codesign.pfx', 'cd artifacts\nsha256sum * > SHA256SUMS\n', 'PICARD_VERSION=$(python -c ""import picard; print(picard.__version__)"")\necho ""version=$PICARD_VERSION"" >> $GITHUB_OUTPUT\n./scripts/tools/changelog-for-version.py $PICARD_VERSION > changes-$PICARD_VERSION.txt\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools\npip install --upgrade -r requirements.txt\n', 'python setup.py test\n', 'python setup.py clean sdist\n', 'pip install --upgrade twine\ntwine upload --non-interactive dist/*\n', 'brew install gettext openssl@1.1\nbrew link gettext --force\nbrew link openssl@1.1 --force\necho ""/usr/local/opt/gettext/bin"" >> $GITHUB_PATH\necho ""/usr/local/opt/openssl@1.1/bin"" >> $GITHUB_PATH\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools wheel\npip install --upgrade -r requirements.txt\n', 'python setup.py test\n', 'python setup.py clean bdist_wheel\n', 'pip install --upgrade twine>=3.0\ntwine upload --non-interactive dist/*\n', 'python -m pip install --upgrade pip\npip install --upgrade setuptools\npip install -r requirements.txt\n', 'pip install flake8 ""isort>=5""\nflake8 picard test --count --show-source --statistics\nisort --check-only --diff --recursive picard test\n', 'pip install pytest pytest-randomly pytest-cov\npytest --verbose --cov=picard --cov-report xml:coverage.xml test\n', 'sudo apt-get update\nsudo apt-get install libdiscid-dev\npython -c ""from picard.disc import discid_version; print(discid_version)""\npytest --verbose test/test_disc.py\npip install python-libdiscid\npython -c ""from picard.disc import discid_version; print(discid_version)""\npytest --verbose test/test_disc.py\n', 'pip install codacy-coverage\npython-codacy-coverage -r coverage.xml\n', 'python -m pip install --upgrade pip\npip install $DEPENDENCIES\npip install -r requirements.txt\n', 'pip install pytest pytest-randomly pytest-cov\npytest --verbose test\n', 'sudo apt-get update\nsudo apt-get install gettext\n', 'brew install gettext\nbrew link gettext --force\necho ""/usr/local/opt/gettext/bin"" >> $GITHUB_PATH\n', 'python -m pip install --upgrade pip\npip install .\n', 'picard --long-version --no-crash-dialog', 'pip install --upgrade setuptools\npip install pytest\nscripts/package/run-sdist-test.sh\n', 'sudo apt-get update\nsudo apt-get install appstream-util gettext\n', 'python setup.py build_appdata\nappstream-util validate-relax org.musicbrainz.Picard.appdata.xml\n']"
"['documentation-generator/run.py', 'pip install build', 'python -m build --sdist --wheel', 'pip install tox', 'tox', 'pip install coveralls', 'coveralls --service=github', 'pip install coveralls', 'coveralls --service=github --finish']"
"['sudo apt install -y gnome-keyring\npython -m pip install --upgrade pip\npython -m pip install poetry\npoetry install -E dnssec -E doh -E idna -E trio\n', 'python -m pip install --upgrade pip\npython -m pip install poetry\npoetry install -E dnssec -E doh -E idna -E trio -E curio\n', 'poetry run pytest --cov=. --cov-report=xml:coverage.xml\n', 'python -m pip install --upgrade pip\npython -m pip install poetry\npoetry install -E dnssec -E doh -E idna -E trio -E doq\n', 'poetry run python -m mypy --install-types --non-interactive --disallow-incomplete-defs dns\n', 'poetry run pytest --cov=dns --cov-branch --cov-report=xml:coverage.xml\n']"
"['python --version\npython -m pip install --upgrade pip wheel\npip install --upgrade -r requirements.txt --no-dependencies\npip install --upgrade -r builder/requirements.txt --no-dependencies\n', 'python builder/package.py installer', 'python --version\npython -m pip install --upgrade pip wheel\npip install --upgrade -r requirements.txt\npip install --upgrade -r builder/requirements.txt\n', 'python builder/package.py binary', 'curl https://www.python.org/ftp/python/${PYTHON_VERSION}/python-${PYTHON_VERSION}-macos11.pkg -o ~/python.pkg', 'sudo installer -pkg ~/python.pkg -target /\nunlink /usr/local/bin/python\nln -s /usr/local/bin/python3 /usr/local/bin/python\n', 'python3 --version\npip3 install --upgrade pip wheel\n\npip3 install --upgrade -r requirements.txt --no-binary cffi\n\npip3 uninstall cryptography -y\npip3 download -r builder/osx/requirements.txt --platform macosx_10_12_universal2 --only-binary :all: --no-deps --dest .\npip3 install -r builder/osx/requirements.txt --no-cache-dir --no-index --find-links .\n\nPYINSTALLER_COMPILE_BOOTLOADER=1 pip3 install --upgrade -r builder/requirements.txt --no-binary pyinstaller --no-dependencies\n', 'echo $CERTIFICATES_P12 | base64 --decode > certificate.p12\nsecurity create-keychain -p ""$MACOS_KEYCHAIN_TEMP_PASSWORD"" build.keychain \nsecurity default-keychain -s build.keychain \nsecurity unlock-keychain -p ""$MACOS_KEYCHAIN_TEMP_PASSWORD"" build.keychain\nsecurity set-keychain-settings -lut 21600 build.keychain\nsecurity import certificate.p12 -k build.keychain -P ""$CERTIFICATES_P12_PASSWORD"" -T /usr/bin/codesign -T /usr/bin/productsign -T /usr/bin/xcrun\nsecurity set-key-partition-list -S apple-tool:,apple:,codesign: -s -k ""$MACOS_KEYCHAIN_TEMP_PASSWORD"" build.keychain\n', 'python builder/package.py source', 'python3 builder/package.py app\npython3 builder/make_dmg.py\n', 'find dist -type f -exec mv {} . \\;', 'pip3 install -r builder/release-requirements.txt\npython3 builder/release.py\n', 'sudo snap install snapcraft --classic\npython3 snap/local/release_snap.py\n', 'sudo apt-get install unrar p7zip-full par2', 'python --version\npython -m pip install --upgrade pip wheel\npip install --upgrade -r requirements.txt\npip install --upgrade -r tests/requirements.txt\n', 'pytest -s', 'python3 tools/extract_pot.py\n', 'curl -o- https://raw.githubusercontent.com/transifex/cli/master/install.sh | bash\n./tx push --source\n./tx pull --all --force\n', 'python3 tools/make_mo.py\n']"
"['pip install black==22.3.0\nblack -S --diff --check msgpack/ test/ setup.py\n', 'pip install -r requirements.txt pytest\nmake cython\npip install .\n', 'pytest -v test\n', 'MSGPACK_PUREPYTHON=1 pytest -v test\n', 'pip install -r requirements.txt\nmake cython\n']"
"['python -m pip install tox', 'tox', 'sudo apt-get update\nsudo apt-get install -y gettext\n', 'python -m pip install coveralls tox', 'tox', 'coveralls', 'python -m pip install coveralls', 'coveralls --finish']"
"['python -m unittest', 'curl --http1.1 -X POST https://packagecontrol.io/test_pr/$PR_NUM.json', 'curl --http1.1 -X POST https://packagecontrol.io/test_pr/$PR_NUM.json']"
"['python -m pip install build --user\n', '# python setup.py sdist\npython -m build --sdist --outdir dist/ .\n', 'echo Resuming CI. Continuing next jobs...', ""echo 'testing=full' >> $GITHUB_ENV\n"", 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends python3-h5py\n', 'python -m pip install --upgrade pip\npython -m pip install --prefer-binary -r requirements-tests.txt\n', 'echo ""Setup SMB environment variable to trigger testing in Petl""\necho \'PETL_TEST_SMB=smb://WORKGROUP;petl:test@localhost/public/\' >> $GITHUB_ENV\necho ""Setup SFTP environment variable to trigger testing in Petl""\necho \'PETL_TEST_SFTP=sftp://petl:test@localhost:2244/public/\' >> $GITHUB_ENV\necho ""::group::Install remote test dependencies""\npython -m pip install --prefer-binary -r requirements-remote.txt\necho ""::endgroup::""\n', 'echo ""::group::Install tricky test dependencies""\nif ! pip install --prefer-binary -r requirements-optional.txt ; then\n  echo \'Dismissed failure installing some optional package.  Resuming tests...\'\nfi\n# DISABLE_BLOSC_AVX2=1 && export DISABLE_BLOSC_AVX2\n# pip install --prefer-binary bcolz\necho ""::endgroup::""\n', 'echo ""::group::Setup docker for SMB at: ${{ env.PETL_TEST_SMB }}$""\ndocker run -it --name samba -p 139:139 -p 445:445 -d ""dperson/samba"" -p -u ""petl;test"" -s ""public;/public-dir;yes;no;yes;all""\necho ""::endgroup::""\necho ""::group::Setup docker for SFTP at: ${{ env.PETL_TEST_SFTP }}$""\ndocker run -it --name sftp -p 2244:22 -d atmoz/sftp petl:test:::public\necho ""::endgroup::""\n', 'echo ""::group::Setup docker for MySQL""\ndocker run -it --name mysql -p 3306:3306 -p 33060:33060 -e MYSQL_ROOT_PASSWORD=pass0 -e MYSQL_DATABASE=petl -e MYSQL_USER=petl -e MYSQL_PASSWORD=test -d mysql:latest\necho ""::endgroup::""\necho ""::group::Setup docker for Postgres""\ndocker run -it --name postgres -p 5432:5432 -e POSTGRES_DB=petl -e POSTGRES_USER=petl -e POSTGRES_PASSWORD=test -d postgres:latest\necho ""::endgroup::""\necho ""::group::Install database test dependencies""\npython -m pip install --prefer-binary -r requirements-database.txt\necho ""::endgroup::""\n', 'python setup.py sdist bdist_wheel', 'pytest --cov=petl petl', 'echo ""::group::Install extra packages test dependencies""\npython -m pip install --prefer-binary -r requirements-formats.txt\necho ""::endgroup::""\necho ""::group::Perform doctest-modules execution with coverage""\npytest --doctest-modules --cov=petl petl\necho ""::endgroup::""\n', 'python -m pip install --upgrade coveralls\ncoveralls --service=github\n', 'coverage report -m', 'python -m pip install --prefer-binary -r requirements-docs.txt\n', 'python setup.py build', 'cd docs\nsphinx-build -W -b singlehtml -d ../build/doctrees . ../build/singlehtml\n']"
"['sudo apt-get install libpq-dev', 'python -m pip install --upgrade pip\npip install flake8 coverage ""Django~=${{ matrix.django-version }}.0"" ""psycopg2==2.8.6"" mysqlclient -e .\n', 'flake8\n', 'coverage run -a tests/runtests.py\n', 'coverage run -a tests/runtests.py -d psql\n', 'coverage run -a tests/runtests.py -d mysql\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install -r linter-requirements.txt', '${{ matrix.lint-command }}', 'python -m pip install --upgrade pip build wheel twine', 'python -m build --sdist --wheel', 'python -m twine check dist/*', 'python -m pip install -e .[docs]', 'python -m sphinx -b html -W docs docs/_build', 'python -m pip install .[test]', 'python -m pip install Django~=""${{ matrix.django-version }}.0""', 'python -m pytest', 'python -m pip install --upgrade pip build wheel twine', 'python -m build --sdist --wheel', 'python -m twine upload dist/*']"
"['python -m pip install --upgrade pip\npip install pytest mock pytest-cov\npython setup.py install\npytest --cov=./ --cov-report=xml\n', 'python -m pytest -v']"
""
"['python -m pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.config[1] }}', 'pip install coveralls coverage-python-version\ncoveralls --service=github\n', 'python -m pip install --upgrade pip\npip install wheel twine\n', 'PACKAGE_VERSION=`python setup.py --version`\nTAG_NAME=v$PACKAGE_VERSION\necho ""Package version $PACKAGE_VERSION with possible tag name $TAG_NAME on $GITHUB_REF_NAME""\n# test that the tag represents the version\n# see https://docs.github.com/en/actions/learn-github-actions/environment-variables\nif [ ""$TAG_NAME"" != ""$GITHUB_REF_NAME"" ]; then\n  echo ""ERROR: This tag is for the wrong version. Got \\""$GITHUB_REF_NAME\\"" expected \\""$TAG_NAME\\"".""\n  exit 1\nfi\n', 'rm -rf dist/*', 'python setup.py bdist_wheel sdist', '# You will have to set the variables TWINE_USERNAME and TWINE_PASSWORD\n# You can use a token specific to your project by setting the user name to\n# __token__ and the password to the token given to you by the PyPI project.\n# sources:\n#   - https://shambu2k.hashnode.dev/gitlab-to-pypi\n#   - http://blog.octomy.org/2020/11/deploying-python-pacakges-to-pypi-using.html?m=1\nif [ -z ""$TWINE_USERNAME"" ]; then\n  echo ""WARNING: TWINE_USERNAME not set!""\nfi\nif [ -z ""$TWINE_PASSWORD"" ]; then\n  echo ""WARNING: TWINE_PASSWORD not set!""\nfi\ntwine check dist/*\ntwine upload dist/*\n']"
""
""
"['sudo apt install -y libgirepository1.0-dev\npython -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi\npython -m unittest discover -s tests\ncoverage run run_tests.py && coverage report -m\n']"
"['pip install --upgrade -e.[test]', 'mkdir ~/.fonts\ncp ./test_non_regression/resources/*.*tf ~/.fonts\nfc-cache -f -v\n', 'python -m pip install .[test] pytest-xdist', 'python -m pytest -n auto', 'python -m flake8', 'python -m isort . --check --diff']"
"['python -m pip install --upgrade pip wheel setuptools\npython -m pip install --upgrade ""django~=${{ matrix.django-version}}""\n', 'python manage.py test']"
"['sudo usermod -a -G systemd-journal ""$USER"" || echo \'no systemd-journal access\'', 'F2B_PY=$(python -c ""import sys; print(sys.version)"")\necho ""Python: ${{ matrix.python-version }} -- ${F2B_PY/$\'\\n\'/ }""\nF2B_PYV=$(echo ""${F2B_PY}"" | grep -oP \'^\\d+(?:\\.\\d+)\')\nF2B_PY=${F2B_PY:0:1}\necho ""Set F2B_PY=$F2B_PY, F2B_PYV=$F2B_PYV""\necho ""F2B_PY=$F2B_PY"" >> $GITHUB_ENV\necho ""F2B_PYV=$F2B_PYV"" >> $GITHUB_ENV\n# for GHA we need to monitor all journals, since it cannot be found using SYSTEM_ONLY(4):\necho ""F2B_SYSTEMD_DEFAULT_FLAGS=0"" >> $GITHUB_ENV\n', 'if [[ ""$F2B_PY"" = 3 ]]; then python -m pip install --upgrade pip || echo ""can\'t upgrade pip""; fi\nif [[ ""$F2B_PY"" = 3 ]] && ! command -v 2to3x -v 2to3 > /dev/null; then\n  #pip install 2to3\n  sudo apt-get -y install 2to3\nfi\n#sudo apt-get -y install python${F2B_PY/2/}-pyinotify || echo \'inotify not available\'\npython -m pip install pyinotify || echo \'inotify not available\'\n#sudo apt-get -y install python${F2B_PY/2/}-systemd || echo \'systemd not available\'\nsudo apt-get -y install libsystemd-dev || echo \'systemd dependencies seems to be unavailable\'\npython -m pip install systemd-python || echo \'systemd not available\'\n#readline if available as module:\npython -c \'import readline\' 2> /dev/null || python -m pip install readline || echo \'readline not available\'\n', 'cd ""$GITHUB_WORKSPACE""\n# Manually execute 2to3 for now\nif [[ ""$F2B_PY"" = 3 ]]; then echo ""2to3 ..."" && ./fail2ban-2to3; fi\n_debug() { echo -n ""$1 ""; err=$(""${@:2}"" 2>&1) && echo \'OK\' || echo -e ""FAIL\\n$err""; }\n# (debug) output current preferred encoding:\n_debug \'Encodings:\' python -c \'import locale, sys; from fail2ban.helpers import PREFER_ENC; print(PREFER_ENC, locale.getpreferredencoding(), (sys.stdout and sys.stdout.encoding))\'\n# (debug) backend availabilities:\necho \'Backends:\'\n_debug \'- systemd:\' python -c \'from fail2ban.server.filtersystemd import FilterSystemd\'\n#_debug \'- systemd (root): \' sudo python -c \'from fail2ban.server.filtersystemd import FilterSystemd\'\n_debug \'- pyinotify:\' python -c \'from fail2ban.server.filterpyinotify import FilterPyinotify\'\n', 'if [[ ""$F2B_PY"" = 2 ]]; then\n  python setup.py test\nelif dpkg --compare-versions ""$F2B_PYV"" lt 3.10; then \n  python bin/fail2ban-testcases --verbosity=2\nelse\n  echo ""Skip systemd backend since systemd-python module must be fixed for python >= v.3.10 in GHA ...""\n  python bin/fail2ban-testcases --verbosity=2 -i ""[sS]ystemd|[jJ]ournal""\nfi\n', 'python setup.py build']"
"[""python -m pip install --upgrade pip\npip install -e '.[MongoTrials, SparkTrials, ATPE, dev]'\npip install pyspark\n"", 'python -m pytest ./hyperopt/tests/unit\n', 'PYSPARK_PIN_THREAD=true pytest hyperopt/tests/integration/test_spark.py\n', 'PYSPARK_PIN_THREAD=false pytest hyperopt/tests/integration/test_spark.py\n']"
""
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-2020 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101 . || true', 'black --check . || true', 'codespell || true', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt || pip install --editable . || true', 'pip install -r dev-requirements.txt', 'pip install .', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest tests/delorean_tests.py', 'pytest --doctest-modules', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
"['python -m pip install --upgrade pip\npip install setuptools>=30.3.0 tox\n', 'Invoke-WebRequest -Uri https://github.com/libusb/libusb/releases/download/v1.0.24/libusb-1.0.24.7z -OutFile libusb-1.0.24.7z\n7z e libusb-1.0.24.7z -oC:\\Windows\\System32 VS2019/MS64/dll/libusb-1.0.dll\n', 'python -c ""import sys; print(\'::set-output name=toxenv::py\' + sys.argv[1].replace(\'.\', \'\'))"" ${{ matrix.python-version }}\n', 'tox -e ${{ steps.pyver2toxenv.outputs.toxenv }}']"
"['python -m pip install --upgrade pip setuptools tox\n', 'python -m tox\n', 'python -m pip install --upgrade pip setuptools tox\n', 'python -m tox\n', 'python -m pip install --upgrade pip setuptools tox\n', 'sudo apt-get install aspell aspell-en\n', 'python -m tox\n', 'python -m pip install --upgrade pip setuptools\npython -m pip install -r docs/src/requirements.txt\n', 'git config user.name facelessuser\ngit config user.email ""${{ secrets.GH_EMAIL }}""\ngit remote add gh-token ""https://${{ secrets.GH_TOKEN }}@github.com/facelessuser/BracketHighlighter.git""\ngit fetch gh-token && git fetch gh-token gh-pages:gh-pages\npython -m mkdocs gh-deploy -v --clean --remote-name gh-token\ngit push gh-token gh-pages\n']"
"['sudo apt-get update -y\nsudo apt-get install -y imagemagick\npython -m pip install --upgrade pip\npip install --upgrade pytest pytest-xdist pytest-forked pytest-cov coveralls\n', 'pytest --skip-pdf --cov wand --forked --durations=20', 'export HOMEBREW_NO_AUTO_UPDATE=1\nbrew install imagemagick\npython -m pip install --upgrade pip\npip install --upgrade pytest pytest-xdist pytest-forked pytest-cov coveralls\n', 'pytest --skip-pdf --skip-fft --cov wand --forked --durations=20', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --service=github --finish\n']"
"['sudo systemctl start mysql.service\necho ""TEST_DB_USER=root"" >> $GITHUB_ENV\necho ""TEST_DB_PASSWORD=root"" >> $GITHUB_ENV\n', 'sudo systemctl start postgresql.service\nsudo -u postgres createuser --createdb $USER\n', 'python -m pip install --upgrade pip\npip install tox==3.25.1\n', 'tox -e ${{ matrix.name }}', 'python -m pip install --upgrade pip\npip install --upgrade build\n', 'python -m build']"
"['make install', 'make lint', 'make install', 'make test', 'docker-compose up -d', 'docker-compose images', 'make install', 'make e2e', 'make clean build']"
"['sudo dnf update -y\nsudo dnf install -y git\ngit config --global --add safe.directory ${GITHUB_WORKSPACE}\n', 'sudo dnf install -y python3-wxpython4 xorg-x11-server-Xvfb python3-pip psmisc\nsudo -H pip install wheel https://extras.wxpython.org/wxPython4/extras/linux/gtk3/fedora-38/wxPython-4.2.1-cp311-cp311-linux_x86_64.whl\nsudo -H pip install -r requirements-dev.txt\n', 'Xvfb &\nexport DISPLAY=:0\ninvoke test-ci &\n', 'pip install .\nxvfb-run --server-args=""-screen 0, 1280x720x24"" -a ride.py &\nsleep 10\nkillall xvfb-run\n', 'git fetch --prune --depth=1 --no-recurse-submodules\n', 'sudo apt update -y\nsudo apt install -y libsdl1.2debian libsdl2-2.0-0 libnotify4\nsudo pip install https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/wxPython-4.2.1-cp310-cp310-linux_x86_64.whl\nsudo pip install -r requirements-dev.txt\nXvfb &\nexport DISPLAY=:0\ninvoke test-ci\npip install .\nwhich ride.py\nxvfb-run --server-args=""-screen 0, 1280x720x24"" -a ride.py &\nsleep 10\nkillall xvfb-run\n', 'sudo apt update -y', 'sudo apt-get install libnotify4 libsdl1.2debian libsdl2.2 -y', 'sudo apt-get install -y xorg openbox', 'sudo apt-get install xvfb -y', 'pip install coverage invoke pytest', 'pip install https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/wxPython-4.2.0-cp310-cp310-linux_x86_64.whl', 'pip install -r requirements-dev.txt', 'Xvfb :1 -noreset &', 'pip install .', 'export DISPLAY=:1\ninvoke test-ci\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['pip install peewee flask wtforms wtf-peewee', 'python runtests.py']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\n', 'python -m pytest . -s\n', 'python -m pip install --upgrade pip\npython -m pip install bumpver\n', 'git config --global user.email ""actions@users.noreply.github.com""\ngit config --global user.name ""Automated Publisher""\n', 'bumpver update --no-fetch --verbose\n', 'git push\n', 'python -m pip install --upgrade pip\npip install -r requirements-maintenance.txt\n', 'python scripts/update_changelog.py', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python scripts/update_contributors.py']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\npip install -r example/requirements.txt\n', 'tox', 'isort .\nblack .\n', 'pip install coveralls\ncoveralls\n']"
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-2020 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade', 'bandit --recursive --skip B101,B102,B307,B404,B603,B607 .', 'black --check . || true', 'codespell', 'flake8 . --builtins=profile --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install --editable .', 'pip install numpy pylab-sdk', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'pip install --upgrade pip wheel', 'pip install pytest safety', 'pip install --editable .', 'pip install numpy pylab-sdk', 'make test', 'safety check']"
"['python -m pip install -U pip setuptools wheel\npip install --upgrade -r py.requirements/ci.github.testing.txt\npip install -e .\n', 'pytest', 'behave --format=progress3 features\nbehave --format=progress3 tools/test-features\nbehave --format=progress3 issue.features\n', 'python -m pip install -U pip setuptools wheel\npip install --upgrade -r py.requirements/ci.github.testing.txt\npip install -e .\n', 'pytest', 'behave --format=progress features\nbehave --format=progress tools/test-features\nbehave --format=progress issue.features\n']"
[]
"['python -m pip install --upgrade pip\npip install pytest wheel\npip install -e .\n', 'pokedex setup -v', 'pytest', 'pokedex dump\ngit --no-pager diff --exit-code pokedex/data/csv/\n']"
"['nix-shell --run true', './ci/check-nix-files.sh', 'nix-shell --run true', 'nix-build', 'nix-shell --run true', './ci/check-formatting.sh', 'nix-shell --run true', './ci/check-mypy.sh', 'nix-shell --run true', './ci/check-flake8.sh', 'nix-shell --run true', './ci/mypy-ratchet.sh', 'nix-shell --run true', './ci/check-tests.sh', 'nix-shell --run true', 'nix-shell --run ""nix build .#checks.x86_64-linux.doc --experimental-features \\""nix-command flakes\\""""', 'nix-build -A docs.x86_64-linux', 'nix-shell --run true', './ci/check-poetry.sh']"
"['export DISPLAY=:99.0\nXvfb -ac :99 -screen 0 1280x1024x16 > /dev/null 2>&1 &\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install robotstatuschecker>=1.4\npip install requests robotframework-pabot\n', 'pip install -U --pre robotframework==${{ matrix.rf-version }}\n', 'invoke gen-stub\n', 'which python\n', 'xvfb-run --auto-servernum python atest/run.py --nounit --zip headlesschrome\n', 'xvfb-run --auto-servernum python atest/run.py --zip chrome\n', 'xvfb-run --auto-servernum python atest/run.py --zip headlessfirefox\n', 'xvfb-run --auto-servernum python atest/run.py --zip firefox\n', 'wget --no-verbose --output-document=./selenium-server-standalone.jar http://selenium-release.storage.googleapis.com/3.141/selenium-server-standalone-3.141.59.jar\nsudo chmod u+x ./selenium-server-standalone.jar\nxvfb-run --auto-servernum python atest/run.py --zip headlesschrome  --grid True\n']"
"[""pip install -e '.[dev]'\n"", 'ruff --version\nruff . --ignore D\n', 'black --version\nblack --check --diff --color pymatgen\n', 'mypy --version\nrm -rf .mypy_cache\nmypy pymatgen\n', 'for pkg in cmd_line/*;\n  do echo ""$(pwd)/cmd_line/$pkg/Linux_64bit"" >> ""$GITHUB_PATH"";\ndone\n', ""python -m pip install --upgrade pip wheel\npython -m pip install numpy cython packaging\npython -m pip install -e '.[dev,optional]'\n"", 'pytest --cov=pymatgen --splits 10 --group ${{ matrix.split }} --durations-path test_files/.pytest-split-durations\n', 'python -m pip install coverage', 'coverage combine coverage*/.coverage*\ncoverage report\ncoverage xml\n', 'python -m pip install build', 'python -m build --sdist']"
""
"['mount | grep cgroup\n', 'docker-compose -f docker-compose.test.yml build cms_test\n', 'docker-compose -f docker-compose.test.yml run --rm cms_test\n']"
""
"['pip install -q black', 'black --check --diff --skip-string-normalization --target-version py33', 'pip install -q pylint', 'pylint *.py', 'pip install -q coveralls pytest-cov', 'pytest --cov', 'coveralls --service=github']"
"['sudo apt-get install -y locales language-pack-it', 'python -m pip install --upgrade pip\npip install -U --upgrade-strategy=eager --pre -e .[testsuite]\n', 'pytest -vv\n']"
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-2020 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101 . || true', 'black --check . || true', 'codespell || true', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt || pip install --editable . || true', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest . || pytest --doctest-modules .', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check']"
"['pip install build cmarkgfm pycodestyle twine', 'find . -name \'*\\.py\' -exec pycodestyle --ignore=E402,E501,E722,W503 ""{}"" \\+\n', 'python3 -m build\ntwine check dist/*\n', 'pip install pytest', 'script -e -c make test', 'make test.integration']"
"['sudo apt-get update -qq\nsudo apt-get install -qq \\\n  libxml2-dev libxslt1-dev gfortran libatlas-base-dev \\\n  libespeak1 libxcb-image0 libxcb-keysyms1 libxcb-render-util0 \\\n  libxkbcommon-x11-0 libxcb-icccm4 libxcb1 openssl \\\n  libxcb-randr0-dev libxcb-xtest0-dev libxcb-xinerama0-dev \\\n  libxcb-shape0-dev libxcb-xkb-dev xvfb \\\n  libopengl0 libegl1\n', 'wget \\\n  https://github.com/AppImage/AppImageKit/releases/download/12/appimagetool-x86_64.AppImage \\\n  -O $HOME/appimagetool-x86_64.AppImage\nchmod a+x $HOME/appimagetool-x86_64.AppImage\n', 'echo ""pip_cache_dir=$(pip cache dir)"" >> $GITHUB_ENV', '# Update pip.\npython -m pip install -U pip setuptools wheel\n\n# Install dependencies for tests.\npip install --progress-bar=off -U -r tests/requirements-tools.txt -r tests/requirements-libraries.txt\n# Compile bootloader\ncd bootloader\npython waf all\ncd ..\n\n# Install PyInstaller.\npip install --progress-bar=off .\n\n# Make sure the help options print.\npython -m pyinstaller -h\n', 'Xvfb :99 &\necho ""DISPLAY=:99"" >> $GITHUB_ENV\necho ""QT_DEBUG_PLUGINS=1"" >> $GITHUB_ENV\n', '# The ``run_tests`` script is invoked with the ``-c`` option to\n# specify a ``pytest.ini``, rather than allowing pytest to find\n# something unexpected in the filesystem (it searches from the root\n# dir all the way to the top of the filesystem per\n# https://docs.pytest.org/en/latest/customize.html).\npython -m PyInstaller.utils.run_tests -c PyInstaller/utils/pytest.ini --include_only=pyi_hooksample.\n', 'git fetch origin ${{ github.base_ref }}\ngit fetch origin ${{ github.ref }}\ngit checkout FETCH_HEAD --\n', 'pip install -U flake8 wheel setuptools', 'git diff -U0 origin/${{ github.base_ref }} -- | flake8 --diff -\n', 'python setup.py sdist bdist_wheel\npip install -v .\n', 'git fetch origin ${{ github.base_ref }}\ngit fetch origin ${{ github.ref }}\ngit checkout FETCH_HEAD --\n', 'git diff --name-status origin/${{ github.base_ref }}\ngit diff --name-status origin/${{ github.base_ref }}\\\n| python scripts/verify-news-fragments.py\n', 'pip install -q -U setuptools wheel towncrier', 'towncrier --draft', 'python -m pip install -U -e . setuptools wheel twine macholib', 'python setup.py sdist bdist_wheels\npython -m twine check dist/*\n']"
"['python -m pip install --upgrade pip\npip install redis==${{ matrix.redis-py-version }}\npip install -r requirements.txt -r dev-requirements.txt\npip install -e .\n', 'RUN_SLOW_TESTS_TOO=1 pytest --durations=5\n', 'python -m pip install --upgrade pip\npip install git+https://github.com/redis/redis-py\npip install git+https://github.com/pallets/click\npip install -r dev-requirements.txt\npip install -e .\n', 'RUN_SLOW_TESTS_TOO=1 pytest --durations=5 > log.txt 2>&1', 'if [[ ""$(curl --url https://api.github.com/repos/${{ github.repository }}/issues?creator=github-actions --request GET)"" != *""\\""""* ]]\n  then curl --request POST \\\n            --url https://api.github.com/repos/${{ github.repository }}/issues \\\n            --header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n            --header \'content-type: application/json\' \\\n            --data ""{\n                \\""title\\"": \\""RQ maybe may not work with dependencies in the future\\"",\n                \\""body\\"": \\""This issue was automatically created by the GitHub Action workflow **${{ github.workflow }}**. \\n\\n View log: \\n\\n \\`\\`\\` \\n $(cat log.txt | while read line; do echo -n ""$line\\n""; done | sed -r \'s/""/\\\\""/g\') \\n \\`\\`\\`\\""\n              }""\nfi\n', '# Parse Version\n# ""master"" -> ""master""\n# ""v1.2.3"" -> ""1.2.3"", ""1.2"", ""1"", ""latest""\n\nVERSIONS=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && {\n  VERSIONS=$(echo $VERSIONS | sed -e \'s/^v//\')\n  i=""$VERSIONS""\n  while [[ ""$i"" == *"".""* ]]\n    do i=""$(echo ""$i"" | sed \'s/\\(.*\\)\\..*/\\1/g\')""\n       VERSIONS=""$VERSIONS $i""\n  done\n  VERSIONS=""$VERSIONS latest""\n}\n\necho Building with tags: $VERSIONS\n\n# Login to registries\necho ""${{ secrets.GITHUB_TOKEN }}"" | docker login docker.pkg.github.com -u ${{ github.actor }} --password-stdin\necho ""${{ secrets.DOCKER_TOKEN }}"" | docker login -u selwin --password-stdin\n\n# Build image\ndocker build . --tag worker\n\n# Tag and Push\nfor VERSION in $VERSIONS\n  do docker tag worker redisqueue/worker:$VERSION\n     docker push redisqueue/worker:$VERSION\n     docker tag worker docker.pkg.github.com/rq/rq/worker:$VERSION\n     docker push docker.pkg.github.com/rq/rq/worker:$VERSION\ndone\n', 'python -m pip install --upgrade pip\npip install black ruff\n', 'black --check --skip-string-normalization --line-length 120 rq tests\n', '# stop the build if there are Python syntax errors.\nruff check --show-source rq tests\n', 'python -m pip install --upgrade pip\npip install redis==${{ matrix.redis-py-version }}\npip install -r requirements.txt -r dev-requirements.txt\npip install -e .\n', 'RUN_SLOW_TESTS_TOO=1 pytest --cov=rq --cov-config=.coveragerc --cov-report=xml --durations=5\n', 'python -m pip install --upgrade pip\npip install redis==${{ matrix.redis-py-version }}\npip install -r requirements.txt -r dev-requirements-36.txt\npip install -e .\n', 'RUN_SLOW_TESTS_TOO=1 pytest --cov=rq --cov-config=.coveragerc --cov-report=xml --durations=5\n']"
"['python -m pip install --upgrade pip\npip install tox>=2.0\ntox -e pep8\n', 'python -m pip install --upgrade pip\npip install tox>=2.0\nmake test\n', 'df -h\nsudo swapoff -a\nsudo rm -f /swapfile\nsudo apt -y clean\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'make docker/pull\n', 'mkdir -p apks\nmake docker/run/make/with-artifact/apk/${{ matrix.bootstrap.target }}\n', 'mv apks/${{ env.APK_ARTIFACT_FILENAME }} apks/${{ matrix.runs_on }}-${{ matrix.bootstrap.name }}-${{ env.APK_ARTIFACT_FILENAME }}\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\npython3 -m pip install -e .\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\npython3 pythonforandroid/prerequisites.py\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\nmake --file ci/makefiles/osx.mk\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\nmake ${{ matrix.bootstrap.target }}\n', 'mv testapps/on_device_unit_tests/${{ env.APK_ARTIFACT_FILENAME }} ${{ matrix.runs_on }}-${{ matrix.bootstrap.name }}-${{ env.APK_ARTIFACT_FILENAME }}\n', 'df -h\nsudo swapoff -a\nsudo rm -f /swapfile\nsudo apt -y clean\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'make docker/pull\n', 'mkdir -p aabs\nmake docker/run/make/with-artifact/aab/${{ matrix.bootstrap.target }}\n', 'mv aabs/${{ env.AAB_ARTIFACT_FILENAME }} aabs/${{ matrix.runs_on }}-${{ matrix.bootstrap.name}}-${{ env.AAB_ARTIFACT_FILENAME }}\n', 'df -h\nsudo swapoff -a\nsudo rm -f /swapfile\nsudo apt -y clean\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'make docker/pull\n', 'mkdir -p aars\nmake docker/run/make/with-artifact/aar/${{ matrix.bootstrap.target }}\n', 'mv aars/${{ env.AAR_ARTIFACT_FILENAME }} aars/${{ matrix.runs_on }}-${{ matrix.bootstrap.name}}-${{ env.AAR_ARTIFACT_FILENAME }}\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\npython3 -m pip install -e .\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\npython3 pythonforandroid/prerequisites.py\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\nmake --file ci/makefiles/osx.mk\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\nmake ${{ matrix.bootstrap.target }}\n', 'mv testapps/on_device_unit_tests/${{ env.AAB_ARTIFACT_FILENAME }} ${{ matrix.runs_on }}-${{ matrix.bootstrap.name }}-${{ env.AAB_ARTIFACT_FILENAME }}\n', 'df -h\nsudo swapoff -a\nsudo rm -f /swapfile\nsudo apt -y clean\ndocker rmi $(docker image ls -aq)\ndf -h\n', 'make docker/pull\n', 'make docker/run/make/rebuild_updated_recipes\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\npython3 -m pip install -e .\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\npython3 pythonforandroid/prerequisites.py\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\nmake --file ci/makefiles/osx.mk\n', 'source ci/osx_ci.sh\narm64_set_path_and_python_version 3.9.7\nmake rebuild_updated_recipes\n', 'python -m pip install --upgrade setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine check dist/*\n']"
"['python -m pip install --upgrade pip setuptools virtualenv wheel', 'python -m pip install --upgrade codecov tox', 'ENV_PREFIX=$(tr -C -d ""0-9"" <<< ""${{ matrix.python-version }}"")\nTOXENV=$(tox --listenvs | grep ""^py$ENV_PREFIX"" | tr \'\\n\' \',\') tox\n', 'tox -e lint\n', 'python -m pip install --upgrade pip setuptools virtualenv wheel', 'python -m pip install --upgrade codecov tox', 'ENV_PREFIX=$(tr -C -d ""0-9"" <<< ""${{ matrix.python-version }}"")\nTOXENV=$(tox --listenvs | grep ""^py$ENV_PREFIX"" | tr \'\\n\' \',\') tox\n', 'tox -e lint\n', 'pip install build twine\n', 'python -m build\ntwine upload dist/*\n']"
"['pip install gevent redis peewee ukt', 'python runtests.py -v 2']"
"['pip install -r requirements.txt -r dev-requirements.txt -e.', 'ckan -c test-core-cypress.ini db init\n', 'pip install flake8 pycodestyle', 'flake8 --count --statistics --show-source', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --extend-exclude=""""', 'pip install -r requirements.txt -r dev-requirements.txt -e.', 'npm ci', 'npx pyright', 'pip install -r requirements.txt -r dev-requirements.txt -e.', 'towncrier check >> $GITHUB_STEP_SUMMARY']"
"['python -m pip install --upgrade pip\npython -m pip install flake8\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox', 'python -m pip install coverage\n', 'coverage combine\ncoverage report --omit=""qrcode/tests/*"" --fail-under=98 -m\ncoverage report --include=""qrcode/tests/*"" --fail-under=100 -m\n']"
"['python -m pip install --upgrade pip', 'python -m pip install -r requirements-dev.txt -r requirements.txt', 'python -m pytest', 'scripts/test-sdist']"
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-2020 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B101,B105,B106,B107,B324 .', 'black --check . || true', 'codespell', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements-test.txt', 'pip install --editable .', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest', 'shopt -s globstar && pyupgrade --py37-plus **/*.py || true', 'safety check']"
"['pip install -U rstcheck doc8 sphinx', 'rstcheck -r docs', 'doc8 --ignore D001 docs', 'sudo apt-get install -y libldap2-dev libsasl2-dev', 'pip install .[test,extra,docs,lint]', './docker/rpms/build-and-install-rpms.sh rl8 docker/rpms/Rocky_Linux_8/Rocky_Linux_8.dockerfile\n', './docker/rpms/build-and-install-rpms.sh fc37 docker/rpms/Fedora_37/Fedora37.dockerfile\n', './docker/rpms/build-and-install-rpms.sh opensuse-leap docker/rpms/opensuse_leap/openSUSE_Leap153.dockerfile\n', './docker/rpms/build-and-install-rpms.sh opensuse-tumbleweed docker/rpms/opensuse_tumbleweed/openSUSE_TW.dockerfile\n', './docker/debs/build-and-install-debs.sh deb10 docker/debs/Debian_10/Debian10.dockerfile\n', './docker/debs/build-and-install-debs.sh deb11 docker/debs/Debian_11/Debian11.dockerfile\n', 'docker pull registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker run --privileged -t -d -v $PWD:/code --name cobbler registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker exec -u 0 -it cobbler bash -c ""./docker/develop/scripts/setup-supervisor.sh""\n', 'docker exec -u 0 -it cobbler bash -c ""pytest --cov=./cobbler --benchmark-only --benchmark-autosave tests/performance""\n', 'docker stop cobbler && docker rm cobbler', 'sudo apt-get -yq update', 'sudo apt-get -yq install wget pycodestyle pyflakes3 liblocale-gettext-perl lsb-release xz-utils bzip2 dpkg-dev tftpd-hpa rsync xorriso fence-agents fakeroot patch pax git hardlink apache2 apache2-dev python3-gunicorn systemd libsasl2-dev', 'pip install Sphinx coverage wheel', 'pip install .', 'python -m pip install build --user', 'python setup.py sdist bdist_wheel', 'sudo apt-get -yq update', 'sudo apt-get -yq install wget pycodestyle pyflakes3 liblocale-gettext-perl lsb-release xz-utils bzip2 dpkg-dev tftpd-hpa rsync xorriso fence-agents fakeroot patch pax git hardlink apache2 apache2-dev python3-gunicorn systemd libsasl2-dev', 'pip install Sphinx coverage wheel', 'echo ""new_version=$(date +\'%Y%m%d%H%M\')"" >> $GITHUB_ENV', 'sed -i \'/VERSION = ""[0-9].[0-9].[0-9]/s/.$/.\'${{ env.new_version }}\'""/g\' setup.py', 'pip install .', 'python -m pip install build --user', 'python setup.py sdist bdist_wheel', 'docker pull registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker run --privileged -t -d -v $PWD:/code --name cobbler registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker exec -u 0 -it cobbler bash -c ""./docker/develop/scripts/setup-supervisor.sh""\n', 'docker exec -u 0 -it cobbler bash -c ""make SYSTESTS=\'basic-*\' system-test""\n', 'docker stop cobbler && docker rm cobbler', 'docker pull registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker run --privileged -t -d -v $PWD:/code --name cobbler registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker exec -u 0 -it cobbler bash -c ""./docker/develop/scripts/setup-supervisor.sh""\n', 'docker exec -u 0 -it cobbler bash -c ""make SYSTESTS=\'import-*\' system-test""\n', 'docker stop cobbler && docker rm cobbler', 'docker pull registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker run --privileged -t -d -v $PWD:/code --name cobbler registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker exec -u 0 -it cobbler bash -c ""./docker/develop/scripts/setup-supervisor.sh""\n', 'docker exec -u 0 -it cobbler bash -c ""make SYSTESTS=\'settings-cli-*\' system-test""\n', 'docker stop cobbler && docker rm cobbler', 'docker pull registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker run --privileged -t -d -v $PWD:/code --name cobbler registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker exec -u 0 -it cobbler bash -c ""./docker/develop/scripts/setup-supervisor.sh""\n', 'docker exec -u 0 -it cobbler bash -c ""make SYSTESTS=\'svc-*\' system-test""\n', 'docker stop cobbler && docker rm cobbler', 'docker pull registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker run --privileged -t -d -v $PWD:/code --name cobbler registry.opensuse.org/home/cobbler-project/github-ci/containers/cobbler-test-github:main', 'docker exec -u 0 -it cobbler bash -c ""./docker/develop/scripts/setup-supervisor.sh""\n', 'docker exec -u 0 -it cobbler bash -c ""pytest --cov=./cobbler --benchmark-skip && git config --global --add safe.directory /code && coverage xml && codecov --token=1064928c-6477-41be-9ac2-7ce5e6d1fd8b --commit=${GITHUB_SHA}""\n', 'docker stop cobbler && docker rm cobbler']"
"['python -V\npython -m pip install build\n\npython -m build .\n\n# Ensure exactly one artifact was produced.\n[[ $(shopt -s nullglob; ls dist/*.tar.gz | wc -w) == 1 ]] || {\n  echo ""Unexpected content in dist dir: \'$(ls dist/*.tar.gz)\'.""\n  exit 1\n}\n', ""# Install some libyaml headers.\n# TODO Should we smoke test the sdist against the libyaml we built?\nsudo apt update\nsudo apt install libyaml-dev -y\n\n# Ensure Cython is not present so we use only what's in the sdist.\npython -m pip uninstall Cython -y || true\n\n# Pass no extra args.\n# We should auto-install with libyaml since it's present.\npython -m pip install dist/*.tar.gz -v\n\npython packaging/build/smoketest.py\n"", 'docker run --rm --volume ""$(pwd):/io"" --env LIBYAML_REF --env LIBYAML_REPO --workdir /io ""$DOCKER_IMAGE"" /io/packaging/build/libyaml.sh\n', 'sudo chmod -R a+r ./libyaml/\n', 'exit 1', 'set -eux\npython3 -V\npython3 -m pip install -U --user cibuildwheel\npython3 -m cibuildwheel --platform auto --output-dir dist .\n', 'set -eux\nbrew install automake coreutils m4\nbash ./packaging/build/libyaml.sh\necho ""finished artifact arch is $(lipo -archs libyaml/src/.libs/libyaml.a)""\n', 'exit 1', 'python3 -V\npython3 -m pip install -U --user cibuildwheel\npython3 -m cibuildwheel --platform auto --output-dir dist .\n', '# git spews all over stderr unless we tell it not to\nexport GIT_REDIRECT_STDERR=""2>&1""\n\nif [[ ! -d ./libyaml ]]; then\n  git clone -b ${{ env.LIBYAML_REF }} ${{ env.LIBYAML_REPO }} 2>&1\nfi\n\npushd libyaml\ngit clean -fdx\npopd\n\nmkdir libyaml/build\n\npushd libyaml/build\ncmake.exe -G ""Visual Studio 16 2019"" -A ${{ matrix.arch }} -DYAML_STATIC_LIB_NAME=yaml ..\ncmake.exe --build . --config Release\npopd\n', 'git config --global core.autocrlf false\ngit config --global core.eol lf\n', 'exit 1', 'set -eux\npython -V\npython -m pip install Cython wheel\n\npython setup.py \\\n--with-libyaml build_ext \\\n-I libyaml/include \\\n-L libyaml/build/Release \\\n-D YAML_DECLARE_STATIC \\\nbuild bdist_wheel\n\n# run tests on built wheel\npython -m pip install dist/*.whl\npython tests/lib/test_all.py\n', 'python -V\npython -m pip install build\n\npython -m build .\n\n# Ensure exactly one artifact was produced.\n[[ $(shopt -s nullglob; ls dist/*.tar.gz | wc -w) == 1 ]] || {\n  echo ""Unexpected content in dist dir: \'$(ls dist/*.tar.gz)\'.""\n  exit 1\n}\n', ""# Install some libyaml headers.\n# TODO Should we smoke test the sdist against the libyaml we built?\nsudo apt update\nsudo apt install libyaml-dev -y\n\n# Ensure Cython is not present so we use only what's in the sdist.\npython -m pip uninstall Cython -y || true\n\n# Pass no extra args.\n# We should auto-install with libyaml since it's present.\npython -m pip install dist/*.tar.gz -v\n\npython packaging/build/smoketest.py\n"", 'docker run --rm --volume ""$(pwd):/io"" --env LIBYAML_REF --env LIBYAML_REPO --workdir /io ""$DOCKER_IMAGE"" /io/packaging/build/libyaml.sh\n', 'sudo chmod -R a+r ./libyaml/\n', 'exit 1', 'set -eux\npython3 -V\npython3 -m pip install -U --user cibuildwheel\npython3 -m cibuildwheel --platform auto --output-dir dist .\n', 'set -eux\nbrew install automake coreutils m4\nbash ./packaging/build/libyaml.sh\necho ""finished artifact arch is $(lipo -archs libyaml/src/.libs/libyaml.a)""\n', 'exit 1', 'python3 -V\npython3 -m pip install -U --user cibuildwheel\npython3 -m cibuildwheel --platform auto --output-dir dist .\n', '# git spews all over stderr unless we tell it not to\nexport GIT_REDIRECT_STDERR=""2>&1""\n\nif [[ ! -d ./libyaml ]]; then\n  git clone -b ${{ env.LIBYAML_REF }} ${{ env.LIBYAML_REPO }} 2>&1\nfi\n\npushd libyaml\ngit clean -fdx\npopd\n\nmkdir libyaml/build\n\npushd libyaml/build\ncmake.exe -G ""Visual Studio 16 2019"" -A ${{ matrix.arch }} -DYAML_STATIC_LIB_NAME=yaml ..\ncmake.exe --build . --config Release\npopd\n', 'git config --global core.autocrlf false\ngit config --global core.eol lf\n', 'exit 1', 'set -eux\npython -V\npython -m pip install Cython wheel\n\npython setup.py \\\n--with-libyaml build_ext \\\n-I libyaml/include \\\n-L libyaml/build/Release \\\n-D YAML_DECLARE_STATIC \\\nbuild bdist_wheel\n\n# run tests on built wheel\npython -m pip install dist/*.whl\npython tests/lib/test_all.py\n']"
"['python -m pip install -U pip\npython -m pip install -U coveralls coverage[toml] tox tox-gh-actions\n', 'python -m tox', 'python -m coverage combine\npython -m coverage xml -i\npython -m coveralls --service=github\n', 'python -m pip install -U pip\npython -m pip install -U coveralls\npython -m coveralls --finish\n', 'python -m pip install -U pip\npython -m pip install tox\n', 'python -m tox -e lint', 'python -m pip install -U pip\npython -m pip install -U build\npython -m build .\n']"
"['python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'pip install --no-binary :all: .', 'pip install --upgrade build twine', 'python -m build', 'twine check dist/*', 'tar xfvz dist/versioneer-*.tar.gz', 'python -c ""import sys; print(sys.version)""', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'cd versioneer-* && tox']"
"['echo ""C:\\msys64\\usr\\bin"" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append\nrm C:\\msys64\\usr\\bin\\bash.exe\n', 'git config --global user.name copier-ci', 'git config --global user.email copier@copier', 'git config --global core.autocrlf input', 'echo ""PY=$((python -VV; pip freeze) | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'python -m pip install poetry poetry-dynamic-versioning\npoetry install --with dev,docs -v\n', 'poetry run poe test --cov=./ --cov-report=xml -ra .', 'copier --version', 'nix flake check -L', 'echo ""PY=$((python -VV; pip freeze) | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'python -m pip install poetry poetry-dynamic-versioning\n', 'poetry build\n']"
['make -C t']
"['python -m pip install --upgrade pip\npip install tox>=2.0\ntox -e pep8\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nbrew install libjpeg\npip3 install wheel\npip3 install -r requirements.txt\nbrew install autoconf automake libtool pkg-config\nbrew link libtool\npip3 install Cython==0.29.33\nsudo gem install xcpretty\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython setup.py install\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ntoolchain build python3 kivy\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\n.ci/test_project.sh\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython -m venv venv\n. venv/bin/activate\nbrew install libjpeg\npip install wheel\npip install -r requirements.txt\npip install sh\nbrew install autoconf automake libtool pkg-config\nbrew link libtool\npip install Cython==0.29.33\nsudo gem install xcpretty\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython setup.py install\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\n. venv/bin/activate\ntoolchain build python3 kivy\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\n. venv/bin/activate\n.ci/test_project.sh\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nbrew install libjpeg\npip3 install wheel\npip3 install -r requirements.txt\nbrew install autoconf automake libtool pkg-config\nbrew link libtool\npip3 install Cython==0.29.33\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython setup.py install\n', 'source .ci/utils.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython3 .ci/rebuild_updated_recipes.py\n', 'python -m pip install --upgrade setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine check dist/*\n', 'pip install --upgrade setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'twine check dist/*\n', 'python -m venv venv\n. venv/bin/activate\npip install dist/kivy-ios-*.tar.gz\npip install Cython==0.29.33\nbrew install autoconf automake libtool pkg-config\n', '. venv/bin/activate\ntoolchain --help\ntoolchain recipes\n']"
""
"['python -VV', 'pip3 install poetry\npoetry config virtualenvs.in-project true\n', 'poetry install', 'poetry run python setup_binary.py download_pandoc', 'poetry run python tests.py', 'python -m pip install -U pip wheel setuptools', 'python setup.py sdist bdist_wheel', 'rm pyproject.toml']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/', 'TAG_ARRAY=\'[\'\n\nif [[ $GITHUB_REF == refs/tags/* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/v}\n  TAG_ARRAY=""$TAG_ARRAY { \\""target\\"": \\""minimal\\"", \\""tag\\"": \\""${VERSION}\\"" },""\n  TAG_ARRAY=""$TAG_ARRAY { \\""target\\"": \\""full\\"", \\""tag\\"": \\""${VERSION}-full\\"" },""\n\nelif [[ $GITHUB_REF == refs/heads/develop ]]; then\n  TAG_ARRAY=""$TAG_ARRAY { \\""target\\"": \\""dev\\"", \\""tag\\"": \\""dev\\"" },""\n\nelse\n  TAG_ARRAY=""$TAG_ARRAY { \\""target\\"": \\""minimal\\"", \\""tag\\"": \\""latest\\"" },""\n  TAG_ARRAY=""$TAG_ARRAY { \\""target\\"": \\""full\\"", \\""tag\\"": \\""latest-full\\"" },""\nfi\n\nTAG_ARRAY=""${TAG_ARRAY::-1} ]""\n\necho ""Tags to build: $TAG_ARRAY""\necho ""::set-output name=tags::$TAG_ARRAY""\n', 'python -m pip install --upgrade pip\npip install flake8\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# Stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude=.git,./docs,./glances/outputs/static\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --exclude=.git,./docs,./glances/outputs/static\n', 'if [ ""${{ matrix.python-version }}"" == ""2.7"" ];\nthen\n  echo ""Skipping static type check for Python 2.7"";\nelse\n  echo ""Skipping static type check for the moment, too much error..."";\n  # pip install pyright\n  # pyright glances\nfi\n', 'python ./unitest.py\n', 'python -m pip install --upgrade pip\nif (Test-Path -PathType Leaf ""requirements.txt"") { python -m pip install -r requirements.txt }\npython setup.py install\n', 'python ./unitest.py\n']"
"['docker build -t ultisnips:${{ matrix.tag }} --build-arg PYTHON_IMAGE=${{ matrix.python_image }} --build-arg VIM_VERSION=${{ matrix.vim_version }} .', 'docker run --rm -t ultisnips:${{ matrix.tag }} docker/run_tests.sh']"
"['pip install -U pip', 'pip install -U bandit mypy pyupgrade safety tox setuptools', 'bandit --recursive --skip B105,B110,B311,B605,B607 --exclude ./.tox .', 'tox -e lint', 'tox -e py', 'mypy --install-types --ignore-missing-imports . || true', 'shopt -s globstar && pyupgrade --py3-only **/*.py', 'safety check -i 42559', 'tox -e build', 'tox -e doc', 'pip install tox', 'tox -e lint', 'tox -e py', 'tox -e build']"
"['echo >>$GITHUB_OUTPUT noxenvs=$(\n  nox --list-sessions |\n  grep \'^* \' |\n  cut -d \' \' -f 2- |\n  jq --raw-input --slurp \'split(""\\n"") | map(select(. != """"))\'\n)\n', 'sudo apt-get update && sudo apt-get install -y libenchant-2-dev', 'brew install enchant', 'echo ""PYTHONUTF8=1"" >> $env:GITHUB_ENV', 'nox -s ""${{ matrix.noxenv }}"" -- ${{ matrix.posargs }}', 'python -m pip install build', 'python -m build .']"
"['python -m pip install cffconvert\ncffconvert --validate\ncffconvert -f bibtex\ncffconvert -f apalike\n', 'python -m pip install --upgrade pip\npython -m pip install build twine\npython -m build --sdist\ntwine check --strict dist/*\n', 'cp ci/wheelbuilder/LICENSE_GEOS .\n', 'cp ci/wheelbuilder/LICENSE_win32 .\n', 'export PKG=$(ls dist/ | grep tar)\nset -- $PKG\necho ""name=$1"" >> $GITHUB_ENV\n', ""echo 'GEOS_INSTALL=${{ github.workspace }}\\geosinstall\\geos-${{ matrix.geos }}' >> $GITHUB_ENV\necho 'GEOS_BUILD=${{ github.workspace }}\\geosbuild' >> $GITHUB_ENV\n"", 'echo ""GEOS_VERSION_SPEC=$(git rev-parse HEAD)"" >> $GITHUB_ENV\n', 'bash ci/install_geos.sh\n', 'python -m pip install --disable-pip-version-check --upgrade pip\npip install --upgrade wheel setuptools\nif [ -z ""${{ matrix.numpy }}"" ]; then\n  pip install --upgrade --pre Cython numpy pytest pytest-cov coveralls;\nelse\n  pip install --upgrade Cython numpy==${{ matrix.numpy }} pytest pytest-cov coveralls;\nfi\nif [ -n ""${{ matrix.matplotlib }}"" ]; then\n  pip install matplotlib\nfi\npip list\n', 'echo ""${{ env.GEOS_INSTALL }}/bin"" >> $GITHUB_PATH\necho ""LD_LIBRARY_PATH=${{ env.GEOS_INSTALL }}/lib"" >> $GITHUB_ENV\n', 'echo ""${{ env.GEOS_INSTALL }}/bin"" >> $GITHUB_PATH\necho ""LDFLAGS=-Wl,-rpath,${{ env.GEOS_INSTALL }}/lib"" >> $GITHUB_ENV\n', ""cp geosinstall/geos-${{ matrix.geos }}/bin/*.dll shapely\necho 'GEOS_LIBRARY_PATH=${{ env.GEOS_INSTALL }}\\lib' >> $GITHUB_ENV\necho 'GEOS_INCLUDE_PATH=${{ env.GEOS_INSTALL }}\\include' >> $GITHUB_ENV\n"", 'pip install -e .\n', 'pip list', 'python -c ""import shapely; print(f\'GEOS version: {shapely.geos_version_string}\')""\npytest shapely/tests -r a --cov --cov-report term-missing ${{ matrix.extra_pytest_args }}\n', 'python -m pytest --doctest-modules docs/manual.rst\n', 'pytest --doctest-modules shapely --ignore=shapely/tests\n', 'coveralls --service=github || echo ""!! intermittent coveralls failure""\n', 'pip3 install --upgrade coveralls\ncoveralls --finish || echo ""!! intermittent coveralls failure""\n']"
"['pip install -e . -r requirements.txt', 'codespell', 'pylint --recursive=y examples pymodbus test', 'pre-commit run --all-files', 'make -C doc/ html', 'mypy pymodbus', 'pip install -e . -r requirements.txt', 'pytest --cov=pymodbus --cov=test --cov-report=term-missing --cov-report=xml -v --full-trace --timeout=20', 'ls', 'cosign version']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'python -m pip install --upgrade pip\npip install setuptools wheel twine typing_extensions\n', 'python setup.py sdist bdist_wheel\npython -m twine upload dist/*\n']"
"['python -m pip install --upgrade -r pip-requirements.txt\npip install tox tox-gh-actions coveralls\nbash ryu/tests/integrated/common/install_docker_test_pkg_for_github_actions.sh\n', 'NOSE_VERBOSE=0 tox']"
"['pip install --upgrade ""tox<4"" tox-gh-actions setuptools\npip list\n', 'python misc/build_helpers/show-tool-versions.py', 'python -m tox', 'pip install twisted pywin32\npython -m tox | python misc/windows-enospc/passthrough.py\n', 'pip3 install --upgrade coveralls==3.0.1\npython3 -m coveralls\n', 'pip3 install --upgrade coveralls==3.0.1\npython3 -m coveralls --finish\n', 'sudo apt install tor', 'brew install tor\n', 'pip install --upgrade ""tox<4""\npip list\n', 'python misc/build_helpers/show-tool-versions.py', 'tox -e integration\n', 'tox -e integration -- --force-foolscap integration/\n', 'pip install --upgrade ""tox<4""\npip list\n', 'python misc/build_helpers/show-tool-versions.py', 'tox -e pyinstaller', 'dist/Tahoe-LAFS/tahoe --version']"
"['pip install tox', 'tox', 'tox -e py', 'pip install tox', 'tox -e py310,coverage', 'pip install tox', 'tox -e docs', 'pip install tox', 'tox -e lint']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'cd tests\n./testserver.sh &\n./run_tests.sh']"
"['python -m pip install -r ci/rstcheck/requirements.txt\n', 'rstcheck -r --ignore-directives automodule --ignore-substitutions version,release,today .\n', 'apt-get update\napt-get -y install software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\napt-get update\n', 'apt-get install -y --no-install-recommends \\\n  python3.10 \\\n  python3.10-dev \\\n  python3.10-venv \\\n  python3-pip \\\n  g++\n', 'apt-get update\napt-get install -qq \\\n  libcurl4-gnutls-dev \\\n  libgeos-dev \\\n  libjpeg-dev \\\n  libnetcdf-dev \\\n  libhdf4-alt-dev \\\n  libhdf5-serial-dev \\\n  libssl-dev \\\n  libsqlite3-dev \\\n  libexpat-dev \\\n  libxerces-c-dev \\\n  libpng-dev \\\n  libopenjp2-7-dev \\\n  libzstd-dev \\\n  libwebp-dev \\\n  cmake \\\n  curl \\\n  git\nbash ci/gdal-compile.sh git\n', 'export PATH=""${GDAL_DIR}/bin/:${PATH}""\npython3.10 -m venv testenv\n. testenv/bin/activate\npython -m pip install --upgrade pip\npython -m pip wheel -r requirements-dev.txt\npython -m pip install -r requirements-dev.txt\npython setup.py clean\npython -m pip install --no-deps --force-reinstall -e .[test]\n', 'export PATH=""${GDAL_DIR}/bin/:${PATH}""\n. testenv/bin/activate\npython -m pytest -v -m ""not wheel or gdal"" -rxXs --cov fiona --cov-report term-missing\n', 'apt-get update\napt-get -y install software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\napt-get update\n', 'apt-get install -y --no-install-recommends \\\n  python${{ matrix.python-version }} \\\n  python${{ matrix.python-version }}-dev \\\n  python${{ matrix.python-version }}-venv \\\n  python3-pip \\\n  g++\n', 'python${{ matrix.python-version }} -m venv testenv\n. testenv/bin/activate\npython -m pip install --upgrade pip\npython -m pip wheel -r requirements-dev.txt\npython -m pip install -r requirements-dev.txt\npython setup.py clean\npython -m pip install --no-deps --force-reinstall -e .[test]\n', '. testenv/bin/activate\npython -m pytest -v -m ""not wheel or gdal"" -rxXs --cov fiona --cov-report term-missing\n', 'conda config --prepend channels conda-forge\nconda config --set channel_priority strict\nconda create -n test python=${{ matrix.python-version }} libgdal geos=3.10.3 cython=0.29\nsource activate test\npython -m pip install -e . || python -m pip install -e .\npython -m pip install -r requirements-dev.txt\n', 'conda config --prepend channels conda-forge\nconda config --set channel_priority strict\nconda create -n test python=${{ matrix.python-version }} libgdal geos=3.10.3 cython=0.29\nsource activate test\nGDAL_VERSION=""3.5"" python setup.py build_ext -I""C:\\\\Miniconda\\\\envs\\\\test\\\\Library\\\\include"" -lgdal_i -L""C:\\\\Miniconda\\\\envs\\\\test\\\\Library\\\\lib"" install\npython -m pip install -r requirements-dev.txt\n', 'source activate test\npython -V\nconda info\nconda list\n', 'source activate test\npytest -v -m ""not wheel"" -rxXs --cov fiona --cov-report term-missing\n', 'source activate test\npython -m pytest -v -m ""not wheel"" -rxXs  --cov fiona --cov-report term-missing\n']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npython -m pip install --upgrade -r dev-requirements.txt\npython -m pip install -e .\n', 'make quality', 'make test_norecord', 'coveralls --service=github', 'pip install --upgrade coveralls\ncoveralls --finish\n', 'python -m pip install --upgrade pip\npython -m pip install build --user\n', 'python -m build --sdist --wheel --outdir dist/ .\n']"
"['pip install --user ruff', 'ruff --format=github .', 'python setup.py sdist\nversion=""$(cat nose2/__init__.py | grep \'^__version__\' |  cut -d \'""\' -f2)""\ncd dist\ntar -xzf ""nose2-${version}.tar.gz""\ncd ""nose2-${version}""\npip install -e \'.[dev]\'\nnose2 -v --pretty-assert\n', 'python -m pip install -U tox', 'python -m tox -e py', 'python -m pip install tox', 'make build', 'git clone ""https://gitlab.com/mailman/mailman/"" .', 'python -m pip install tox', 'tox -e py-nocov --devenv testenv', 'source testenv/bin/activate\npip install --force-reinstall -v ./nose2-build/*.${{ matrix.pkg-extension }}\n', 'source testenv/bin/activate\npython -m nose2 -v\n']"
"['sudo apt update\nsudo apt install latexmk texlive-xetex\n', 'pip install --upgrade --force-reinstall setuptools pip\nif [[ ${{matrix.python-version}} != ""3.12-dev"" ]]\nthen\n  pip install --upgrade .[develop,gmpy,docs]\nelse\n  pip install --upgrade pytest\n  pip install --upgrade .[docs]\nfi\npip install --upgrade pytest-xdist\n', 'python -We:invalid -m compileall -f mpmath -q\nflake518\n', 'pytest\nMPMATH_STRICT=Y pytest mpmath/tests/test_basic_ops.py\n', 'pip uninstall -y gmpy2', 'pytest\nMPMATH_STRICT=Y pytest mpmath/tests/test_basic_ops.py\ncoverage html\ncoverage xml\n', 'sphinx-build --color -W --keep-going -b html docs build/sphinx/html\nsphinx-build --color -W --keep-going -b latex docs build/sphinx/latex\nmake -C build/sphinx/latex all-pdf\n', 'python -m build']"
""
""
"['pip install mypy', 'mypy --strict certifi', 'python -m pip install --upgrade pip\npython -m pip install pytest\n', ""python -W error -W 'ignore:Running attrs on Python 3.6' -m pytest\n""]"
"['gh pr merge --auto --rebase ""$PR_URL""', 'gh pr comment ""$PR_URL"" -b ""Looks like a major version upgrade! Skipping auto-merge.""', 'git checkout HEAD^2']"
"['pip install tox flake8', 'tox -e pep8', 'pip install tox ""django==${{ matrix.django }}""', 'tox -e py3.10-django${{ matrix.django }}']"
"['python -m pip install --upgrade pip\npip install setuptools wheel\npython setup.py sdist\n', 'python -m pip install cibuildwheel', 'python -m cibuildwheel --output-dir wheelhouse', 'echo ""::set-output name=dir::$(pip cache dir)""', 'echo ""PY=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_ENV', 'pip install -r requirements-dev.txt', 'pip install .', 'py.test src/geventhttpclient/tests']"
"['set -x -e\n\necho ""github event ref is ${{ github.ref }}""\n\nif [ ""x${{ startsWith(github.ref, \'refs/tags/v\') }}"" == ""xtrue"" ]\nthen\n  echo ""Trigger was a version tag - ${{ github.ref }}""\n  echo ::set-output name=q_version::${GITHUB_REF#refs/tags/v}\n  echo ::set-output name=is_release::true\nelse\n  # For testing version propagation inside the PR\n  echo ""Either branch of a non-version tag - setting version to 0.0.0""\n  echo ::set-output name=q_version::0.0.0\n  echo ::set-output name=is_release::false\nfi\n', 'set -e -x\n\necho ""outputs: ${{ toJson(needs.version_info) }}""\n', 'set -x -e\ngem install ronn\n\nronn doc/USAGE.markdown\n# Must be gzipped, otherwise debian does not install it\ngzip doc/USAGE\n', 'set -e -x\n\nsudo apt-get update\nsudo apt-get install -y zip sqlite3 rpm\n\ncurl -o pyoxidizer.zip -L ""https://github.com/indygreg/PyOxidizer/releases/download/pyoxidizer%2F0.17/pyoxidizer-0.17.0-linux_x86_64.zip""\nunzip pyoxidizer.zip\nchmod +x ./pyoxidizer\n', 'set -e -x\n\n./pyoxidizer build --release\n\nexport Q_EXECUTABLE=./build/x86_64-unknown-linux-gnu/release/install/q\nchmod 755 $Q_EXECUTABLE\n\nseq 1 100 | $Q_EXECUTABLE -c 1 ""select sum(c1),count(*) from -"" -S test.sqlite\n\nmkdir -p packages/linux/\ncp $Q_EXECUTABLE packages/linux/linux-q\n', 'set -e -x\n\npip3 install -r test-requirements.txt\n', 'set -x -e\n\nfind ./ -ls\n\nchmod 755 ./linux-q\n\nQ_EXECUTABLE=`pwd`/linux-q Q_SKIP_EXECUTABLE_VALIDATION=true ./run-tests.sh -v\n', 'set -e -x\n\nmkdir -p packages/linux/\n\nfind ./ -ls\n\nchmod 755 ./linux-q\n\nexport q_version=${{ needs.version_info.outputs.q_version }}\n\ngem install fpm\ncp dist/fpm-config ~/.fpm\nfpm -s dir -t deb --deb-use-file-permissions -p packages/linux/q-text-as-data-${q_version}-1.x86_64.deb --version ${q_version} ./linux-q=/usr/bin/q USAGE.gz=/usr/share/man/man1/q.1.gz\n', 'set -e -x\n\npip3 install -r test-requirements.txt\n', './dist/test-using-deb.sh ./q-text-as-data-${{ needs.version_info.outputs.q_version }}-1.x86_64.deb', 'set -e -x\n\nmkdir -p packages/linux\n\n\nchmod 755 ./linux-q\n\nexport q_version=${{ needs.version_info.outputs.q_version }}\n\ngem install fpm\ncp dist/fpm-config ~/.fpm\nfpm -s dir -t rpm --rpm-use-file-permissions -p packages/linux/q-text-as-data-${q_version}.x86_64.rpm --version ${q_version} ./linux-q=/usr/bin/q USAGE.gz=/usr/share/man/man1/q.1.gz\n', './dist/test-using-rpm.sh ./q-text-as-data-${{ needs.version_info.outputs.q_version }}.x86_64.rpm', 'set -e -x\n\ncurl -o  pyoxidizer.zip -L ""https://github.com/indygreg/PyOxidizer/releases/download/pyoxidizer%2F0.17/pyoxidizer-0.17.0-macos-universal.zip""\nunzip pyoxidizer.zip\nmv macos-universal/pyoxidizer ./pyoxidizer\n\nchmod +x ./pyoxidizer\n', 'set -e -x\n\n./pyoxidizer build --release\n\nexport Q_EXECUTABLE=./build/x86_64-apple-darwin/release/install/q\nchmod 755 $Q_EXECUTABLE\n\nseq 1 100 | $Q_EXECUTABLE -c 1 ""select sum(c1),count(*) from -"" -S test.sqlite\n\nmkdir -p packages/macos/\ncp $Q_EXECUTABLE packages/macos/macos-q\n', 'set -e -x\n\npip3 install wheel\n\npip3 install -r test-requirements.txt\n', 'set -e -x\n\nchmod 755 ./macos-q\n\nQ_EXECUTABLE=`pwd`/macos-q Q_SKIP_EXECUTABLE_VALIDATION=true ./run-tests.sh -v\n', 'echo ""homebrew mac cannot be packaged from the source code itself, due to the package build process of homebrew. See https://github.com/harelba/homebrew-q""\n', 'echo ""homebrew mac packaging cannot be tested here, due to the package build process of homebrew. See https://github.com/harelba/homebrew-q""\n', 'set -x -e\n\npython3 -V\npip3 -V\n\npip3 install pyoxidizer\n', 'set -e -x\n\npyoxidizer build --release --var Q_VERSION ${{ needs.version_info.outputs.q_version }}\n\nexport Q_EXECUTABLE=./build/x86_64-pc-windows-msvc/release/install/q\nchmod 755 $Q_EXECUTABLE\n\nseq 1 100 | $Q_EXECUTABLE -c 1 ""select sum(c1),count(*) from -"" -S test.sqlite\n\nmkdir -p packages/windows/\ncp $Q_EXECUTABLE packages/windows/win-q.exe\n\nfind ./ -ls\n', 'echo ""Tests are not compatible with Windows (path separators, tmp folder names etc.). Only a sanity wil be tested""\n\nchmod +x ./win-q.exe\n\nseq 1 10000 | ./win-q.exe -c 1 ""select sum(c1),count(*) from -"" -S some-db.sqlite\n', 'set -x -e\n\npython3 -V\npip3 -V\n\npip3 install pyoxidizer\n', 'set -e -x\n\npyoxidizer build --release msi_installer --var Q_VERSION ${{ needs.version_info.outputs.q_version }}\n\nexport Q_MSI=./build/x86_64-pc-windows-msvc/release/msi_installer/q-text-as-data-${{ needs.version_info.outputs.q_version }}.msi\nchmod 755 $Q_MSI\n\nmkdir -p packages/windows/\ncp $Q_MSI packages/windows/q-text-as-data-${{ needs.version_info.outputs.q_version }}.msi\n', '$process = Start-Process msiexec.exe -ArgumentList ""/i q-text-as-data-${{ needs.version_info.outputs.q_version }}.msi -l* msi-install.log /norestart /quiet"" -PassThru -Wait\n$process.ExitCode\ngc msi-install.log\n\nexit $process.ExitCode\n', '$process = Start-Process msiexec.exe -ArgumentList ""/u q-text-as-data-${{ needs.version_info.outputs.q_version }}.msi /norestart /quiet"" -PassThru -Wait\n$process.ExitCode\nexit $process.ExitCode\n', 'set -e -x\n\necho ""Workflow finished at $(date)"" >> artifacts/workflow-finish-time.txt\n']"
"['pip install poetry\n', 'poetry config virtualenvs.in-project true\npoetry install\n', 'poetry run python -m flake8 sh.py tests/*.py\npoetry run black --check --diff sh.py tests/*.py\npoetry run rstcheck README.rst\npoetry run mypy sh.py\n', 'pip install poetry\n', 'poetry config virtualenvs.in-project true\npoetry install\n', 'SH_TESTS_RUNNING=1 SH_TESTS_USE_SELECT=${{ matrix.use-select }} LANG=${{ matrix.lang }} poetry run coverage run -a -m unittest\n', 'pip install coverage coveralls\n', 'find coverage-artifacts -name .coverage | xargs coverage combine -a\n', 'coverage report\ncoveralls --service=github\n', 'echo ""::set-output name=version::$(sed -n \'s/^version = ""\\(.*\\)""/\\1/p\' pyproject.toml)""', 'git tag ""${{steps.get_version.outputs.version}}"" ""${{github.ref_name}}""\ngit push -f origin ""${{steps.get_version.outputs.version}}""\n', 'pip install build', 'python -m build']"
"['pip install -r requirements/ci.txt\n', 'pip install .\n', 'pytest tests/bench.py --benchmark-json output.json\n', 'python -m pip install --upgrade pip\n', 'pip install -r requirements/ci.txt\n', 'pip install -r requirements/ci-3.11.txt\n', 'pip uninstall pycryptodome -y\n', 'pip install .\n', 'python -m coverage run --parallel-mode -m pytest tests -vv\n', 'mypy pypdf --show-error-codes --disallow-untyped-defs --disallow-incomplete-defs\n', 'python -m pip install --upgrade pip\n', 'pip install -r requirements/ci-3.11.txt\n', 'pip install .\n', 'ruff .\n', 'python -m pip install flit check-wheel-contents', 'flit build', 'ls -l dist', 'check-wheel-contents dist/*.whl', 'python -m pip install .', 'python -c ""import pypdf;print(pypdf.__version__)""', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine\npython -m coverage xml\n']"
""
"['pip install -r requirements_build.txt\npython setup.py install\n', 'cd doc\npip install -r requirements.txt\nmake html\n', 'PY_TAG=${{ matrix.python_version }}\nPY_TAG=""${PY_TAG//.}""\nif [[ $PY_TAG -lt 38 ]]; then\n  PY_TAG_FULL=""cp${PY_TAG}-cp${PY_TAG}m""\nelse\n  PY_TAG_FULL=""cp${PY_TAG}-cp${PY_TAG}""\nfi\nif [[ ${{ matrix.os }} == ""ubuntu-latest"" ]]; then\n  PLAT_NAME=manylinux2014_x86_64\nelif [[ ${{ matrix.os }} == ""windows-latest"" ]]; then\n  if [[ ${{ matrix.python_arch }} == ""x64"" ]]; then\n    PLAT_NAME=win_amd64\n  else\n    PLAT_NAME=win32\n  fi\nelse\n  PLAT_NAME=macosx_10_9_x86_64\nfi\nPACKAGE_VERSION=$(python -c ""import lightning;print(lightning.__version__)"")\necho ""PY_TAG=$PY_TAG"" >> $GITHUB_ENV\necho ""PY_TAG_FULL=$PY_TAG_FULL"" >> $GITHUB_ENV\necho ""PLAT_NAME=$PLAT_NAME"" >> $GITHUB_ENV\necho ""PACKAGE_NAME=sklearn_contrib_lightning"" >> $GITHUB_ENV\necho ""PACKAGE_VERSION=$PACKAGE_VERSION"" >> $GITHUB_ENV\n', 'echo ""/opt/python/${{ env.PY_TAG_FULL }}/bin"" >> $GITHUB_PATH', 'if [[ $(which python) != ""/opt/python/${{ env.PY_TAG_FULL }}/bin/python"" ]]; then\n  exit -1\nfi\n', 'import struct\nimport sys\n\nassert sys.version_info[:2] == tuple(map(int, ""${{ matrix.python_version }}"".split(""."")))\nassert f""x{struct.calcsize(\'P\') * 8}"".replace(""32"", ""86"") == ""${{ matrix.python_arch }}""\n', 'python -m pip install --upgrade pip\npip install -r requirements_build.txt -r requirements_test.txt\npython setup.py install\n', 'pytest -v --pyargs lightning', 'python setup.py sdist', 'pip install wheel\npython setup.py bdist_wheel --python-tag=""cp${{ env.PY_TAG }}"" --plat-name=${{ env.PLAT_NAME }}\n', 'pip install auditwheel\nauditwheel repair --plat ${{ env.PLAT_NAME }} dist/${{ env.PACKAGE_NAME }}*.whl\nmv -f wheelhouse/${{ env.PACKAGE_NAME }}*.whl dist/${{ env.PACKAGE_NAME }}-${{ env.PACKAGE_VERSION }}-${{ env.PY_TAG_FULL }}-${{ env.PLAT_NAME }}.whl\n', 'pip install twine\nrm -f dist/*.egg\ntwine upload --skip-existing dist/*\n']"
""
"['pipx install poetry', 'poetry env use ""3.7""\npoetry install\npoetry config repositories.testpypi https://test.pypi.org/legacy/\n', 'poetry version ${{ github.event.release.tag_name }}', 'poetry build', 'poetry publish -r testpypi -u __token__ -p ${{ secrets.TEST_PYPI_PASSWORD }}', 'pipx install poetry', 'poetry env use ""3.7""\npoetry install\n', 'poetry version ${{ github.event.release.tag_name }}', 'poetry build', 'poetry publish -u __token__ -p ${{ secrets.PYPI_PASSWORD }}', 'python -m pip install --upgrade pip tox', 'make tests']"
"['git remote add unittest_upstream_comparison https://github.com/ros/rosdistro.git || git remote set-url unittest_upstream_comparison https://github.com/ros/rosdistro.git\ngit fetch --no-tags --depth=1 unittest_upstream_comparison master\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -r test/requirements.txt\n', 'pytest -s test', 'python -m pip install --upgrade pip setuptools wheel\npip install yamllint\n', 'yamllint */']"
['./unrpyc.py --clobber testcases/script.rpyc\ndiff -u testcases/script.orig.rpy testcases/script.rpy\ncd un.rpyc;\n./compile.py -p 1\ncd ..\n']
"['python -m pip install pyperf>=2.3.0\npython extras/profiling/run.py --fresh --complex  --min-speed=6 --file output.txt\nbody=$(cat output.txt)\nbody=""${body//\'%\'/\'%25\'}""\nbody=""${body//$\'\\n\'/\'%0A\'}""\nbody=""${body//$\'\\r\'/\'%0D\'}""\necho ""::set-output name=body::$body""\n', 'make venv', 'make codestyle', 'make content', 'make install', 'make test-cover', 'make codecov-upload', 'make test-dist', 'sudo gem install mdl', 'make doc-check', 'sudo pip install httpie', 'http --ignore-stdin POST ${{ secrets.DOCS_UPDATE_VERCEL_HOOK }}', 'choco pack -v', 'choco info httpie -s .', 'choco install httpie -y -dv -s ""\'.;https://community.chocolatey.org/api/v2/\'""\n', '# Source: https://stackoverflow.com/a/46760714/15330941\n\n# Make `refreshenv` available right away, by defining the $env:ChocolateyInstall\n# variable and importing the Chocolatey profile module.\n$env:ChocolateyInstall = Convert-Path ""$((Get-Command choco).Path)\\..\\..""\nImport-Module ""$env:ChocolateyInstall\\helpers\\chocolateyProfile.psm1""\nrefreshenv\n\nhttp --version\nhttps --version\nhttpie --version\nchoco uninstall -y httpie\n', 'choco apikey --key $CHOCO_API_KEY --source https://push.chocolatey.org/\nchoco push httpie*.nupkg --source https://push.chocolatey.org/\n', 'cd extras/packaging/linux\n./get_release_artifacts.sh\n', 'pip install httpie\nexport API_URL=""api.github.com/repos/httpie/httpie/releases/tags/${{ github.event.inputs.tag_name }}""\nexport UPLOAD_URL=`https --ignore-stdin GET $API_URL | jq -r "".upload_url""`\necho ""::set-output name=UPLOAD_URL::$UPLOAD_URL""\n', 'make install && make build', 'sudo snap install --dangerous ${{ steps.snapcraft.outputs.snap }}', 'httpie.http --version\nhttpie.https --version\nhttpie --version\n# Auto-aliases cannot be tested when installing a snap outside the store.\n# http --version\n# https --version\n', 'brew developer on\nbrew update\n', 'make brew-test', ""python -m pip install --upgrade pip wheel\npython -m pip install --upgrade '.[dev]'\npython -m pytest --verbose ./httpie ./tests\n"", 'make install\nmake test\n']"
"['pip install --user ruff', 'ruff --format=github --select=""E,F,PLC,PLE,UP,W,YTT"" --ignore=""PLC1901,S101,UP031"" --target-version=py37 .', 'npm install --no-progress\npip install pytest\n', ""echo 'GYP_MSVS_VERSION=2015' >> $Env:GITHUB_ENV\necho 'GYP_MSVS_OVERRIDE_PATH=C:\\\\Dummy' >> $Env:GITHUB_ENV\n"", 'python -m pytest', 'npx envinfo', 'npm test', 'npm install --no-progress\n# npm audit fix --force\n', ""echo 'GYP_MSVS_VERSION=${{ matrix.msvs-version }}' >> $Env:GITHUB_ENV\necho 'GYP_MSVS_OVERRIDE_PATH=C:\\\\Dummy' >> $Env:GITHUB_ENV\n"", 'npx envinfo', 'npm test']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install .[testing]', 'python -m pytest', 'pip install .[qa]', 'python -m flake8 jedi setup.py\npython -m mypy jedi sith.py setup.py\n', 'pip install .[testing] coverage', 'python -m coverage run --source jedi -m pytest\npython -m coverage report\n', 'pip install --quiet codecov coveralls\npython -m coverage xml\npython -m coverage report -m\nbash <(curl -s https://codecov.io/bash) -X gcov -X coveragepy -X search -X fix -X xcode -f coverage.xml\n']"
"['sudo ./ci/apt-install $CI_DATABASE', './ci/print-versions', './ci/pip-install', 'sudo apt update\nsudo apt install -y graphviz\n', 'pip install -r docs/requirements.txt', 'echo ""::add-matcher::.github/matchers/sphinx.json""\n./ci/run-docs\necho ""::remove-matcher owner=sphinx::""\n', 'pip install -r docs/requirements.txt', './ci/run-docs linkcheck\n', 'echo ""::add-matcher::.github/matchers/sphinx-linkcheck.json""\necho ""::add-matcher::.github/matchers/sphinx-linkcheck-warn.json""\nsed \'s@^@docs/@\' docs/_build/linkcheck/output.txt\necho ""::remove-matcher owner=sphinx::""\necho ""::remove-matcher owner=sphinx-warn::""\n', 'python -m pip install --upgrade pip wheel\npip install -r requirements-lint.txt\n', './scripts/generate-license-data', './ci/services-up ${{ matrix.database }}', 'sudo ./ci/apt-install $CI_DATABASE', './ci/pip-install migrations\n', './ci/run-checkmigrate', './ci/run-migrate 4.0', './ci/run-migrate 4.0.4', './ci/run-migrate 4.1', './ci/run-migrate 4.2', './ci/run-migrate 4.3', './ci/run-migrate 4.4', './ci/run-migrate 4.5', './ci/run-migrate 4.6', './ci/run-migrate 4.7', './ci/run-migrate 4.8', './ci/run-migrate 4.9', './ci/run-migrate 4.10', './ci/run-migrate 4.11', './ci/run-migrate 4.12', './ci/run-migrate 4.13', './ci/run-migrate 4.14', './ci/run-migrate 4.15', './ci/run-migrate 4.16', './ci/run-migrate 4.17', 'coverage combine\ncoverage xml\n', './ci/services-down ${{ matrix.database }}', 'python -m pip install --upgrade pip wheel\npip install -r requirements-lint.txt\n', 'pre-commit run --all', 'python -m pip install --upgrade pip wheel\npip install -r requirements-lint.txt\n', 'pylint weblate/', './rundev.sh build', './rundev.sh start', './rundev.sh wait', './rundev.sh check', './rundev.sh test --failfast weblate.vcs', './rundev.sh logs', './rundev.sh stop', ""pip install $(grep -E '^(weblate-schemas)==' requirements.txt) -r requirements-lint.txt"", 'make -C docs update-schemas', 'pre-commit run --files $(git diff --name-only)', 'semgrep ci', 'sudo ./ci/apt-install', './ci/print-versions', './ci/pip-install', 'coverage run ./setup.py install', ""diff -ruNqp weblate $(python -c 'import weblate; import os; print(os.path.dirname(weblate.__file__))')"", 'coverage combine\ncoverage xml\n', 'sudo ./ci/apt-install', './ci/print-versions', './ci/pip-install', ""pip install '.[all]'"", ""diff -ruNqp weblate $(python -c 'import weblate; import os; print(os.path.dirname(weblate.__file__))')"", 'sudo ./ci/apt-install', './ci/pip-install', 'coverage run ./setup.py sdist', 'pip install ./dist/*.tar.gz\npip uninstall --yes Weblate\n', 'coverage run ./setup.py bdist_wheel', 'pip install ./dist/*.whl\npip uninstall --yes Weblate\n', 'twine check dist/*', 'coverage combine\ncoverage xml\n', './ci/services-up ${{ matrix.database }}', 'sudo ./ci/apt-install $CI_DATABASE', './ci/print-versions', './ci/pip-install ${{ matrix.requirements }}', './ci/prepare-database', 'coverage run ./manage.py compilemessages', 'coverage run ./manage.py collectstatic --noinput --verbosity 0', 'coverage run ./manage.py migrate --noinput --traceback', 'coverage run ./manage.py check', 'coverage run ./manage.py test -v2', 'coverage run weblate/wsgi.py', 'coverage combine\ncoverage xml\n', 'curl https://deepsource.io/cli | sh\n./bin/deepsource report --analyzer test-coverage --key python --value-file ./coverage.xml\n', './ci/services-down ${{ matrix.database }}', './scripts/yarn-update']"
['cd tests\npython tests.py']
"['echo ::set-output name=github-ref::${{ github.ref }}\necho ::set-output name=docker-push::${{ github.ref == \'refs/heads/main\' }}\necho ::set-output name=oci-reference::ghcr.io/$( echo ""${{ github.repository}}"" | tr \'[:upper:]\' \'[:lower:]\' )\n', 'echo ""steps.vars.github-ref = ${{ steps.vars.outputs.github-ref }}""\necho ""steps.vars.docker-push = ${{ steps.vars.outputs.docker-push }}""\necho ""steps.vars.oci-reference = ${{ steps.vars.outputs.oci-reference }}""\n', 'pip install -r devtools/requirements-poetry.in\n', 'task \\\n  OCI_REFERENCE=""${{ needs.setup.outputs.oci-reference }}"" \\\n  docker:latest docker:unstable\n', 'pip install -r devtools/requirements-poetry.in\n', 'task \\\n  DOCKER_PUSH=true \\\n  OCI_REFERENCE=""${{ needs.setup.outputs.oci-reference }}"" \\\n  docker:latest docker:unstable\n', 'pip install -r devtools/requirements-poetry.in\n', '${{ matrix.PREPARATION }}\n', ""task \\\n  OS=${{ matrix.os }} \\\n  MATRIX_SUFFIX=${{ matrix.suffix }} \\\n  EXTENSIVE=${{ matrix.extensive-tests || 'false' }} \\\n  TOX_PYTHON_VERSION=${{ matrix.python-version }} \\\n  TOXENV_SUFFIX=${{ matrix.TOXENV_SUFFIX }} \\\n  TOX_JUNIT_XML_PREFIX=${{ matrix.python-version }}-${{ matrix.os }}${{matrix.suffix}}- \\\n  gha:validate\n"", 'pip install -r devtools/requirements-poetry.in\n', 'task ${{ matrix.task }}\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['python .github/workflows/validate_release_tag.py $GITHUB_REF', 'sudo rm -f /etc/apt/sources.list.d/dotnetdev.list /etc/apt/sources.list.d/microsoft-prod.list\nsudo apt update\nsudo apt install libev-dev libevent-dev\nsudo apt install gcc make libffi-dev pkg-config zlib1g-dev libbz2-dev libsqlite3-dev libncurses5-dev libexpat1-dev libssl-dev libgdbm-dev tk-dev libgc-dev python-cffi liblzma-dev libncursesw5-dev\nsudo ldconfig\n', 'pip install tox coveralls', 'tox -v -e py', 'pip install tox coveralls', 'tox -v -e py', 'pip install flit~=3.4', 'flit publish', 'sudo rm -f /etc/apt/sources.list.d/dotnetdev.list /etc/apt/sources.list.d/microsoft-prod.list\nsudo apt update\nsudo apt install libev-dev libevent-dev\nsudo apt install gcc make libffi-dev pkg-config zlib1g-dev libbz2-dev libsqlite3-dev libncurses5-dev libexpat1-dev libssl-dev libgdbm-dev tk-dev libgc-dev python-cffi liblzma-dev libncursesw5-dev\nsudo ldconfig\n', 'pip install tox coveralls', 'tox -v -e py', 'pip install tox coveralls\n', 'tox -v -e py']"
"['pip install --user ruff', 'ruff --format=github --ignore=""E722,F40,F841"" --line-length=681 --target-version=py37 .', 'make test-docker version=${{ matrix.python-version }}', 'python -m pip install --upgrade pip\npip install build\npip install wheel\npython setup.py sdist bdist_wheel\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py clean sdist bdist_wheel\ntwine upload dist/*\n']"
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
"['pip install flake8', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['sudo apt-get install python3 python3-dev python-is-python3\nsudo apt-get install build-essential clang\nsudo apt-get install libboost-dev libboost-system-dev libboost-iostreams-dev libboost-python-dev systemtap-sdt-dev\nsudo apt-get install libfuzzy-dev graphviz libgraphviz-dev tesseract-ocr unrar xfonts-base xfonts-75dpi\nwget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb\nsudo dpkg -i wkhtmltox_0.12.6-1.bionic_amd64.deb\n', 'git clone https://github.com/buffer/libemu.git\ncd libemu\nautoreconf -v -i\n./configure\nsudo make install\ncd -\nsudo ldconfig\n', 'pip3 install --upgrade pip\npip3 install tox tox-gh-actions\npip3 install appdirs\n', 'sudo mkdir -p /etc/thug/\nsudo cp -R thug/conf/* /etc/thug/\n', 'tox\n', 'sudo apt-get install python python3 python3-dev\nsudo apt-get install build-essential clang\nsudo apt-get install libboost-dev libboost-system-dev libboost-iostreams-dev libboost-python-dev systemtap-sdt-dev\nsudo apt-get install libfuzzy-dev graphviz libgraphviz-dev tesseract-ocr unrar xfonts-base xfonts-75dpi\nwget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.focal_amd64.deb\nsudo dpkg -i wkhtmltox_0.12.6-1.focal_amd64.deb\n', 'git clone https://github.com/buffer/libemu.git\ncd libemu\nautoreconf -v -i\n./configure\nsudo make install\ncd -\nsudo ldconfig\n', 'pip3 install --upgrade pip\npip3 install tox tox-gh-actions\npip3 install appdirs\n', 'sudo mkdir -p /etc/thug/\nsudo cp -R thug/conf/* /etc/thug/\n', 'tox\n', 'sudo apt-get install python python3 python3-dev\nsudo apt-get install build-essential clang\nsudo apt-get install libboost-dev libboost-system-dev libboost-iostreams-dev libboost-python-dev systemtap-sdt-dev\nsudo apt-get install libfuzzy-dev graphviz libgraphviz-dev tesseract-ocr unrar xfonts-base xfonts-75dpi\nwget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.focal_amd64.deb\nsudo dpkg -i wkhtmltox_0.12.6-1.focal_amd64.deb\n', 'git clone https://github.com/buffer/libemu.git\ncd libemu\nautoreconf -v -i\n./configure\nsudo make install\ncd -\nsudo ldconfig\n', 'pip3 install --upgrade pip\npip3 install tox tox-gh-actions\npip3 install appdirs\n', 'sudo mkdir -p /etc/thug/\nsudo cp -R thug/conf/* /etc/thug/\n', 'tox\n', 'git checkout HEAD^2']"
""
"['python -m pip install --upgrade pip tox\n', 'tox']"
"['pip install -e .[dev,test]\n', 'pip install regex\n', 'pylint guessit', 'pytest --cov=guessit\n', 'pip install -e .[dev]\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""github-actions""\n', 'semantic-release -v DEBUG version', 'python setup.py sdist bdist_wheel', 'pip install -e .[dev]\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""github-actions""\n', 'semantic-release -v DEBUG version', 'pyinstaller --dist ./dist guessit.spec', './dist/guessit ""Treme.1x03.Right.Place,.Wrong.Time.HDTV.XviD-NoTV.avi""', 'pip install -e .[dev]\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""github-actions""\n', 'semantic-release -v DEBUG version', 'pyinstaller --dist ./dist guessit.spec', './dist/guessit ""Treme.1x03.Right.Place,.Wrong.Time.HDTV.XviD-NoTV.avi""', 'pip install -e .[dev]\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""github-actions""\n', 'semantic-release -v DEBUG version', 'pyinstaller --dist ./dist guessit.spec', './dist/guessit ""Treme.1x03.Right.Place,.Wrong.Time.HDTV.XviD-NoTV.avi""', 'mkdir -p ./dist\nmv artifacts/guessit-bin-linux/guessit ./dist/guessit-linux\nmv artifacts/guessit-bin-macos/guessit ./dist/guessit-macos\nmv artifacts/guessit-bin-windows/guessit.exe ./dist/guessit-windows.exe\nmv artifacts/guessit-python/* ./dist\n', 'git config --global user.email ""action@github.com""\ngit config --global user.name ""github-actions""\n', 'pip install python-semantic-release', 'semantic-release -v DEBUG publish', 'pip install mkdocs mkdocs-material', 'mkdocs build']"
['git checkout HEAD^2']
""
[]
"['pip install --upgrade pip pipenv', 'pipenv install --dev --skip-lock --python ${{ matrix.python }}', 'make ci', 'python -m pip install --upgrade pip\npip install build setuptools wheel twine\n', 'python -m build\npython -m twine upload dist/*\n']"
""
"['sudo apt-get update\nsudo apt-get install -y libgeos-c1v5\n', ""psql -U postgres -h localhost -c 'CREATE EXTENSION hstore;' flask_admin_test"", 'pip install tox', 'tox -e ${{ matrix.tox-version }}']"
"['bash .github/workflows/install_ci_python_dep.sh', 'pre-commit run -a', 'bash .github/workflows/install_mongo.sh ${{ matrix.MONGODB }}\nbash .github/workflows/install_ci_python_dep.sh\nbash .github/workflows/start_mongo.sh ${{ matrix.MONGODB }}\n', 'tox -e $(echo py${{ matrix.python-version }}-mg${{ matrix.PYMONGO }} | tr -d . | sed -e \'s/pypypy/pypy/\') -- -a ""-k=test_ci_placeholder""', 'tox -e $(echo py${{ matrix.python-version }}-mg${{ matrix.PYMONGO }} | tr -d . | sed -e \'s/pypypy/pypy/\') -- -a ""--cov=mongoengine""', 'coveralls', 'pip install -e .\npip install -r docs/requirements.txt\n', 'cd docs\nmake html-readthedocs\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n', 'pip install wheel\npython setup.py sdist bdist_wheel\n']"
""
"['pip install -r requirements/style.txt', 'pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install -r requirements/ci-release.txt', 'python -m build', 'twine check dist/*', 'pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install -r requirements/ci-tests.txt', 'tox -e ${{ matrix.tox }}', 'coveralls --service=github']"
"['if [ ! -f data/country_osm_grid.sql.gz ]; then\n    wget --no-verbose -O data/country_osm_grid.sql.gz https://www.nominatim.org/data/country_grid.sql.gz\nfi\ncd ..\ntar czf nominatim-src.tar.bz2 Nominatim\nmv nominatim-src.tar.bz2 Nominatim\n', 'tar xf nominatim-src.tar.bz2', 'sudo apt-get install -y -qq python3-behave', 'pip3 install behave==1.2.6', 'sudo apt-get install -y -qq python3-pytest python3-pytest-asyncio uvicorn', 'pip3 install -U pytest-asyncio', 'pip3 install pytest pytest-asyncio uvicorn', 'pip3 install falcon sanic sanic-testing sanic-cors starlette', 'pip3 install -U pylint asgi_lifespan', 'phpcs --report-width=120 .', 'python3 -m pylint nominatim', 'phpunit ./', 'python3 -m pytest test/python', 'python3 -m behave -DREMOVE_TEMPLATE=1 -DBUILDDIR=$GITHUB_WORKSPACE/build --format=progress3\n', 'pip3 install -U mypy osmium uvicorn types-PyYAML types-jinja2 types-psycopg2 types-psutil types-requests types-ujson types-Pygments typing-extensions', 'python3 -m mypy --strict nominatim', 'tar xf nominatim-src.tar.bz2', 'sudo apt-get install postgresql-server-dev-13', 'sudo apt-get install -y -qq python3-behave', 'python3 -m behave -DREMOVE_TEMPLATE=1 -DBUILDDIR=$GITHUB_WORKSPACE/build -DTOKENIZER=legacy --format=progress3\n', 'export APT_LISTCHANGES_FRONTEND=none\nexport DEBIAN_FRONTEND=noninteractive\napt-get update -qq\napt-get install -y git sudo wget\nln -snf /usr/share/zoneinfo/$CONTAINER_TIMEZONE /etc/localtime && echo $CONTAINER_TIMEZONE > /etc/timezone\n', 'useradd -m nominatim\necho \'nominatim   ALL=(ALL:ALL) NOPASSWD: ALL\' > /etc/sudoers.d/nominiatim\necho ""/home/nominatim/Nominatim/vagrant/Install-on-${OS}.sh no $INSTALL_MODE"" > /home/nominatim/vagrant.sh\n', 'export USERNAME=nominatim\nexport USERHOME=/home/nominatim\nexport NOSYSTEMD=yes\nexport HAVE_SELINUX=no\ntar xf nominatim-src.tar.bz2\n. vagrant.sh\n', 'mv Nominatim/test/testdb/apidb-test-data.pbf test.pbf\nrm -rf Nominatim\nmkdir data-env-reverse\n', 'nominatim --version', 'nominatim admin --collect-os-info', 'nominatim import --osm-file ../test.pbf', 'nominatim special-phrases --import-from-wiki', 'nominatim admin --check-database', 'nominatim admin --warm', 'apt-get install -y python3-pip', 'pip3 install --user osmium\nnominatim replication --init\nNOMINATIM_REPLICATION_MAX_DIFF=1 nominatim replication --once\n', 'nominatim refresh --postcodes --word-tokens', 'echo \'NOMINATIM_DATABASE_DSN=""pgsql:dbname=reverse""\' >> .env\nnominatim import --osm-file ../test.pbf --reverse-only --no-updates\n', 'nominatim admin --check-database', 'nominatim refresh --postcodes --word-tokens']"
"['flyctl deploy --remote-only --app alerta-api', 'docker build -t $IMAGE_NAME -t $REPOSITORY_URL/$IMAGE_NAME:$(cat VERSION) -t $REPOSITORY_URL/$IMAGE_NAME:$(git rev-parse --short HEAD) -t $REPOSITORY_URL/$IMAGE_NAME:latest .', 'docker push --all-tags $REPOSITORY_URL/$IMAGE_NAME', 'docker run --network host ghcr.io/act10ns/hey -z 2s -m POST -T application/json -d \'{""environment"":""Production"",""service"":[""network""],""resource"":""router01"",""event"":""node_down""}\' http://localhost:8080/alert', 'python3 -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install .\n', 'pre-commit run -a --show-diff-on-failure\n', 'pytest --cov=alerta tests/*.py\n', 'python3 -m pip install --upgrade build\npython3 -m build\n', 'python3 -m pip install --upgrade twine\npython3 -m twine check dist/*\npython3 -m twine upload --verbose dist/*\n', 'python3 -m pip install --upgrade alerta-server\npython3 -m pip freeze\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install .\n', 'pre-commit run -a --show-diff-on-failure\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=50 --max-line-length=127 --statistics\n', 'pytest --cov=alerta tests/*.py\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install .\n', 'pre-commit run -a --show-diff-on-failure\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=50 --max-line-length=127 --statistics\n', 'pytest --cov=alerta tests/*.py\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install .\n', 'pre-commit run -a --show-diff-on-failure\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=50 --max-line-length=127 --statistics\n', 'pytest --cov=alerta tests/*.py\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install .\n', 'pre-commit run -a --show-diff-on-failure\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=50 --max-line-length=127 --statistics\n', 'pytest --cov=alerta tests/*.py\n', 'sudo apt-get update\nsudo apt-get install -y \\\n  build-essential \\\n  python3-dev \\\n  libldap2-dev \\\n  libsasl2-dev \\\n  xmlsec1\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-ci.txt\npip install .\n', 'pytest tests/integration\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install Django~=${{ matrix.django }}.0\n', 'pytest --cov-report=xml\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip\npip install coverage tox tox-gh-actions\n', 'tox', 'coverage report\ncoverage xml\n', 'python -m pip install build twine check-wheel-contents', 'python -m build --sdist --wheel .', 'ls -l dist', 'check-wheel-contents dist/*.whl', 'python -m twine check dist/*']"
"['echo ""$GITHUB_CONTEXT""', 'git fetch --recurse-submodules=no https://github.com/linuxcnc/linuxcnc refs/tags/*:refs/tags/*', 'set -x\nsudo apt-get install -y eatmydata\neatmydata ./scripts/travis-install-build-deps.sh\neatmydata curl -O https://snapshot.debian.org/archive/debian/20220716T154603Z/pool/main/p/po4a/po4a_0.67-2_all.deb\nsudo eatmydata apt install ./po4a_0.67-2_all.deb\ncd src\neatmydata ./autogen.sh\neatmydata ./configure --with-realtime=uspace --disable-check-runtime-deps\neatmydata make -O -j$((1+$(nproc))) default pycheck V=1\n# Note that the package build covers html docs\neatmydata ../scripts/rip-environment runtests -p\n', 'echo ""$GITHUB_CONTEXT""', 'git fetch --recurse-submodules=no https://github.com/linuxcnc/linuxcnc refs/tags/*:refs/tags/*', 'set -x\nsudo apt-get install -y eatmydata\neatmydata ./scripts/travis-install-build-deps.sh\nsudo eatmydata apt-get install -y clang\neatmydata curl -O https://snapshot.debian.org/archive/debian/20220716T154603Z/pool/main/p/po4a/po4a_0.67-2_all.deb\nsudo eatmydata apt install ./po4a_0.67-2_all.deb\ncd src\neatmydata ./autogen.sh\nCC=clang CXX=clang++ eatmydata ./configure --with-realtime=uspace --disable-check-runtime-deps\neatmydata make -O -j$((1+$(nproc))) default pycheck V=1\n# Note that the package build covers html docs\neatmydata ../scripts/rip-environment runtests -p\n', 'echo ""$GITHUB_CONTEXT""', 'git fetch --recurse-submodules=no https://github.com/linuxcnc/linuxcnc refs/tags/*:refs/tags/*', './scripts/travis-install-build-deps.sh\nsudo apt-get install -y eatmydata\ncurl -O https://snapshot.debian.org/archive/debian/20220716T154603Z/pool/main/p/po4a/po4a_0.67-2_all.deb\nsudo apt install ./po4a_0.67-2_all.deb\ncd src\neatmydata ./autogen.sh\neatmydata ./configure --with-realtime=uspace --disable-check-runtime-deps --enable-build-documentation=html\neatmydata make -O -j$((1+$(nproc))) manpages\neatmydata make -O -j$((1+$(nproc))) translateddocs\neatmydata make -O -j$((1+$(nproc))) docs\n# Note that the package build covers html docs\n', 'echo ""$GITHUB_CONTEXT""', 'set -e\nset -x\napt-get --quiet update\napt-get --yes --quiet install eatmydata\n# Install stuff needed to check out the linuxcnc repo and turn it into a debian source package.\neatmydata apt-get --yes --quiet install --no-install-suggests git lsb-release python3 devscripts\n', 'case ""${{matrix.image}}"" in\n    debian:sid|debian:bookworm)\n        exit 0\n        ;;\n    *)\n        ;;\nesac\nset -e\nset -x\neatmydata apt-get --yes --quiet install --no-install-recommends gpg software-properties-common\neatmydata gpg --homedir=""${PWD}/gnupg"" --output /etc/apt/trusted.gpg.d/linuxcnc-deb-archive.gpg --export 3CB9FD148F374FEF\nDIST=$(echo ${{matrix.image}} | cut -d : -f 2)\neatmydata add-apt-repository ""deb http://linuxcnc.org $DIST base""\neatmydata apt-get --quiet update\n', 'set -e\nset -x\neatmydata git config --global --add safe.directory ""${PWD}""\neatmydata debian/configure\neatmydata debian/update-dch-from-git\neatmydata scripts/get-version-from-git | sed -re \'s/^v(.*)$/\\1/\' >| VERSION; cat VERSION\neatmydata git diff\neatmydata apt-get --yes --quiet build-dep --arch-only .\neatmydata debuild -us -uc --build=any\n', 'set -e\nset -x\neatmydata apt-get --yes --quiet install ../*.deb\neatmydata apt-get --yes --quiet install sudo # some tests run sudo...\neatmydata adduser --disabled-password --gecos """" testrunner\neatmydata passwd -d testrunner\neatmydata adduser testrunner sudo\nchmod 0777 $(find tests/ -type d) # make test dirs world-writable for the testrunner\nsu -c ""eatmydata ./scripts/runtests -p ./tests"" testrunner\n', 'echo ""$GITHUB_CONTEXT""', 'set -e\nset -x\napt-get --quiet update\napt-get --yes --quiet install eatmydata\n# Install stuff needed to check out the linuxcnc repo and turn it into a debian source package.\neatmydata apt-get --yes --quiet install --no-install-suggests git lsb-release python3 devscripts\n', 'case ""${{matrix.image}}"" in\n    debian:sid|debian:bookworm)\n        exit 0\n        ;;\n    *)\n        ;;\nesac\nset -e\nset -x\neatmydata apt-get --yes --quiet install gpg software-properties-common\neatmydata gpg --homedir=""${PWD}/gnupg"" --output /etc/apt/trusted.gpg.d/linuxcnc-deb-archive.gpg --export 3CB9FD148F374FEF\nDIST=$(echo ${{matrix.image}} | cut -d : -f 2)\neatmydata add-apt-repository ""deb http://linuxcnc.org $DIST base""\neatmydata apt-get --quiet update\n', 'set -e\nset -x\neatmydata git config --global --add safe.directory ""${PWD}""\neatmydata debian/configure\neatmydata debian/update-dch-from-git\neatmydata scripts/get-version-from-git | sed -re \'s/^v(.*)$/\\1/\' >| VERSION; cat VERSION\neatmydata git diff\neatmydata apt-get --yes --quiet build-dep --indep-only .\neatmydata debuild -us -uc --build=all\n', 'set -e\nset -x\neatmydata apt-get --yes --quiet install ../*.deb\n']"
""
[]
[]
"['sudo apt-get update\nsudo apt-get install -y libgettextpo-dev\n', 'pip install --upgrade pip wheel\npip install -r requirements/dev.txt\n', 'make docs', 'sudo apt-get update\nsudo apt-get install -y libgettextpo-dev libxml2-dev libxmlsec1-dev gettext hunspell-af\n', 'pip install --upgrade pip wheel', 'pip install -r requirements/test.txt', 'make requirements/min-versions.txt\ncat requirements/min-versions.txt\npip install -r requirements/min-versions.txt\n', 'pip install -e .', 'make test', 'make test-functional', 'coverage xml\n', 'brew install gettext\necho ""/usr/local/opt/gettext/bin"" >> $GITHUB_PATH\n', 'pip install --upgrade pip wheel', 'pip install -r requirements/test.txt', 'pip install .', 'make test', 'make test-functional', 'coverage xml\n', 'python -m pip install --upgrade pip wheel\npip install -r requirements/lint.txt\n', 'pre-commit run --all', 'python -m pip install --upgrade pip wheel\npip install -r requirements/lint.txt\n', 'echo ""::add-matcher::.github/matchers/flake8.json""\npre-commit run --all pydocstyle\necho ""::remove-matcher owner=flake8::""\n', 'python -m pip install --upgrade pip wheel\npip install -r requirements/lint.txt\n', 'echo ""::add-matcher::.github/matchers/flake8.json""\npre-commit run --all flake8\necho ""::remove-matcher owner=flake8::""\n', 'sudo apt-get update\nsudo apt-get install -y libgettextpo-dev\n', 'pip install --upgrade pip wheel', 'pip install -r requirements/dev.txt\n', 'make build', 'twine check dist/*', 'virtualenv test-ttk-release-src\n. ./test-ttk-release-src/bin/activate\npip install dist/translate-toolkit-*.tar.gz\nmoz2po --help\npython ./test-ttk-release-src/lib/python*/site-packages/translate/lang/identify.py README.rst\n', 'virtualenv test-ttk-release-whl\n. ./test-ttk-release-whl/bin/activate\npip install dist/translate_toolkit*.whl\nmoz2po --help\npython ./test-ttk-release-whl/lib/python*/site-packages/translate/lang/identify.py README.rst\n', 'coverage run ./setup.py build', 'coverage run --append ./setup.py sdist', 'coverage xml', 'python -m pip install --upgrade pip wheel', 'python -m pip install -r requirements/test.txt', 'pip install .', 'make test', 'coverage xml\n']"
[]
"['pip install django bs4', 'python runtests.py']"
"['python -m pip install --upgrade pip\npip install pipenv\npipenv install\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pipenv run test\n']"
"['echo $TMPDIR\n./ci_build.sh\n', 'make -f .github/workflows/Makefile-scripts sandbox/bin/buildout\n', 'sandbox/bin/buildout -v -c .github/workflows/scripts-${PYTHON_VERSION}.cfg annotate buildout\nsandbox/bin/buildout -c .github/workflows/scripts-${PYTHON_VERSION}.cfg\n', 'ls -al sandbox/eggs\nls -al sandbox/downloads/dist\n', 'echo $TMPDIR\n./ci_build.sh\n', './ci_build.sh\n', './ci_build.sh\n', './ci_build.sh\n', 'make -f .github/workflows/Makefile-scripts-setuptools-head sandbox/bin/buildout\n', 'sandbox/bin/buildout -v -c .github/workflows/scripts-${PYTHON_VERSION}.cfg annotate buildout\nsandbox/bin/buildout -c .github/workflows/scripts-${PYTHON_VERSION}.cfg\n', 'ls -al sandbox/eggs\nls -al sandbox/downloads/dist\n']"
"['sudo apt-get install -y libsnappy-dev && pip install python-snappy', 'pip install -r developer_requirements.txt', 'SKIP_BLACK=""1"" ./run-tests.sh', 'pip install -r developer_requirements.txt', 'make clean\nmake all\n', 'curl -X POST -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{github.repository}}/releases -d \'{""tag_name"": ""${{github.event.ref}}""}\'\n', 'release_id=$(curl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{github.repository}}/releases/tags/${{github.event.ref}} | jq -r \'.id\')\necho ${release_id}\necho ""release_id=${release_id}"" >> $GITHUB_ENV\n', 'for i in 1 2 3; do\n  echo ""try $i"" && \\\n  brew update -vvv && \\\n  brew install snappy && \\\n  CPPFLAGS=""-I/usr/local/include -L/usr/local/lib"" python -m pip install python-snappy && \\\n  exit 0 || true;\ndone\nexit 1\n', 'brew install openblas\nOPENBLAS=""$(brew --prefix openblas)"" pip install numpy\n', 'pip install -r developer_requirements.txt', 'SKIP_BLACK=""1"" ./run-tests.sh', 'python setup.py bdist_wheel', 'curl -X POST -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{github.repository}}/releases -d \'{""tag_name"": ""${{github.event.ref}}""}\'\n', 'release_id=$(curl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{github.repository}}/releases/tags/${{github.event.ref}} | jq -r \'.id\')\necho ${release_id}\necho ""release_id=${release_id}"" >> $GITHUB_ENV\n', 'pip install -r developer_requirements.txt', 'set SKIP_BLACK=1 && .\\run-tests.cmd', 'python setup.py bdist_wheel', 'curl -X POST -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{github.repository}}/releases -d \'{""tag_name"": ""${{github.event.ref}}""}\'\n', 'release_id=$(curl -H ""Authorization: token ${{ secrets.GITHUB_TOKEN }}"" https://api.github.com/repos/${{github.repository}}/releases/tags/${{github.event.ref}} | jq -r \'.id\')\necho ""release_id=${release_id}"" >> $GITHUB_ENV\n', 'pip install ""pip<20.3""', 'pip install -r developer_requirements.txt', 'black --target-version py37 --diff fastavro/ tests/ setup.py\nblack --target-version py37 --check fastavro/ tests/ setup.py\n', 'make docs\n']"
"['cd /tmp\ncurl -s -L ""$PYTHON_SOURCE_URL"" | tar -zxf - -C ./\ncd Python-${{ matrix.python_version }}\n./configure --enable-optimizations\nmake\nsudo make install\necho \'export PATH=""/opt/python/bin:$PATH""\' >> $HOME/.bashrc\n', 'python -m pip install --upgrade pip\npip install ""coverage<7.2"" flake8 pytest pytest-cov freezegun requests scrutinizer-ocular codecov\n', 'set -e\nflake8 --show-source --max-line-length=160 .\npy.test --cov qiniu\nocular --data-file .coverage\ncoverage run test_qiniu.py\ncodecov\n']"
"['pip install -r requirements.txt', 'asv machine --machine ubuntu-latest --yes > /dev/null\necho \'Beginning benchmarks...\'\nasv continuous --interleave-processes -a processes=2 --split --show-stderr \'HEAD^\' \'HEAD\' |\\\nsed -n -E \'/(before.*after.*ratio)|(BENCHMARKS)/,$p\' >> out.txt\necho \'Benchmarks Done.\'\necho \'```\' >> $GITHUB_STEP_SUMMARY\ncat out.txt >> $GITHUB_STEP_SUMMARY\necho \'```\' >> $GITHUB_STEP_SUMMARY\nif grep -q ""PERFORMANCE DECREASED"" out.txt;\nthen\n  exit 1\nfi', 'python -m pip install -r docs/requirements.txt', 'cd docs\nsphinx-build -b spelling -n -q -W --keep-going -d _build/doctrees -D language=en_US -j auto . _build/spelling\n', 'python -m pip install blacken-docs', 'cd docs\nmake black\nRESULT=`cat _build/black/output.txt`\nif [ ""$RESULT"" -gt 0 ]; then\n    echo ""ðŸ’¥ ðŸ“¢ Code blocks in documentation must be reformatted with blacken-docs ðŸ“¢ ðŸ’¥""\nfi;\nexit $RESULT\n', 'python -m pip install flake8', 'python -m pip install isort', 'python -m pip install --upgrade pip setuptools wheel', 'python -m pip install -r tests/requirements/py3.txt -e .', 'python tests/runtests.py -v2', 'npm install', 'npm test', 'python -m pip install --upgrade pip setuptools wheel', 'python -m pip install -r tests/requirements/py3.txt -e .', 'python tests/runtests.py -v2', 'npm install', 'npm test']"
"['python3 -m pip install -r python/test_requirements.txt', 'python3 run_tests.py --quiet python/ycm/tests', 'coverage xml', 'sudo -H pip3 install -r python/test_requirements.txt', 'python3 ./install.py --force-sudo --ts-completer --clangd-completer --java-completer', './test/run_vim_tests --vim /usr/bin/vim', './test/run_vim_tests', 'coverage combine && coverage xml', './update-vim-docs', 'git diff', 'echo ""Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}""\necho ""Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}""\n']"
"['python --version\npip --version\n', 'pip install -r requirements.txt', 'flake8', 'python makeHosts.py', 'python testUpdateHostsFile.py']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r test/requirements.txt\n', '# stop the build if there are Python Flake8 violations\nflake8 . --count --show-source --statistics\n', 'python -m pytest -n auto --dist loadfile --ignore=test/integration/modules/ --durations=5 --cov-report term --cov=. .\n']"
"['hub pr checkout ${{ github.event.issue.number }}\necho ""Checked out SHA:""\ngit log -1 --format=\'%H\'\n', 'pip install --upgrade pip\npip install -r requirements.txt\npip install .\n', '# extracting the regex, see https://stackoverflow.com/a/36798723\nREGEX=$(echo ""$COMMENT"" | sed -n ""s/^@benchmark\\s\\+-b\\s*\\(\\S*\\).*$/\\1/p"")\nif [ -z ""$REGEX"" ]; then\n  BENCHMARKS=""""\nelse\n  BENCHMARKS=""-b $REGEX""\nfi\ncd benchmarks\nasv check -E existing\ngit remote add upstream https://github.com/dedupeio/dedupe.git\ngit fetch upstream\nasv machine --yes\nasv continuous --show-stderr -f 1.1 $BENCHMARKS upstream/main HEAD | cat\necho \'BENCH_OUTPUT<<EOF\' >> $GITHUB_ENV\nasv compare -f 1.1 upstream/main HEAD | python ../.github/scripts/asv_markdown.py >> $GITHUB_ENV\necho \'EOF\' >> $GITHUB_ENV\necho ""REGEX=$REGEX"" >> $GITHUB_ENV\n', 'pip install --upgrade pip\npip install -e . --config-settings editable_mode=compat --use-pep517\npip install -r requirements.txt\n', 'flake8 dedupe tests benchmarks/benchmarks', 'isort --check-only .', 'black . --check', 'mypy', 'pip install --upgrade pip\npip install -e . --config-settings editable_mode=compat --use-pep517\n', 'pip install -r requirements.txt', 'pytest', 'python -m pip install -e ./benchmarks\npython benchmarks/benchmarks/canonical.py\npython benchmarks/benchmarks/canonical.py\npython benchmarks/benchmarks/canonical_matching.py\npython benchmarks/benchmarks/canonical_matching.py\npython benchmarks/benchmarks/canonical_gazetteer.py\npython benchmarks/benchmarks/canonical_gazetteer.py\n', 'pip install --upgrade pip\npip install -e . --config-settings editable_mode=compat\npython -m pip install -e ./benchmarks\n', 'python benchmarks/benchmarks/canonical.py', 'pip install -e . --config-settings editable_mode=compat --use-pep517', 'python benchmarks/benchmarks/canonical.py', 'pip install build\npython -m build --sdist\n', 'pip install twine\ntwine upload --skip-existing wheelhouse/*.whl\ntwine upload dist/*\n']"
"['LABEL_SERVICE_API_KEY=$(az keyvault secret show \\\n  --vault-name issue-labeler \\\n  -n issue-labeler-func-key \\\n  -o tsv \\\n  --query value)\n\necho ""::add-mask::$LABEL_SERVICE_API_KEY""\necho ""LABEL_SERVICE_API_KEY=$LABEL_SERVICE_API_KEY"" >> $GITHUB_ENV\n', 'dotnet tool install Azure.Sdk.Tools.GitHubEventProcessor --version 1.0.0-dev.20230505.2 --add-source https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-net/nuget/v3/index.json --global\n', ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json\n"", 'dotnet tool install Azure.Sdk.Tools.GitHubEventProcessor --version 1.0.0-dev.20230505.2 --add-source https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-net/nuget/v3/index.json --global\n', ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json CloseStaleIssues\n"", ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json IdentifyStalePullRequests\n"", ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json CloseStalePullRequests\n"", ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json IdentifyStaleIssues\n"", ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json CloseAddressedIssues\n"", ""cat > payload.json << 'EOF'\n${{ toJson(github.event) }}\nEOF\ngithub-event-processor ${{ github.event_name }} payload.json LockClosedIssues\n""]"
"['python -m pip install --upgrade pip flit\nflit install --deps=develop\n', 'flake8 sqlparse --count --max-complexity=31 --show-source --statistics', 'pytest --cov=sqlparse']"
"['python -m pip install --upgrade pip\npip install .\npip install -U coverage wheel\n', 'coverage run --include=""more_itertools/*.py"" -m unittest\n', 'coverage report --show-missing --fail-under=99\n', 'pip install -U flake8\nflake8 .\n', 'pip install -U black\nblack --check .\n', 'pip install -U mypy\nstubtest more_itertools.more more_itertools.recipes\n', 'pip install -U sphinx sphinx_rtd_theme\nsphinx-build -W -b html docs docs/_build/html\n', 'pip install -U flit twine\nflit build --setup-py\ntwine check dist/*\n']"
""
"['echo ""::add-matcher::$GITHUB_WORKSPACE/.github/matchers/pylint.json""\npipx run --python python nox -s pylint\n', 'pip install wheel coveralls pytest-github-actions-annotate-failures\npip install -e .[dev]\n', 'chmod 755 ~\nmkdir -p ~/.ssh\nchmod 755 ~/.ssh\necho ""NoHostAuthenticationForLocalhost yes"" >> ~/.ssh/config\necho ""StrictHostKeyChecking no"" >> ~/.ssh/config\nssh-keygen -q -f ~/.ssh/id_rsa -N \'\'\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nchmod 644 ~/.ssh/authorized_keys\nls -la ~\nssh localhost -vvv ""echo \'Worked!\'""\n', 'pytest --cov --run-optional-tests=ssh,sudo', 'coveralls --service=github', 'pip install coveralls', 'coveralls --service=github --finish']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade ""tox<4"" tox-gh-actions\n', 'REDIS_PRIMARY=$(tests/start_redis.sh)\nREDIS_SENTINEL=$(tests/start_redis.sh --sentinel)\nCONTAINERS=""$REDIS_PRIMARY $REDIS_SENTINEL""\ntrap ""docker stop $CONTAINERS && docker rm $CONTAINERS"" EXIT\ntests/wait_for_redis.sh $REDIS_PRIMARY 6379\ntests/wait_for_redis.sh $REDIS_SENTINEL 26379\n\ntox\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade ""tox<4""\n', 'tox -e ${{ matrix.tool }}', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade towncrier\n', 'if ! towncrier check; then\n  echo \'\'\n  echo ""Please add a description of your changes to \'changelog.d/{issue or PR number}.{feature,bugfix,misc,doc,removal}\'""\n  exit 1\nfi\n', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n']"
"['python -m pip install --upgrade-strategy eager -U pip setuptools wheel\n', 'python -m pip install --upgrade-strategy eager -Ur requirements-tests.txt\n', 'python -m pip install .\n', 'py.test tests/\n', 'nikola\n', 'nikola help\n', 'python -m pip install --upgrade-strategy eager -U pip setuptools wheel\n', 'python -m pip install --upgrade-strategy eager -Ur requirements-extras.txt freezegun\n', 'python -m pip install .\n', 'scripts/baseline.sh check\n', 'python -m pip install --upgrade-strategy eager -U pip setuptools wheel\n', 'python -m pip install flake8 pydocstyle\n', 'flake8 nikola/ tests/\n', ""pydocstyle --count --match-dir='(?!^\\\\.)(?!data).*' nikola/\n"", 'python -m pip install --upgrade-strategy eager -U pip setuptools wheel\n', 'python -m pip install --upgrade-strategy eager -Ur requirements.txt\n', 'python -m pip install .\n', 'nikola help\n', 'nikola init -qd nsite\n', 'nikola build\n']"
['./runtests']
"['python -m pip install -U pip\npython -m pip install -U setuptools wheel twine\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'pip install -r requirements.txt\npip install ""Django~=${{ matrix.django-version }}.0"" .\n', 'python -m mypy dj_database_url\n', 'echo ""$(python --version) / Django $(django-admin --version)""\ncoverage run --source=dj_database_url --branch -m unittest discover -v\ncoverage report\ncoverage xml\n', 'pip install .\ncd tests\npython -m mypy .\n']"
"['sudo apt-get install gettext\npython -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['poetry install -v -E all', 'source $VENV\nnox -e cov -- xml\n', 'source $VENV\npytest tests/unit\n', 'poetry install -v -E all', 'source $VENV\nnox -e stress -- ${{ env.STRESS_TEST_MULTIPLIER }}\n', 'poetry install -v', 'source $VENV\npytest -n auto tests/unit\n', 'poetry add requests@${{ matrix.requests-version }} --lock\npoetry lock --check\npoetry lock --no-update\npoetry install -v -E all\n', 'poetry run pytest -n auto tests/unit', 'poetry version $(poetry version -s).${{ env.pre-release-suffix }}${{ env.pre-release-version }}\npoetry version\n', 'poetry build']"
"['python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox -e release']"
"['mkdir ./installdir\npython ./install.py ./installdir\n', './installdir/bin/rez/rez-benchmark --out ./out\n\n# remove benchmarking suite package repo\nrm -rf ./out/packages\n', 'python ./.github/scripts/validate_benchmark.py\n', 'sudo apt-get update\nsudo apt-get install -y gnuplot || /bin/true\n', 'python ./.github/scripts/store_benchmark.py\n', ""git config user.name 'github-actions[bot]'\ngit config user.email 'github-actions[bot]@users.noreply.github.com'\n"", 'if [[ ""$(git status --porcelain)"" == """" ]]; then\n  echo ""Nothing new to commit""\nelse\n  git add --all\n  git commit -m ""Generated from GitHub ""${{ github.workflow }}"" Workflow""\n  git push origin master\nfi\n', 'bash ./src/build_utils/license/apply_copyright\nif [[ ""$(git status | grep modified)"" != """" ]]; then\n  echo ""Some sourcefiles are missing copyright notice!"" 1>&2\n  echo ""Run ./src/build_utils/license/apply_copyright to apply."" 1>&2\n  exit 1\nfi\n', 'mkdir ./installdir\npython ./install.py ./installdir\n', './installdir/bin/rez/rez-python -m pip install pytest-cov\n', './installdir/bin/rez/rez-selftest -v \\\n  --config \\\n  --copy_package \\\n  --formatter \\\n  --imports \\\n  --packages \\\n  --package_filter \\\n  --packages_order \\\n  --resources_ \\\n  --rex \\\n  --schema \\\n  --solver \\\n  --version\n', 'pip install flake8\n', ""find -name '*.py' -not -path './rez/vendor/*' -not -path './rez/data/*' -not -path './rez/backport/*' -not -path './build_utils/*' -not -path './support/*' -not -path './rezgui/*' | xargs flake8"", '${{ matrix.REZ_INSTALL_COMMAND }}\n', '${{ matrix.REZ_SET_PATH_COMMAND }}\nrez-status\n', '${{ matrix.REZ_SET_PATH_COMMAND }}\nrez-pip --install .', 'cmake --version\n', 'pwsh --version\n', 'mkdir ./installdir\npython ./install.py ./installdir\n', './installdir/bin/rez/rez-python -m pip install pytest-cov\n./installdir/bin/rez/rez-python -m pip install parameterized\n', './installdir/bin/rez/rez-selftest -v\n', 'pip install wheel\n', 'python setup.py sdist bdist_wheel\n', 'python docs/build.py --no-docker', 'git init .\ngit config --global user.name ""github.com/${{ github.actor }}""\ngit config --global user.email ""${{ github.actor }}@${{ github.sha }}""\ngit remote add origin ""https://${{ github.actor }}:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git""\ngit checkout -B gh-pages\n', 'git add .\ngit commit \\\n  -m ""Generated from GitHub ""${{ github.workflow }}"" Workflow"" \\\n  && git push --force origin gh-pages \\\n  || echo ""Nothing new to commit""\n', 'sudo apt-get update\n', 'cmake --version\n', 'pwsh --version\n', 'sudo apt-get install -y csh\n', 'sudo apt-get install -y tcsh\n', 'sudo apt-get install -y zsh\n', 'mkdir ./installdir\npython ./install.py ./installdir\n', './installdir/bin/rez/rez-python -m pip install pytest-cov\n./installdir/bin/rez/rez-python -m pip install parameterized\n', './installdir/bin/rez/rez-selftest -v\n', 'python generate-wiki.py \\\n  --github-repo=""${{ github.repository }}"" \\\n  --out=""${{ github.workspace }}/out""\n', 'git config --global user.name ""github.com/${{ github.actor }}""\ngit config --global user.email ""${{ github.actor }}@${{ github.sha }}""\n', 'git clone https://${{ github.actor }}:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.wiki.git .\n', 'git add --all\n( git commit -m ""Generated from GitHub ${{ github.workflow }} Workflow"" \\\n  && git push origin master \\\n) || echo ""Nothing new to commit""\n', 'tag=$( \\\n  echo ""${{ hashFiles(\'.github/docker/rez-win-base/*\') }}${IMAGE_TAG_SALT}"" \\\n  | md5sum - \\\n  | awk \'{print $1}\' \\\n)\necho ""base tag is ${tag}""\necho ""::set-output name=tag::${tag}""\n', 'tag=$( \\\n  echo ""${{ hashFiles(\'.github/docker/rez-win-base/*\', \'.github/docker/rez-win-py/*\') }}${IMAGE_TAG_SALT}"" \\\n  | md5sum - \\\n  | awk \'{print $1}\' \\\n)\necho ""py tag is ${tag}""\necho ""::set-output name=tag::${tag}""\n', '# Try to get the image from the pub namepsace first.\n$pub_namespace = ""${Env:PUB_NAMESPACE}""\n$docker_image = ""${pub_namespace}/rez-win-${{ matrix.os-version }}-py-${{ matrix.py-version }}:${{ needs.image_tags.outputs.py }}"".ToLower()\n\nWrite-Output ""Inspecting image ${docker_image}...""\n$ErrorActionPreference = ""Continue""\ndocker manifest inspect $docker_image *>$null || Write-Output ""(no such image)""\n$ErrorActionPreference = ""Stop""\n\nif ($LastExitCode -eq 0) {\n  Write-Output ""Found ${docker_image}""\n  Write-Output ""::set-output name=namespace::${pub_namespace}""\n  Write-Output ""::set-output name=needs_rebuild::false""\n}\nelse {\n\n  Write-Output ""${docker_image} not found""\n\n  # Image not found in pub namespace, look into the current\'s\n  # repo registry or in the originating repo when the workflow is\n  # triggered from a PR.\n  if (\'${{ github.event_name }}\' -eq \'pull_request\') {\n    # This is quite important since workflows don\'t have write\n    # permissions when the source branch is from a fork.\n    $github_namespace = ""ghcr.io/${{ github.event.pull_request.head.repo.full_name }}""\n  }\n  else {\n    $github_namespace = ""ghcr.io/${{ github.repository }}""\n  }\n  $docker_image = ""${github_namespace}/rez-win-${{ matrix.os-version }}-py-${{ matrix.py-version }}:${{ needs.image_tags.outputs.py }}"".ToLower()\n\n  Write-Output ""Inspecting image ${docker_image}...""\n  $ErrorActionPreference = ""Continue""\n  docker manifest inspect $docker_image *>$null || Write-Output ""(no such image)""\n  $ErrorActionPreference = ""Stop""\n\n  # Inform the next jobs that they need to use the ""private""\n  # registry.\n  Write-Output ""::set-output name=namespace::${github_namespace}""\n\n  if ($LastExitCode -ne 0) {\n    # Well, no images found at all! We will need to build the images.\n    Write-Output ""${docker_image} not found""\n    Write-Output ""::set-output name=needs_rebuild::true""\n  } else {\n    Write-Output ""Found ${docker_image}""\n    Write-Output ""::set-output name=needs_rebuild::false""\n  }\n}\n\nexit 0\n', ""echo '# Action required!\n\nThis branch is coming from a fork and the appropriate docker images were\nnot found in `${{ needs.public_py_image.outputs.namespace }}`.\n\nPlease ensure that you run the workflow in your fork. Once this is done,\nplease let the reviewers know so that they can re-run the workflow in\nthe context of the PR.' > $GITHUB_STEP_SUMMARY\n\nexit 1\n"", '$docker_image = ""${{ needs.public_py_image.outputs.namespace }}/rez-win-${{ matrix.os-version }}-base:${{ needs.image_tags.outputs.base }}"".ToLower()\n\nWrite-Output ""::set-output name=docker_image::${docker_image}""\n', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u $ --password-stdin', 'Write-Output ""Inspecting image ${{ steps.vars.outputs.docker_image }}...""\n$ErrorActionPreference = ""Continue""\ndocker manifest inspect ${{ steps.vars.outputs.docker_image }} *>$null || Write-Output ""(no such image)""\n$ErrorActionPreference = ""Stop""\n\nif ($LastExitCode -ne 0) {\n  Write-Output ""::set-output name=image_exists::false""\n}\nexit 0\n', 'Write-Output ""Building image ${{ steps.vars.outputs.docker_image }}...""\ncd .github\\docker\\rez-win-base\ndocker build `\n  --tag ${{ steps.vars.outputs.docker_image }} `\n  --build-arg WINDOWS_VERSION=""${{ matrix.windows-version }}"" `\n  .\n', 'Write-Output ""Pushing image ${{ steps.vars.outputs.docker_image }}...""\ndocker push ${{ steps.vars.outputs.docker_image }}\n', '# When publishing the images, we always publish in the current repo\'s package registry\n$base_docker_image = ""ghcr.io/${{ github.repository }}/rez-win-${{ matrix.os-version }}-base:${{ needs.image_tags.outputs.base }}"".ToLower()\n$docker_image = ""ghcr.io/${{ github.repository }}/rez-win-${{ matrix.os-version }}-py-${{ matrix.py-version }}:${{ needs.image_tags.outputs.py }}"".ToLower()\n\nWrite-Output ""::set-output name=base_docker_image::${base_docker_image}""\nWrite-Output ""::set-output name=docker_image::${docker_image}""\n', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | docker login ghcr.io -u $ --password-stdin', 'Write-Output ""Inspecting image ${{ steps.vars.outputs.docker_image }}...""\n$ErrorActionPreference = ""Continue""\ndocker manifest inspect ${{ steps.vars.outputs.docker_image }} *>$null || Write-Output ""(no such image)""\n$ErrorActionPreference = ""Stop""\n\nif ($LastExitCode -ne 0) {\n  Write-Output ""::set-output name=image_exists::false""\n}\nexit 0\n', 'Write-Output ""Pulling base image ${{ steps.vars.outputs.base_docker_image }}...""\ndocker pull ${{ steps.vars.outputs.base_docker_image }}\n', 'Write-Output ""Building image ${{ steps.vars.outputs.docker_image }}...""\ncd .github\\docker\\rez-win-py\n\ndocker build `\n  --tag ${{ steps.vars.outputs.docker_image }} `\n  --build-arg BASE_IMAGE_NAME=""${{ steps.vars.outputs.base_docker_image }}"" `\n  --build-arg IMAGE_NAME=""${{ steps.vars.outputs.docker_image }}"" `\n  --build-arg PYTHON_VERSION=""${{ matrix.py-version }}"" `\n  .\n', 'Write-Output ""Pushing image ${{ steps.vars.outputs.docker_image }}...""\ndocker push ${{ steps.vars.outputs.docker_image }}\n', '$docker_image = ""${{ needs.public_py_image.outputs.namespace }}/rez-win-${{ matrix.os-version }}-py-${{ matrix.py-version }}:${{ needs.image_tags.outputs.py }}"".ToLower()\n\nWrite-Output ""::set-output name=docker_image::${docker_image}""\nWrite-Output ""Using image ${docker_image}...""\n', 'docker pull ${{ steps.vars.outputs.docker_image }}\n', 'docker run --mount type=bind,src=$pwd,dst=C:\\checkout,readonly ${{ steps.vars.outputs.docker_image }}']"
"['python -m pip install ""Django~=${{ matrix.django-version }}.0""\n', 'python -m pip install -e "".[test]""\n', 'python runtests.py\n']"
"['python -m pip install --upgrade pip setuptools wheel\n', 'python setup.py sdist bdist_wheel', 'sudo apt-get install -y libusb-1.0-0-dev\ncd tools/ci\n./install-librtlsdr.sh\ncd ../..\necho ""NEW_LD_LIBPATH=$HOME/.local:$LD_LIBRARY_PATH"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -U pytest pytest-forked pytest-cov\npip install numpy\n', 'rm -Rf rtlsdr\n', 'pip install dist/*.whl', 'pip install dist/*.tar.gz', 'LD_LIBRARY_PATH=${{ env.NEW_LD_LIBPATH }} py.test --cov-config .coveragerc --cov=rtlsdr\nLD_LIBRARY_PATH=${{ env.NEW_LD_LIBPATH }} py.test --cov-append --cov-config .coveragerc --cov=rtlsdr --forked --no-overrides --pyargs tests/no_override*\n', 'python -m pip install --upgrade pip setuptools wheel twine\n', 'twine upload dist/*', 'sudo apt-get install -y libusb-1.0-0-dev\ncd tools/ci\n./install-librtlsdr.sh\ncd ../..\necho ""NEW_LD_LIBPATH=$HOME/.local:$LD_LIBRARY_PATH"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -U pytest pytest-forked pytest-cov\npip install numpy\npip install -e .\n', 'LD_LIBRARY_PATH=${{ env.NEW_LD_LIBPATH }} py.test --cov-config .coveragerc --cov=rtlsdr\nLD_LIBRARY_PATH=${{ env.NEW_LD_LIBPATH }} py.test --cov-append --cov-config .coveragerc --cov=rtlsdr --forked --no-overrides --pyargs tests/no_override*\n', 'python -m pip install coveralls\npython -m coveralls --service=github\n', 'pip3 install --upgrade coveralls\ncoveralls --finish\n']"
"['python -m pip install --upgrade wheel gevent\npython setup.py sdist bdist_wheel --universal\n', 'pip install --upgrade twine\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install gevent requests pytest nose\n', 'pytest tests.py\n']"
"['echo ::set-output name=tag::${GITHUB_REF#refs/tags/}', 'echo ""BRANCH=$(echo $GITHUB_REF | cut -d\'/\' -f 3)"" >> $GITHUB_ENV\ngit config remote.origin.fetch +refs/heads/*:refs/remotes/origin/*\ngit fetch --unshallow --tags\ngit tag\nif [[ $(git rev-parse --abbrev-ref HEAD) == ""master"" ]]; then\n    echo ""reattaching HEAD on master""\n    git symbolic-ref --short HEAD || git checkout -b ${GITHUB_ENV}-test $GITHUB_ENV\nfi\n\nsudo apt update -q\nyes | sudo ./scripts/bootstrap-dev-debian.sh\n./scripts/bootstrap-dev-pip.sh system\n#sudo apt install --no-install-recommends -y xvfb gir1.2-gtk-3.0 python3-gi python3-gi-cairo gir1.2-wnck-3.0 gobject-introspection libgirepository1.0-dev\n\nexport DISPLAY=:99.0\n', 'set -e\nmkdir test-rtd\nvirtualenv test-rtd\n. ./test-rtd/bin/activate\npip install -r requirements-dev.txt\nrm -rf test-rtd\n\n# for dev\nmake dev-actions\npipenv run pip install pycairo pygobject platformdirs importlib_metadata\n# check\nmake check\nmake build\nmake test-actions\n# prepare for deployment\nmake generate-paths\n', 'echo ::set-output name=tag::${GITHUB_REF#refs/tags/}', 'echo ""BRANCH=$(echo $GITHUB_REF | cut -d\'/\' -f 3)"" >> $GITHUB_ENV\ngit config remote.origin.fetch +refs/heads/*:refs/remotes/origin/*\ngit fetch --unshallow --tags\ngit tag\nif [[ $(git rev-parse --abbrev-ref HEAD) == ""master"" ]]; then\n    echo ""reattaching HEAD on master""\n    git symbolic-ref --short HEAD || git checkout -b ${GITHUB_ENV}-test $GITHUB_ENV\nfi\n\nsudo apt update -q\nyes | sudo ./scripts/bootstrap-dev-debian.sh\n./scripts/bootstrap-dev-pip.sh system\n#sudo apt install --no-install-recommends -y xvfb gir1.2-gtk-3.0 python3-gi python3-gi-cairo gir1.2-wnck-3.0 gobject-introspection libgirepository1.0-dev\n\nexport DISPLAY=:99.0\n', 'echo ""SETUPTOOLS_SCM_PRETEND_VERSION_FOR_GUAKE=$(echo $GITHUB_REF_NAME)"" >> $GITHUB_ENV', 'echo ""SETUPTOOLS_SCM_PRETEND_VERSION_FOR_GUAKE=$SETUPTOOLS_SCM_PRETEND_VERSION_FOR_GUAKE""\nset -e\nmkdir test-rtd\nvirtualenv test-rtd\n. ./test-rtd/bin/activate\npip install -r requirements-dev.txt\nrm -rf test-rtd\n\n# for dev\nmake dev-actions\npipenv run pip install pycairo pygobject\n# check\nmake build\n# prepare for deployment\nmake generate-paths\n']"
"['npm ci', 'npm run build', 'npm test']"
"['pip install -r tests/requirements.txt', 'python -m black --check .', 'python -m flake8', 'pip install -r docs/requirements.txt', 'mkdocs gh-deploy']"
"['python -m pip install --upgrade pip\npip install \\\n  pytest \\\n  tox\ncp tests/config-ci.py config.py\n', 'tox -e py\n', 'tox -e dist-check\n', 'tox -e codestyle\n', 'tox -e sort\n', 'tox -e security\n', 'tox -e docs\n']"
"['pip install --upgrade pip\npip install codecov tox tox-gh-actions\n', 'tox', 'codecov']"
"['python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'cd docs\nmake clean\nmake html\n', 'git clone https://github.com/cve-search/cve-search.git --branch gh-pages --single-branch gh-pages\ncp -r docs/build/html/* gh-pages/\ncd gh-pages\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""Update documentation"" -a || true\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'python3 ./test/test_data/setup_test_data.py\n', 'pytest --doctest-modules --junitxml=junit/test-results_${{ matrix.python-version }}.xml --cov-report=xml --cov-report=html --cov=./ test/unit\n', 'curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable""\nsudo apt-get update\nsudo apt-get -y -o Dpkg::Options::=""--force-confnew"" install docker-ce\n', 'git clone https://github.com/cve-search/CVE-Search-Docker.git\ncd CVE-Search-Docker\ndocker-compose build\ndocker-compose up -d\nsleep 180\n', 'git clone https://github.com/cve-search/CVE-Search-Docker.git\ncd CVE-Search-Docker\ndocker-compose build --build-arg REPO=$PR_REPO_OWNER$PR_REPO_NAME --build-arg BRANCH=$PR_BRANCH\ndocker-compose up -d\nsleep 180\n', 'pytest --doctest-modules --junitxml=junit/test-results_web_${{ matrix.python-version }}.xml test/web/\n']"
""
"['python -m pip install --upgrade pip\npip install django==${{ matrix.django-version }}\npip install redis django-redis rq sentry-sdk rq-scheduler\n', '`which django-admin` test django_rq --settings=django_rq.tests.settings --pythonpath=.\n']"
"[""python -m pip install --upgrade pip 'setuptools<58' --force-reinstall\npip install 'tox<4' tox-gh-actions flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n"", 'tox\n', 'tox\n', 'pip install codecov\ncodecov\n']"
"['python -m pip install build --user', 'python -m build -C--global-option=egg_info -C--global-option=--tag-build="""" --sdist --wheel --outdir dist/ .', 'python3 -m pip install --upgrade pip\npip install tox\n', 'tox -e ${TOX_VENV}', 'sudo apt-get -y install libevent-dev krb5-kdc krb5-admin-server libkrb5-dev\npython3 -m pip install --upgrade pip\npip install tox\n', 'tox -e ${TOX_VENV}']"
"['python -m pip install --upgrade pip\npip install tox flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox\n', 'tox\n', 'pip install codecov\ncodecov\n']"
""
"['python -m pip install --upgrade pip\npip install nox virtualenv\n', ""nox -s 'latest-${{ matrix.python }}(wtf=True, toolbar=True)' -- --cov-report=xml --cov-report=html"", 'python -m pip install --upgrade pip\npip install nox virtualenv\n', 'nox -s lint', 'python -m pip install --upgrade pip\npip install nox virtualenv\n', 'nox -s ci_cd_tests --python ${{ matrix.python }}']"
"['pip install black', 'black --quiet --check --diff .', 'pip install flake8 flake8-import-order flake8-bugbear pep8-naming', 'flake8', 'pip install mypy hypothesis pytest pytest-mock fastnumbers', 'mypy --strict natsort tests', 'pip install twine check-manifest', 'check-manifest --ignore "".github*,*.md,.coveragerc""\npython setup.py sdist\ntwine check dist/*\n', 'pip install wheel\npython setup.py sdist --format=gztar\npip wheel . -w dist\n', 'sudo apt-get update\nsudo apt-get install language-pack-de language-pack-en language-pack-cs\n', 'sudo apt-get install libicu-dev', 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions codecov\n', 'echo WITH_EXTRAS=fast,icu >> $GITHUB_ENV', 'tox', 'coverage xml']"
"['pip install -q wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install -U -q poetry pip\npoetry install -q\npoetry run pip install -q ""${{ matrix.django }}""\n', 'poetry run python -m pytest\n']"
"['python -m pip install --upgrade pip wheel setuptools\npip wheel . -w dist/\nls dist/\n', 'pip install twine\nTWINE_USERNAME=""__token__"" \\\nTWINE_PASSWORD=""${{ secrets.pypi_password }}"" \\\ntwine upload dist/*\n']"
""
"['pip install --upgrade pip', 'pip install black codespell flake8 isort mypy pytest pyupgrade tox', 'black --check .', 'codespell --quiet-level=2', 'flake8 . --count --show-source --statistics', 'isort --profile black .', 'tox', 'pip install -e .', 'mypy --ignore-missing-imports . || true', 'pytest .', 'pytest --doctest-modules . || true', 'shopt -s globstar && pyupgrade --py37-plus **/*.py']"
"['python -c ""import sqlmap; import sqlmapapi""', 'python sqlmap.py --smoke', 'python sqlmap.py --vuln']"
"['python -m pip install --upgrade pip\npip install setuptools wheel\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py sdist bdist_wheel\n']"
"['pip install --upgrade pip\npip install .[stats,docs]\n', 'sudo apt-get install pandoc\n', 'git clone https://github.com/mwaskom/seaborn-data.git\nls $SEABORN_DATA\n', 'cd doc\nmake -j `nproc` notebooks\nmake html\n', ""pip install --upgrade pip wheel\nif [[ ${{matrix.install}} == 'full' ]]; then EXTRAS=',stats'; fi\nif [[ ${{matrix.deps }} == 'pinned' ]]; then DEPS='-r ci/deps_pinned.txt'; fi\npip install .[dev$EXTRAS] $DEPS\n"", 'make test', 'pip install mypy flake8', 'make lint', 'make typecheck']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\npip install -e .\n', 'pytest --cov=misago\n', 'black --check .\nruff misago/notifications misago/apiv2\n']"
""
""
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install .\npython -m pip --quiet install ipython requests pytest\npython -m pip list\n', 'pytest tests\n']"
"['which python\npython --version\npython -c ""import struct; print(struct.calcsize(\'P\') * 8)""\nwhich pip\npip --version\n', 'pip3 install -U setuptools wheel pip\npip3 install -r requirements.txt\npip3 install -r requirements_gui.txt\npip3 install py2exe\n', 'python3 -c ""from PySide2 import __version__; print(__version__)""\npython3 -c ""from PySide2.QtCore import __version__; print(__version__)""\npython3 -c ""from PySide2.QtCore import QLibraryInfo; print(QLibraryInfo.location(QLibraryInfo.LibrariesPath))""\n', '$ver = (findstr version .\\syncplay\\__init__.py).split(""\'"")[1]\necho $ver\necho ""VER=$ver"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\npython buildPy2exe.py\nNew-Item -Path syncplay_v$ver -Name ""syncplay.ini"" -Value "" ""\n', 'dir', 'wget https://www.python.org/ftp/python/3.10.6/python-3.10.6-macos11.pkg\nsudo installer -verbose -pkg ./python-3.10.6-macos11.pkg -target /\necho ""/Library/Frameworks/Python.framework/Versions/3.10/bin"" >> $GITHUB_PATH\n', 'which python3\npython3 --version\nwhich pip3\npip3 --version\nfile $(which python3)\n', 'pip3 install -U pip setuptools wheel\npip3 install -r requirements.txt\npip3 install -r requirements_gui.txt\npip3 install py2app\n', 'pip3 uninstall zope.interface -y\npip3 install --no-binary :all: zope.interface\n\npip3 uninstall cffi -y\npip3 install --no-binary :all: cffi\n\npip3 uninstall cryptography -y\npip3 download --platform macosx_10_10_universal2 --only-binary :all: --no-deps --dest . cryptography\npip3 install --no-cache-dir --no-index --find-links . cryptography\n\npip3 uninstall charset-normalizer -y\npip3 download --platform macosx_10_9_universal2 --only-binary :all: --no-deps --dest . charset-normalizer\npip3 install --no-cache-dir --no-index --find-links . charset-normalizer\n', 'python3 -c ""from PySide6 import __version__; print(__version__)""\npython3 -c ""from PySide6.QtCore import __version__; print(__version__)""\npython3 -c ""from PySide6.QtCore import QLibraryInfo; print(QLibraryInfo.location(QLibraryInfo.LibrariesPath))""\npython3 -c ""import ssl; print(ssl)""\npython3 -c ""from py2app.recipes import pyside6""\npython3 -c \'from distutils.sysconfig import get_config_var; print(get_config_var(""LDLIBRARY""))\'\n', 'python3 buildPy2app.py py2app\n', 'ls -al\nexport VER=""$(cat syncplay/__init__.py | awk \'/version/ {gsub(""\\047"", """", $3); print $NF}\')""\necho ""VER=$VER"" >> $GITHUB_ENV\nmkdir dist_actions\nci/macos-deploy.sh\nls -al dist_actions\n', 'sudo apt-get install --no-install-recommends \\\n  libglib2.0-0 \\\n  libxkbcommon-x11-0 \\\n  libxcb1 \\\n;\n', 'ci/appimage-script.sh', 'ls -al\nexport VER=""$(cat syncplay/__init__.py | awk \'/version/ {gsub(""\\047"", """", $3); print $NF}\')""\necho ""VER=$VER"" >> $GITHUB_ENV\nmkdir dist_actions\nci/appimage-deploy.sh\nls -al dist_actions\n', 'ci/deb-script.sh', 'ci/deb-server-script.sh', 'ci/deb-installation-test.sh', 'ls -al\nexport VER=""$(cat syncplay/__init__.py | awk \'/version/ {gsub(""\\047"", """", $3); print $NF}\')""\necho ""VER=$VER"" >> $GITHUB_ENV\nmkdir dist_actions\nmv /tmp/syncplay.deb dist_actions/syncplay_${VER}.deb\nmv /tmp/syncplay-server.deb dist_actions/syncplay-server_${VER}.deb\nls -al dist_actions\n']"
"['python -m pip install --upgrade pip\npip install -r dev_requirements.txt\n', 'python setup.py build install\n']"
"['python -m pip install --upgrade pip\npip install redis==${{ matrix.redis-py-version }}\npython setup.py install\n', 'python run_tests.py\n']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest coverage pytest-cov\n', 'coverage run --source dpkt -m pytest dpkt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['git remote add freedoom https://github.com/freedoom/freedoom.git\ngit fetch --all\n', 'sudo apt update\nsudo apt install python3-pil asciidoc unzip zip ruby dos2unix\nsudo gem install asciidoctor-pdf --pre\n', 'git clone https://github.com/Doom-Utils/deutex.git\ncd deutex\ngit checkout v5.2.1\nsudo apt install libpng-dev\n./bootstrap\n./configure\nmake\nsudo make install\n', 'make dist\nmkdir -p artifacts/freedoom\nmkdir -p artifacts/freedm\nunzip wads/freedoom-*.zip -d artifacts/freedoom\nunzip wads/freedm-*.zip -d artifacts/freedm\nexport VERSION=$(git describe --abbrev=8)\nif [[ ${VERSION:0:1} == ""v"" ]]; then\n  export VERSION=${VERSION:1}\nfi\necho ""VERSION=$VERSION"" >> $GITHUB_OUTPUT\n']"
"['pip install virtualenv\nmake venv reqs-install\n', 'make mypy\n', 'make lint\n', 'make samples-download\n', 'pip install -e .\n', 'find exif-samples-master -name *.tiff -o -name *.jpg | xargs EXIF.py -dc\n', 'find exif-samples-master -name *.tiff -o -name *.jpg | sort -f | xargs EXIF.py > exif-samples-master/dump_test\ndiff -Z --side-by-side --suppress-common-lines exif-samples-master/dump exif-samples-master/dump_test\n']"
"['sudo apt-get install -y python3 python3-dev python3-setuptools virtualenv build-essential libpcap-dev libgraph-easy-perl libffi-dev\nmkdir venv\nvirtualenv venv\nsource venv/bin/activate\npip3 install Cython==0.29.32\npip3 install -e .\n', 'source venv/bin/activate\npython3 setup.py test\n', 'source venv/bin/activate\nsphinx-build -b html doc/documentation/source/ doc/documentation/build/\n']"
"['python -m pip install --upgrade pip\npip install Django==${{ matrix.django-version }}\npip install -e .\npip install -r requirements-test.txt\n', './runtests.py\n', './runtests.py --use-tz=false\n', 'python -m pip install --upgrade pip\npip install -e .\npip install -r requirements-dev.txt\n', 'flake8 --version\nflake8 .\n', 'python -m pip install --upgrade pip\npip install -e .\npip install -r requirements-dev.txt\n', 'isort --version\nisort -c .\n', 'python -m pip install --upgrade pip\npip install -e .\npip install -r requirements-dev.txt\n', 'check-manifest\n']"
"['python -m pip install --upgrade tox', 'tox ${{ matrix.toxargs }} -e ${{ matrix.toxenv }} -- ${{ matrix.toxposargs }}', 'python -m pip install --upgrade tox', 'tox ${{ matrix.toxargs }} -e ${{ matrix.toxenv }} -- ${{ matrix.toxposargs }}', 'python -m pip install --upgrade tox', 'tox ${{ matrix.toxargs }} -e ${{ matrix.toxenv }} -- ${{ matrix.toxposargs }}', 'python setup.py egg_info']"
"['python -m pip install --upgrade pip\npython -m pip install flake8 pytest\n', 'pip install .', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n', 'pip install twine wheel\npip wheel -w dist --no-deps .\npython setup.py sdist\ntwine upload dist/*\n']"
"['python -Im pip install --upgrade wheel nox', 'nox --python ${{ matrix.python-version }}', 'nox --python ${{ matrix.python-version }} -e tests', 'python -Im pip install --upgrade coverage[toml]', 'python -Im coverage combine\npython -Im coverage html --skip-covered --skip-empty\npython -Im coverage report --fail-under=100\n', 'python -Im pip install -e .[dev]', ""python -c 'import doc2dash; print(doc2dash.__title__)'"", 'python -Im doc2dash --version', 'python -Im pip install nox', 'nox -e pin_for_pyoxidizer', 'echo ""count=$(git status --porcelain=v1 2>/dev/null | wc -l)"" >>$GITHUB_OUTPUT', 'git config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""Automated dependency upgrades for ${{ matrix.runs-on}}""\ngit push -f origin ${{ github.ref_name }}:${{ env.BRANCH_NAME }}\n', 'PR=$(gh pr list --head ${{ env.BRANCH_NAME }} --json number -q \'.[0].number\')\nif [ -z $PR ]; then\n  gh pr create \\\n  --head ${{ env.BRANCH_NAME }} \\\n  --title ""Automated dependency upgrades for ${{ matrix.runs-on}}"" \\\n  --body ""Full log: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}""\nelse\n  echo ""Pull request already exists, won\'t create a new one.""\nfi\n', 'python -Im pip install nox', 'nox -e oxidize', 'cd build/*/release/install/\n./doc2dash --version\n', 'cd build/*/release/install/\n./doc2dash.exe --version\n']"
"['[ ! -e out/crash-* ]\necho No legacy crash detected\n', 'python3 .github/workflows/system-info.py', '.ci/install.sh\n', '.ci/build.sh\n', 'make doccheck\n', 'python3 .github/workflows/system-info.py', 'python3 -m pip install -U pip\npython3 -m pip install -U tox\n', 'tox -e lint', 'git config --global core.autocrlf input\n', 'ln -sf c:/cygwin/bin/python3.${{ matrix.python-minor-version }} c:/cygwin/bin/python3\n', 'dash.exe -c ""python3 .github/workflows/system-info.py""\n', 'bash.exe .ci/install.sh\n', 'python3 -m pip install -U numpy\n', 'SETUPTOOLS_USE_DISTUTILS=stdlib .ci/build.sh\n', ""bash.exe xvfb-run -s '-screen 0 1024x768x24' .ci/test.sh\n"", 'dash.exe -c ""mkdir -p Tests/errors""\n', 'bash.exe .ci/after_success.sh\n', 'echo Cygwin Test Successful', 'python3 .github/workflows/system-info.py', 'docker run --rm --privileged aptman/qus -s -- -p ${{ matrix.qemu-arch }}\n', 'docker pull pythonpillow/${{ matrix.docker }}:${{ matrix.dockerTag }}\n', '# The Pillow user in the docker container is UID 1000\nsudo chown -R 1000 $GITHUB_WORKSPACE\ndocker run --name pillow_container  -v $GITHUB_WORKSPACE:/Pillow pythonpillow/${{ matrix.docker }}:${{ matrix.dockerTag }}\nsudo chown -R runner $GITHUB_WORKSPACE\n', 'PATH=""$PATH:~/.local/bin""\ndocker start pillow_container\npil_path=`docker exec pillow_container /vpy3/bin/python -c \'import os, PIL;print(os.path.realpath(os.path.dirname(PIL.__file__)))\'`\ndocker stop pillow_container\nsudo mkdir -p $pil_path\nsudo cp src/PIL/*.py $pil_path\n.ci/after_success.sh\n', 'echo Docker Test Successful', 'echo ""C:\\msys64\\usr\\bin\\"" >> $env:GITHUB_PATH', 'pacman -S --noconfirm \\\n    ${{ matrix.package }}-freetype \\\n    ${{ matrix.package }}-gcc \\\n    ${{ matrix.package }}-ghostscript \\\n    ${{ matrix.package }}-lcms2 \\\n    ${{ matrix.package }}-libimagequant \\\n    ${{ matrix.package }}-libjpeg-turbo \\\n    ${{ matrix.package }}-libraqm \\\n    ${{ matrix.package }}-libtiff \\\n    ${{ matrix.package }}-libwebp \\\n    ${{ matrix.package }}-openjpeg2 \\\n    ${{ matrix.package }}-python3-cffi \\\n    ${{ matrix.package }}-python3-numpy \\\n    ${{ matrix.package }}-python3-olefile \\\n    ${{ matrix.package }}-python3-pip \\\n    ${{ matrix.package }}-python3-setuptools\n\nif [ ${{ matrix.package }} == ""mingw-w64-x86_64"" ]; then\n    pacman -S --noconfirm \\\n        ${{ matrix.package }}-python-pyqt6\nfi\n\npython3 -m pip install pyroma pytest pytest-cov pytest-timeout\n\npushd depends && ./install_extra_test_images.sh && popd\n', 'SETUPTOOLS_USE_DISTUTILS=""stdlib"" CFLAGS=""-coverage"" python3 -m pip install --global-option=""build_ext"" .', 'python3 selftest.py --installed\npython3 -c ""from PIL import Image""\npython3 -m pytest -vx --cov PIL --cov Tests --cov-report term --cov-report xml Tests\n', 'echo MinGW Test Successful', 'python3 .github/workflows/system-info.py', 'docker pull pythonpillow/${{ matrix.docker }}:${{ matrix.dockerTag }}\n', '# The Pillow user in the docker container is UID 1000\nsudo chown -R 1000 $GITHUB_WORKSPACE\ndocker run --name pillow_container -e ""PILLOW_VALGRIND_TEST=true"" -v $GITHUB_WORKSPACE:/Pillow pythonpillow/${{ matrix.docker }}:${{ matrix.dockerTag }}\nsudo chown -R runner $GITHUB_WORKSPACE\n', 'python3 .github/workflows/system-info.py', 'python3 -m pip install setuptools wheel pytest pytest-cov pytest-timeout defusedxml', '7z x winbuild\\depends\\nasm-2.16.01-win64.zip ""-o$env:RUNNER_WORKSPACE\\""\necho ""$env:RUNNER_WORKSPACE\\nasm-2.16.01"" >> $env:GITHUB_PATH\n\nchoco install ghostscript --version=10.0.0.20230317\necho ""C:\\Program Files\\gs\\gs10.00.0\\bin"" >> $env:GITHUB_PATH\n\n# Install extra test images\nxcopy /S /Y Tests\\test-images\\* Tests\\images\n\n# make cache key depend on VS version\n& ""C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe"" `\n  | find """"""catalog_buildVersion"""""" `\n  | ForEach-Object { $a = $_.split("" "")[1]; echo ""vs=$a"" >> $env:GITHUB_OUTPUT }\n', '& python.exe winbuild\\build_prepare.py -v --python $env:pythonLocation\n', '& winbuild\\build\\build_dep_libjpeg.cmd', '& winbuild\\build\\build_dep_zlib.cmd', '& winbuild\\build\\build_dep_xz.cmd', '& winbuild\\build\\build_dep_libwebp.cmd', '& winbuild\\build\\build_dep_libtiff.cmd', '& winbuild\\build\\build_dep_libpng.cmd', '& winbuild\\build\\build_dep_brotli.cmd', '& winbuild\\build\\build_dep_freetype.cmd', '& winbuild\\build\\build_dep_lcms2.cmd', '& winbuild\\build\\build_dep_openjpeg.cmd', '& winbuild\\build\\build_dep_libimagequant.cmd', '& winbuild\\build\\build_dep_harfbuzz.cmd', '& winbuild\\build\\build_dep_fribidi.cmd', 'rmdir /S /Q winbuild\\build\\src', '$FLAGS=""""\nif (\'${{ github.event_name }}\' -ne \'pull_request\') { $FLAGS=""--disable-imagequant"" }\n& winbuild\\build\\build_pillow.cmd $FLAGS install\n& $env:pythonLocation\\python.exe selftest.py --installed\n', '& reg.exe add ""HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Image File Execution Options\\python.exe"" /v ""GlobalFlag"" /t REG_SZ /d ""0x02000000"" /f\n', 'path %GITHUB_WORKSPACE%\\\\winbuild\\\\build\\\\bin;%PATH%\npython.exe -m pytest -vx -W always --cov PIL --cov Tests --cov-report term --cov-report xml Tests\n', 'mkdir -p Tests/errors\n', '.ci/after_success.sh\n', 'mkdir fribidi\\${{ matrix.architecture }}\ncopy winbuild\\build\\bin\\fribidi* fribidi\\${{ matrix.architecture }}\nsetlocal EnableDelayedExpansion\nfor %%f in (winbuild\\build\\license\\*) do (\n  set x=%%~nf\n  rem Skip FriBiDi license, it is not included in the wheel.\n  set fribidi=!x:~0,7!\n  if NOT !fribidi!==fribidi (\n    rem Skip imagequant license, it is not included in the wheel.\n    set libimagequant=!x:~0,13!\n    if NOT !libimagequant!==libimagequant (\n      echo. >> LICENSE\n      echo ===== %%~nf ===== >> LICENSE\n      echo. >> LICENSE\n      type %%f >> LICENSE\n    )\n  )\n)\nfor /f ""tokens=3 delims=/"" %%a in (""${{ github.ref }}"") do echo dist=dist-%%a >> %GITHUB_OUTPUT%\nwinbuild\\\\build\\\\build_pillow.cmd --disable-imagequant bdist_wheel\n', 'echo Windows Test Successful', 'python3 .github/workflows/system-info.py', '.ci/install.sh\n', '.github/workflows/macos-install.sh\n', '.ci/build.sh\n', 'if [ $REVERSE ]; then\n  python3 -m pip install pytest-reverse\nfi\nif [ ""${{ matrix.os }}"" = ""ubuntu-latest"" ]; then\n  xvfb-run -s \'-screen 0 1024x768x24\' sway&\n  export WAYLAND_DISPLAY=wayland-1\n  .ci/test.sh\nelse\n  .ci/test.sh\nfi\n', 'mkdir -p Tests/errors\n', '.ci/after_success.sh\n', 'echo Test Successful']"
"['pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=13 --max-line-length=127 --statistics\n', 'python -m pip install --upgrade pip\npip install celery==${{ matrix.celery-version }}\npip install tornado==${{ matrix.tornado-version }}\npip install -r requirements/default.txt\npip install -r requirements/test.txt\n', 'python -m flower --version\npython -m tests.unit.__main__\n']"
""
"['git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install ""${{matrix.numpy}}""', 'pip install ""${{matrix.uncertainties}}""', 'pip install ${{matrix.extras}}', 'sudo apt install -y graphviz\npip install pytest pytest-cov pytest-subtests packaging\npip install .\n', 'pip install pytest-mpl', 'pytest $TEST_OPTS\n', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install ""${{matrix.numpy}}""', '# sudo apt install -y graphviz\npip install pytest pytest-cov pytest-subtests packaging\npip install .\n', 'pytest ${env:TEST_OPTS}', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install ""${{matrix.numpy}}""', 'pip install pytest pytest-cov pytest-subtests packaging\npip install .\n', 'pytest $TEST_OPTS\n', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'echo ""::set-output name=dir::$(pip cache dir)""', 'sudo apt install -y pandoc\npip install --upgrade pip setuptools wheel\npip install -r ""requirements_docs.txt""\npip install docutils==0.14 commonmark==0.8.1 recommonmark==0.5.0 babel==2.8\npip install .\n', 'sphinx-build -n -j auto -b html -d build/doctrees docs build/html', 'sphinx-build -a -j auto -b doctest -d build/doctrees docs build/doctest', 'python -m pip install --upgrade pip', 'python -m pip install --upgrade pre-commit', 'python -m pip list', 'python -m pip install build', 'python -m build']"
"['python -m pip install pre-commit', 'pre-commit run --all-files', 'python -m pip install --upgrade pip wheel\npip install --upgrade --upgrade-strategy only-if-needed pytest\npip install .\n', 'pytest\n']"
"['sudo apt update', 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox -v']"
['exit 1']
"['pip install -e .\npip install Cython\n', 'buildozer --help', 'buildozer init', 'sed -i.bak ""s/# android.accept_sdk_license = False/android.accept_sdk_license = True/"" buildozer.spec\nsed -i.bak ""s/#p4a.branch = master/p4a.branch = develop/"" buildozer.spec\nbuildozer android p4a -- --help\n', 'sudo apt -y install automake', 'brew install automake\nsudo ln -sfn /usr/local/opt/openssl /usr/local/ssl\n', 'touch main.py\nbuildozer android debug\n', 'touch main.py\nexport BUILDOZER_ALLOW_ORG_TEST_DOMAIN=1\nbuildozer android release\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npip install -e .\npip install Cython cookiecutter pbxproj\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nbuildozer --help\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nbuildozer init\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nbrew install autoconf automake libtool pkg-config\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ntouch main.py\nbuildozer ios debug\n', 'python -m pip install --upgrade setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine check dist/*\n', 'pip install -U coveralls setuptools tox>=2.0\n', 'tox', 'coveralls', 'docker build --tag=kivy/buildozer .', 'docker run kivy/buildozer --version', '# we don\'t want the build to fail with the exit 1 so we catch with ""||""\npython2 -m pip install -e . 2> error.log || echo Failing as expected\ncat error.log\ngrep ""Unsupported Python version"" error.log\n']"
"['sudo apt-get install -y gcc python-dev libkrb5-dev', 'pip install coveralls\npip install .[credssp,kerberos]\npip install -r requirements-test.txt\n', 'pytest -v --flake8 --cov=winrm --cov-report=term-missing winrm/tests/\n', 'pytest -v --flake8 --cov=winrm --cov-report=term-missing winrm/tests/\n', 'coveralls --service=github']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade ""tox<4"" ""tox-gh-actions<3""\n', 'tox --verbose\n']"
"['python3 setup.py build', 'ant -f native/build.xml']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'PYTHON_ENV=""py$(echo $PYTHON_VERSION | sed \'s/\\.//;s/\\-dev//\')""\ntox -e ""${PYTHON_ENV}-${PANDAS_PRESENCE}""\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-t-pre37.txt ]; then pip install -r requirements-t-pre37.txt; fi\n', './scripts/testing.sh\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f requirements-t.txt ]; then pip install -r requirements-t.txt; fi\n', './scripts/flake8.sh\n', 'pytest\n']"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install .\npython -m pip --quiet install ipythonblocks matplotlib pytest\npython -m pip list\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'make test\n']"
"['pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.check }}\n', 'pip install --upgrade pip\npip install tox\n', 'tox -e py', 'pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'python -m build', 'twine upload dist/*\n']"
"['short_sha=$(git rev-parse --short HEAD)\nimage=""jupyter/nbviewer:$short_sha""\necho ""::set-output name=image::$image""\ntags=""$image""\nif ""${{ github.ref_name }}"" == ""main"" ]]; then\n  tags=""jupyter/nbviewer:latest $tags""\nfi\necho ""::set-output name=tags::$tags""\n', 'echo ${{ steps.vars.outputs.sha_short }}', 'docker run --rm -i ${{ steps.vars.outputs.image }} python3 -c ""import nbviewer, pycurl, pylibmc""\ndocker run --rm -i ${{ steps.vars.outputs.image }} python3 -m nbviewer --help-all\n', 'sudo apt-get update && sudo apt-get -y install \\\n    libcurl4-gnutls-dev \\\n    libgnutls28-dev\n', 'python3 -m pip install -r requirements.in -r requirements-dev.txt\n', 'python3 -m pip install mypy types-pycurl types-requests types-Markdown pyflakes\n', 'mypy nbviewer\n', 'pyflakes nbviewer\n', 'python3 -m pip freeze\n', 'pytest -v nbviewer/tests -s\n']"
"['python -m pip install --upgrade pip\npython -m pip install coverage coveralls ninja\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'src/test/runall.sh\n']"
"['python -m pip install cibuildwheel~=2.12.3\npython -m cibuildwheel --output-dir dist\n', 'source .ci/utils.sh\nensure_python_version 3.11\npython -m pip install cibuildwheel~=2.12.3\npython -m cibuildwheel --output-dir dist\n', 'pip install -U setuptools\npython setup.py sdist\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nbrew install ant\n', 'sudo apt-get update && sudo apt-get install -y ant\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\nant all\n', 'ant all', 'python -m pip install --find-links=dist pyjnius[dev,ci]', 'source .ci/utils.sh\nensure_python_version ${{ matrix.python }}\npython -m pip install --find-links=dist pyjnius[dev,ci]\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\npython -m pip install --find-links=dist pyjnius[dev,ci]\n', 'source .ci/utils.sh\nensure_python_version ${{ matrix.python }}\ncd tests\nCLASSPATH=../build/test-classes:../build/classes python -m pytest -v\n', 'source .ci/osx_ci.sh\narm64_set_path_and_python_version ${{ matrix.python }}\ncd tests\nCLASSPATH=../build/test-classes:../build/classes python -m pytest -v\n', 'cd tests\n$env:PATH +="";$env:JAVA_HOME\\jre\\bin\\server\\;$env:JAVA_HOME\\jre\\bin\\client\\;$env:JAVA_HOME\\bin\\server\\""\n$env:CLASSPATH =""../build/test-classes;../build/classes""\npython -m pytest -v\n', 'cd tests\n$env:CLASSPATH =""../build/test-classes;../build/classes""\npython -m pytest -v\n', ""linux32 sh -ec '\n  cat /etc/*release || true\n  uname -a\n  git version\n  python --version\n'\n"", ""linux32 sh -ec '\n  git config --global --add safe.directory '*'\n  git init .\n  git remote add origin https://github.com/$GITHUB_REPOSITORY\n  git config --local gc.auto 0\n  git fetch --no-tags --prune --progress --no-recurse-submodules \\\n    --depth=1 origin +$GITHUB_SHA:refs/remotes/origin/${GITHUB_REF##*/}\n  git checkout --progress --force -B ${GITHUB_REF##*/} \\\n    refs/remotes/origin/${GITHUB_REF##*/}\n  git log -1 --format=%H\n'\n"", ""linux32 sh -ec '\n  apt update\n  apt install -y openjdk-11-jdk-headless ant\n  java -version\n'\n"", ""linux32 sh -ec '\n  pip install --timeout=120 .[dev,ci]\n'\n"", ""linux32 sh -ec '\n  ant all\n  cd tests\n  CLASSPATH=../build/test-classes:../build/classes pytest -v\n'\n"", 'brew install ant', 'ant all', 'pip install --timeout=120 .[dev,ci]\n', '$env:PATH +="";$env:JAVA_HOME\\jre\\bin\\server\\;$env:JAVA_HOME\\jre\\bin\\client\\;$env:JAVA_HOME\\bin\\server\\""\n$env:CLASSPATH =""../build/test-classes;../build/classes""\ncd tests\npytest -v\n', 'cd tests\nCLASSPATH=../build/test-classes:../build/classes python -m pytest -v\n']"
"['pip install --upgrade pip setuptools wheel', 'pip install .[all]', 'black --check --skip-string-normalization . || true', 'ruff --format=github .', 'pip install -r pex-requirements.txt -r tests/requirements.txt', 'mypy .', 'safety check || true', 'pip install pre-commit', 'pre-commit --version', 'pre-commit install', 'pre-commit run --all-files', 'pip install setuptools==""${{ matrix.setuptools-version }}""', 'pip install .', 'pip install tox', 'tox -e py']"
"['pipx run check-manifest\n', 'echo ""PACKAGES=cython=0.29.24 matplotlib-base=3.4 numpy=1.21 owslib=0.24.1 pyproj=3.1 proj=8.0 scipy=1.6.3 shapely=1.7.1"" >> $GITHUB_ENV\n', 'echo ""PACKAGES=cython fiona matplotlib-base numpy pyproj pykdtree scipy shapely"" >> $GITHUB_ENV\n', 'echo ""PACKAGES=cython fiona matplotlib-base numpy pyproj pykdtree scipy geos"" >> $GITHUB_ENV\n', 'echo ""PACKAGES=$PACKAGES pytest-cov coveralls"" >> $GITHUB_ENV\necho ""CYTHON_COVERAGE=1"" >> $GITHUB_ENV\necho ""EXTRA_TEST_ARGS=--cov=cartopy -ra"" >> $GITHUB_ENV\n', 'PACKAGES=""$PACKAGES owslib pep8 pillow pyshp pytest pytest-mpl!=0.16.0""\nPACKAGES=""$PACKAGES pytest-xdist setuptools_scm""\nconda install $PACKAGES\nconda info -a\nconda list\n', 'python -m pip install git+https://github.com/shapely/shapely.git@main\n', 'MPL_CONFIG_DIR=~/.config/matplotlib\nmkdir -p $MPL_CONFIG_DIR\necho ""backend : agg"" > $MPL_CONFIG_DIR/matplotlibrc\npip install --no-deps -e .\npython -c ""import cartopy; print(\'Version \', cartopy.__version__)""\n', '# Check that the downloader tool at least knows where to get the data from (but don\'t actually download it)\npython tools/cartopy_feature_download.py gshhs physical --dry-run\nCARTOPY_GIT_DIR=$PWD\nPYPROJ_GLOBAL_CONTEXT=ON pytest -ra -n 4 --doctest-modules \\\n    --color=yes \\\n    --mpl --mpl-generate-summary=html \\\n    --mpl-results-path=""cartopy_test_output-${{ matrix.os }}-${{ matrix.python-version }}"" \\\n    --pyargs cartopy ${EXTRA_TEST_ARGS}\n', 'coveralls --service=github', 'PACKAGES=""cython fiona matplotlib-base numpy pyproj pykdtree scipy""\nPACKAGES=""$PACKAGES owslib pep8 pillow pyshp pytest""\nPACKAGES=""$PACKAGES pytest-xdist setuptools_scm shapely""\nconda install $PACKAGES\n', 'python setup.py build_ext sdist', 'mkdir dist\ncp packages-*/* dist/\n']"
"['pip install wheel\npython setup.py bdist_wheel\npython setup.py sdist\n', 'python -m pip install .[tests]\n', 'pytest --pylama pylama', 'pytest tests']"
"['python -m pip install --upgrade cibuildwheel', 'python -m cibuildwheel', 'python setup.py sdist', 'ls -l dist', 'python -m pip install "".[test]""', 'python -m pytest']"
"['python -m pip install --upgrade pip\npython -m pip install build --user\n', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip\npip install pytz\npip install ""Django==${{ matrix.django-version }}.*""\n', '`which django-admin` test post_office --settings=post_office.test_settings --pythonpath=.\n']"
"['pip install tox-gh-actions', 'tox']"
"['git checkout HEAD^2', 'version=$(echo ""${{ github.ref }}"" | cut -d/ -f3)\ngrep ${version} VERSION.txt || exit 1\n', 'echo ""${{ steps.create_release.outputs.upload_url }}"" > upload_url.txt\n', 'apt update\nexport DEBIAN_FRONTEND=noninteractive\napt -y install python3-stdeb dh-python\n# https://bugs.launchpad.net/bugs/1916551\nsed -i -e ""s/python-all/python3-all/g"" /usr/lib/python3/dist-packages/stdeb/util.py\n', 'export upload_url=$(cat upload_url.txt)\nrm upload_url.txt\necho ""upload_url=${upload_url}"" >> $GITHUB_ENV\n', 'python3 setup.py --command-packages=stdeb.command bdist_deb\n', 'echo ""version=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', 'dnf -y install rpm-build python3 python3-setuptools git\n', 'export upload_url=$(cat upload_url.txt)\nrm upload_url.txt\necho ""upload_url=${upload_url}"" >> $GITHUB_ENV\n', 'python3 setup.py bdist_rpm\n', 'echo ""version=${GITHUB_REF#refs/*/}"" >> $GITHUB_ENV', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'pip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['python -m pip install -U tox\n', 'tox -e py${{ matrix.python-version }}-django${{ matrix.django-version }}', 'python -m pip install -U build\npython -m build\n', 'echo ""ðŸŽ‰""']"
"['pip install .', './check-migrations.sh']"
""
"['pip install setuptools twine\n', 'python setup.py release sdist\ntwine upload dist/*\n']"
"['pip install -r requirements.txt\npip install tox tox-gh-actions\n', 'tox', 'pip install flake8\nflake8 ./\n', 'pip install coverage[toml] django==3.2.15\ncoverage run ./example_project/manage.py test photologue\n']"
""
""
"[""python -m pip install --upgrade pip 'tox<4.0'\n"", ""sudo /etc/init.d/mysql start\nmysql -e 'create database IF NOT EXISTS luigi_test;' -uroot -proot || true\nmysql -e 'create user 'travis'@'localhost';' -uroot -proot || true\nmysql -e 'grant all privileges ON *.* TO 'travis'@'localhost';' -uroot -proot || true\n"", 'tox', 'pip install codecov\ncodecov -e ${{ matrix.tox-env }}\n', ""python -m pip install --upgrade pip 'tox<4.0'\n"", ""PGPASSWORD=postgres psql -h localhost -p 5432 -c 'create database spotify;' -U postgres\n"", 'tox', 'pip install codecov\ncodecov -e ${{ matrix.tox-env }}\n', ""python -m pip install --upgrade pip 'tox<4.0'\n"", 'tox', 'pip install codecov\ncodecov -e ${{ matrix.tox-env }}\n']"
"['python --version --version', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install -U tox virtualenv', 'tox --notest -p auto --parallel-live', 'tox', 'pip install tox', 'tox --notest -p auto --parallel-live', 'tox', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install -U tox virtualenv', 'tox --notest -p auto --parallel-live', 'tox', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'pip install tox', 'tox --notest -p auto --parallel-live', 'tox', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'echo ""sha-256=$(python -VV | sha256sum | cut -d\' \' -f1)"" >> $GITHUB_OUTPUT', 'pip install tox', 'tox --notest -p auto --parallel-live', 'tox', 'python -m pip install -U pip\npython -m pip install -U twine build setuptools-scm\n', 'python -m setuptools_scm\npython -m build\ntwine check --strict dist/*\n']"
"['sudo apt-get install -y ccache sed gcc\n', 'brew install ccache\necho CFLAGS=$CFLAGS -Wno-parentheses-equality >>$GITHUB_ENV\n', 'echo G_USE_COV=--coverage >> $GITHUB_ENV\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install -U pip\npip install -U -q setuptools wheel twine\npip install -q -U \'faulthandler; python_version == ""2.7"" and platform_python_implementation == ""CPython""\'\npip install -q -U \'cffi;platform_python_implementation==""CPython""\'\npip install -q -U \'cython>=3.0a9\'\npip install \'greenlet>=2.0.0 ;platform_python_implementation==""CPython""\'\n', '# Next, build the wheel *in place*. This helps ccache, and also lets us cache the configure\n# output (pip install uses a random temporary directory, making this difficult)\npython setup.py build_ext -i\npython setup.py bdist_wheel\n', 'ls -l dist\ntwine check dist/*\n', 'twine upload --skip-existing dist/*\n', 'pip install -U -e .[test]\n', 'python --version\npython -c \'import greenlet; print(greenlet, greenlet.__version__)\'\npython -c \'import gevent; print(gevent.__version__)\'\npython -c \'from gevent._compat import get_clock_info; print(get_clock_info(""perf_counter""))\'\npython -c \'import gevent.core; print(gevent.core.loop)\'\npython -c \'import gevent.ares; print(gevent.ares)\'\nccache -s\n', ""pip install -U 'pylint<2.15'\npython -m pylint --rcfile=.pylintrc gevent\n"", 'python -m gevent.tests --second-chance $G_USE_COV\n', 'python -m gevent.tests --second-chance $G_USE_COV `(cd src/gevent/tests >/dev/null && ls test__*subprocess*.py)`\n', 'python -mgevent.tests --second-chance $G_USE_COV --ignore tests_that_dont_use_resolver.txt\n', 'python -mgevent.tests --second-chance $G_USE_COV --ignore tests_that_dont_use_resolver.txt\n', 'python -m gevent.tests --second-chance --ignore tests_that_dont_do_leakchecks.txt\n', 'python -mgevent.tests --second-chance --coverage\n', 'python -m gevent.tests --second-chance $G_USE_COV\n', 'python -m gevent.tests --second-chance $G_USE_COV\n', 'python -m coverage combine || true\npython -m coverage report -i || true\n', 'sudo apt-get install -y ccache sed gcc\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install -U pip\npip install -U -q setuptools wheel twine\npip install -q -U \'faulthandler; python_version == ""2.7"" and platform_python_implementation == ""CPython""\'\npip install -q -U \'cffi;platform_python_implementation==""CPython""\'\npip install -q -U \'cython>=3.0a5\'\npip install \'greenlet>=2.0.0; platform_python_implementation==""CPython""\'\n', '# These need to be absolute paths\nexport BUILD_LIBS=""$HOME/.libs/""\nmkdir -p $BUILD_LIBS\nexport LDFLAGS=-L$BUILD_LIBS/lib\nexport CPPFLAGS=""-I$BUILD_LIBS/include""\nenv | sort\necho which sed? `which sed`\necho LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$BUILD_LIBS/lib >>$GITHUB_ENV\n(pushd deps/libev && sh ./configure -C --prefix=$BUILD_LIBS && make install && popd)\n(pushd deps/c-ares && sh ./configure -C --prefix=$BUILD_LIBS && make -j4 install && popd)\n\n(pushd deps/libuv && ./autogen.sh && sh ./configure -C --disable-static --prefix=$BUILD_LIBS && make -j4 install && popd)\n# libev builds a manpage each time, and it includes today\'s date, so it frequently changes.\n# delete to avoid repacking the archive\nrm -rf $BUILD_LIBS/share/man/\nls -l $BUILD_LIBS $BUILD_LIBS/lib $BUILD_LIBS/include\npython setup.py bdist_wheel\npip uninstall -y gevent\npip install -U `ls dist/*whl`[test]\n# Test that we\'re actually linking\n# to the .so file.\nobjdump -p build/lib*/gevent/libev/_corecffi*so | grep ""NEEDED.*libev.so""\nobjdump -p build/lib*/gevent/libev/corecext*so | grep ""NEEDED.*libev.so""\nobjdump -p build/lib*/gevent/libuv/_corecffi*so | grep ""NEEDED.*libuv.so""\nobjdump -p build/lib*/gevent/resolver/cares*so | grep ""NEEDED.*libcares.so""\n', ""# Verify that we got non-embedded builds\npython -c 'import gevent.libev.corecffi as CF; assert not CF.LIBEV_EMBED'\npython -c 'import gevent.libuv.loop as CF; assert not CF.libuv.LIBUV_EMBED'\npython -mgevent.tests --second-chance\n"", 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install -U pip', 'docker run --rm --privileged hypriot/qemu-register\n', 'scripts/releases/make-manylinux', 'sudo chown -R $(whoami) ${{ steps.pip-cache.outputs.dir }}']"
"['sudo pip install pytest\nvim --version\n', 'make test', 'vim --version\nmake check\n', 'sudo add-apt-repository ppa:neovim-ppa/stable -y\nsudo apt-get update -q\nsudo apt-get install neovim -y\n\nsudo pip install pynvim pytest-cov\nsudo pip list\nnvim --version\n', 'make --keep-going test_coverage BUILD_VIRTUAL_ENV=$VIRTUAL_ENV\n', 'coverage xml\nbash <(curl -s https://codecov.io/bash) -X fix -f coverage.xml -F py${TRAVIS_PYTHON_VERSION//./}\n']"
[]
"['python -m pip install --upgrade pip\npython -m pip install black\n', 'black --check termgraph/\n']"
"['docker compose create\ndocker compose start\n# Wait for the services to accept connections,\n# TODO: do that smarter, poll connection attempt until it succeeds\nsleep 30\n', 'pip install .\npip install pytest\n', 'pytest -k ""not test_no_trailing_rotate_event and not test_end_log_pos""\n', 'python3 -m pip install build --user', 'python3 -m build --sdist --outdir dist/']"
"['./ci/get_hdf5_if_needed.sh\n', 'python -m pip install --upgrade pip\npython3 -m pip install tox\n', 'tox\necho -n ""api.h5py.org"" > docs_api/_build/html/CNAME\n']"
""
['python -m pip install -U pip\n# Use tox-gh to configure the python environment\n# See https://github.com/tox-dev/tox-gh\npython -m pip install tox_gh pymongo==${{ matrix.pymongo-version }}\ntox\n']
"['sudo apt install fontforge\nsudo python3 -m pip install -r requirements.txt\n', 'make travistest\n']"
""
"['pip install tox tox-gh-actions coverage', 'tox', 'coverage xml', 'pip install tox', 'tox -e isort', 'tox -e flake8', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n']"
"['PACKAGES=""""\n# Install a specific python version as configured\nPACKAGES=""$PACKAGES python${{matrix.python-version}}""\n# Normal dependencies\nPACKAGES=""$PACKAGES gettext intltool python3-gi python3-cairo python3-gi-cairo python3-distutils python3-dbus python3-xdg libglib2.0-dev libglib2.0-bin gir1.2-gtk-3.0 gtk-update-icon-cache""\n# For dbus-launch\nPACKAGES=""$PACKAGES dbus-x11""\n\nsudo apt-get update\nsudo apt-get install ${PACKAGES}\n', './waf configure build\nsudo ./waf install\n', 'dbus-launch python${{ matrix.python-version }} -m unittest\n', 'pip install flake8\n', 'flake8 --count --show-source --statistics\n', 'PACKAGES=""flatpak flatpak-builder dbus-x11""\n\nsudo apt-get update\nsudo apt-get install -yq ${PACKAGES}\n', 'flatpak remote-add --user --if-not-exists --from flathub https://flathub.org/repo/flathub.flatpakrepo\nflatpak install --user -y flathub org.gnome.Platform//41 org.gnome.Sdk//41\n', 'flatpak-builder --repo=build/flatpak/repo build/flatpak/tmp org.gnome.Hamster.yml\n', 'dbus-launch flatpak-builder --run build/flatpak/tmp org.gnome.Hamster.yml python3 -m unittest\n', 'mkdir -p dist\nflatpak build-bundle --runtime-repo=https://flathub.org/repo/flathub.flatpakrepo build/flatpak/repo dist/Hamster.flatpak org.gnome.Hamster\nflatpak --user -y install dist/Hamster.flatpak\n']"
"['echo ""CIBW_SKIP=$CIBW_SKIP *aarch64"" >> ""$GITHUB_ENV""\n', 'echo ""CIBW_BUILD: $CIBW_BUILD""\necho ""CIBW_SKIP: $CIBW_SKIP""\necho ""CIBW_TEST_COMMAND: $CIBW_TEST_COMMAND""\n', 'pipx run build --sdist', 'python -m pip install .\n', 'cd misc/docs\nmake html\n', 'cd misc/docs/build/html/\ntar cfJ obspydoc.tar.xz *\n', 'curl --request POST \\\n--url https://api.github.com/repos/${{ github.repository }}/statuses/${{ github.event.pull_request.head.sha }} \\\n--header \'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}\' \\\n--header \'content-type: application/json\' \\\n--data \'{\n  ""state"": ""success"",\n  ""context"": ""tests / all reports"",\n  ""description"": ""See Details link"",\n  ""target_url"": ""https://tests.obspy.org/?pr=${{ github.event.pull_request.number }}""\n  }\'\n', 'python -m pip install wheel\npython -m pip install flake8\n', 'mkdir -p $HOME/bin\ncurl -sfL \\\n  https://github.com/reviewdog/reviewdog/raw/master/install.sh | \\\n    sh -s -- -b $HOME/bin\necho ""$HOME/bin"" >> $GITHUB_PATH\n', 'set -o pipefail\npython -m flake8 obspy | \\\n  reviewdog -f=pep8 -name=flake8 \\\n    -tee -reporter=github-check -filter-mode nofilter\n', 'echo ""::set-output name=date::$(date +\'%Y-%m-%d\')""', 'mamba env update -n test -f ${{ env.env_file }}', 'conda info -a\nconda list\n', 'python -m pip install --no-deps .\n', ""obspy-runtests --cov obspy --cov-report='xml' --cov-append --report ${{ env.runtest_options }}\n""]"
"['python --version', 'python -m pip install --user --pre black flake8 mypy types-PyYAML', 'python -m mypy .', 'python -m flake8', 'python -m black --check .', 'python --version', 'python -m pip install --user pytest ${{ matrix.dependencies }}', 'python -m pytest', 'python --version', 'python -m pip install wheel --user', 'python setup.py sdist bdist_wheel']"
"['chmod +x ./scripts/docker-neo4j.sh\nsh ./scripts/docker-neo4j.sh ${{ matrix.neo4j-version }}\nsleep 30s\n', ""python -m pip install --upgrade pip\npip install -e '.[dev]'\n"", 'pytest --cov=neomodel --cov-report=html:coverage_report\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install -U pip\npython -m pip install "".[test]""\n', 'python -m pytest -v test', 'python -m pip install -U pip\npython -m pip install -U build\npython -m build .\n', 'python -m pip install twine\ntwine check dist/*\n']"
"['python -m pip install --upgrade pip tox\n', 'tox -e lint\n', 'python -m pip install --upgrade pip .[test] ""django==${DJ_VER}.*""', 'py.test tests/ -v']"
"['# Create schema ""gis"" into database ""gis""\npsql -h localhost -p 5432 -U gis -d gis -c \'CREATE SCHEMA gis;\'\n\n# Add PostGIS extension to ""gis"" database\npsql -h localhost -p 5432 -U gis -d gis -c \'CREATE EXTENSION IF NOT EXISTS postgis;\'\n\n# Drop PostGIS Tiger Geocoder extension to ""gis"" database\npsql -h localhost -p 5432 -U gis -d gis -c \'DROP EXTENSION IF EXISTS postgis_tiger_geocoder CASCADE;\'\n', 'sudo systemctl start mysql\nsudo mysql --user=root --password=root --host=127.0.0.1 -e ""CREATE USER \'gis\'@\'%\' IDENTIFIED BY \'gis\';""\nsudo mysql --user=root --password=root --host=127.0.0.1 -e ""GRANT ALL PRIVILEGES ON *.* TO \'gis\'@\'%\' WITH GRANT OPTION;""\nmysql --user=gis --password=gis -e ""CREATE DATABASE gis;""\n', '/home/runner/micromamba-bin/micromamba info\n/home/runner/micromamba-bin/micromamba list\npython -c ""import sys; print(sys.version)""\n', 'python -m pip install --upgrade pip setuptools\npip install tox-gh-actions\n', '# Run the unit test suite with SQLAlchemy=1.4.* and then with the latest version of SQLAlchemy\ntox -vv\n', 'pip install --upgrade pip setuptools build twine\npython -m build -o dist\ntwine upload dist/*\n']"
"['wget https://s3.amazonaws.com/quantopian-orchestration/packages/ta-lib-0.4.0-src.tar.gz\ntar xvfz ta-lib-0.4.0-src.tar.gz\ncd ta-lib\n./configure\nmake\nsudo make install\nsudo ldconfig\n', 'brew install ta-lib\n', 'echo ::set-env name=PIP_CONSTRAINT::etc/requirements_locked.txt\n', 'echo ::set-env name=PIP_CONSTRAINT::etc/requirements_py36_locked.txt\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install wheel\npython -m pip install -r etc/requirements_build.in\npython -m pip install --no-binary=bcolz -e .[all] -r etc/requirements_blaze.in\n', 'nosetests tests\n', 'C:\\Miniconda\\condabin\\conda.bat init powershell\n', 'conda create -y --name test python=${{matrix.python-version}} pip pandas=0.22.0 numpy=1.19.1 scipy=1.5.0 cython=0.29.21\n', 'conda activate test\nconda install -y -c quantopian ta-lib\n', 'conda activate test\npip install --no-binary=bcolz -e .[dev] -c etc/requirements_py36_locked.txt\n', 'conda activate test\nnosetests tests\n']"
"['set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade virtualenv tox tox-gh-actions\n', 'mongo eve_test --eval \'db.createUser({user:""test_user"", pwd:""test_pw"", roles:[""readWrite""]});\'\n', 'tox -e ${{ matrix.tox }}']"
"['case ""${{ runner.os }}"" in\nLinux)\n  sudo apt-get update -yy\n  sudo apt-get install -yy  libsamplerate0\n  ;; \nmacOS)\n  brew install libsamplerate\n  ;;\nesac\n', 'conda info -a\nconda list\n', 'python -m pip install --upgrade-strategy only-if-needed -e .[tests]', 'pytest', 'conda info -a\nconda list\n', 'python -m pip install --upgrade-strategy only-if-needed -e .[docs]', 'sphinx-multiversion -D smv_latest_version=$(./scripts/get_latest_release.sh) docs build/html\ntouch build/html/.nojekyll\ncp docs/docsite-index-redirect.html build/html/index.html\nln -srf build/html/$(./scripts/get_latest_release.sh) build/html/latest  # auto-link the latest tag\n', 'conda info -a\nconda list\n', 'codespell --ignore-words-list=""ba,trough,ue"" --skip=""*demo.ipynb,./AUTHORS.md,./docs/changelog.rst""', 'bandit --recursive --skip B101,B110 .', 'flake8 librosa --count --select=E9,F63,F7,F82 --show-source --statistics', 'python -m velin --check librosa', 'python -m mypy --pretty librosa', 'python -m mypy --pretty tests', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"[""gh run list \\\n  --branch main \\\n  --workflow tests \\\n  --limit 10 \\\n  --json databaseId \\\n  --jq '.[].databaseId' \\\n| xargs \\\n  -n 1 \\\ngh run download \\\n  --dir ${{ runner.temp }}/artifacts/ \\\n  --pattern '*-all' \\\n|| true\n"", 'python tests/durations/combine.py ${{ runner.temp }}/artifacts/', ""echo ${{ join(steps.stale.outputs.*, ',') }}"", 'echo ""TMPDIR=$env:USERPROFILE\\AppData\\Local\\Temp"" >> $env:GITHUB_ENV\necho ""TEMP=$env:USERPROFILE\\AppData\\Local\\Temp"" >> $env:GITHUB_ENV\necho ""TMP=$env:USERPROFILE\\AppData\\Local\\Temp"" >> $env:GITHUB_ENV\n', 'call .\\dev\\windows\\setup.bat\n', 'call .\\dev\\windows\\setup.bat\n', 'call .\\dev\\windows\\${{ matrix.test-type }}.bat\n', ""docker run --rm -v ${PWD}:/opt/conda-src -e TEST_SPLITS -e TEST_GROUP ghcr.io/conda/conda-ci:main-linux-python${{ matrix.python-version }}${{ matrix.default-channel == 'conda-forge' && '-conda-forge' || '' }} /opt/conda-src/dev/linux/${{ matrix.test-type }}.sh\n"", ""docker run --rm -v ${PWD}:/opt/conda-src --platform linux/${{ matrix.platform }} ghcr.io/conda/conda-ci:main-linux-python${{ matrix.python-version }}${{ matrix.default-channel == 'conda-forge' && '-conda-forge' || '' }} /opt/conda-src/dev/linux/qemu.sh\n"", './dev/macos/setup.sh\n', './dev/macos/${{ matrix.test-type }}.sh\n']"
"['echo -e ""sphinx\\npygments"" > sphinx-requirements.txt', 'pip install -U -r sphinx-requirements.txt', 'make site', 'make apidoc-init apidoc', 'cp -r apidoc/_output/ docs/_build/html/docs/api', 'sed -i ""s|isso-js-testbed|${{ env.TESTBED_IMAGE }}|g"" docker-compose.yml\n# Also remove the `build`: section so image is pulled, not built\nsed -i \'/container_name: isso-client/{n;N;N;d}\' docker-compose.yml\n', 'docker pull ${{ env.TESTBED_IMAGE }}', 'docker compose build isso-server', 'docker compose up -d', 'make docker-js-unit', '[ ""$(docker inspect --format={{.State.Health.Status}} isso-server)"" = ""healthy"" ] || sleep 5', 'make docker-js-integration', 'make docker-compare-screenshots', 'make docker-update-screenshots', '/bin/false', 'npm install', 'npm run test-unit', 'make init', 'make js', 'make init', 'make js', 'python setup.py sdist\necho ""::set-output name=package_file::$(ls dist/)""\n', 'pip install dist/${{ steps.generate-package.outputs.package_file }}', 'pip uninstall --y isso', 'pip install -e .', 'pip install pytest pytest-cov', 'make test', 'pip install -e .', 'pip install pytest pytest-cov', 'pip install flake8', 'make flakes', 'make coverage\ncoverage report --fail-under 70\n']"
"['pip install flake8\nflake8 funcy\nflake8 --select=F,E5,W tests\n', 'pip install -r requirements.txt\nsphinx-build -b html -W . _build/html\n', 'python -m pip install --upgrade pip\npip install -r test_requirements.txt\n', 'pytest -W error']"
"['python -m pip install --upgrade pip setuptools tox', 'tox -e py', 'python -m pip install --upgrade pip setuptools tox', 'tox -e ${{ matrix.environment }}']"
"['pip install tox', 'tox -e check-manifest', 'tox -e lint', 'tox -e mypy', 'python -m pip install build setuptools twine wheel', 'python -m build\npython -m twine check dist/*\n', 'python -m twine upload dist/*', 'pip install tox', 'sudo locale-gen fr_FR zh_TW.EUC-TW\nsudo update-locale\n', 'tox -e py-${{ matrix.psycopg }}']"
"['.github/workflows/setup-${{ matrix.os }}.sh', 'pip install -e .', 'python test.py -v', 'pip install -e .', 'python examples/perft/perft.py -t 1 examples/perft/random.perft --max-nodes 10000', 'python examples/perft/perft.py -t 1 examples/perft/chess960.perft --max-nodes 100000', 'python examples/perft/perft.py -t 1 examples/perft/tricky.perft', 'python examples/perft/perft.py -t 1 --variant giveaway examples/perft/giveaway.perft', 'python examples/perft/perft.py -t 1 --variant atomic examples/perft/atomic.perft', 'python examples/perft/perft.py -t 1 --variant racingkings examples/perft/racingkings.perft', 'python examples/perft/perft.py -t 1 --variant horde examples/perft/horde.perft', 'python examples/perft/perft.py -t 1 --variant crazyhouse examples/perft/crazyhouse.perft', 'python examples/perft/perft.py -t 1 --variant 3check examples/perft/3check.perft', 'pip install -e .', 'pip install mypy', 'python -m mypy --strict chess', 'python -m mypy --strict examples/**/*.py', 'sudo apt-get update && sudo apt-get install -y docutils-common', 'python setup.py --long-description | rst2html --strict --no-raw > /dev/null', 'pip install -e .', '.github/workflows/setup-ubuntu-latest.sh', 'python -m doctest README.rst']"
"['[[ -z $CONFIRM_REF || $GITHUB_REF =~ ^refs/(heads|tags)/$CONFIRM_REF$ || $CONFIRM_REF == ""test"" ]]\nif [[ $CONFIRM_REF == ""test"" ]]; then\n  echo ""Build and deploy to test.pypi.org.""\nelif [[ -z $CONFIRM_REF ]]; then\n  echo ""Build only.  Nothing will be uploaded to PyPI.""\nelse\n  echo ""Full build and deploy.  Wheels and source will be uploaded to PyPI.""\nfi\n', ""python -m pip install 'build'\n"", 'if [[ ! -z ""$OVERRIDE_VERSION"" ]]; then echo ""$OVERRIDE_VERSION"" > VERSION; fi\n# The build package is the reference PEP 517 package builder.  All\n# dependencies are specified by our setup code.\npython -m build --sdist .\n', '# First assert that there is exactly one tarball, and find its name.\nshopt -s failglob\ntarball_pattern=""*.tar.gz""\ntarballs=($tarball_pattern)\n[[ ${#tarballs[@]} == 1 ]]\ntarball=""${tarballs[0]}""\n# Get the stem and make the zipfile name.\nstem=""${tarball%.tar.gz}""\nzipfile=""${stem}.zip""\n# Extract the tarball and rezip it.\ntar -xzf ""$tarball""\nzip ""$zipfile"" -r ""$stem""\nrm -r ""$stem""\n', ""# cibuildwheel does the heavy lifting for us. Originally tested on\n# 2.11.3, but should be fine at least up to any minor new release.\npython -m pip install 'cibuildwheel==2.11.*'\n"", '# If the version override was specified, then write it the VERSION\n# file with it.\nif [[ ! -z ""$OVERRIDE_VERSION"" ]]; then echo ""$OVERRIDE_VERSION"" > VERSION; fi\npython -m cibuildwheel --output-dir wheelhouse\n', 'python -m pip install wheels/*-cp38-cp38-manylinux*.whl\npython -c \'import qutip; print(qutip.__version__); assert ""dev"" not in qutip.__version__; assert ""+"" not in qutip.__version__\'\n', 'python -m pip install ""twine""\npython -m twine upload --verbose wheels/*.whl sdist/*.tar.gz\n', 'python -m pip install wheels/*-cp38-cp38-manylinux*.whl\npython -c \'import qutip; print(qutip.__version__); assert ""dev"" not in qutip.__version__; assert ""+"" not in qutip.__version__\'\n', 'python -m pip install ""twine""\npython -m twine upload --repository testpypi --verbose wheels/*.whl sdist/*.tar.gz\n', 'pip install pip --upgrade\npython -mpip install -r doc/requirements.txt\nsudo apt-get update\nsudo apt-get install texlive-full\n', ""# Build without build isolation so that we use the build\n# dependencies already installed from doc/requirements.txt.\npython -m pip install -e .[full] --no-build-isolation\n# Install in editable mode so it doesn't matter if we import from\n# inside the installation directory, otherwise we can get some errors\n# because we're importing from the wrong location.\npython -c 'import qutip; qutip.about()'\n"", 'make latexpdf SPHINXOPTS=""-W --keep-going -T""\n# Above flags are:\n#   -W : turn warnings into errors\n#   --keep-going : do not stop after the first error\n#   -T : display a full traceback if a Python exception occurs\n', 'make html SPHINXOPTS=""-W --keep-going -T""\n# Above flags are:\n#   -W : turn warnings into errors\n#   --keep-going : do not stop after the first error\n#   -T : display a full traceback if a Python exception occurs\n', 'QUTIP_TARGET=""tests,graphics,semidefinite,ipython""\nif [[ -z ""${{ matrix.nocython }}"" ]]; then\n  QUTIP_TARGET=""$QUTIP_TARGET,runtime_compilation""\nfi\nexport CI_QUTIP_WITH_OPENMP=${{ matrix.openmp }}\nif [[ -z ""${{ matrix.nomkl }}"" ]]; then\n  conda install blas=*=mkl ""numpy${{ matrix.numpy-requirement }}"" ""scipy${{ matrix.scipy-requirement }}""\nelif [[ ""${{ matrix.os }}"" =~ ^windows.*$ ]]; then\n  # Conda doesn\'t supply forced nomkl builds on Windows, so we rely on\n  # pip not automatically linking to MKL.\n  pip install ""numpy${{ matrix.numpy-requirement }}"" ""scipy${{ matrix.scipy-requirement }}""\nelse\n  conda install nomkl ""numpy${{ matrix.numpy-requirement }}"" ""scipy${{ matrix.scipy-requirement }}""\nfi\nif [[ -n ""${{ matrix.conda-extra-pkgs }}"" ]]; then\n  conda install ""${{ matrix.conda-extra-pkgs }}""\nfi\npython -m pip install -e .[$QUTIP_TARGET]\npython -m pip install ""coverage${{ matrix.coverage-requirement }}""\npython -m pip install pytest-cov coveralls pytest-fail-slow\n', 'conda list\npython -c ""import qutip; qutip.about()""\npython -c ""import qutip; print(qutip.settings)""\n', 'uname -a\nif [[ ""ubuntu-latest"" == ""${{ matrix.os }}"" ]]; then\n    hostnamectl\n    lscpu\n    free -h\nfi\n', 'if [[ -n ""${{ matrix.openmp }}"" ]]; then\n  # Force OpenMP runs to use more threads, even if there aren\'t\n  # actually that many CPUs.  We have to check any dispatch code is\n  # truly being executed.\n  export QUTIP_NUM_PROCESSES=2\nfi\npytest -Werror --strict-config --strict-markers --fail-slow=300 --durations=0 --durations-min=1.0 --verbosity=1 --cov=qutip --cov-report= --color=yes ${{ matrix.pytest-extra-options }} qutip/tests\n# Above flags are:\n#  -Werror\n#     treat warnings as errors\n#  --strict-config\n#     error out if the configuration file is not parseable\n#  --strict-markers\n#     error out if a marker is used but not defined in the\n#     configuration file\n#  --timeout=300\n#     error any individual test that goes longer than the given time\n#  --durations=0 --durations-min=1.0\n#     at the end, show a list of all the tests that took longer than a\n#     second to run\n#  --verbosity=1\n#     turn the verbosity up so pytest prints the names of the tests\n#     it\'s currently working on\n#  --cov=qutip\n#     limit coverage reporting to code that\'s within the qutip package\n#  --cov-report=\n#     don\'t print the coverage report to the terminal---it just adds\n#     cruft, and we\'re going to upload the .coverage file to Coveralls\n#  --color=yes\n#     force coloured output in the terminal\n', 'coveralls --service=github', 'python -m pip install towncrier\n', '# Fetch the pull request\' base branch so towncrier will be able to\n# compare the current branch with the base branch.\n# Source: https://github.com/actions/checkout/#fetch-all-branches.\ngit fetch --no-tags origin +refs/heads/${BASE_BRANCH}:refs/remotes/origin/${BASE_BRANCH}\ntowncrier check --compare-with origin/${BASE_BRANCH}\ntowncrier build --version ""$(cat VERSION)"" --draft\n', 'python -m pip install coveralls\ncoveralls --service=github --finish\n']"
"['pip install -U pip setuptools wheel\npip install -r dev-requirements.txt\n', 'python manage.py makemigrations --check --dry-run\n', 'python -Wd -m coverage run manage.py test -v2\n', 'coverage report -m --fail-under=75\n']"
"['python -m pip install --upgrade pip\npip install -e .\npip install coveralls --upgrade\n', 'pip install flake8 --upgrade\nflake8 --exclude=build --ignore=E501,F403,F401,E241,E225,E128 .\n', 'pip install pycodestyle --upgrade\npycodestyle --ignore=E128,E261,E225,E501,W605 slugify test.py setup.py\n', 'coverage run --source=slugify test.py\n', 'coveralls --service=github', 'python -m pip install --upgrade pip\npip install -e .\npip install coveralls --upgrade\n', 'pip install flake8 --upgrade\nflake8 --exclude=build --ignore=E501,F403,F401,E241,E225,E128 .\n', 'pip install pycodestyle --upgrade\npycodestyle --ignore=E128,E261,E225,E501,W605 slugify test.py setup.py\n', 'coverage run --source=slugify test.py\n', 'coveralls --service=github', 'python -m pip install --upgrade pip\npip install -e .\npip install coveralls --upgrade\n', 'pip install flake8 --upgrade\nflake8 --exclude=build --ignore=E501,F403,F401,E241,E225,E128 .\n', 'pip install pycodestyle --upgrade\npycodestyle --ignore=E128,E261,E225,E501,W605 slugify test.py setup.py\n', 'coverage run --source=slugify test.py\n', 'coveralls --service=github']"
"['pip install -q wheel\npython setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip\npip install flake8 black\n', 'flake8 modeltranslation\nblack --check modeltranslation *.py\n', 'python -m pip install --upgrade pip\npython setup.py sdist\npip install dist/*\n', 'if [[ $DB == mysql ]]; then\n  pip install -q mysqlclient\nfi\nif [[ $DB == postgres ]]; then\n  pip install -q psycopg2-binary\nfi\npip install typing-extensions coverage pytest pytest-django pytest-cov $(./get-django-version.py ${{ matrix.django }})\n', 'pytest --cov-report term\n']"
"['sudo apt-get install -y graphviz idle python3-matplotlib\n', 'python -m pip install --upgrade pip wheel\npip install build pycodestyle coverage coveralls numpy mypy\npip install -r doc/requirements.txt\nif [ ""$RUNNER_OS"" == ""Windows"" ]; then\n  pip install pywin32\nfi\n', 'python -Wall setup.py try\npython setup.py install\npython -Wall setup.py test\nif [ ""$RUNNER_OS"" == ""Linux"" ]; then\n  COVERAGE=1 coverage run test/runtest.py\nfi\npython run.py --doctest\n', 'python -m build\n', 'pycodestyle pympler\n', 'mypy pympler\n', 'python setup.py build_sphinx -W\n']"
[]
"['python -m pip install poetry\n', 'poetry publish --build\n', 'python -m pip install poetry\npoetry install\n', 'sudo apt-get install redis -y\n', 'poetry run pytest -vv']"
"['python -m pip install --upgrade tox', 'tox -e codestyle --', 'sudo apt update\nsudo apt install -y texlive-latex-recommended\nsudo apt install -y texlive-latex-extra\nsudo apt install -y cm-super\nsudo apt install -y dvipng\nlatex --version\n', 'python -m pip install --upgrade tox', 'tox ${{ matrix.toxargs }} -e ${{ matrix.target.env }} -- ${{ matrix.target.toxargs }}', 'python setup.py egg_info']"
"['pip install --upgrade pip\npip install flake8\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python setup.py sdist']"
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\nsudo apt-get install libmemcached-dev\n', 'tox -e lint,mypy\n', 'tox -e docs\n', ""sudo sed -i '/::1/d' /etc/hosts\n"", ""tox -- pymemcache/test/ \\\n    -m 'unit or integration' \\\n    --port ${{ job.services.memcached.ports[11211] }} \\\n    --tls-port ${{ job.services.tls_memcached.ports[11211] }}\n""]"
""
"['pip install poetry', 'poetry install', 'poetry build --format wheel', 'rm -rf django_object_actions\npip install --force-reinstall dist/*.whl\n', 'pip install ""Django==${{ matrix.django-version }}.*""', 'make test', 'pip install black', 'black --check .']"
"['python -m pip install --upgrade pip\npip install setuptools wheel\n', 'python setup.py sdist bdist_wheel\n', 'python -m pip install flake8\n', 'python -m pip install tox\n', 'tox', 'python -m pip install isort\n', 'python -m pip install tox\n', 'tox', 'python -m pip install tox\n', 'tox', 'python -m pip install mypy\n', 'python -m pip install tox coveralls', 'coveralls --service=github', 'python -m pip install tox coveralls', 'tox -e py', 'coveralls --service=github', 'python -m pip install tox==3.27.1\n', 'tox', 'python -m pip install tox==3.27.1\n', 'tox']"
"[""python scripts/new-change -t '${{ github.event.inputs.type }}' -c '${{ github.event.inputs.category }}' -d '${{ github.event.inputs.description }}'\n"", 'git config --global user.name ""Github Actions""\ngit config --global user.email ""<>""\ngit fetch\ngit add .changes\ngit commit -m ""Add changelog entry""\ngit pull --rebase\ngit push\n', 'echo ""PRs must be made against the develop branch.""\nexit 1\n', 'python scripts/ci/install\npython scripts/ci/install-check\n', 'python scripts/ci/run-tests --with-cov', 'python scripts/ci/run-check', 'python -m pip install -r requirements-test.txt\n', 'python scripts/regenerate-lock-files --show-files\necho ""CHANGES=$(git status --porcelain=v1)"" >> $env:GITHUB_ENV\n', 'python scripts/regenerate-lock-files --show-files\necho ""CHANGES<<EOF"" >> $GITHUB_ENV\necho ""CHANGES=$(git status --porcelain=v1)"" >> $GITHUB_ENV\necho ""EOF"" >> $GITHUB_ENV\n', 'echo ""PLATFORMS=Windows"" >> $env:GITHUB_ENV\n', 'echo ""PLATFORMS=macOS and Linux"" >> $GITHUB_ENV\n', 'git config --global user.name ""Github Actions""\ngit config --global user.email ""<>""\ngit fetch\ngit add requirements\ngit commit -m ""Regenerate lock files for ${{ env.PLATFORMS }}""\ngit pull --rebase\ngit push\n']"
"['python -m pip install --upgrade pip', 'pip install mkdocs && pip install mkdocs-material markdown-include', 'python -m mkdocs build', 'pip install -r requirements.txt', 'python -m howdoi --sanity-check --explain', 'npm ci\nnpm run build --if-present\nnpm run lint\nnpm test\n', 'npm ci\nnpm run build --if-present\nnpm run precompile\nnpm run lint\nnpm test\n', 'python -m pip install --upgrade pip\nif [ -f requirements/dev.txt ]; then pip install -r requirements/dev.txt; fi\n', 'flake8 . --count --show-source --statistics\n', 'pylint howdoi *.py --rcfile=.pylintrc\n', 'nose2\n', 'python -m pip install --upgrade pip\nif [ -f requirements/dev.txt ]; then pip install -r requirements/dev.txt; fi\n', 'flake8 . --count --show-source --statistics\n', 'pylint howdoi *.py --rcfile=.pylintrc\n', 'nose2\n']"
"['git fetch --prune --unshallow\n', 'git fetch --prune --unshallow\n', 'pipx run build --sdist', 'python -m pip install --disable-pip-version-check --upgrade pip\npip install -U -r requirements.txt -r requirements-dev.txt\n#Install locally to support tests\npip install -e .\n', 'pytest', 'docker run --rm --privileged hypriot/qemu-register\n']"
"['pip install -U pip setuptools wheel\npip install -U tox tox-gh-actions\n', 'tox', 'pip install -U pip setuptools wheel\npip install -U tox\n', 'tox -e lint -- --show-diff-on-failure', 'tox -e docs']"
"[""sudo /etc/init.d/mysql start\nmysql -e 'CREATE DATABASE ${{ env.DB_NAME }};'  -u${{ env.IMPORT_EXPORT_MYSQL_USER }} -p${{ env.IMPORT_EXPORT_MYSQL_PASSWORD }}\n"", 'python -m pip install --upgrade pip \npip install tox coverage coveralls\n', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'coverage combine', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --service=github --finish\n']"
"['tests/geckodriver.sh', 'pip install tox', 'export PATH=$PATH:$PWD\ntox -e py\n', 'tox -e flake8,docs', 'pip install tox', 'tox -e py', 'pip install tox', 'tox -e py']"
"['sudo apt install calibre', 'npm install honkit --save-dev\nnpx honkit build . public --log=debug\nnpx honkit pdf . byte-of-python.pdf\nnpx honkit epub . byte-of-python.epub\n']"
"['python -m pip install --upgrade pip setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip setuptools\npython -m pip install tox\n', 'python -m pip install coveralls coverage-lcov toml\n', 'tox\n', 'coverage-lcov --output_file_path lcov.info\n']"
""
"['echo ""PRs must be made aginst the develop branch.""\nexit 1\n', 'python scripts/ci/install --extras crt\n', 'python scripts/ci/run-crt-tests --with-cov --with-xdist\n', 'python scripts/ci/install\n', 'python scripts/ci/run-tests --with-cov --with-xdist\n']"
""
"['sudo apt-get install -y libffi-dev libffi-dev bc\n', 'echo ""JASMIN_RELEASE=$(python setup.py --version)"" >> $GITHUB_ENV\necho ""Building Jasmin ${{ env.JASMIN_RELEASE }}""\n', 'python -m pip install --upgrade pip', 'pip install -r ./requirements.txt', 'pip install -r ./requirements-test.txt', 'sudo mkdir -p /var/log/jasmin /etc/jasmin/resource /etc/jasmin/store\nsudo cp ./misc/config/*.cfg /etc/jasmin/\nsudo cp ./misc/config/resource/* /etc/jasmin/resource/\nsudo chmod -R 777 /var/log/jasmin /etc/jasmin/store\nchmod +x ./misc/pylint/run.sh\n', './misc/pylint/run.sh', 'coverage run --source=jasmin -m twisted.trial tests', 'coveralls', 'cd misc/doc;make html;cd ../../']"
"['python -m pip install --upgrade pip\npython -m pip install ""cython>=0.29"" oldest-supported-numpy setuptools versioneer\npython setup.py sdist --formats=gztar --with-cython --fail-on-error\n', 'conda install -n test_env --quiet --yes -c conda-forge gsl pip\nconda activate test_env\npython -m pip install --pre pytest cython sympy pyparsing numpy jinja2 scipy sphinx\npython -m pip install .\n', 'cd  $GITHUB_WORKSPACE/.. # move out of the workspace to avoid direct import\npython -Wd $GITHUB_WORKSPACE/dev/continuous-integration/run_test_suite.py\n', 'pip3 install -r .devcontainer/dev-requirements.txt', 'pre-commit run --all-files --show-diff-on-failure', 'conda install -n test_env --quiet --yes -c conda-forge pip pytest cython sympy future pyparsing numpy jinja2 six scipy sphinx gsl coverage\npip install .\n', 'cd  $GITHUB_WORKSPACE/.. # move out of the workspace to avoid direct import\ncoverage run --rcfile=$GITHUB_WORKSPACE/.coveragerc $GITHUB_WORKSPACE/$SCRIPT_NAME\n', 'cp  $GITHUB_WORKSPACE/../.coverage .\npython -m pip install --upgrade coveralls\ncoveralls --service=github --rcfile=$GITHUB_WORKSPACE/.coveragerc\n', 'python -m pip install --upgrade coveralls\ncoveralls --service=github --finish --rcfile=$GITHUB_WORKSPACE/.coveragerc\n', 'pip install -r rtd-requirements.txt', 'pip install .', 'cd docs_sphinx\nsphinx-build -b html . ../docs\n']"
"['./misc/trigger_wheel_build.sh', ""pip install --upgrade 'setuptools!=50' tox==4.4.4"", 'tox run -e ${{ env.TOXENV }} --notest', 'tox run -e ${{ env.TOXENV }} --skip-pkg-install', 'python -m pip install -U pip\npip install git+https://github.com/hauntsaninja/mypy_primer.git\n', 'cd mypy_to_test\necho ""new commit""\ngit rev-list --format=%s --max-count=1 $GITHUB_SHA\n\nMERGE_BASE=$(git merge-base $GITHUB_SHA origin/$GITHUB_BASE_REF)\ngit checkout -b base_commit $MERGE_BASE\necho ""base commit""\ngit rev-list --format=%s --max-count=1 base_commit\n\necho \'\'\ncd ..\n# fail action if exit code isn\'t zero or one\n(\n  mypy_primer \\\n  --repo mypy_to_test \\\n  --new $GITHUB_SHA --old base_commit \\\n  --num-shards 5 --shard-index ${{ matrix.shard-index }} \\\n  --debug \\\n  --additional-flags=""--debug-serialize"" \\\n  --output concise \\\n  | tee diff_${{ matrix.shard-index }}.txt\n) || [ $? -eq 1 ]\n', 'echo ${{ github.event.pull_request.number }} | tee pr_number.txt\n', 'unzip diff.zip', 'cat diff_*.txt | tee fulldiff.txt\n', ""git config --global user.name mypybot\ngit config --global user.email '<>'\n"", 'python -m pip install requests==2.28.1\nGITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }} python misc/sync-typeshed.py --make-pr\n', 'PYTHONVERSION=${{ matrix.python }}\nPYTHONDIR=~/python-debug/python-$PYTHONVERSION\nVENV=$PYTHONDIR/env\n./misc/build-debug-python.sh $PYTHONVERSION $PYTHONDIR $VENV\nsource $VENV/bin/activate\n', ""pip install --upgrade 'setuptools!=50' tox==4.4.4"", 'pip install -r test-requirements.txt\nCC=clang MYPYC_OPT_LEVEL=0 MYPY_USE_MYPYC=1 pip install -e .\n', 'tox run -e ${{ matrix.toxenv }} --notest', 'tox run -e ${{ matrix.toxenv }} --skip-pkg-install -- ${{ matrix.tox_extra_args }}', ""pip install --upgrade 'setuptools!=50' tox==4.4.4"", 'tox run -e py --notest', 'tox run -e py --skip-pkg-install -- ""-n 2""', 'exit 0', 'sudo dpkg --add-architecture i386 && \\\nsudo apt-get update && sudo apt-get install -y \\\n  zlib1g-dev:i386 \\\n  libgcc-s1:i386 \\\n  g++-i686-linux-gnu \\\n  gcc-i686-linux-gnu \\\n  libffi-dev:i386 \\\n  libssl-dev:i386 \\\n  libbz2-dev:i386 \\\n  libncurses-dev:i386 \\\n  libreadline-dev:i386 \\\n  libsqlite3-dev:i386 \\\n  liblzma-dev:i386 \\\n  uuid-dev:i386\n', ""pip install --upgrade 'setuptools!=50' tox==4.4.4"", 'tox run -e py --notest', 'tox run -e py --skip-pkg-install -- -n 2 mypyc/test/', 'misc/test-stubgenc.sh']"
"[""python -m pip install --upgrade pip\npip --version\npip install 'setuptools>=47' 'wheel>=0.34'\npip wheel -w dist -v --no-deps .\n"", 'pip install tox\ntox -e wheel_check -- ${{ matrix.pytest-extra }}\n', 'pip install -U twine\ntwine upload --skip-existing dist/*\n', 'platform=${py_tag%%-*}\nversion=${platform:2:1}.${platform:3:3}\necho $version\necho ""::set-output name=python-version::$version""\n', 'pip install tox\ntox -e wheel_check -- ${{ matrix.pytest-extra }}\n', 'pip install -U twine\ntwine upload --skip-existing dist/*manylinux*\n', 'python -m pip install --upgrade pip\npip --version\npip install setuptools>=47 wheel>=0.34\npython setup.py sdist --dist-dir dist\npython .github/workflows/scripts/verify_tag.py dist\n', 'pip install -U twine\ntwine upload --skip-existing dist/*\n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'pip install -U twine\ntwine upload --skip-existing dist/*manylinux*\n', 'python -m pip install --upgrade pip\npip install -U tox\npython --version\npip --version\ntox --version\n', 'tools/sed_coverage_rc.py\n', 'tools/tox_sdist.py\n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'sudo chmod -R 777 .pip\nsudo chmod -R 777 .tox\n', 'sudo apt-get update\nsudo apt-get install -y libunwind-dev\n', 'python -m pip install --upgrade pip\npip install -U coverage fixtures setuptools tox wheel\npython --version\npip --version\ntox --version\ncoverage --version\n', 'tox -e ${{ matrix.toxenv }}', 'coverage --version\ncoverage combine\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['python -m pip install wheel --user\n', 'python setup.py sdist bdist_wheel\n', 'pip install -e .[develop]\n', 'pre-commit run --all-files --show-diff-on-failure\n', 'pip install -e .[develop]\n', 'pytest\n', 'npm -g install node-qunit-puppeteer@2.1.0\n', 'node-qunit-puppeteer tests/static/js/test-helpers.html\nnode-qunit-puppeteer tests/static/js/test-client-base.html\n', 'pip install dist/OctoPrint-*-py2.py3-none-any.whl\n', 'pip install dist/OctoPrint-*.tar.gz\n', 'pip install dist/OctoPrint-*-py2.py3-none-any.whl\n', 'mkdir e2econfig\ncp -r .github/fixtures/with_acl/* e2econfig\n', 'npm ci\nPLAYWRIGHT_VERSION=$(npm ls --json @playwright/test | jq --raw-output \'.dependencies[""@playwright/test""].version\')\necho ""PLAYWRIGHT_VERSION=$PLAYWRIGHT_VERSION"" >> $GITHUB_ENV\n', 'npx playwright install --with-deps\n', 'npx playwright install-deps\n', 'npx playwright test\n', 'log=${{ github.workspace }}/e2econfig/logs/octoprint.log\nif grep ""\\- ERROR \\-"" $log; then\n  echo ""::error::Errors were logged to octoprint.log""\n  grep -Pazo \'(?m)^\\N+\\- ERROR \\-\\N*\\n(^\\N*?\\n)*?(?=\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3} \\- )\' $log\n  exit 1\nfi\n', 'OCTOPRINT_VERSION=$(echo $GITHUB_REF | cut -d/ -f3)\necho ""OCTOPRINT_VERSION=$OCTOPRINT_VERSION"" >> $GITHUB_ENV\n', 'echo ""CUTOFF=`date --date=\'14 days ago\' +\'%Y-%m-%d\'`"" >> $GITHUB_OUTPUT\n', 'pip install .\n', 'OCTOPRINT_VERSION=$(curl -i https://github.com/OctoPrint/OctoPrint/releases/latest | grep -i location: | head -n1 | awk -F/ \'{print $NF}\' | tr -d \'\\r\')\necho ""OCTOPRINT_VERSION=$OCTOPRINT_VERSION"" >> $GITHUB_ENV\n', 'pip install https://github.com/OctoPrint/OctoPrint/archive/${{ env.OCTOPRINT_VERSION }}.zip\n']"
"['python -m pip install -r requirements-dev.txt\nmake spec\nmake regen\n', 'echo ""pr_title=Updates from spec version $(jq -r .ResourceSpecificationVersion CloudFormationResourceSpecification.json)"" >> $GITHUB_OUTPUT\n', 'pip install -r requirements-dev.txt\n', 'npm install', 'make lint', 'make test', 'python -m pip install -r requirements-dev.txt\n', 'make spec\nmake regen\ngit diff --exit-code\n']"
"['git config --global core.autocrlf false', ""if [[ ${{ matrix.python }} = pyodide ]] ; then\n    npm install pyodide@0.22.1\n      # 0.23.0 has a regression: https://github.com/pyodide/pyodide/issues/3730\n    pip install 'pip >= 22.3.1'\n      # Older pips may fail to install `pyodide-build`.\n    pip install 'pyodide-build == 0.22.1'\n    pyodide venv .venv-pyodide\n    source .venv-pyodide/bin/activate\nfi\npip install .\nrm -r hy\n  # We want to be sure we're testing the installed version,\n  # instead of running from the source tree.\npip install pytest\n"", 'if [[ ${{ matrix.python }} = pyodide ]] ; then\n    source .venv-pyodide/bin/activate\nfi\npython -m pytest tests\n']"
"['pip install tox', 'pip install python-dateutil && tox -e mypy', 'pip install tox', 'pip install python-dateutil && tox -e py']"
"['./cargo install --version 0.17.5 cargo-audit\n./cargo audit --ignore RUSTSEC-2020-0128\n', 'git config --local user.email ""pantsbuild+github-automation@gmail.com""\ngit config --local user.name ""Worker Pants (Pantsbuild GitHub Automation Bot)""\nbash -x build-support/bin/cherry_pick.sh ${{ github.event.pull_request.number || inputs.PR_number }} origin\n', 'MODE=debug ./pants package build-support/bin/cache_comparison.py\ngit fetch --no-tags --depth=1024 origin ""$BASE_REF""\n', 'dist/build-support.bin/cache_comparison_py.pex \\\n  --args=""$PANTS_ARGS"" \\\n  --build-commit=""$BUILD_COMMIT"" \\\n  --source-diffspec=""$SOURCE_DIFFSPEC"" \\\n  --source-diffspec-step=$SOURCE_DIFFSPEC_STEP\n', 'git config --global safe.directory ""$GITHUB_WORKSPACE""', 'curl --proto \'=https\' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -v -y --default-toolchain none\necho ""${HOME}/.cargo/bin"" >> $GITHUB_PATH\n', 'echo ""/opt/python/cp37-cp37m/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp38-cp38/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp39-cp39/bin"" >> $GITHUB_PATH\n', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', './build-support/bin/deploy_to_s3.py', 'git config --global safe.directory ""$GITHUB_WORKSPACE""', 'curl --proto \'=https\' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -v -y --default-toolchain none\necho ""${HOME}/.cargo/bin"" >> $GITHUB_PATH\n', 'echo ""/opt/python/cp37-cp37m/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp38-cp38/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp39-cp39/bin"" >> $GITHUB_PATH\n', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', './build-support/bin/deploy_to_s3.py', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', './build-support/bin/deploy_to_s3.py', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', './build-support/bin/deploy_to_s3.py', 'if [[ -n ""$REF"" ]]; then\n    ref=""$REF""\nelse\n    ref=""${GITHUB_REF#refs/tags/}""\nfi\necho ""build-ref=${ref}"" >> $GITHUB_OUTPUT\nif [[ ""${ref}"" =~ ^release_.+$ ]]; then\n    echo ""is-release=true"" >> $GITHUB_OUTPUT\nfi\n', './pants run build-support/bin/release.py -- fetch-and-stabilize --dest=dest/pypi_release', 'tag=""${{ needs.determine_ref.outputs.build-ref }}""\ncommit=""$(git rev-parse ${tag}^{commit})""\n\necho ""Recording tag ${tag} is of commit ${commit}""\nmkdir -p dist/deploy/tags/pantsbuild.pants\necho ""${commit}"" > ""dist/deploy/tags/pantsbuild.pants/${tag}""\n', './build-support/bin/deploy_to_s3.py --scope tags/pantsbuild.pants', 'echo ""hash=$(./build-support/bin/rust/print_engine_hash.sh)"" >> $GITHUB_OUTPUT', './pants version > ${{ runner.temp }}/_pants_version.stdout && [[ -s ${{ runner.temp }}/_pants_version.stdout ]]', './pants list ::\n./pants roots\n./pants help goals\n./pants help targets\n./pants help subsystems\n', './cargo test --tests -- --nocapture --test-threads=8', 'echo ""hash=$(./build-support/bin/rust/print_engine_hash.sh)"" >> $GITHUB_OUTPUT', './pants version > ${{ runner.temp }}/_pants_version.stdout && [[ -s ${{ runner.temp }}/_pants_version.stdout ]]', './pants list ::\n./pants roots\n./pants help goals\n./pants help targets\n./pants help subsystems\n', './pants run build-support/bin/generate_github_workflows.py -- --check\n', 'sudo apt-get install -y pkg-config fuse libfuse-dev\n./build-support/bin/check_rust_pre_commit.sh\n./cargo test --all --tests -- --nocapture\n./cargo check --benches\n./cargo doc', 'echo ""hash=$(./build-support/bin/rust/print_engine_hash.sh)"" >> $GITHUB_OUTPUT', './pants version > ${{ runner.temp }}/_pants_version.stdout && [[ -s ${{ runner.temp }}/_pants_version.stdout ]]', './pants list ::\n./pants roots\n./pants help goals\n./pants help targets\n./pants help subsystems\n', './cargo test --tests -- --nocapture', 'git config --global safe.directory ""$GITHUB_WORKSPACE""', 'curl --proto \'=https\' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -v -y --default-toolchain none\necho ""${HOME}/.cargo/bin"" >> $GITHUB_PATH\n', 'echo ""/opt/python/cp37-cp37m/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp38-cp38/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp39-cp39/bin"" >> $GITHUB_PATH\n', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', 'git config --global safe.directory ""$GITHUB_WORKSPACE""', 'curl --proto \'=https\' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -v -y --default-toolchain none\necho ""${HOME}/.cargo/bin"" >> $GITHUB_PATH\n', 'echo ""/opt/python/cp37-cp37m/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp38-cp38/bin"" >> $GITHUB_PATH\necho ""/opt/python/cp39-cp39/bin"" >> $GITHUB_PATH\n', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', './pants run build-support/bin/release.py -- build-local-pex\n./pants run build-support/bin/release.py -- build-wheels\n', 'if [[ -z $GITHUB_EVENT_PULL_REQUEST_BASE_SHA ]]; then\n  # push: compare to the immediate parent, which should already be fetched\n  # (checkout\'s fetch_depth defaults to 10)\n  comparison_sha=$(git rev-parse HEAD^)\nelse\n  # pull request: compare to the base branch, ensuring that commit exists\n  git fetch --depth=1 ""$GITHUB_EVENT_PULL_REQUEST_BASE_SHA""\n  comparison_sha=""$GITHUB_EVENT_PULL_REQUEST_BASE_SHA""\nfi\necho ""comparison_sha=$comparison_sha""\n\naffected=$(git diff --name-only ""$comparison_sha"" HEAD | python build-support/bin/classify_changed_files.py)\necho ""Affected:""\nif [[ ""${affected}"" == ""docs"" ]]; then\n  echo ""docs_only=true"" | tee -a $GITHUB_OUTPUT\nfi\nfor i in ${affected}; do\n  echo ""${i}=true"" | tee -a $GITHUB_OUTPUT\ndone\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'chmod +x src/python/pants/bin/native_client', './pants lint check ::\n', 'merge_ok=""${{ needs.set_merge_ok.outputs.merge_ok }}""\nif [[ ""${merge_ok}"" == ""true"" ]]; then\n    echo ""Merge OK""\n    exit 0\nelse\n    echo ""Merge NOT OK""\n    exit 1\nfi\n', ""echo 'merge_ok=true' >> ${GITHUB_OUTPUT}"", 'chmod +x src/python/pants/bin/native_client', './pants --tag=+platform_specific_behavior test :: -- -m platform_specific_behavior\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-ARM64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=0/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=1/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=2/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=3/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=4/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=5/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=6/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=7/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=8/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'mkdir -p ~/bazel-remote\nif [[ -z ""${AWS_ACCESS_KEY_ID}"" ]]; then\n  CACHE_WRITE=false\n  # If no secret read/write creds, use hard-coded read-only creds, so that\n  # cross-fork PRs can at least read from the cache.\n  # These creds are hard-coded here in this public repo, which makes the bucket\n  # world-readable. But since putting raw AWS tokens in a public repo, even\n  # deliberately, is icky, we base64-them. This will at least help hide from\n  # automated scanners that look for checked in AWS keys.\n  # Not that it would be terrible if we were scanned, since this is public\n  # on purpose, but it\'s best not to draw attention.\n  AWS_ACCESS_KEY_ID=$(echo \'QUtJQVY2QTZHN1JRVkJJUVM1RUEK\' | base64 -d)\n  AWS_SECRET_ACCESS_KEY=$(echo \'d3dOQ1k1eHJJWVVtejZBblV6M0l1endXV0loQWZWcW9GZlVjMDlKRwo=\' | base64 -d)\nelse\n  CACHE_WRITE=true\nfi\ndocker run --detach -u 1001:1000                   -v ~/bazel-remote:/data                   -p 9092:9092                   buchgr/bazel-remote-cache:v2.4.1                   --s3.auth_method=access_key                   --s3.access_key_id=""${AWS_ACCESS_KEY_ID}""                   --s3.secret_access_key=""${AWS_SECRET_ACCESS_KEY}""                   --s3.bucket=cache.pantsbuild.org                   --s3.endpoint=s3.us-east-1.amazonaws.com                   --max_size 30\necho ""PANTS_REMOTE_STORE_ADDRESS=grpc://localhost:9092"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_READ=true"" >> ""$GITHUB_ENV""\necho ""PANTS_REMOTE_CACHE_WRITE=${CACHE_WRITE}"" >> ""$GITHUB_ENV""\n', 'mkdir -p ""${HOME}/.thrift""\ncurl --fail -L https://binaries.pantsbuild.org/bin/thrift/linux/x86_64/0.15.0/thrift -o ""${HOME}/.thrift/thrift""\nchmod +x ""${HOME}/.thrift/thrift""\necho ""${HOME}/.thrift"" >> $GITHUB_PATH\n', 'chmod +x src/python/pants/bin/native_client', './pants test --shard=9/10 ::\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/Linux-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n', 'chmod +x src/python/pants/bin/native_client', './pants --tag=+platform_specific_behavior test :: -- -m platform_specific_behavior\n', 'export S3_DST=s3://logs.pantsbuild.org/test/reports/macOS11-x86_64/$(git show --no-patch --format=%cd --date=format:%Y-%m-%d)/${GITHUB_REF_NAME//\\//_}/${GITHUB_RUN_ID}/${GITHUB_RUN_ATTEMPT}/${GITHUB_JOB}\necho ""Uploading test reports to ${S3_DST}""\n./build-support/bin/copy_to_s3.py                   --src-prefix=dist/test/reports                   --dst-prefix=${S3_DST}                   --path=""""\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox', 'python -m pip install --upgrade pip\npip install wheel\n', 'python setup.py sdist bdist_wheel\n']"
"['pip install tox', 'tox --verbose -e pep8', 'sudo apt install -y --no-install-recommends ccache libffi-dev default-libmysqlclient-dev libpq-dev libssl-dev libzmq3-dev', 'pip install codecov tox', 'env', 'tox --verbose --verbose -e ${{ matrix.toxenv }}', ""codecov --flags=$(echo ${{ matrix.toxenv }} |tr -d -- '-.')""]"
""
"['pip install .', 'pip install pytest', 'pytest', 'pip install pre-commit', 'pip install . tomli', 'pre-commit run --all-files --show-diff-on-failure', 'pip install .', 'python test_fuzz.py ./*.py', 'pip install build twine', 'python -m build --wheel --sdist', 'twine upload --skip-existing dist/*']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'sudo apt-get install -y libxml2-dev libxslt1-dev python-dev zlib1g-dev', 'pip install lxml', 'pip install -e .[test]', 'pytest tests/']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.config[1] }}', 'pip install coveralls coverage-python-version\ncoveralls --service=github\n']"
"[""python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade 'tox>=4.0.0rc3'\n"", 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine\npython -m coverage html --skip-covered --skip-empty\npython -m coverage report --fail-under=100\n']"
"['python -m pip install --upgrade pip wheel\npip install -r dev-requirements.txt\n', 'tox -s\n', 'python -m pip install --upgrade pip wheel\ncat dev-requirements.txt | grep yapf== | xargs pip install\n', 'yapf -p -r -d docs/ werobot/ tests/ *.py\n', 'python -m pip install --upgrade pip wheel\npip install -r docs-requirements.txt\n', 'sphinx-build -W docs docs/_build/html\n']"
"['python -m pip install --upgrade pip\npip install pytest flake8 black ruff\n', 'make lint\nmake test_flake8\n', 'pytest\n']"
"['python -c ""import sys; print(sys.version)""', 'pip install --upgrade pip\npip install wheel coverage\npip install -r requirements_prod.txt\npip install -e .\n', ""wger create-settings\ncoverage run --source='.' ./manage.py test\ncoverage lcov\n"", 'wger create-settings\npython manage.py test\n', 'pip install yapf isort', 'yapf --recursive --in-place wger', 'isort .', 'git config user.name Github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""Automatic linting""\ngit push\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['mkdir data\nmongod --fork --dbpath=$(pwd)/data --logpath=$PWD/mongo.log --setParameter enableTestCommands=1\n', 'python -m pip install -U pip tox tox-gh-actions setuptools\n', 'tox\n', 'mkdir data\nmongod --fork --dbpath=$(pwd)/data --logpath=$PWD/mongo.log --setParameter enableTestCommands=1\n', 'python -m pip install -U pip tox\n', 'tox -e py3-sphinx-docs\ntox -e py3-sphinx-doctest\ntox -e py3-sphinx-linkcheck\n']"
"['chmod +x ./ci/before-script.sh\n./ci/before-script.sh\npython -m pip install --upgrade pip\npip install nose\n', 'nosetests -w tests']"
""
"['source ci/ci_osx.sh\ndependencies\n', 'source ci/ci_osx.sh\ntests\n', 'source ci/ci_ubuntu.sh\nstyle_dependencies\n', 'source ci/ci_ubuntu.sh\nstyle\n', 'source ci/ci_ubuntu.sh\ndependencies\n', 'source ci/ci_ubuntu.sh\ntests\n', 'source ci/ci_ubuntu.sh\nupload_coverage\n', '. .\\ci\\ci_windows.ps1\nDependencies\n', '. .\\ci\\ci_windows.ps1\nTests\n', 'source ci/ci_ubuntu.sh\ndeployment_dependencies\n', 'source ci/ci_ubuntu.sh\nbuild\n', 'source ci/ci_ubuntu.sh\ndeploy\n']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python setup.py test', 'python -m pip install wheel\npython setup.py sdist bdist_wheel\n']"
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install --upgrade pip\npip install astropy==3.0.1 docutils==0.17.1 matplotlib==3.3.0 numpy==1.15.4 pandas==1.0.0 pyflakes scipy==1.2.2 sphinx==1.7.2 spktype21\n', 'python setup.py sdist\npip install -e .\n', './test-docs.sh\n', ""pyflakes $(find skyfield/ -name '*.py')\n"", 'python -m pip install --upgrade pip\npython setup.py sdist\npip install dist/*\npip install mock pandas\npip install https://github.com/brandon-rhodes/assay/archive/master.zip\n', 'cd ci && ../test-code.sh\n']"
"['python -VV\npython -m pip install -r requirements-dev.txt\n', 'make test\n']"
"['python -m pip install -U pip\npython -m pip install -U wheel\npython -m pip install -U tox\n', 'tox -e py\n', 'echo Test successful']"
"['python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'git config --global user.email user@example.com\ngit config --global user.name ""Example User""\n', 'poetry run python -m pytest -v --cov=PyGitUp\n', 'poetry build\npoetry run twine check dist/*\n', 'poetry run coveralls\n', 'python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'poetry publish --build\n']"
"['python -m pip install poetry pre-commit', 'poetry lock', 'poetry export --without-hashes --with dev -f requirements.txt -o requirements_dev.txt', 'poetry export -f requirements.txt --without-hashes -o requirements.txt', 'pre-commit autoupdate', 'git status', 'echo ""PULL_ID=$(jq --raw-output .pull_request.number ""$GITHUB_EVENT_PATH"")"" >> $GITHUB_ENV\n', 'echo ""PULL_ID=$(jq --raw-output .pull_request.number ""$GITHUB_EVENT_PATH"")"" >> $GITHUB_ENV\n', '# The name of the owner and of the repository: owner/repository\nIMAGE_NAME=$(echo ${{ github.repository }} | tr \'[:upper:]\' \'[:lower:]\')\necho ""image_name=$IMAGE_NAME"" >> $GITHUB_OUTPUT\n', 'set -x\n\n# Remove everything else than the tagged version\nVERSION=${GITHUB_REF#refs/tags/}\n\n# Tag as stable (:latest) if there is no letters in the version\n# Otherwise, tag as preview (:snapshot)\nif [[ $VERSION =~ ^[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n  # Matches 1.0.0, 1.0.1, etc.\n  NAMED_VERSION=""latest""\nelse\n  # Matches 1.0.0a1, 1.0.0rc1, etc.\n  NAMED_VERSION=""snapshot""\nfi\n\nTAGS=$""\\\nghcr.io/${{ steps.image.outputs.image_name }}:${NAMED_VERSION},\\\nghcr.io/${{ steps.image.outputs.image_name }}:${VERSION}\\\n""\n\n# Output the target tags\necho ""\nCONTAINER_TAGS=${TAGS}\nNAMED_VERSION=${NAMED_VERSION}\nVERSION=${VERSION}\n"" >> ""${GITHUB_ENV}""\n', 'set -x\n\n# Version name is the branch name\n# Slashes are substitued by dashes\nCLEAN_BRANCH_NAME=$(echo ${GITHUB_REF#refs/heads/} | tr / -)\n\n# Target GHCR with the branch name as the tag, e.g. unstable-main\nTAGS=$""\\\nghcr.io/${{ steps.image.outputs.image_name }}:unstable-${CLEAN_BRANCH_NAME}\n""\n\n# Set version name for open-containers version label as:\n# <branch>-<drift>-g<6-digit-hash>\nNAMED_VERSION=$(git describe --all --long | tr -d $\'\\n\')\n\n# Output the target tags\necho ""\nCONTAINER_TAGS=${TAGS}\nNAMED_VERSION=${NAMED_VERSION}\n"" >> ""${GITHUB_ENV}""\n', 'echo $""\\\n  Digest: ${{ steps.docker_build.outputs.digest }}\n  Tags: ${{ env.CONTAINER_TAGS }}""\n', 'curl -f -X POST \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.SALEOR_RELEASE_TOKEN }}"" \\\n  https://api.github.com/repos/saleor/saleor-multitenant/dispatches \\\n  -d ""{\\""event_type\\"":\\""deploy-staging\\"",\\""client_payload\\"":{\\""version\\"":\\""$VERSION\\""}}""\n\ncurl -f -X POST \\\n  -H ""Accept: application/vnd.github.v3+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.SALEOR_RELEASE_TOKEN }}"" \\\n  https://api.github.com/repos/saleor/saleor-cloud-deployments/dispatches \\\n  -d ""{\\""event_type\\"":\\""deploy-demo-staging\\"",\\""client_payload\\"":{\\""version\\"":\\""$VERSION\\""}}""\n', 'python3 ./.github/workflows/notify/notify-slack.py\n', 'apt-get install -y libpq-dev', 'python -m pip install wheel\npython -m pip install -r requirements_dev.txt\n', 'pytest \\\n  --cov \\\n  --junitxml=junit/test-results.xml \\\n  --django-db-bench=${{ env.BENCH_PATH }}\n', 'git config --global --add safe.directory ""$GITHUB_WORKSPACE""\npre-commit run --all\n', 'echo ""domain=${{ env.GITHUB_HEAD_REF_SLUG_URL }}.api.saleor.rocks"" >> $GITHUB_OUTPUT\n', 'apt-get install -y libpq-dev', 'python -m pip install wheel\npython -m pip install -r requirements_dev.txt\n', 'export DJANGO_SETTINGS_MODULE=saleor.tests.settings\n./manage.py migrate\n', 'export PYTEST_DB_URL=$DATABASE_URL\npytest -n 0 --reuse-db\n']"
""
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'echo ""::set-output name=dir::$(pip cache dir)""', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox', 'docker run -d --rm -p 15672:15672 -p 5672:5672 -p 5671:5671 --name nameko-rabbitmq ghcr.io/nameko/nameko-rabbitmq:3.6.6\ndocker cp nameko-rabbitmq:/srv/ssl certs\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox', 'docker run -d --rm -p 15672:15672 -p 5672:5672 -p 5671:5671 --name nameko-rabbitmq ghcr.io/nameko/nameko-rabbitmq:3.6.6\ndocker cp nameko-rabbitmq:/srv/ssl certs\n', 'mkdir $PWD/bin\nwget -O $PWD/bin/toxiproxy-server https://github.com/Shopify/toxiproxy/releases/download/v2.0.0/toxiproxy-server-linux-amd64\nchmod +x $PWD/bin/toxiproxy-server\nsudo ln -s $PWD/bin/toxiproxy-server /usr/local/bin/toxiproxy-server\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox', 'sudo apt-get install -y libenchant-dev\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox', 'pip install wheel\npython setup.py sdist bdist_wheel\n']"
""
"['sudo apt-get install -y libxkbcommon-x11-0', 'pip install tox', 'tox -e build_doc', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'tox -e pylint-ci', 'sudo apt-get update\nsudo apt-get install -y libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-xfixes0 libegl1-mesa libxcb-shape0 libxcb-cursor0\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', ""echo 'SKIP_EXAMPLE_WORKFLOWS=1' >> $GITHUB_ENV\n"", 'catchsegv xvfb-run -a -s ""$XVFBARGS"" tox -e ${{ matrix.tox_env }}', 'brew install libomp', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'tox -e ${{ matrix.tox_env }}\n']"
"['git checkout HEAD^2', 'git checkout HEAD^2', 'python -m pip install --upgrade pip\npip install setuptools build wheel twine\n', 'python -m build\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install ruff\n', 'ruff --format=github .', 'python -m pip install --upgrade pip wheel setuptools\npip install .\n# https://stackoverflow.com/a/69439779/2988107\npip install --upgrade numpy tinysegmenter jieba konlpy hebrew_tokenizer ""tweepy<4.0.0"" greek-stemmer-pos\npython -c ""import nltk; nltk.download(\'punkt\')""\npip install --upgrade pytest codecov pytest-cov\n', 'pytest tests', 'codecov']"
"['python -m pip install --upgrade pip setuptools', 'python setup.py -q sdist', 'python -m pip install --upgrade pip setuptools wheel Babel Jinja2', 'python setup.py -q build_locale bdist_wheel', 'sudo apt-get update && sudo apt-get -y --no-install-recommends install libcurl4-openssl-dev', 'echo ""day_of_year=$(date +%j)"" >> $GITHUB_ENV', 'pip install tox', 'tox -e ${{ matrix.toxenv }} --workdir ${{ env.toxworkdir }}', 'sudo apt-get update && sudo apt-get -y --no-install-recommends install libcurl4-openssl-dev', 'echo ""day_of_year=$(date +%j)"" >> $GITHUB_ENV', 'pip install tox\n[ ${{ startsWith(matrix.os, \'macos\') }} = true ] && { python -m pip install --upgrade pip==21.3.1; python -m pip install --install-option=""--openssl-dir=/usr/local/opt/openssl@1.1"" pycurl; } || true\n[ ${{ startsWith(matrix.os, \'windows\') }} = true ] && { pip install pipwin; pipwin install pycurl; } || true\n', 'tox -e ${{ matrix.toxenv || env.toxenv }} --sitepackages --workdir ${{ env.toxworkdir }}']"
"['pip install --upgrade pre-commit', 'pre-commit run --all-files', 'pip install --upgrade check-manifest setuptools', 'check-manifest', 'pip install --upgrade setuptools wheel', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade pip\npip install -e .[test]\n', 'pytest scrapyd --cov=./ --cov-report=xml\n', 'printf ""[scrapyd]\\nusername = hello12345\\npassword = 67890world\\n"" > scrapyd.conf\nmkdir logs\nscrapyd > scrapyd.log 2>&1 &\nsleep 1\npytest integration_tests\ncat scrapyd.log\nkill %%\n']"
"['pip install tox', 'tox', 'pip install tox twine wheel', 'tox', 'pip install --upgrade tox', 'tox -e cz', 'tox -e black -- --check', 'tox -e flake8', 'tox -e mypy', 'tox -e isort -- --check', 'tox -e pylint', 'pip install tox==3.26.0', 'tox -e pre-commit', 'resp=$(curl -H ""Authorization: bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN"" ""$ACTIONS_ID_TOKEN_REQUEST_URL&audience=pypi"")\noidc_token=$(jq \'.value\' <<< ""${resp}"")\n\nresp=$(curl -X POST https://pypi.org/_/oidc/github/mint-token -d ""{\\""token\\"":${oidc_token}}"")\napi_token=$(jq \'.token\' <<< ""${resp}"" | tr -d \'""\')\n\necho ""::add-mask::${api_token}""\necho ""api-token=${api_token}"" >> ""${GITHUB_OUTPUT}""\n', 'pip install tox', 'tox --skip-missing-interpreters false', 'pip install tox', ""tox -- --override-ini='log_cli=True'"", 'pip install tox', 'tox', 'pip install -r requirements-test.txt\n', 'python -m build -o dist/', 'pip install dist/*.whl -r requirements-test.txt tox', 'tox -e install']"
"['pip install pip==20.0.2\npip install -r requirements.txt\npython setup.py bdist_wheel\npip install dist/*.whl\n', 'cd tests/ && py.test --cov jmespath --cov-report term-missing\n']"
"['pip install mkdocs-material mkdocs-redirects', 'mkdocs gh-deploy --force --config-file mkdocs-gh-pages.yml', 'python3 -m pip install --upgrade pip\npython3 -m pip install --upgrade wheel\npython3 -m pip install mkdocs-material\nmkdocs build -d help\n', 'python3 -m pip install pyoxidizer==0.22.0', 'cd $GITHUB_WORKSPACE\n7z e ffmpeg-macos.7z -obin ""ffmpeg""\n', 'cd $GITHUB_WORKSPACE\ncp static/build_files/macos/pyoxidizer.bzl pyoxidizer.bzl\ncp static/build_files/macos/requirements.txt requirements.txt\nbasename $(rustc --print sysroot) | sed -e ""s/^stable-//"" > triple.txt\npyoxidizer build --release\ncd build/$(head -n 1 triple.txt)/release\nmkdir -p ""Hydrus Network.app/Contents/MacOS""\nmkdir -p ""Hydrus Network.app/Contents/Resources""\nmkdir -p ""Hydrus Network.app/Contents/Frameworks""\nmv install/static/icon.icns ""Hydrus Network.app/Contents/Resources/icon.icns""\ncp install/static/build_files/macos/Info.plist ""Hydrus Network.app/Contents/Info.plist""\ncp install/static/build_files/macos/ReadMeFirst.rtf ./ReadMeFirst.rtf\ncp install/static/build_files/macos/running_from_app ""install/running_from_app""\nln -s /Applications ./Applications\nmv install/* ""Hydrus Network.app/Contents/MacOS/""\nrm -rf install\n', 'cd $GITHUB_WORKSPACE\ntemp_dmg=""$(mktemp).dmg""\nhdiutil create ""$temp_dmg"" -ov -volname ""HydrusNetwork"" -fs HFS+ -format UDZO -srcfolder ""$GITHUB_WORKSPACE/build/$(head -n 1 triple.txt)/release""\nmv ""$temp_dmg"" HydrusNetwork.dmg\n', 'sudo apt-get update\nsudo apt-get install -y libmpv1\n', 'python3 -m pip install -r hydrus/static/build_files/linux/requirements.txt', 'mkdocs build -d help', 'cp hydrus/static/build_files/linux/hydrus_client.spec hydrus_client.spec\ncp hydrus/static/build_files/linux/hydrus_server.spec hydrus_server.spec\npyinstaller hydrus_server.spec\npyinstaller hydrus_client.spec\n', 'find dist/hydrus_client/ -type f -name ""*.pyc"" -delete\nwhile read line; do find dist/hydrus_client/ -type f -name ""${line}"" -delete ; done < hydrus/static/build_files/linux/files_to_delete.txt\n', 'rm -f dist/hydrus_client/libxkbcommon.so*\n', 'sudo chown --recursive 1000:1000 dist/hydrus_client\nsudo find dist/hydrus_client -type d -exec chmod 0755 {} \\;\nsudo chmod +x dist/hydrus_client/hydrus_client dist/hydrus_client/hydrus_server dist/hydrus_client/bin/swfrender_linux\n', 'mv dist/hydrus_client ""dist/Hydrus Network""\ntar -czvf Ubuntu-Extract.tar.gz -C dist ""Hydrus Network""\n', 'python3 -m pip install -r hydrus/static/build_files/windows/requirements.txt', 'mkdocs build -d help', '7z x mpv-dev-x86_64.7z -ompv\nmove mpv\\libmpv-2.dll hydrus\\mpv-2.dll\n', '7z e ffmpeg-release-full.7z -ohydrus/bin ""**/bin/ffmpeg.exe""\n', 'move hydrus\\static\\build_files\\windows\\sqlite3.dll hydrus\\\nmove hydrus\\static\\build_files\\windows\\sqlite3.exe hydrus\\db\nmove hydrus\\static\\build_files\\windows\\file_version_info_maker.py file_version_info_maker.py\npython file_version_info_maker.py ${{ github.ref_name }}\nmove hydrus\\static\\build_files\\windows\\hydrus_client.spec hydrus_client.spec\nmove hydrus\\static\\build_files\\windows\\hydrus_server.spec hydrus_server.spec\npyinstaller hydrus_server.spec\npyinstaller hydrus_client.spec\nmove dist\\hydrus_client ""dist\\Hydrus Network""\n', 'move hydrus\\static\\build_files\\windows\\InnoSetup.iss InnoSetup.iss\nISCC.exe InnoSetup.iss /DVersion=${{ github.ref_name }}\n', ""cd .\\dist\n7z.exe a -tzip -mm=Deflate -mx=5 ..\\Windows-Extract.zip 'Hydrus Network'\ncd ..\n"", 'echo ""version=${GITHUB_REF##*/}"" >> $GITHUB_ENV\necho ""version_short=${GITHUB_REF##*/v}"" >> $GITHUB_ENV\n', 'mkdir ubuntu windows\nmv Windows-Install/HydrusInstaller.exe    Hydrus.Network.${{ env.version_short }}.-.Windows.-.Installer.exe\nmv Windows-Extract/Windows-Extract.zip  Hydrus.Network.${{ env.version_short }}.-.Windows.-.Extract.only.zip\nmv Ubuntu-Extract/Ubuntu-Extract.tar.gz Hydrus.Network.${{ env.version_short }}.-.Linux.-.Executable.tar.gz\nmv MacOS-DMG/HydrusNetwork.dmg          Hydrus.Network.${{ env.version_short }}.-.macOS.-.App.dmg\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['make publish_deps deps', 'make test', 'make clean dist publish', 'make deps', 'make test']"
"['python -m pip install --upgrade pip pip install -r requirements.txt pytest', 'pytest']"
"['pip install tox', 'tox -e flake8,isort', 'bash .ci/install_mssql.sh', ""psql -U postgres -d sqlalchemy_utils_test -c 'CREATE EXTENSION hstore;'"", 'pip install tox', 'tox -e ${{ matrix.tox_env }}']"
"['echo ""127.0.0.1 authomatic.org"" | sudo tee -a /etc/hosts\necho ""127.0.0.1 authomatic.com"" | sudo tee -a /etc/hosts\necho ""127.0.0.1 authomatic.test"" | sudo tee -a /etc/hosts\n', 'openssl aes-256-cbc -pass env:SECRETS_KEY -d -md sha512 -pbkdf2 -iter 100000 -salt -in tests/functional_tests/config_secret.py.enc -out tests/functional_tests/config_secret.py\n', 'sudo pip install tox', 'sudo tox -e py3', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .']"
"['sudo apt-get update\nsudo apt-get install binutils libproj-dev gdal-bin -y\n', 'pip install -U pip wheel setuptools\npip install -U -r requirements-test.txt\npip install tox docutils pygments twine\n', './run-qa-checks\n', 'tox -e ${{ matrix.env.TOXENV }}\n', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --finish\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['set -x\ngit submodule update --init --recursive\n', 'set -x\npip install pylint\npip install yapf\ntest/run_pylint.sh\nyapf -dr .\n', 'set -x\npython gsutil version -l\npython gsutil test -u\n']"
"['git config --global url.""https://"".insteadOf git://\n./scripts/setup-npm.sh\nrm -f package-lock.json\n', 'python -m pip install --upgrade ""pip>=20,<21"" setuptools py\npython -m pip install wheel coveralls requirements-builder configparser\ncat requirements.txt > .prod-requirements.txt\n', 'export VIRTUAL_ENV=${pythonLocation}\ncat .prod-requirements.txt\nsudo apt-get update\nsudo apt-get -y install libkrb5-dev\npip install -r .prod-requirements.txt\npip install -e .[all]\npip freeze\n./scripts/setup-assets.sh\n', ""docker-compose -f ${{ matrix.SERVICE }} up -d\ntimeout 30 bash -c 'until curl -k -s -f -o /dev/null http://localhost:9200; do sleep 5; done' || false\n./run-tests.sh\n""]"
"['python -m pip install --upgrade pip\npython -m pip install .\n', 'python tests/test_mnemonic.py\n', 'python -m pip install --upgrade pip\npip install --upgrade black flake8 wheel mypy\n', 'make style_check', 'make dist']"
"['sudo apt-get install gettext python3 libgpgme-dev', 'sudo pip3 install .', 'torbrowser-launcher -h']"
"['python -m pip install --upgrade pip\npip install pytest tox tox-gh-actions\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox', 'tox -e coverage', 'python -m pip install --upgrade pip\npip install flake8 pylint black\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --statistics\n', 'black --check **/*.py', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install tox\n', 'tox -e py,lint,coveralls,release', 'ls -R', 'echo ""ðŸŽ‰""']"
"['git checkout HEAD^2', 'git checkout HEAD^2', 'python -m pip install --upgrade pip\npip install setuptools build wheel twine\n', 'python -m build\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\n# pytest 4.6.X works best for older Python versions\npip install -U lxml ""pytest==4.6.9"" codecov \'coverage==4.5.4\' pytest-cov\n', 'python -m pip install --upgrade pip\npip install -U lxml pytest codecov coverage pytest-cov\n', 'py.test tests']"
"['python -m pip install --upgrade pip tox tox-gh-actions\n', 'tox\n']"
"['wget https://packages.microsoft.com/config/ubuntu/21.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\nsudo dpkg -i packages-microsoft-prod.deb\nrm packages-microsoft-prod.deb\nsudo apt-get update\nsudo apt-get install -y apt-transport-https && \\\n  sudo apt-get update && \\\n  sudo apt-get install -y dotnet-sdk-6.0\n', 'TASK=${{ matrix.task }} ./build.sh', 'pip install --upgrade setuptools pip wheel\npip install -r requirements/coverage.txt\npip install hypothesis-python/[all]\n', 'python -m pytest --numprocesses auto hypothesis-python/tests/ --ignore=hypothesis-python/tests/quality/ --ignore=hypothesis-python/tests/ghostwriter/', 'TASK=${{ matrix.task }} ./build.sh', 'TASK=${{ matrix.task }} ./build.sh', './build.sh upgrade-requirements']"
"['python -m pip install --upgrade pip wheel\npip install -r requirements.txt -r requirements-devel.txt\n', ""coverage run --source=taiga --omit='*tests*,*commands*,*migrations*,*admin*,*.jinja,*dashboard*,*settings*,*wsgi*,*questions*,*documents*' -m pytest -v --tb=native --pythonwarnings=default\n""]"
"['pip install poetry\npoetry install\n', 'poetry run pre-commit run --all-files --show-diff-on-failure\n', 'pip install poetry\npoetry config installer.parallel false\npoetry install\n', 'poetry run pytest -n auto\n', 'pip install poetry\npoetry install\n', 'poetry run coverage run -m pytest\n', 'poetry run coveralls\n', 'pip install poetry dunamai', 'RX_VERSION=$(dunamai from any --no-metadata --style pep440)\npoetry version $RX_VERSION\necho ""__version__ = \\""$RX_VERSION\\"""" > reactivex/_version.py\n', 'poetry build', ""poetry publish -u __token__ -p ${{ secrets.PYPI_API_TOKEN }} || echo 'Version exists'""]"
""
"['sudo apt -y update\nsudo apt -y install libcurl4-openssl-dev libssl-dev pkg-config libxml2-dev libxslt-dev\n', 'pip install -U setuptools wheel pip\npip install -r requirements.txt --no-deps\npip install -r requirements/dev.txt\npip check\n', 'sudo apt -y update\nsudo apt -y install libcurl4-openssl-dev libssl-dev pkg-config libxml2-dev libxslt-dev\n', '${{ matrix.command }}', 'sudo apt-get update\nsudo apt install libcurl4-openssl-dev libssl-dev\n', 'sourcebranches=""${BRANCHES_TO_COMBINE%\\""}""\nsourcebranches=""${sourcebranches#\\""}""\n\nbasebranch=""${BASE_BRANCH%\\""}""\nbasebranch=""${basebranch#\\""}""\n\ngit config pull.rebase false\ngit config user.name github-actions\ngit config user.email github-actions@github.com\n\ngit branch $COMBINE_BRANCH_NAME $basebranch\ngit checkout $COMBINE_BRANCH_NAME\ngit pull origin $sourcebranches --no-edit\ngit push origin $COMBINE_BRANCH_NAME\n', 'npm ci', '${{ matrix.command }}']"
"['sudo apt-get update\nsudo apt-get -y install libglu1-mesa-dev libgl1-mesa-dev libxi-dev libglfw3-dev libgles2-mesa-dev libsdl2-2.0-0 mesa-utils x11-utils\n', 'export DISPLAY=:99.0\n/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1400x900x24 -ac +extension GLX +render\nsleep 5\n', 'export DISPLAY=:99.0\nglxinfo\n', 'export DISPLAY=:99.0\nxdpyinfo\n', 'python -m pip install --upgrade pip\npip install flake8 numpy cython pytest numpydoc\n', 'pip install -e .\n', 'python make test lineendings\n', 'python make test flake\n', '# opengl system libraries\nsudo apt-get update;\ncat ci/requirements/linux_full_deps_apt.txt | xargs sudo apt-get -y install\n# Additional latex dependencies\nsudo apt-get -y install texlive-latex-base texlive-latex-extra texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra\nsudo apt-get -y install dvipng\n# Start xvfb daemon\nexport DISPLAY=:99.0\n/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1400x900x24 -ac +extension GLX +render\nsleep 5\n# export python_version\necho ""python_version=310"" >> $GITHUB_OUTPUT\n', 'micromamba install -y python=3.9 -f ci/requirements/linux_full_deps_conda.txt -c conda-forge\n', 'python -m pip install --upgrade pip\npip install -r ci/requirements/linux_full_deps_pip.txt\npip install -r ci/requirements/linux_website_deps_pip.txt\n', 'pip install -e .[doc]\n', 'export DISPLAY=:99.0\nmicromamba list\npython -c ""import vispy; print(vispy.version.version)""\npython -c ""import vispy; print(vispy.sys_info())""\n', 'export DISPLAY=:99.0\nexport XDG_RUNTIME_DIR=/tmp/runtime-runner\nsource ./ci/build_website.sh\n', '# opengl system libraries\nif [ ""${{ matrix.deps }}"" == \'full\' ]; then\n  sudo apt-get update;\n  if [ ""${{ matrix.new_qt }}"" == \'true\' ]; then\n    cat ci/requirements/linux_full_newqtdeps_apt.txt | xargs sudo apt-get -y install\n  else\n    cat ci/requirements/linux_full_deps_apt.txt | xargs sudo apt-get -y install\n  fi\nelse\n  sudo apt-get -y install x11-utils\nfi\n# Start xvfb daemon\nexport DISPLAY=:99.0\n/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1400x900x24 -ac +extension GLX +render\nsleep 5\n# export python_version\nPY_VER=${{ matrix.python_version }}\necho ""python_version=${PY_VER//.}"" >> $GITHUB_OUTPUT\n', 'if [ ""${{ matrix.new_qt }}"" == \'true\' ]; then\n  micromamba install -y python=${{ matrix.python_version }} -f ci/requirements/linux_${{ matrix.deps }}_newqtdeps_conda.txt -c conda-forge\nelse\n  micromamba install -y python=${{ matrix.python_version }} -f ci/requirements/linux_${{ matrix.deps }}_deps_conda.txt -c conda-forge\nfi\n', 'if [ ""${{ matrix.deps }}"" == \'full\' ]; then\n  if [ ""${{ matrix.new_qt }}"" == \'true\' ]; then\n    pip install -r ci/requirements/linux_full_newqtdeps_pip.txt\n  else\n    pip install -r ci/requirements/linux_full_deps_pip.txt\n  fi\n  rm -rf vispy/ext/_bundled\nfi\n', 'pip install -e .\n', 'if [ ""${{ matrix.test }}"" != \'osmesa\' ]; then\n  export DISPLAY=:99.0\nfi\nif [ ""${{ matrix.test }}"" == \'osmesa\' ]; then\n  export OSMESA_LIBRARY=~/micromamba-root/envs/vispy-tests/lib/libOSMesa32.so;\n  export VISPY_GL_LIB=$OSMESA_LIBRARY\nfi\nmicromamba list\npython -c ""import vispy; print(vispy.version.version)""\npython -c ""import vispy; print(vispy.sys_info())""\n', 'if [ ""${{ matrix.test }}"" == \'standard\' ]; then\n  export DISPLAY=:99.0\n  python make test unit --tb=short\nfi\nif [ ""${{ matrix.test }}"" == \'osmesa\' ]; then\n  export OSMESA_LIBRARY=~/micromamba-root/envs/vispy-tests/lib/libOSMesa32.so\n  export VISPY_GL_LIB=$OSMESA_LIBRARY\n  make osmesa\nfi\nCOVERAGE_FILE=.vispy-coverage coverage combine\nmv .vispy-coverage .coverage\n', 'python -m pip install --upgrade pip setuptools build wheel\npython -m pip install numpy Cython\npython -m build -s -o dist/\n']"
"['npm install yarn', 'yarn install --registry https://registry.yarnpkg.com', 'yarn build', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'make flake8-in-ci\n', 'make test-in-ci\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['python -m pip install --upgrade pip\npip install flake8\n', 'tmpafter=$(mktemp)\nfind src -name \\*.py -exec flake8 --ignore=E402,E501,W504 {} + | egrep -v ""alembic/versions/|usr/local/share/pysnmp/mibs/"" > $tmpafter\nnum_errors_after=`cat $tmpafter | wc -l`\necho ""CURRENT_ERROR_FILE=${tmpafter}"" >> $GITHUB_ENV\necho ""CURRENT_ERRORS=${num_errors_after}"" >> $GITHUB_ENV\n', 'tmpbefore=$(mktemp)\nfind src -name \\*.py -exec flake8 --ignore=E402,E501,W504 {} + | egrep -v ""alembic/versions/|usr/local/share/pysnmp/mibs/"" > $tmpbefore\nnum_errors_before=`cat $tmpbefore | wc -l`\necho ""OLD_ERROR_FILE=${tmpbefore}"" >> $GITHUB_ENV\necho ""OLD_ERRORS=${num_errors_before}"" >> $GITHUB_ENV\n', 'if [ ${{ env.CURRENT_ERRORS }} -gt ${{ env.OLD_ERRORS }} ]; then\n  echo ""New flake8 errors were introduced""\n  diff -u ${{ env.OLD_ERROR_FILE }} ${{ env.CURRENT_ERROR_FILE }}\n  exit 1\nelse\n  echo ""No new flake 8 errors were introduced""\nfi\n', './src/freenas/usr/bin/install-dev-tools\ncd src/middlewared ; make reinstall_container\ncp -a /__w/middleware/middleware/src/middlewared/middlewared/pytest /usr/local/lib/python3.11/dist-packages/middlewared/\n     \n', 'pytest-3 -v --disable-pytest-warnings /usr/local/lib/python3.11/dist-packages/middlewared/pytest']"
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install virtualenv\n', './script/cibuild\n', './script/cibuild-setup-py\n']"
"['conda info\nconda list\n', 'python -m pip install -U pip\npip install .[testing]\n', 'flake8 --select F401, F405, E231 quantecon\n', 'coverage run -m pytest quantecon\ncoverage lcov\n', 'pip install flit~=3.6\n', 'flit publish\n']"
"['sudo apt-get update --fix-missing;\nif [ ""${{ matrix.qt-version }}"" == 5 ]; then\n  cat ci/qt5_linux.txt | xargs sudo apt-get -y install\nelse\n  cat ci/qt6_linux.txt | xargs sudo apt-get -y install\nfi\nsudo apt-get install -y herbstluftwm scrot\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'python -m pip install --upgrade pip\npip install setuptools-scm[toml]\npip install numpy\npip install git+https://github.com/we-like-parsers/pegen --no-deps\npip install git+https://github.com/MatthieuDartiailh/bytecode@main\npip install git+https://github.com/nucleic/cppy@main\npip install git+https://github.com/nucleic/atom@main\npip install git+https://github.com/nucleic/kiwi@main\n', 'pip install -e .[qt${{ matrix.qt-version }}-${{ matrix.qt-binding }}]\n', 'pip install -e .[qt${{ matrix.qt-version }}-${{ matrix.qt-binding }},ipython-qt,matplotlib-qt,scintilla-qt${{ matrix.qt-version }}-pyqt,webview-qt${{ matrix.qt-version }}-pyqt]\n', 'pip install -e .[qt${{ matrix.qt-version }}-${{ matrix.qt-binding }},ipython-qt,matplotlib-qt]\n', 'pip install -r test_requirements.txt\n', 'python -X dev -m pytest tests -v --cov --cov-report xml -rs', 'pip uninstall python3-xlib --yes\npip install python-xlib\nexport DISPLAY=:99.0\n/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1920x1200x24 -ac +extension GLX +render -noreset\nsleep 3\nherbstluftwm &\nsleep 1\npython -X dev -m pytest tests -v --cov --cov-report xml -rs\n', 'bash -c ""find . -type f -name \'*.gcno\' -exec gcov -pb --all-blocks {} +"" || true\nls\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'python -m pip install --upgrade pip\npip install wheel\npip install -r docs/requirements.txt\n', 'pip install .\n', 'cd docs\nmkdir docs_output;\nsphinx-build source docs_output -W -b html;\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'pip install --upgrade pip\npip install wheel build\npython -m build . -s\n', 'pip install pytest\npip install dist/*.tar.gz\ncd ..\npython -m pytest enaml/tests\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'python -m pip install --upgrade pip\npython -m pip install wheel cibuildwheel\n', 'python -m cibuildwheel . --output-dir dist\n']"
"['sudo apt-get install graphviz libxml2-dev libxslt-dev python3-dev\npython -m pip install --upgrade pip\npip install -e .\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 soco\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\npylint soco\n', 'pytest --cov-config .coveragerc --cov soco .\n', 'black --check soco tests\n', 'make docs\n']"
"['python -m pip install --upgrade pip setuptools wheel pycodestyle\n', 'make pycodestyle test']"
"['python -m pip install poetry==1.3.1\n', 'python -m poetry config virtualenvs.in-project true\n', 'python -m poetry install --only=dev\n', 'source .venv/bin/activate\ntox -e mypy\n', 'source .venv/bin/activate\ncoverage erase\ntox run-parallel -f ${{ matrix.toxfactor }} --parallel-no-spinner --parallel-live\ncoverage combine\ncoverage xml\n']"
"['pip install -r dev-requirements.txt', 'echo ""VERSION=$(python -m pbxproj --version).${{ github.sha }}"" >> $GITHUB_ENV', 'make coverage', 'pip install .\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -c ""import sys; print(sys.version)""', 'sudo apt-get install gcc ipmitool', 'python -m pip install --upgrade pip wheel\npip install pytest pytest-cov\npip install -r requirements.txt\n', 'pytest --junitxml=junit/test-results.xml --cov=com --cov-report=xml --cov-report=html\n']"
"['python -m pip install scons\nInvoke-WebRequest -Uri ""https://inkscape.org/gallery/item/37363/inkscape-1.2.2_2022-12-09_732a01da63-x64.exe"" -OutFile ""inkscape.exe""\nStart-Process .\\inkscape.exe /S -NoNewWindow -Wait\n', ""Xvfb :99 -screen 0 1280x1024x24 &\nmetacity --display :99.0 &\nuseradd -m testUser\necho LD_PRELOAD=libSegFault.so >> $GITHUB_ENV\n# The Docker container configures bash shells such that they enable the\n# software collections we want. If we could set GitHub's\n# `defaults.run.shell` to `bash` then all our build steps would pick up\n# this environment automatically. But we can't do that because it\n# breaks the build on Windows, and we can't configure a different shell\n# per platform because GitHub won't allow it. But we can run _this_\n# Linux-only step in bash, and transfer the environment out to be used\n# in later steps.\necho $PATH > $GITHUB_PATH\necho LD_LIBRARY_PATH=$LD_LIBRARY_PATH >> $GITHUB_ENV\necho DISPLAY=:99.0 >> $GITHUB_ENV\n"", 'python --version\npip install PyJWT==1.7.1 PyGitHub==1.45\n', '.github/workflows/main/setBuildVars.py\necho GAFFER_SPHINX=`which sphinx-build` >> $GITHUB_ENV\n', 'echo GAFFER_DEPENDENCIES_HASH=`python .github/workflows/main/installDependencies.py --dependenciesDir ${{ env.GAFFER_BUILD_DIR }} --outputFormat ""{archiveDigest}""` >> $GITHUB_ENV\n./.github/workflows/main/installDelight.py\necho DELIGHT=$GITHUB_WORKSPACE/3delight >> $GITHUB_ENV\n', 'curl.exe -L --output mesa.7z --url https://github.com/pal1000/mesa-dist-win/releases/download/22.3.1/mesa3d-22.3.1-release-msvc.7z\n& ""C:\\Program Files\\7-Zip\\7z.exe"" x mesa.7z -omesa\n./mesa/systemwidedeploy.cmd 1\n', 'scons -j 2 build BUILD_TYPE=${{ matrix.buildType }} OPTIONS=.github/workflows/main/sconsOptions\n', 'echo ""::add-matcher::./.github/workflows/main/problemMatchers/unittest.json""\n${{ matrix.testRunner }} ""${{ env.GAFFER_BUILD_DIR }}/bin/gaffer test ${{ matrix.testArguments }}""\necho ""::remove-matcher owner=unittest::""\n', 'import subprocess\nimport sys\nimport os\n\nfor arnoldVersion in [ ""7.1.1.0"", ""7.2.1.0"" ] :\n  arnoldRoot = os.path.join( os.environ[""GITHUB_WORKSPACE""], ""arnoldRoot"", arnoldVersion )\n  os.environ[""ARNOLD_ROOT""] = arnoldRoot\n\n  subprocess.check_call(\n    [\n      sys.executable,\n      "".github/workflows/main/installArnold.py"",\n      ""--version"",\n      arnoldVersion\n    ]\n  )\n  #Build Arnold extension\n  subprocess.check_call( ""scons -j 2 build BUILD_TYPE=${{ matrix.buildType }} OPTIONS=.github/workflows/main/sconsOptions"", shell = True )\n\n  if os.name != ""nt"" :\n    # Test Arnold extension\n    print( ""::add-matcher::./.github/workflows/main/problemMatchers/unittest.json"" )\n    subprocess.check_call( ""${{ matrix.testRunner }} \\"""" + os.path.join( os.environ[""GAFFER_BUILD_DIR""], ""bin"", ""gaffer"" ) + "" test IECoreArnoldTest GafferArnoldTest GafferArnoldUITest\\"""", shell = True )\n    print( ""::remove-matcher owner=unittest::"" )\n\n  # Publish ARNOLD_ROOT to the environment for subsequent steps,\n  # so we can build the docs for GafferArnold.\n  with open( os.environ[""GITHUB_ENV""], ""a"" ) as f :\n    print( ""Setting $ARNOLD_ROOT to \'%s\'"" % arnoldRoot )\n    f.write( \'ARNOLD_ROOT=%s\\n\' % arnoldRoot )\n', '# Treats warnings-as-errors so we know about broken links\necho ""::add-matcher::./.github/workflows/main/problemMatchers/sphinx.json""\nscons -j 2 package BUILD_TYPE=${{ matrix.buildType }} OPTIONS=.github/workflows/main/sconsOptions\necho ""::remove-matcher owner=sphinx::""\n', 'echo ""::add-matcher::./.github/workflows/main/problemMatchers/validateRelease.json""\npython ./config/validateRelease.py --archive ${{ env.GAFFER_BUILD_NAME }}.${{ env.PACKAGE_EXTENSION }} ${{ env.GAFFER_VALIDATE_EXTRA_FLAGS }}\necho ""::remove-matcher owner=validateRelease::""\n', 'python ./config/publishRelease.py --archive ${{ env.GAFFER_BUILD_NAME }}.${{ env.PACKAGE_EXTENSION }} --repo ${{ github.repository }} --releaseId ${{ env.GAFFER_GITHUB_RELEASEID }}\n', 'python ./.github/workflows/main/limitDirectorySize.py --directory ./sconsCache --megabytes ${{ matrix.sconsCacheMegabytes }} --verbose', '# Print SCons logs\nshopt -s nullglob\nfor logFile in config.log\ndo\n echo $logFile\n cat $logFile\ndone\n', 'git -c core.whitespace=indent-with-non-tab,tabwidth=1 diff --check refs/remotes/origin/${{ github.base_ref }} include src python\n']"
"['python -m pip install --upgrade pip wheel build\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python -m build\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'find . -path \\*/vendor -prune -false -o -name \\*.y*ml | xargs yamllint -d relaxed\n']"
"['# install pip=>20.1 to use ""pip cache dir""\npython3 -m pip install --upgrade pip\n', 'echo ""PY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_OUTPUT\necho ""PIP_CACHE=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python3 -m pip install -r ./requirements.txt\npython3 -m pip install .\n', '# Build html and link check\nmake -C docs\n', 'cat docs/_build/linkcheck/output.txt | while read line\ndo\n  echo -e ""$line \\n""\ndone\n', 'echo ""PY=$(python -c \'import hashlib, sys;print(hashlib.sha256(sys.version.encode()+sys.executable.encode()).hexdigest())\')"" >> $GITHUB_OUTPUT\necho ""PIP_CACHE=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-build.txt\n# Install virtualenv to ensure this runs on window\npython3 -m pip install ""build[virtualenv]""\n', 'python -m pip freeze', ""git fetch origin 'refs/tags/*:refs/tags/*'"", '# Build package\npython3 -m build\n', '# Install package from the built wheel\npython -m pip install --no-deps dist/*.whl\n', 'make test\n', 'curl https://developers.strava.com/swagger/swagger.json > stravalib/tests/resources/strava_swagger.json', ""git fetch origin 'refs/tags/*:refs/tags/*'"", 'python -m pip install -r requirements-build.txt\npip list\n', 'python3 -m build\necho """"\necho ""Generated files:""\nls -lh dist/\n', 'twine check dist/*']"
"['pip install -r DEPENDS.txt -r DEPENDS-tests.txt -r DEPENDS-docs.txt\n', 'tools/header.py ""Dependency versions""\ntools/build_versions.py\npython setup.py build_ext --inplace\npip install -e .\n', 'nosetests -v --exe --with-coverage --cover-package=skfuzzy skfuzzy\nflake8 --exclude=test_* skfuzzy docs/examples\n', 'make -C docs/ html\n']"
"['npm install', 'pip3 install -r requirements-dev.txt', 'npm run test']"
"['# Remove existing google-cloud-sdk packages in Ubuntu.\nsudo rm -rf /usr/lib/google-cloud-sdk\ncurl https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-410.0.0-linux-x86_64.tar.gz | tar -zx > /dev/null\n# Substitute the downloaded google-cloud-sdk packages, due to https://stackoverflow.com/questions/42697026/install-google-cloud-components-error-from-gcloud-command.\nsudo mv google-cloud-sdk /usr/lib/\nsudo gcloud components update\nsudo gcloud components install app-engine-python beta cloud-datastore-emulator app-engine-python-extras\ngcloud config set project cr-status-staging\ngcloud version\n', 'npm install -g gulp\nnpm ci\nnpm run deps\n', 'npm run lint', 'mypy --ignore-missing-imports --exclude cs-env/ --exclude gen/ --exclude appengine_config.py .', 'npm test', 'npm run build', '# Remove existing google-cloud-sdk packages in Ubuntu.\nsudo rm -rf /usr/lib/google-cloud-sdk\ncurl https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-410.0.0-linux-x86_64.tar.gz | tar -zx > /dev/null\n# Substitute the downloaded google-cloud-sdk packages, due to https://stackoverflow.com/questions/42697026/install-google-cloud-components-error-from-gcloud-command.\nsudo mv google-cloud-sdk /usr/lib/\nsudo gcloud components update\nsudo gcloud components install app-engine-python beta cloud-datastore-emulator app-engine-python-extras\ngcloud config set project cr-status-staging\ngcloud version\n', 'npm install -g gulp\n', 'npm ci', 'npm run deps', 'npm run build --if-present', 'npx playwright install-deps', 'npm run webtest']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-py\n', 'tox --py current']"
"['python setup.py install', 'mackup --help', 'pip install black', 'black --check --target-version py310 .', 'mdl .', 'pip install docopt six nose', 'nosetests --with-coverage --cover-tests --cover-inclusive --cover-branches --cover-package=mackup', 'pip install pipenv', 'pipenv install -d', 'make test']"
"['sudo apt-get update\nsudo apt-get install -y --no-install-recommends -o Acquire::Retries=3 \\\n  gdb gdbserver socat \\\n  qemu-user-static \\\n  binutils-aarch64-linux-gnu \\\n  binutils-arm-linux-gnueabihf \\\n  libc6-dbg\n', ""source travis/setup_avd_fast.sh\nsed -i 's/skip_android = True/skip_android = False/' docs/source/conf.py\nset | grep ^PATH >.android.env\n"", 'pip install --upgrade pip\npython setup.py egg_info\npip install --upgrade --editable .\n', ""PWNLIB_NOTERM=1 python -c 'from pwn import *; print(pwnlib.term.term_mode)'"", 'pip install -r docs/requirements.txt', 'source .android.env\nPWNLIB_NOTERM=1 coverage run -m sphinx -b doctest docs/source docs/build/doctest docs/source/adb.rst\n', ""if git log --stat ${GITHUB_BASE}..HEAD | grep CHANGELOG.md; then\n   echo 'Changelog updated :D'\nelse\n   if git log --stat ${GITHUB_BASE}..HEAD pwnlib pwn | grep '++\\|--'; then\n      echo 'Major changes detected, changelog required!'\n      false\n   else\n      echo 'Minor changes detected, no changelog required!'\n   fi\nfi\n"", 'git fetch origin\ngit log --oneline --graph -10\n', 'set -x\nGITHUB_TAG=${GITHUB_REF#refs/tags/}\necho ""$GITHUB_TAG"" | grep -E \'^[0-9.]*(beta[0-9])?$\'\nvsetup=$(grep -o ""version\\\\s*=\\\\s*[\\""\'].*[\\""\'],$"" setup.py | grep -o ""[0-9][^\\""\']*"")\nvpwnlib=$(grep -o ""__version__\\\\s*=\\\\s*[\\""\'].*[\\""\']$"" pwnlib/version.py | grep -o ""[0-9][^\\""\']*"")\n[ ""$vsetup"" = ""$vpwnlib"" ]\n[ ""$GITHUB_TAG"" = ""$vsetup"" ]\n', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends -o Acquire::Retries=3 \\\n  ash bash-static dash ksh mksh zsh \\\n  pandoc gdb gdbserver socat \\\n  binutils-multiarch qemu-user-static \\\n  binutils-aarch64-linux-gnu \\\n  binutils-arm-linux-gnueabihf \\\n  binutils-mips-linux-gnu \\\n  binutils-msp430 \\\n  binutils-powerpc-linux-gnu \\\n  binutils-s390x-linux-gnu \\\n  binutils-sparc64-linux-gnu \\\n  gcc-multilib \\\n  libc6-dbg \\\n  elfutils\n', 'sudo apt-get install -y python3-pip\n/usr/bin/python3 -m pip install rpyc\n', ""ulimit -a\nulimit -c unlimited\ncat /proc/sys/kernel/core_pattern\ncat /proc/sys/kernel/core_uses_pid\n( cd $(mktemp -d); sh -c 'kill -11 $$' || true; ls -la ./*core* /var/crash/*.crash /var/lib/apport/coredump/core*) || true\n"", 'travis/ssh_setup.sh\n', 'pip install --upgrade pip\npip install --upgrade wheel build\npip install --upgrade flake8 appdirs\npip install --upgrade --editable .\n', ""PWNLIB_NOTERM=1 python -bb -c 'from pwn import *; print(pwnlib.term.term_mode)'"", 'pip install -r docs/requirements.txt', 'pip install unicorn==2.0.0rc7', 'echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope # required by some gdb doctests\n', 'PWNLIB_NOTERM=1 python -bb -m coverage run -m sphinx -b doctest docs/source docs/build/doctest\n', 'export TERM=linux\nset -x\npython -bb travis/coverage_chdir.py examples/fmtstr examples/fmtstr/exploit.py\npython -bb travis/coverage_chdir.py examples/fmtstr examples/fmtstr/exploit2.py || : # can fail randomly?\npython -bb -m coverage run examples/asm.py\npython -bb -m coverage run examples/asm.py\npython -bb -m coverage run examples/text.py\n# for f in examples/sigreturn_corefile_*.py; do coverage run ""$f""; done # XXX something is wrong\n', 'export TERM=linux\npwn() { ( set +x; cmd=$1; shift; PYTHONUNBUFFERED=1 exec python -bb -m coverage run -m pwnlib.commandline.""$cmd"" ""$@"" ) }\nset -x\n\npwn cyclic 32\npwn cyclic -l 0x62616161\npwn cyclic -a ab\necho\n\npwn shellcraft --list |tail\npwn shellcraft -l --syscalls |tail\npwn shellcraft -l execve\npwn shellcraft --show i386.linux.loader_append\npwn shellcraft -f asm --color amd64.linux.sh\npwn shellcraft -f elf amd64.linux.syscalls.exit 0 </dev/null |pwn hex\npwn shellcraft -f i   --color amd64.linux.cat /etc/passwd </dev/null\npwn shellcraft -f c   amd64.linux.syscalls.exit 0 </dev/null\npwn shellcraft -f str aarch64.linux.sh </dev/null\npwn shellcraft -abr -f elf -o /dev/null amd64.linux.cat /etc/passwd </dev/null\npwn shellcraft -nzr thumb.linux.syscalls.execve /bin/cat \'[""/bin/cat"", ""/etc/os-release""]\' </dev/null\npwn shellcraft -fp aarch64.trap\n\npwn disasm --color ff3424c3ebfe\npwn asm -f hex nop\n\npwn hex ABCD\npwn hex ABCD --separator \' \'\npwn hex ABCD --prefix \'\\x\'\npwn hex ABCD -p \'0x\' -s \' \'\n\npwn hex abcd\npwn unhex 4141 4141\n\ncat /dev/urandom | pwn phd --color -c 256 -s 2\npwn phd -l 0x3d --color=always /etc/os-release\n\npwn checksec /bin/bash\n\npwn errno 2\npwn errno -1\npwn errno EADDRINUSE\n\npwn constgrep -c freebsd -m ^PROT_ \'3 + 4\'\npwn constgrep ^MAP_ 0\npwn constgrep -e O_RDWR\n\npwn libcdb file /lib/x86_64-linux-gnu/libc.so.6\npwn libcdb lookup puts 5f0 __libc_start_main_ret d0a\npwn libcdb hash b229d1da1e161f95e839cf90cded5f719e5de308\n', 'python -m build\n', 'coverage combine\nCOVERALLS_REPO_TOKEN=PP20MEgztXIQJJTguQwe2jeCh6Bm4lkbv coveralls\n', 'BRANCH=${GITHUB_REF#refs/heads/}\nTARGET=${BRANCH%-staging}\ngit branch -f ""$TARGET""\ngit push origin ""$TARGET""\ngit push origin --delete ""$BRANCH""\n', 'ls -R', 'pip install flake8\nflake8 . --count --select=E9,F63,F7,E71 --show-source --statistics --exclude=android-?dk  # TODO: Add F82\n', ""flake8 pwnlib setup.py docs travis --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nflake8 examples --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --ignore='F403,F405'\nflake8 pwn --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --ignore='F401,F403,F405'\n"", ""if git show ${GITHUB_BASE}..HEAD | grep -EB15 --color=always '^\\+\\+(>>>>>|=====|<<<<<)'; then\n    echo 'WARNING: Merge marker commits found in history.'\n    if git diff ${GITHUB_BASE}..HEAD | grep -EB15 --color=always '^\\+(>>>>>|=====|<<<<<)'; then\n        echo 'ERROR: Merge markers still present! Do something!'\n        exit 1\n    fi\nelse\n    echo 'Congrats! All the merges you did were clean or resolved correctly :)'\nfi\n"", 'set -x\npip install pylint\npip install --upgrade -e .\npylint --exit-zero --errors-only pwnlib -f parseable | cut -d \' \' -f2- > current.txt\ngit fetch origin\ngit checkout origin/""$GITHUB_BASE_REF""\npylint --exit-zero --errors-only pwnlib -f parseable | cut -d \' \' -f2- > base.txt\nif diff base.txt current.txt | grep \'>\'; then\n  false\nfi\n']"
"['python -m pip install --upgrade pip\npip install codespell flake8 -r requirements.txt\n', 'echo index.%EXT% > tmp_wordlist.txt\necho home.html >> tmp_wordlist.txt\necho testphp.vulnweb.com > tmp_targets.txt\necho ""GET / HTTP/1.1"" > tmp_raw.txt\necho ""Host: google.com"" >> tmp_raw.txt\necho ""User-Agent: dirsearch"" >> tmp_raw.txt\necho ""Accept: */*"" >> tmp_raw.txt\npython3 dirsearch.py -w tmp_wordlist.txt -u https://example.com -o tmp_report.json --format json --force-recursive -R 3 --full-url -q -O\npython3 dirsearch.py -w tmp_wordlist.txt -l tmp_targets.txt --subdirs /,admin/ --exclude-extensions conf -q -L -f -i 200 --user-agent a --log tmp_log.log\npython3 dirsearch.py -w tmp_wordlist.txt -u https://localhost --ip 93.184.216.34 --max-rate 2 -H K:V --random-agent --overwrite-extensions --no-color\npython3 dirsearch.py -w tmp_wordlist.txt --raw tmp_raw.txt --prefixes . --suffixes ~ --skip-on-status 404 -m POST -d test=1 --crawl --min-response-size 9\necho https://self-signed.badssl.com | python3 dirsearch.py -w tmp_wordlist.txt --stdin --max-time 9 --auth u:p --auth-type basic --scheme http\n', 'python3 testing.py', 'flake8 .\n', 'codespell', 'docker build . --file Dockerfile --tag my-image-name:$(date +%s)']"
"['sudo apt-get update\nsudo apt-get install patchelf gdb ccache libfuse2\npython -m pip install --no-python-version-warning --disable-pip-version-check -r requirements-devel.txt\npython -m pip install --no-python-version-warning --disable-pip-version-check .\n', 'python -m nuitka --module --show-scons --run --report=compilation-report-module.xml tests/basics/EmptyModuleTest.py\npython -m nuitka --show-scons --run --report=compilation-report-exe.xml tests/basics/EmptyModuleTest.py\n', 'python -m nuitka --version\nenv | sort\npython ./tests/run-tests --no-other-python --skip-reflection-test --skip-all-cpython-tests --assume-yes-for-downloads\n', 'python ./bin/check-nuitka-with-pylint\n', 'python ./bin/check-nuitka-with-restlint\n', 'python ./bin/check-nuitka-with-yamllint\n', 'python ./bin/check-nuitka-with-codespell\n', 'python ./bin/autoformat-nuitka-source --check-only\n', 'set -x\nwhich python\notool -L $(which python)\notool -l $(which python)\n', 'pip install --no-python-version-warning --disable-pip-version-check -r requirements-devel.txt\npip install --no-python-version-warning --disable-pip-version-check .\n', 'python -m nuitka --module --show-scons --run --assume-yes-for-downloads tests/basics/EmptyModuleTest.py\npython -m nuitka --show-scons --run --assume-yes-for-downloads tests/basics/EmptyModuleTest.py\n', 'python -m nuitka --version\nenv | sort\npython ./tests/run-tests --no-other-python --skip-reflection-test --skip-all-cpython-tests\n', 'set -x\nwhich python\notool -L $(which python)\notool -l $(which python)\nls -lR $(dirname $(dirname $(which python)))\n', 'pip install --no-python-version-warning --disable-pip-version-check -r requirements-devel.txt\npip install --no-python-version-warning --disable-pip-version-check .\n', 'python -m nuitka --module --show-scons --run --assume-yes-for-downloads tests/basics/EmptyModuleTest.py\npython -m nuitka --show-scons --run --assume-yes-for-downloads tests/basics/EmptyModuleTest.py\n', 'python -m nuitka --version\nenv | sort\npython ./tests/run-tests --no-other-python --skip-reflection-test --skip-all-cpython-tests\n', 'pip install --no-python-version-warning --disable-pip-version-check -r requirements-devel.txt\npip install --no-python-version-warning --disable-pip-version-check .\n', 'python -m nuitka --module --show-scons --run --assume-yes-for-downloads tests\\basics\\EmptyModuleTest.py\npython -m nuitka --show-scons --run tests\\basics\\EmptyModuleTest.py\n', 'Get-ChildItem env:\npython -m nuitka --version\npython .\\tests\\run-tests --no-other-python --no-debug --skip-reflection-test --skip-all-cpython-tests --assume-yes-for-downloads\n']"
"['echo ""$GITHUB_REF""\n', 'sudo apt-get -qq update\npip install -e "".[dev]""\n', 'make test\n', 'make test\n', 'make test\n', 'flake8 --ignore=E501,E123,E124,E126,E127,E128 dataset\n', 'python setup.py sdist bdist_wheel\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox -e manifest', 'tox -e build', 'python -m pip install -e .', ""python -c 'import environ; print(environ.__version__)'"", 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'python setup.py --fullname\npython setup.py --long-description\npython setup.py --classifiers\n', 'tox', 'tox -e coverage-report', 'python -m pip install coveralls\n# Do not fail job if coveralls.io is down\ncoveralls || true\n', 'git log --format=fuller -5', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox -e lint', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox -e linkcheck', 'tox -e docs']"
"['python -m pip install --upgrade pip\npip install nox\n', 'nox', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel                 \ntwine upload --repository pypi dist/*']"
"['sudo apt-get update -y \\\n&& sudo apt-get install -y \\\n  postfix librrd-dev rrdtool libldap2-dev libsasl2-dev libssl-dev\npython -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r ldap-requirements.txt\npip install -r test-requirements.txt\npython setup.py develop\n', 'pip install -r postgresql-requirements.txt\npip install coverage\necho ""DB=postgres"" >> $GITHUB_ENV\n', 'pip install -r mysql-requirements.txt\necho ""DB=mysql"" >> $GITHUB_ENV\n', 'python ./tests.py\ncd test_project\npython3 manage.py test modoboa\n', 'python ./tests.py\ncd test_project\ncoverage run manage.py test modoboa\ncoverage combine\ncoverage xml\ncoverage report\n', 'sudo apt-get install gettext\ncd frontend\nnvm install 14\nyarn install\nmake translations\nyarn build\ncd ..\n', 'sudo apt-get install librrd-dev rrdtool libssl-dev\npython -m pip install --upgrade pip build\npip install -r requirements.txt\ncd modoboa\ndjango-admin compilemessages\ncd ..\npython -m build\n']"
""
"['python toxver.py ${{ matrix.python }} ${{ matrix.deps }} >> $env:GITHUB_ENV', 'python toxver.py ${{ matrix.python }} ${{ matrix.deps }} >> $GITHUB_ENV', 'pip install tox', 'tox', 'python debug-info.py']"
"['git checkout HEAD^2', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['python -m pip install --upgrade pip\npip install -e .\npip install --upgrade coveralls ruff\npip install ""django~=${{ matrix.django-version }}.0""\n', 'ruff --format=github .', 'coverage run --source=ipware manage.py test', 'coveralls --service=github']"
"['sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=linter', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=repo', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=unit', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=lab', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=docs', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=neutron', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=integration', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'sudo make nox session=minimal', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'sudo make nox session=linux-3.6', 'sudo make nox session=linux-3.10', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'sudo make nox session=linux-3.6', 'sudo make nox session=linux-3.10', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', '/home/github/openbsd_runner.sh', 'sudo chown -R $USER:$USER $GITHUB_WORKSPACE', 'make nox session=linter']"
"['python -m pip install -U setuptools wheel build', 'python -m build .']"
"['sudo apt-get install -y graphviz pandoc\n', ""echo '${{ secrets.PYPIRC_FILE }}' > ~/.pypirc\n""]"
"['pip install -r requirements.txt\n', 'cd ./documentation/src/\n./boxes2rst generators.inc\nmake html #linkcheck\ncp index.html ../build/\ncd ../build/ && ls && ls html/\ncp -r ../../static .\nrm -rf doctrees\ntouch .nojekyll    \n', 'pip install -r requirements.txt\npip install mypy\npip install types-Markdown\n', 'mypy --ignore-missing-imports ./boxes/\nmypy --ignore-missing-imports ./scripts/boxes\nmypy --ignore-missing-imports ./scripts/boxes2inkscape\nmypy --ignore-missing-imports ./scripts/boxes2pot\nmypy --ignore-missing-imports ./scripts/boxesserver\nmypy --ignore-missing-imports ./documentation/src/boxes2rst\nmypy --ignore-missing-imports ./setup.py\n']"
"['pip install tox tox-gh-actions', 'tox', 'pip3 install coveralls\ncoveralls --service=github\n', 'pip3 install coveralls\ncoveralls --finish --service=github\n', 'pip install tox', 'tox -e docs', 'pip install tox', 'tox -e format', 'pip install tox', 'tox -e setuppy']"
""
"['pip install -U flake8', 'flake8 docker/ tests/', 'python3 -m pip install --upgrade pip\npip3 install -r test-requirements.txt -r requirements.txt\n', 'docker logout\nrm -rf ~/.docker\npy.test -v --cov=docker tests/unit\n', 'docker logout\nrm -rf ~/.docker\nmake ${{ matrix.variant }}\n', 'pip3 install wheel\npython setup.py sdist bdist_wheel\n']"
"['python -m pip install -e . --no-deps --force-reinstall', 'set -e\ncp examples/Quickstart.ipynb docs/quickstart.ipynb\npushd docs\nmake clean html linkcheck\npopd\n', 'git fetch --depth=1 origin +refs/tags/*:refs/tags/*', 'python -m pip install --upgrade pip wheel build twine\n', 'python -m build --sdist --wheel . --outdir dist', 'ls dist\n', 'cd dist && python -m pip install folium*.whl\npython -m twine check *\n', 'python -m pip install -e . --no-deps --force-reinstall', 'python -m pytest -vv --ignore=tests/selenium', 'python -m pip install -e . --no-deps --force-reinstall', 'python -m pytest --nbval-lax examples --ignore=examples/WmsTimeDimension.ipynb', 'python -m pip install -e . --no-deps --force-reinstall', 'micromamba remove branca --yes --force\npython -m pip install git+https://github.com/python-visualization/branca.git\npython -m pytest -vv --ignore=tests/selenium\n', 'python -m pip install -e . --no-deps --force-reinstall\n', 'mypy folium\n', 'python -m pip install -e . --no-deps --force-reinstall', 'python -m pytest tests/selenium -vv', 'python -m pip install -e . --no-deps --force-reinstall', 'for fname in examples/*.ipynb; do echo $fname $(flake8-nb $fname --ignore=W391,E226,E402,W504,I100,I201,I202,E703 --max-line-length=120 --show-source --count); done\n']"
"['python3 -m pip install nox\n', 'nox -s lint', 'python3 -m pip install nox\n', 'nox -s docs', 'python -m pip install nox\n', '.ci/run-nox.sh', './.ci/make.sh assemble ${{ env.STACK_VERSION }}']"
['docker run -it splash-tests\n']
"['(cat setup.cfg) | %{$_ -replace ""tag_build.?=.?dev"",""""} | set-content setup.cfg\n', 'python -m pip install --upgrade pip\npip --version\npip install build\npip list\nDISABLE_SQLALCHEMY_CEXT=y python -m build --wheel --outdir ./wheelhouse\n', 'pip install -U twine\ntwine upload --skip-existing ./wheelhouse/*\n', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e github-${{ matrix.build-type }} -- -q --nomemory --notimingintensive ${{ matrix.pytest-args }}', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e ${{ matrix.tox-env }} ${{ matrix.pytest-args }}', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e github-${{ matrix.build-type }} -- -q --nomemory --notimingintensive ${{ matrix.pytest-args }}', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'python -m pip install --upgrade pip\npip install --upgrade tox setuptools\npip list\n', 'tox -e ${{ matrix.tox-env }} ${{ matrix.pytest-args }}']"
"['pip install -U setuptools wheel\npython setup.py sdist bdist_wheel\n', 'docker run --rm -d -p 8050:8050 --network host scrapinghub/splash\n', 'pip install -U tox\nSPLASH_URL=http://127.0.0.1:8050 tox\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n', 'echo Test successful']"
"['pip install -U -r requirements_dev.txt\npip install -U -r requirements_backend.txt\npip install -U -e .[cssselect,pyquery]\n', 'make pylint && make flake8\n', 'pip install -U -r requirements_dev.txt\npip install -U -r requirements_backend.txt\npip install -U -e .[cssselect,pyquery]\n', 'make mypy\n', 'pip install -U -r requirements_dev.txt\npip install -U -r requirements_backend.txt\npip install -U -e .[cssselect,pyquery]\n', 'pytest -n=logical -x --cov grab --cov-report term-missing\ncoverage lcov -o .coverage.lcov\n']"
"['python -m pip install -U pip wheel setuptools', 'python -m pip install ""tox<4.0.0"" ""tox-gh-actions<3.0.0""', 'python -m tox', 'python -m pip install -U pip wheel setuptools', 'python -m pip install ""tox<4.0.0"" ""tox-gh-actions<3.0.0""', 'python -m tox', 'python -m pip install -U pip wheel setuptools', 'python -m pip install ""tox<4.0.0"" ""tox-gh-actions<3.0.0""', 'python -m tox -e py27,py27-without-extensions', 'python -m pip install -U pip wheel setuptools', 'python -m pip install ""tox<4.0.0"" ""tox-gh-actions<3.0.0""', 'python -m tox', 'python setup.py sdist', 'pip install wheel', 'WRAPT_INSTALL_EXTENSIONS=false python setup.py bdist_wheel', 'python -m pip install -U coverage', 'python -m coverage combine', 'python -m coverage report', 'python -m coverage xml']"
""
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox -eflake8', 'tox -edocs', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox']"
"['pip install --upgrade pip setuptools\npip install --upgrade tox-gh-actions coveralls\n', 'tox\n', 'coveralls --service=github\n', 'pip install --upgrade coveralls\ncoveralls --service=github --finish\n']"
"['pip install tox', 'tox', 'pip install tox', 'tox', 'pip install tox', 'tox', '/venv/bin/pip install tox', '/venv/bin/tox', '/opt/python/${{ matrix.PYTHON.VERSION }}/bin/python -m venv .venv', '.venv/bin/pip install -U pip wheel setuptools-rust', '.venv/bin/python setup.py sdist', 'tar zxvf dist/bcrypt*.tar.gz && mkdir tmpwheelhouse', 'if [ -n ""${{ matrix.PYTHON.ABI_VERSION }}"" ]; then\n    PY_LIMITED_API=""--py-limited-api=${{ matrix.PYTHON.ABI_VERSION }}""\nfi\ncd bcrypt* && ../.venv/bin/python setup.py bdist_wheel $PY_LIMITED_API && mv dist/bcrypt*.whl ../tmpwheelhouse\n', 'auditwheel repair tmpwheelhouse/bcrypt*.whl -w wheelhouse/', '.venv/bin/pip install bcrypt --no-index -f wheelhouse/', '.venv/bin/python -c ""import bcrypt; password = b\'super secret password\';hashed = bcrypt.hashpw(password, bcrypt.gensalt());bcrypt.checkpw(password, hashed)""\n', 'mkdir bcrypt-wheelhouse', 'mv wheelhouse/bcrypt*.whl bcrypt-wheelhouse/', 'curl ""${{ matrix.PYTHON.DOWNLOAD_URL }}"" -o python.pkg\nsudo installer -pkg python.pkg -target /\n', '${{ matrix.PYTHON.BIN_PATH }} -m venv venv', 'venv/bin/pip install -U pip wheel setuptools-rust', 'venv/bin/python setup.py sdist', 'tar zxvf dist/bcrypt*.tar.gz && mkdir wheelhouse', 'cd bcrypt* && ../venv/bin/python setup.py bdist_wheel --py-limited-api=${{ matrix.PYTHON.ABI_VERSION }} && mv dist/bcrypt*.whl ../wheelhouse', 'venv/bin/pip install -f wheelhouse --no-index bcrypt', 'venv/bin/python -c ""import bcrypt;password = b\'super secret password\';hashed = bcrypt.hashpw(password, bcrypt.gensalt());bcrypt.checkpw(password, hashed)""\n', 'mkdir bcrypt-wheelhouse', 'mv wheelhouse/bcrypt*.whl bcrypt-wheelhouse/', 'python -m pip install -U pip wheel setuptools-rust', 'python setup.py sdist', 'tar zxvf dist/bcrypt*.tar.gz && mkdir wheelhouse', 'cd bcrypt* && python setup.py bdist_wheel --py-limited-api=${{ matrix.PYTHON.ABI_VERSION }} && mv dist/bcrypt*.whl ../wheelhouse', 'pip install -f wheelhouse --no-index bcrypt', 'python -c ""import bcrypt; password = b\'super secret password\';hashed = bcrypt.hashpw(password, bcrypt.gensalt());bcrypt.checkpw(password, hashed)""\n', 'mkdir bcrypt-wheelhouse', 'move wheelhouse\\bcrypt*.whl bcrypt-wheelhouse\\']"
"['curl -sSL \\\n  ""https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py"" | python\n\n# Adding `poetry` to `$PATH`:\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry config virtualenvs.in-project true\npoetry run pip install --upgrade pip setuptools\npoetry install\n\npoetry run pip install --upgrade ""${{ matrix.django-version }}""\n', 'poetry run flake8 .\npoetry run mypy split_settings\npoetry run pytest\npoetry run doc8 -q docs\npoetry check\npoetry run pip check\n']"
"['pip install wheel\npython setup.py bdist_wheel\npython setup.py sdist\n', 'python -m venv env\nenv/bin/pip install -e .[tests]\n', 'env/bin/pytest tests']"
"['sudo apt-get update -qq', 'sudo apt-get install -qq -y --no-install-recommends gir1.2-gtk-3.0 python3-gi python3-gi-cairo python3-numpy python3-setuptools graphviz xvfb twine', 'python3 setup.py sdist', '.github/scripts/test.sh', 'sudo apt-get update -qq', 'sudo apt-get install -qq -y --no-install-recommends gir1.2-gtk-3.0 python3-gi python3-gi-cairo python3-numpy python3-setuptools graphviz xvfb twine', 'python3 setup.py sdist', '.github/scripts/test.sh', 'twine upload dist/*']"
"['python -m pip install --upgrade pip\npip install setuptools wheel\n', 'python setup.py sdist bdist_wheel', 'sudo apt-get install libxml2-dev libxslt1-dev\npython -m pip install --upgrade pip setuptools py wheel requirements-builder\nrequirements-builder -e all --level=${{ matrix.requirements-level }} setup.py > .${{ matrix.requirements-level }}-${{ matrix.python-version }}-requirements.txt\n', 'pip install -r .${{ matrix.requirements-level }}-${{ matrix.python-version }}-requirements.txt\npip install .[all]\npip freeze\n', './run-tests.sh\n']"
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox']"
"[""python -m pip install --upgrade pip\npython -m pip install --upgrade 'tox>=4.0'\n"", 'tox -e docs', ""python -m pip install --upgrade pip\npython -m pip install --upgrade 'tox>=4.0'\n"", 'tox -e prospector', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-py\n', 'tox --py 36', ""python -m pip install --upgrade pip\npython -m pip install --upgrade 'tox>=4.0'\n"", 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)']"
"['git fetch --force --tags --depth=1\n', 'echo ""EXTRA_CHECKLIST_ARGS=--no-verify-tags --no-verify-docs-next-version"" >> $GITHUB_ENV\n', 'admin/release_checklist $EXTRA_CHECKLIST_ARGS 6.0+master\n', ""sudo apt update\nsudo apt-get install -y \\\n  make \\\n  ninja-build \\\n  $CC \\\n  $(echo $CC | sed -e 's/gcc/g\\+\\+/')\nsudo apt-get clean\n"", 'sudo apt-get install -y libxml2-utils\nsudo apt-get clean\n', 'python -m pip install nox\n', 'nox --non-interactive --session lint\n', 'nox --non-interactive --session ""tests_compiler($CC)""\n', 'nox --non-interactive --session doc\n', 'nox --non-interactive --session bundle_app\n', 'nox --non-interactive --session build_wheel\n', 'nox --non-interactive --session check_wheel\n', 'python -m nox --session upload_wheel\n', 'if [ -z ""$PR_MILESTONE"" ]; then\n  echo \'No milestone selected for PR\'\n  exit 1\nfi\nexit 0\n', 'if [ -z ""${{ github.event.pull_request.number }}"" ]; then\n  echo \'No PR defined\'\nelse\n  if grep -qE \'^\\[no changelog\\]\' <<<""$PR_BODY""; then\n    echo \'Marker ""[no changelog]"" found in PR body\'\n    if [ ""$(grep -F ""$CHANGELOG_ISSUE"" CHANGELOG.rst)"" != """" ]; then\n      echo ""ERROR: $CHANGELOG_ISSUE found in CHANGELOG.rst.""\n      exit 1\n    else\n      echo ""OK: $CHANGELOG_ISSUE not found in CHANGELOG.rst""\n    fi\n  else\n    echo \'Marker ""[no changelog]"" not found in PR body\'\n    if [ ""$(grep -F ""$CHANGELOG_ISSUE"" CHANGELOG.rst)"" == """" ]; then\n      echo ""ERROR: $CHANGELOG_ISSUE not found in CHANGELOG.rst.""\n      exit 1\n    else\n      echo ""OK: $CHANGELOG_ISSUE found in CHANGELOG.rst""\n    fi\n  fi\nfi\nexit 0\n', '# Enable coverage for specific target configurations\ncase ""${{ matrix.os }}/${{ matrix.gcc }}/${{ matrix.python-version }}"" in\n  windows-2019/gcc-8/3.7)   USE_COVERAGE=true ;;\n  ubuntu-22.04/gcc-11/3.11) USE_COVERAGE=true ;;\n  *)                        USE_COVERAGE=false ;;\nesac\necho ""USE_COVERAGE=$USE_COVERAGE"" >> $GITHUB_ENV\n', 'choco install ninja\n', ""sudo apt update\nsudo apt-get install -y \\\n  make \\\n  ninja-build \\\n  ${{ matrix.gcc }} \\\n  $(echo ${{ matrix.gcc }} | sed -e 's/gcc/g\\+\\+/')\nsudo apt-get clean\n"", 'sudo apt-get install -y \\\n  libxml2-utils\nsudo apt-get clean\n', 'python3 -m pip install nox\n', 'nox --non-interactive --session lint\n', 'nox --non-interactive --session ""tests_compiler(${{ matrix.gcc }})"" -- --archive_differences\n', 'nox --non-interactive --session doc\n', 'nox --non-interactive --session bundle_app\n', 'nox --non-interactive --session build_wheel\n', 'nox --non-interactive --session check_wheel\n', '# Enable coverage for specific target configurations\ncase ""${{ matrix.gcc }}"" in\n  gcc-5)    USE_COVERAGE=true ;;\n  gcc-13)    USE_COVERAGE=true ;;\n  clang-10) USE_COVERAGE=true ;;\n  clang-14) USE_COVERAGE=true ;;\n  *)        USE_COVERAGE=false ;;\nesac\necho ""USE_COVERAGE=$USE_COVERAGE"" >> $GITHUB_ENV\n', 'python3 -m pip install nox\n', 'python3 -m nox --non-interactive --session ""docker_build_compiler(${{ matrix.gcc }})""\n', 'python3 -m nox --non-interactive --session ""docker_run_compiler(${{ matrix.gcc }})"" -- --session lint\n', 'python3 -m nox --non-interactive --session ""docker_run_compiler(${{ matrix.gcc }})"" -- --session tests\n', 'python3 -m nox --non-interactive --session ""docker_run_compiler(${{ matrix.gcc }})"" -- --session doc\n', 'python3 -m nox --non-interactive --session ""docker_run_compiler(${{ matrix.gcc }})"" -- --session bundle_app\n', 'python3 -m nox --non-interactive --session ""docker_run_compiler(${{ matrix.gcc }})"" -- --session build_wheel\n', 'python3 -m nox --non-interactive --session ""docker_run_compiler(${{ matrix.gcc }})"" -- --session check_wheel\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions coveralls\npip install -r requirements-test.txt\n', 'tox\n', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --service=github --finish\n']"
"['mkdir -p ~/.cache/downloads\nmkdir -p ~/.cache/pip\nmkdir -p ~/.cache/wheelhouse\nmkdir -p ~/.cache/testmon\nrm -rf node_modules  ## TODO remove this later\n', 'sudo apt update\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n', 'invoke test_travis_addons -n 1', 'sudo apt update\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n', 'invoke test_travis_website -n 1', 'sudo apt update\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n', 'invoke assets --dev\n', 'invoke test_travis_api1_and_js -n 1', 'sudo apt update\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n', 'invoke test_travis_api2 -n 1', 'sudo apt update\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n', 'invoke test_travis_api3_and_osf -n 1']"
"['python --version\npip --version\n', 'make pip-install-build', 'make test', 'python setup.py bdist_wheel\n']"
""
"['python -c ""from dpath.version import VERSION; print(f\'::set-output name=version::v{VERSION}\');""\n', 'pip install flake8\nflake8 setup.py dpath/ tests/\n', 'python -c ""import os\nfrom random import randint\nhashseed = randint(0, 4294967295)\nprint(f\'{hashseed=}\')\nopen(os.environ[\'GITHUB_OUTPUT\'], \'a\').write(f\'hashseed={hashseed}\')""\n']"
""
"['python -m pip install --upgrade build twine\npython -m build\ntwine check --strict dist/*\n', 'export PKG=$(ls dist/ | grep tar)\nset -- $PKG\necho ""name=$1"" >> $GITHUB_ENV\n', 'python -V\npython -c ""import geopandas; geopandas.show_versions();""\nmicromamba info\n# save conda list to file and print out\n# so that we can do the HAS_PYGEOS check without calling conda again\nmicromamba list 2>&1 | tee conda.txt\nif ( cat conda.txt | grep -q pygeos  )\nthen\n  echo ""Setting HAS_PYGEOS=1""\n  echo ""HAS_PYGEOS=1"" >> $GITHUB_ENV\nelse\n  echo ""Setting HAS_PYGEOS=0""\n  echo ""HAS_PYGEOS=0"" >> $GITHUB_ENV\nfi\n', 'pytest -v -r a -n auto --color=yes --cov=geopandas --cov-append --cov-report term-missing --cov-report xml geopandas/\n', 'pytest -v -r a -n auto --color=yes --cov=geopandas --cov-append --cov-report term-missing --cov-report xml geopandas/\n', 'conda install postgis -c conda-forge\nsh ci/scripts/setup_postgres.sh\npytest -v -r a --color=yes --cov=geopandas --cov-append --cov-report term-missing --cov-report xml geopandas/io/tests/test_sql.py | tee /dev/stderr | if grep SKIPPED >/dev/null;then echo ""TESTS SKIPPED, FAILING"" && exit 1;fi\n', 'pytest -v --color=yes --doctest-only geopandas --ignore=geopandas/datasets\n', 'pip install versioneer\nversioneer install\n', 'git reset -- geopandas/__init__.py\ngit checkout -- geopandas/__init__.py\n']"
"['sudo apt-get update\nsudo apt-get install python3-pyqt5\nmake modules\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt -r requirements-extra.txt\n', 'python build.py --modules\n', 'pytest core hscommon\n']"
['python -m unittest discover']
"['python -m pip install --upgrade pip\npip install build\n', 'python -m build --sdist', 'python -m build --wheel', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
['python test/all_tests.py\n']
"['python -m pip install --upgrade pip\npip install .\n', 'python demo.py', 'pip install .[tests]\n', 'pytest']"
"['pip install -r requirements.txt', 'python test/test_parsers.py', ""echo IMAGE_REPO=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]') >> $GITHUB_ENV\necho IMAGE_TAG=$(./.github/workflows/docker-tag.sh ${{github.ref}}) >> $GITHUB_ENV\n"", 'echo IMAGE=docker.pkg.github.com/$IMAGE_REPO/git-auto-deploy:$IMAGE_TAG >> $GITHUB_ENV\n', 'docker login docker.pkg.github.com -u ${{ github.actor }} -p ${{ secrets.GITHUB_TOKEN }}\n', 'docker build \\\n    --rm \\\n    -f docker/image-gitautodeploy/Dockerfile \\\n    -t $IMAGE \\\n    .\n', 'docker push $IMAGE\n']"
"['git fetch origin +refs/tags/*:refs/tags/*\n', 'python -m pip install pep517', 'python -m pep517.build --source --out-dir dist/ .', 'pip install coverage pytest\ncoverage run -m pytest tests\n']"
""
"['python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox', 'python -m pip install coverage\n', 'coverage combine\ncoverage report --include=""django_countries/tests/*"" --fail-under=100 -m\ncoverage report --omit=""django_countries/tests/*"" --fail-under=90 -m\n']"
"['python3 -m pip install --quiet --editable .[dev]', 'flake8\nreuse lint --quiet\npytest\ncheck-manifest --verbose\nsphinx-build -q -E docs _build\n']"
"['pip install -U tox\n', 'tox', 'python -m pip install poetry githubrelease httpx==0.18.2 autopub twine wheel\necho ""release=$(autopub check)"" >> $GITHUB_OUTPUT\n', 'git remote set-url origin https://$GITHUB_TOKEN@github.com/${{ github.repository }}\nautopub deploy\n']"
"['python -m pip install --upgrade pip\npip install -r test_requirements.txt\n', 'pip install .', 'cd tests\npython -E -Wall -bb run_testsuite.py\n']"
"['python -m pip install --upgrade pip\npip install -e .\npip install -U flake8 coveralls argparse mypy django-stubs types-setuptools\npip install -U Django~=${{ matrix.django_version }}\n', 'flake8 --ignore=E501,W504 safedelete\n', 'mypy --config-file mypy.ini safedelete\n', 'python -W error::DeprecationWarning -W error::PendingDeprecationWarning \\\n  -m coverage run `which django-admin` test --settings=safedelete.tests.settings\n', 'coveralls --service=github\n']"
"['python -m pip install --upgrade pip\npip --version\npip install -U setuptools\npip install pytest\npip install ""sseclient<0.0.23,>=0.0.18""\npip install mwparserfromhell\npip install ""PyMySQL >= 0.9.3""\npip install coverage\npython -c ""import setuptools; print(\'setuptools:\', setuptools.__version__)""\n', 'python -Werror::UserWarning -m pwb generate_user_files -site:wikipedia:test -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\n', 'python pwb.py version\npytest --version\ncoverage run -m pytest pywikibot --doctest-modules --ignore-glob=""*gui.py"" --ignore-glob=""*memento.py""\n', 'coverage report\n', 'python -m pip install --upgrade pip\npip --version\npip install -U setuptools\npip install requests\npip install mwparserfromhell\npip install coverage\npython -c ""import setuptools; print(\'setuptools:\', setuptools.__version__)""\n', ""if  [ ${{matrix.family || 0}} == wpbeta ]; then\n  python pwb.py generate_family_file http://${{matrix.code}}.wikipedia.beta.wmflabs.org/ wpbeta y\nfi\nif [ ${{matrix.site || 0}} == 'wsbeta:en' ]; then\n  python pwb.py generate_family_file http://en.wikisource.beta.wmflabs.org/ wsbeta y\nfi\n"", 'if [ ${{matrix.site}} != false ]; then\n  python -Werror::UserWarning -m pwb generate_user_files -site:${{matrix.site}} -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\nelse\n  python -Werror::UserWarning -m pwb generate_user_files -family:${{matrix.family}} -lang:${{matrix.code}} -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\nfi\necho ""usernames[\'wikipedia\'][\'en\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikisource\'][\'zh\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikipedia\'][\'test\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikidata\'][\'test\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'commons\'][\'commons\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'meta\'][\'meta\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""max_retries = 3"" >> user-config.py\necho ""noisysleep = float(\'inf\')"" >> user-config.py\necho ""maximum_GET_length = 5000"" >> user-config.py\necho ""console_encoding = \'utf8\'"" >> user-config.py\necho ""import os"" >> user-config.py\necho ""password_file = os.path.expanduser(\'passwordfile\')"" >> user-config.py\necho ""(\'${{ env.PYWIKIBOT_USERNAME }}\', \'${{ secrets.PYWIKIBOT_USERPWD }}\')"" > passwordfile\n', 'python pwb.py version\ncoverage run -m unittest -vv tests/site_login_logout_tests.py\n', 'coverage report\n', 'python -m pip install --upgrade pip\npip --version\npip install -U setuptools\npip install requests\npip install mwparserfromhell\npip install mwoauth\npip install coverage\npython -c ""import setuptools; print(\'setuptools:\', setuptools.__version__)""\n', 'python pwb.py generate_family_file http://${{matrix.code}}.wikipedia.beta.wmflabs.org/ wpbeta y\n', 'python -Werror::UserWarning -m pwb generate_user_files -family:${{matrix.family}} -lang:${{matrix.code}} -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\necho ""authenticate[\'${{ matrix.domain }}\'] = (\'${{ steps.split.outputs._0 }}\', \'${{ steps.split.outputs._1 }}\', \'${{ steps.split.outputs._2 }}\', \'${{ steps.split.outputs._3 }}\')"" >> user-config.py\necho ""max_retries = 3"" >> user-config.py\necho ""noisysleep = float(\'inf\')"" >> user-config.py\necho ""maximum_GET_length = 5000"" >> user-config.py\necho ""console_encoding = \'utf8\'"" >> user-config.py\n', 'python pwb.py version\ncoverage run -m unittest -vv tests/oauth_tests.py\n', 'coverage report\n', 'python -m pip install --upgrade pip\npip --version\npip install -U setuptools\nif [ -f dev-requirements.txt ]; then pip install -r dev-requirements.txt; fi\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npip install wikitextparser\npython -c ""import setuptools; print(\'setuptools:\', setuptools.__version__)""\n', ""if  [ ${{matrix.family || 0}} == wpbeta ]; then\n  python pwb.py generate_family_file http://${{matrix.code}}.wikipedia.beta.wmflabs.org/ wpbeta y\nfi\nif [ ${{matrix.site || 0}} == 'wsbeta:en' ]; then\n  python pwb.py generate_family_file http://en.wikisource.beta.wmflabs.org/ wsbeta y\nfi\n"", 'if [ ${{matrix.site}} != false ]; then\n  python -Werror::UserWarning -m pwb generate_user_files -site:${{matrix.site}} -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\nelse\n  python -Werror::UserWarning -m pwb generate_user_files -family:${{matrix.family}} -lang:${{matrix.code}} -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\nfi\necho ""usernames[\'wikipedia\'][\'en\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikisource\'][\'zh\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikipedia\'][\'test\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikidata\'][\'test\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'commons\'][\'commons\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'meta\'][\'meta\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""max_retries = 3"" >> user-config.py\necho ""noisysleep = float(\'inf\')"" >> user-config.py\necho ""maximum_GET_length = 5000"" >> user-config.py\necho ""console_encoding = \'utf8\'"" >> user-config.py\necho ""import os"" >> user-config.py\necho ""password_file = os.path.expanduser(\'passwordfile\')"" >> user-config.py\necho ""(\'${{ env.PYWIKIBOT_USERNAME }}\', \'${{ secrets.PYWIKIBOT_USERPWD }}\')"" > passwordfile\n', 'python pwb.py version\nif [ ${{matrix.site || 0}} != \'wikipedia:test\' ]; then\n  coverage run -m unittest discover -vv -p \\""*_tests.py\\"";\nelse\n  pytest --cov=.;\nfi\n', 'coverage report\n', 'exit 1', 'python -m pip install --upgrade pip\npip --version\npip install -U setuptools\nif [ -f dev-requirements.txt ]; then pip install -r dev-requirements.txt; fi\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -c ""import setuptools; print(\'setuptools:\', setuptools.__version__)""\n', 'python -Werror::UserWarning -m pwb generate_user_files -site:${{matrix.site}} -user:${{ env.PYWIKIBOT_USERNAME }} -v -debug;\necho ""usernames[\'wikipedia\'][\'en\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikisource\'][\'zh\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikipedia\'][\'test\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'wikidata\'][\'test\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'commons\'][\'commons\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""usernames[\'meta\'][\'meta\'] = \'${{ env.PYWIKIBOT_USERNAME }}\'"" >> user-config.py\necho ""max_retries = 3"" >> user-config.py\necho ""noisysleep = float(\'inf\')"" >> user-config.py\necho ""maximum_GET_length = 5000"" >> user-config.py\necho ""console_encoding = \'utf8\'"" >> user-config.py\necho ""import os"" >> user-config.py\necho ""password_file = os.path.expanduser(\'passwordfile\')"" >> user-config.py\necho ""(\'${{ env.PYWIKIBOT_USERNAME }}\', \'${{ secrets.PYWIKIBOT_USERPWD }}\')"" > passwordfile\n', 'python pwb.py version\npytest -a write --cov=.;\n', 'coverage report\n', 'exit 1']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\npip install nox virtualenv\n', 'nox -s docs', 'python -m pip install --upgrade pip\npip install nox virtualenv\n', 'nox -s lint', 'python -m pip install --upgrade pip\npip install nox virtualenv\n', 'nox -p ${{ matrix.python }} -s tests', 'nox -p ${{ matrix.python }} -s safety_tests']"
"['python -m pip install --upgrade pip twine\npip install poetry\npoetry install\n', 'poetry run py.test -v --cov=tinydb\n', 'poetry run pip install pytest-mypy\npoetry run pytest --mypy -m mypy tinydb tests\n', 'poetry build\ntwine check dist/*\n', 'poetry run coveralls\n', 'python -m pip install --upgrade pip\npip install poetry\npoetry install\n', 'poetry publish --build\n']"
"['pip install tox', 'tox $TOX_OPTIONS -e lint,package_readme', 'pip install tox coverage', 'tox $TOX_OPTIONS -e py-cov', 'tox $TOX_OPTIONS -e py-cov-noextra', 'coverage combine\ncoverage xml\n', 'pip install tox', 'tox $TOX_OPTIONS -e py-cy', 'pip install tox', 'tox $TOX_OPTIONS -e pypy3', 'pip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\n', 'pip install git+https://github.com/anthrotype/cibuildwheel.git@test_command_wheel#egg=cibuildwheel', 'python -m cibuildwheel --output-dir wheelhouse .', 'pip install git+https://github.com/anthrotype/cibuildwheel.git@test_command_wheel#egg=cibuildwheel', 'python -m cibuildwheel --output-dir wheelhouse .']"
"['python -m pip install --upgrade pip\npip install `python setup.py --list-test`\n', 'ruff .\n', 'brew install --cask openscad\n', 'pip install .\npython tests/test_minimal.py\n', 'pip install .[easy,test]', 'pytest tests/', 'python -m pip install --upgrade pip\npip install build twine\n', 'pyproject-build --outdir dist .\ntwine upload dist/*\n', 'echo ${{ secrets.DH_PASS }} | docker login --username mikedh --password-stdin', 'make tests # build docker images and run unit tests\nmake publish-docker # push images to docker hub\nmake docs # build sphinx docs\n', 'docker logout\n', 'pip install .[easy,test]', 'python tests/corpus.py', 'export VER=$(python trimesh/version.py)\necho ""::set-output name=tag_name::${VER}""\n', 'pip install `python setup.py --list-test`\n', 'ruff .\n', 'pip install .\npython tests/test_minimal.py\n', 'pip install .[easy,test]', 'pytest', 'make tests', 'pip install .[easy,test]', 'python tests/corpus.py']"
"['uname -a\ndf -h\nulimit -a\n# ccache -s\nwhich python\npython --version\n# sudo apt-get install libatlas-base-dev\npip install --upgrade pip build\n# Set numpy version first, other packages link against it\nif [ ""${MINIMUM_REQUIREMENTS}"" == ""1"" ]; then\n    pip install ${CYTHON_MIN}\n    pip install ${NUMPY_MIN}\n    if [ ""${USE_SCIPY}"" == ""1"" ]; then pip install ${SCIPY_MIN}; fi\nelse\n    pip install cython\n    pip install numpy\n    if [ ""${USE_SCIPY}"" == ""1"" ]; then pip install scipy; fi\nfi\npip install matplotlib pytest\n\nset -o pipefail\nif [ ""${USE_WHEEL}"" == ""1"" ]; then\n    # Need verbose output or TravisCI will terminate after 10 minutes\n    pip wheel . -v\n    pip install pywavelets*.whl\nelif [ ""${USE_SDIST}"" == ""1"" ]; then\n    python -m build --sdist\n    pip install dist/pyw*.tar.gz -v\nelif [ ""${REFGUIDE_CHECK}"" == ""1"" ]; then\n    pip install sphinx numpydoc\n    pip install . -v\nelse\n    pip install . -v\nfi\n', 'set -o pipefail\n# Move out of source directory to avoid finding local pywt\npushd demo\nif [ ""${USE_WHEEL}"" == ""1"" ]; then\n    pytest --pyargs pywt\n    python ../pywt/tests/test_doc.py\nelif [ ""${USE_SDIST}"" == ""1"" ]; then\n    pytest --pyargs pywt\n    python ../pywt/tests/test_doc.py\nelif [ ""${REFGUIDE_CHECK}"" == ""1"" ]; then\n    python util/refguide_check.py --doctests\nelse\n    pytest --pyargs pywt\nfi\npopd\n', 'uname -a\ndf -h\nulimit -a\nwhich python\npython --version\npip install --upgrade pip build\n# Set numpy version first, other packages link against it\nif [ ""${MINIMUM_REQUIREMENTS}"" == ""1"" ]; then\n    pip install ${CYTHON_MIN} ${NUMPY_MIN}\n    if [ ""${USE_SCIPY}"" == ""1"" ]; then pip install ${SCIPY_MIN}; fi\nelse\n    pip install cython numpy\n    if [ ""${USE_SCIPY}"" == ""1"" ]; then pip install scipy; fi\nfi\npip install matplotlib pytest\n\nset -o pipefail\nif [ ""${USE_WHEEL}"" == ""1"" ]; then\n    pip wheel . -v\n    pip install pywavelets*.whl -v\nelif [ ""${USE_SDIST}"" == ""1"" ]; then\n    python -m build --sdist\n    pip install pywavelets* -v\nelif [ ""${REFGUIDE_CHECK}"" == ""1"" ]; then\n    pip install sphinx numpydoc\n    pip install . -v\nelse\n    pip install . -v\nfi\n', '# Move out of source directory to avoid finding local pywt\npushd demo\nif [ ""${USE_WHEEL}"" == ""1"" ]; then\n    pytest --pyargs pywt\n    python ../pywt/tests/test_doc.py\nelif [ ""${USE_SDIST}"" == ""1"" ]; then\n    pytest --pyargs pywt\n    python ../pywt/tests/test_doc.py\nelif [ ""${REFGUIDE_CHECK}"" == ""1"" ]; then\n    python util/refguide_check.py --doctests\nelse\n    pytest --pyargs pywt\nfi\npopd\n', 'python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n', 'python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n', 'python -m pip install cibuildwheel\n', '# We need to set both MACOS_DEPLOYMENT_TARGET and MACOSX_DEPLOYMENT_TARGET\n# until there is a new release with this commit:\n# https://github.com/mesonbuild/meson-python/pull/309 (should be in 0.13.0)\nif [[ ""$CIBW_ARCHS_MACOS"" == arm64 ]]; then\n    export MACOSX_DEPLOYMENT_TARGET=11.0\n    export MACOS_DEPLOYMENT_TARGET=11.0\nelse\n    export MACOSX_DEPLOYMENT_TARGET=10.13\n    export MACOS_DEPLOYMENT_TARGET=10.13\nfi\necho MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET}\n\npython -m cibuildwheel --output-dir dist\n', 'python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n', 'python -m pip install --upgrade pip\npip install twine\npip install ""cython<3"" numpy\n', 'PYWT_VERSION=$(git describe --tags)\npython -m build --sdist\nls -la ${{ github.workspace }}/dist\n# We prefer to release wheels before source because otherwise there is a\n# small window during which users who pip install pywt will require compilation.\ntwine upload ${{ github.workspace }}/dist/*.whl\ntwine upload ${{ github.workspace }}/dist/pywavelets-${PYWT_VERSION:1}.tar.gz\n']"
"['pip install -U wheel\npip install -U setuptools\npython -m pip install -U pip\n', 'pip install tox', 'tox run -e ${{ matrix.tox }}']"
"['python -m pip install --upgrade pip tox\npip install -r requirements.txt\n', 'tox\n']"
"['python -m pip install --upgrade pip\npython -m pip install numpy flake8 pycodestyle pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 *.py --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 *.py --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pycodestyle *.py\n', 'pytest minisom.py\n']"
"['pip install build -e .', 'make import-cldr', 'python -m build', ""python -m pip install --upgrade pip setuptools wheel\npython -m pip install 'tox~=4.0' 'tox-gh-actions~=3.0'\n"", 'tox --skip-missing-interpreters']"
"['python -m pip install --upgrade pip\npip install flake8 nose chardet\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'nosetests -v\n']"
"[""python -m pip install -U pip \npython -m pip install -e '.[dev]'\n"", 'python -m build', 'twine check --strict dist/*', 'ls -l dist', ""python -m pip install -U pip \npython -m pip install -e '.[dev]'\n"", './check.sh', ""python -m pip install -U pip \npython -m pip install -e '.[test]'\n"", 'sphinx-build -d docs docs/source docs_out --color -W -bhtml\n', ""python -m pip install -U pip wheel \npython -m pip install -e '.[dev,test]'\n"", 'xvfb-run python -m pytest', 'python -m pytest']"
"['pip install black==22.6.0 flake8==5.0.4', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'pytest\n']"
"['curl -XPOST -u ""${{ secrets.VALIDATION_USER }}:${{ secrets.VALIDATION_PASS }}"" \\\n-H ""Accept: application/vnd.github.v3+json"" \\\n-H ""Content-Type: application/json"" https://api.github.com/repos/poliastro/validation/actions/workflows/ci_actions.yml/dispatches \\\n--data \'{""ref"": ""main""}\' \\\n--fail\n']"
"['import re\nfrom packaging.version import parse\n\nversion = ""${{ github.ref }}"".replace(""refs/tags/v"", """")\nimage = ""ghcr.io/${{ github.repository }}""\n\ntags = set()\n\n# full version\ntags.add(f""{image}:{version}"")\n\nif not parse(version).is_prerelease:\n    # only final and post-releases should get the tags\n    # used for automatic use of latest *stable* version\n\n    # major_version\n    major_version = re.search(r\'(\\d+?)\\.\', version).group(1)\n    tags.add(f""{image}:{major_version}"")\n\n    # major_version.minor_version\n    major_and_minor_version = re.search(r\'(\\d+?\\.\\d+?)\\.\', version).group(1)\n    tags.add(f""{image}:{major_and_minor_version}"")\n\n    tags.add(f""{image}:latest"")\n\ntags = "","".join(sorted(list(tags)))\n\nprint(f""::set-output name=tags::{tags}"")\n', 'pip install wheel\n', 'python setup.py build sdist bdist_wheel\n', 'pip install --upgrade wheel setuptools\npython setup.py develop\npip install -r requirements-test.txt\nmypy --install-types --non-interactive puppetboard/ test/\n', 'pytest --cov=. --cov-report=xml --strict-markers --mypy -v puppetboard test\npylint --errors-only puppetboard test\n', 'pip install bandit\n', 'bandit -r puppetboard\n', 'echo Test suite completed']"
"['sed -i \'s/^__version__ = ""[^""]*""/__version__ = ""${{ github.event.inputs.version }}""/g\' duolingo.py', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -U pytest\npip install pytest-cov\npip install coveralls\n', 'pytest tests.py']"
"['npm ci --also=dev\npython -m pip install --upgrade pip\npython -m pip install build --user\n', 'npm run build\nnpm run minify\n', 'mkdir -p adminsortable2/templates/adminsortable2/edit_inline\nDJANGO_VERSIONS=(""4.0"" ""4.1"" ""4.2"")\nfor django_version in ${DJANGO_VERSIONS[@]}; do\n  echo $django_version\n  curl --silent --output adminsortable2/templates/adminsortable2/edit_inline/stacked-django-$django_version.html https://raw.githubusercontent.com/django/django/stable/$django_version.x/django/contrib/admin/templates/admin/edit_inline/stacked.html\n  curl --silent --output adminsortable2/templates/adminsortable2/edit_inline/tabular-django-$django_version.html https://raw.githubusercontent.com/django/django/stable/$django_version.x/django/contrib/admin/templates/admin/edit_inline/tabular.html\n  patch -p0 adminsortable2/templates/adminsortable2/edit_inline/stacked-django-$django_version.html patches/stacked-django-4.0.patch\n  patch -p0 adminsortable2/templates/adminsortable2/edit_inline/tabular-django-$django_version.html patches/tabular-django-4.0.patch\ndone\n', 'python -m build --sdist --wheel --outdir dist/', 'npm ci --also=dev\npython -m pip install --upgrade pip\npython -m pip install ""Django==${{ matrix.django-version }}""\npython -m pip install -r testapp/requirements.txt\npython -m playwright install\npython -m playwright install-deps\n', 'npm run build\nnpm run minify\n', 'mkdir -p adminsortable2/templates/adminsortable2/edit_inline\nDJANGO_VERSIONS=(""4.0"" ""4.1"" ""4.2"")\nfor django_version in ${DJANGO_VERSIONS[@]}; do\n  echo $django_version\n  curl --silent --output adminsortable2/templates/adminsortable2/edit_inline/stacked-django-$django_version.html https://raw.githubusercontent.com/django/django/stable/$django_version.x/django/contrib/admin/templates/admin/edit_inline/stacked.html\n  curl --silent --output adminsortable2/templates/adminsortable2/edit_inline/tabular-django-$django_version.html https://raw.githubusercontent.com/django/django/stable/$django_version.x/django/contrib/admin/templates/admin/edit_inline/tabular.html\n  patch -p0 adminsortable2/templates/adminsortable2/edit_inline/stacked-django-$django_version.html patches/stacked-django-4.0.patch\n  patch -p0 adminsortable2/templates/adminsortable2/edit_inline/tabular-django-$django_version.html patches/tabular-django-4.0.patch\ndone\n', 'python -m pytest testapp\n']"
"['curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\nbash miniconda.sh -b -p $HOME/miniconda\nexport PATH=""$HOME/miniconda/bin:$PATH""\nhash -r\nconda config --set always_yes yes --set changeps1 no\n', 'export PATH=$HOME/miniconda/bin:$PATH\nconda create -n foo -q --yes -c conda-forge -c bioconda python=3.7 twine\n', 'export PATH=$HOME/miniconda/bin:$PATH\nsource activate foo\nrm -f dist/*\npython setup.py sdist\n', 'export PATH=$HOME/miniconda/bin:$PATH\nsource activate foo\ntwine upload dist/*\n', 'find /home/runner/work/deepTools/deepTools -name ""_version.py""', 'echo ""deeptools_version=$(grep ""__version__"" /home/runner/work/deepTools/deepTools/deeptools/_version.py | awk \'{print substr($NF, 2, length($NF) - 2)}\')"" >> $GITHUB_ENV', 'echo ""galaxy_deeptools_version=$(grep ""token.*TOOL_VERSION"" /home/runner/work/deepTools/deepTools/galaxy/wrapper/deepTools_macros.xml | awk -F \'>|<\' \'{print $3}\')"" >> $GITHUB_ENV', 'echo deeptools version=${deeptools_version}\necho deeptools version in galaxy=${galaxy_deeptools_version}\n', 'if [ -z $galaxy_deeptools_version ]; then\n  echo ""galaxy_deeptools_version not defined change the github action""\n  exit 1\nfi\nif [ -z $deeptools_version ]; then\n  echo ""deeptools_version not defined change the github action""\n  exit 1\nfi\nif [ ""$galaxy_deeptools_version"" != ""$deeptools_version"" ]; then\n  echo ""versions do not match""\n  exit 1\nfi\n', 'source activate foo\nflake8 . --exclude=.venv,.build,build --ignore=E501,F403,E402,F999,F405,E722,W504,W605\n', 'source activate foo\nnosetests --with-doctest -sv deeptools\n', 'source activate foo\nrm -f dist/*\npython setup.py sdist\n', '\nsource activate foo\nnosetests --with-doctest -sv deeptools\n', 'source activate foo\nconda update -c conda-forge -c bioconda samtools\n./.planemo.sh ${{ matrix.chunk }} ${{ env.GALAXY_BRANCH }}\n']"
"['pip install --upgrade pip', 'pip install tox', 'tox -e ${{ matrix.job }}', 'pip install --upgrade pip', 'pip install tox', 'tox -e py']"
"['{\necho imageid: ""${{ steps.docker_build.outputs.imageid }}""\necho digest: ""${{ steps.docker_build.outputs.digest }}""\necho labels: ""${{ steps.meta.outputs.labels }}""\necho tags: ""${{ steps.meta.outputs.tags }}""\necho version: ""${{ steps.meta.outputs.version }}""\n} >> ""$GITHUB_STEP_SUMMARY""\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'mongorestore dump/explainshell && mongorestore -d explainshell_tests dump/explainshell\nmake tests\n']"
"['pip install -r requirements.txt', 'pytest -n auto tests', 'pip install -r requirements.txt', 'sh tests/test_docker.sh ${{ matrix.script.args }}', 'pip install -r requirements.txt', 'sh tests/test_bare.sh ${{ matrix.script.args }}', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python scripts/create_django_issue.py', 'pip install pre-commit', 'pre-commit autoupdate', 'pre-commit autoupdate', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'git config --global user.name ""github-actions""\ngit config --global user.email ""action@github.com""\n', 'python scripts/update_changelog.py', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'python scripts/update_contributors.py']"
"['python -m pip install --upgrade wheel pip\npip install .[lint]\n', 'flake8 examples --show-source', 'flake8 setup.py docs/conf.py scripts --show-source', 'flake8 tests --show-source', 'flake8 moviepy --show-source', 'python -m pip install --upgrade pip\npip install isort\n', 'isort --check-only moviepy tests examples docs/conf.py scripts', 'conda install python.app', 'pythonw --version\nwhich python\npythonw -m site --user-site\necho $PYTHONPATH\necho $PYTHONHOME\n', 'brew install imagemagick\n# needed for installing matplotlib\nbrew install pkg-config\npython -m pip install --upgrade wheel setuptools coveralls\npython -m pip install "".[test, optional]""\n', 'pythonw -m pytest --doctest-glob ""moviepy/**/**.py"" -v --cov moviepy --cov-report term-missing\n', 'coveralls', 'python3 scripts/get-latest-imagemagick-win.py >im-url.txt\nset /p IMAGEMAGICK_URL= <im-url.txt\nrm im-url.txt\necho %IMAGEMAGICK_URL%\ncurl %IMAGEMAGICK_URL% -o ImageMagick.exe\n', 'set IMAGEMAGICK_INSTALL_DIR=%CD%\\imagemagick\nmkdir %IMAGEMAGICK_INSTALL_DIR%\necho %IMAGEMAGICK_INSTALL_DIR%\n\nImageMagick.exe /SILENT /SP /DIR=%IMAGEMAGICK_INSTALL_DIR%\ndir imagemagick\n\nmove imagemagick\\ffmpeg.exe ffmpeg.exe\nmove imagemagick\\magick.exe convert.exe\n', 'python -m pip install --upgrade wheel setuptools coveralls\npython -m pip install "".[test]""\n', 'python moviepy\\config.py\n', 'python -m pytest --cov moviepy --cov-report term-missing\n', 'coveralls', 'python --version\nwhich python\npython -m site --user-site\n', 'python -m pip install --upgrade wheel setuptools\n', 'python -m pip install "".[test, doc]""\npython -m pip install python-dotenv\n', 'python -m pip install "".[test, optional, doc]""\n', ""cat /etc/ImageMagick-6/policy.xml \\\n| sed 's/none/read,write/g' \\\n| sudo tee /etc/ImageMagick-6/policy.xml\n"", 'python -m pytest --doctest-glob ""moviepy/**/**.py"" --cov moviepy --cov-report term-missing\n', 'pip install -e .\npip install -e .[optional]\npip install -e .[test]\npip install -e .[doc]\n', 'coveralls']"
"['python -m venv .venv-base\n.venv-base/bin/pip install -v -c ./cryptography-base/ci-constraints-requirements.txt ""./cryptography-base[test]"" ./cryptography-base/vectors/\n', 'python -m venv .venv-pr\n.venv-pr/bin/pip install -v -c ./cryptography-pr/ci-constraints-requirements.txt ""./cryptography-pr[test]"" ./cryptography-pr/vectors/\n', '.venv-base/bin/pytest --benchmark-enable --benchmark-only ./cryptography-pr/tests/bench/ --benchmark-json=bench-base.json', '.venv-pr/bin/pytest --benchmark-enable --benchmark-only ./cryptography-pr/tests/bench/ --benchmark-json=bench-pr.json', 'python ./cryptography-pr/.github/compare_benchmarks.py bench-base.json bench-pr.json | tee -a $GITHUB_STEP_SUMMARY', 'SHA=$(git ls-remote https://boringssl.googlesource.com/boringssl refs/heads/master | cut -f1)\nLAST_COMMIT=$(grep boringssl .github/workflows/ci.yml | grep TYPE | grep -oE \'[a-f0-9]{40}\')\nif ! grep -q ""$SHA"" .github/workflows/ci.yml; then\n  echo ""COMMIT_SHA=${SHA}"" >> $GITHUB_OUTPUT\n  echo ""COMMIT_MSG<<EOF"" >> $GITHUB_OUTPUT\n  echo -e ""## BoringSSL\\n[Commit: ${SHA}](https://boringssl.googlesource.com/boringssl/+/${SHA})\\n\\n[Diff](https://boringssl.googlesource.com/boringssl/+/${LAST_COMMIT}..${SHA}) between the last commit hash merged to this repository and the new commit."" >> $GITHUB_OUTPUT\n  echo ""EOF"" >> $GITHUB_OUTPUT\n\nfi\n', 'SHA=$(git ls-remote https://github.com/openssl/openssl refs/heads/master | cut -f1)\nLAST_COMMIT=$(grep openssl .github/workflows/ci.yml | grep TYPE | grep -oE \'[a-f0-9]{40}\')\nif ! grep -q ""$SHA"" .github/workflows/ci.yml; then\n  echo ""COMMIT_SHA=${SHA}"" >> $GITHUB_OUTPUT\n  echo ""COMMIT_MSG<<EOF"" >> $GITHUB_OUTPUT\n  echo -e ""## OpenSSL\\n[Commit: ${SHA}](https://github.com/openssl/openssl/commit/${SHA})\\n\\n[Diff](https://github.com/openssl/openssl/compare/${LAST_COMMIT}...${SHA}) between the last commit hash merged to this repository and the new commit."" >> $GITHUB_OUTPUT\n  echo ""EOF"" >> $GITHUB_OUTPUT\nfi\n', 'set -xe\nCURRENT_DATE=$(date ""+%b %d, %Y"")\nsed -E -i ""s/Latest commit on the BoringSSL master branch.*/Latest commit on the BoringSSL master branch, as of ${CURRENT_DATE}./"" .github/workflows/ci.yml\nsed -E -i ""s/TYPE: \\""boringssl\\"", VERSION: \\""[0-9a-f]{40}\\""/TYPE: \\""boringssl\\"", VERSION: \\""${{ steps.check-sha-boring.outputs.COMMIT_SHA }}\\""/"" .github/workflows/ci.yml\ngit status\n', 'set -xe\nCURRENT_DATE=$(date ""+%b %d, %Y"")\nsed -E -i ""s/Latest commit on the OpenSSL master branch.*/Latest commit on the OpenSSL master branch, as of ${CURRENT_DATE}./"" .github/workflows/ci.yml\nsed -E -i ""s/TYPE: \\""openssl\\"", VERSION: \\""[0-9a-f]{40}\\""/TYPE: \\""openssl\\"", VERSION: \\""${{ steps.check-sha-openssl.outputs.COMMIT_SHA }}\\""/"" .github/workflows/ci.yml\ngit status\n', 'rustup component add llvm-tools-preview', 'DEFAULT_CONFIG_FLAGS=""shared no-ssl2 no-ssl3""\nCONFIG_FLAGS=""$DEFAULT_CONFIG_FLAGS $CONFIG_FLAGS""\nOPENSSL_HASH=$(echo ""${{ matrix.PYTHON.OPENSSL.TYPE }}-${{ matrix.PYTHON.OPENSSL.VERSION }}-$CONFIG_FLAGS"" | sha1sum | sed \'s/ .*$//\')\necho ""CONFIG_FLAGS=${CONFIG_FLAGS}"" >> $GITHUB_ENV\necho ""OPENSSL_HASH=${OPENSSL_HASH}"" >> $GITHUB_ENV\necho ""OSSL_INFO=${{ matrix.PYTHON.OPENSSL.TYPE }}-${{ matrix.PYTHON.OPENSSL.VERSION }}-${CONFIG_FLAGS}"" >> $GITHUB_ENV\necho ""OSSL_PATH=${{ github.workspace }}/osslcache/${{ matrix.PYTHON.OPENSSL.TYPE }}-${{ matrix.PYTHON.OPENSSL.VERSION }}-${OPENSSL_HASH}"" >> $GITHUB_ENV\n', '.github/workflows/build_openssl.sh', 'echo ""OPENSSL_DIR=${OSSL_PATH}"" >> $GITHUB_ENV\necho ""CFLAGS=${CFLAGS} -Werror=implicit-function-declaration"" >> $GITHUB_ENV\necho ""RUSTFLAGS=-Clink-arg=-Wl,-rpath=${OSSL_PATH}/lib -Clink-arg=-Wl,-rpath=${OSSL_PATH}/lib64"" >> $GITHUB_ENV\n', ""python -m pip install -c ci-constraints-requirements.txt 'nox'"", 'nox -v --install-only\n', 'nox --no-install --  --color=yes --wycheproof-root=wycheproof ${{ matrix.PYTHON.NOXARGS }}\n', '# This modifies /etc/os-release so the JS actions\n# from GH can\'t detect that it\'s on alpine:aarch64. It will\n# then use a glibc nodejs, which works fine when gcompat\n# is installed in the container (which it is)\nsed -i ""s:ID=alpine:ID=NotpineForGHA:"" /etc/os-release\n', 'mkdir -p ""${HOME}/.cache/pip""', 'echo ""OPENSSL_FORCE_FIPS_MODE=1"" >> $GITHUB_ENV\n', ""/venv/bin/python -m pip install -c ci-constraints-requirements.txt 'nox'"", '/venv/bin/nox -v --install-only', '/venv/bin/nox --no-install --  --color=yes --wycheproof-root=""wycheproof""', 'rustup component add llvm-tools-preview', ""python -m pip install -c ci-constraints-requirements.txt 'nox'"", 'OPENSSL_DIR=$(readlink -f ../openssl-macos-universal2/) \\\n  OPENSSL_STATIC=1 \\\n  CFLAGS=""-Werror -Wno-error=deprecated-declarations -Wno-error=incompatible-pointer-types-discards-qualifiers -Wno-error=unused-function -mmacosx-version-min=10.12"" \\\n  nox -v --install-only\n', 'nox --no-install --  --color=yes --wycheproof-root=wycheproof', 'rustup component add llvm-tools-preview', 'python -m pip install -c ci-constraints-requirements.txt ""nox""', 'echo ""OPENSSL_DIR=C:/openssl-${{ matrix.WINDOWS.WINDOWS }}"" >> $GITHUB_ENV\n', 'nox -v --install-only', 'nox --no-install --  --color=yes --wycheproof-root=wycheproof', './.github/downstream.d/${{ matrix.DOWNSTREAM }}.sh install', 'pip install .', 'import json\nimport pkg_resources\nimport shutil\nimport urllib.request\n\nd = pkg_resources.get_distribution(""cryptography"")\nwith urllib.request.urlopen(""https://pypi.org/pypi/cryptography/json"") as r:\n    latest_version = json.load(r)[""info""][""version""]\nnew_path = d.egg_info.replace(d.version, latest_version)\nshutil.move(d.egg_info, new_path)\n', './.github/downstream.d/${{ matrix.DOWNSTREAM }}.sh run', 'pip install -c ci-constraints-requirements.txt coverage[toml]', 'set +e\npython -m coverage combine\necho ""## Python Coverage"" >> $GITHUB_STEP_SUMMARY\npython -m coverage report -m --fail-under=100 > COV_REPORT\nCOV_EXIT_CODE=$?\ncat COV_REPORT\nif [ $COV_EXIT_CODE -ne 0 ]; then\n  echo ""ðŸš¨ Python Coverage failed. Under 100"" | tee -a $GITHUB_STEP_SUMMARY\nfi\necho \'```\' >> $GITHUB_STEP_SUMMARY\ncat COV_REPORT >> $GITHUB_STEP_SUMMARY\necho \'```\' >> $GITHUB_STEP_SUMMARY\nexit $COV_EXIT_CODE\n', 'set +e\nsudo apt-get install -y lcov\nRUST_COVERAGE_OUTPUT=$(lcov $(for f in *.lcov; do echo --add-tracefile ""$f""; done) -o combined.lcov | grep lines)\necho ""## Rust Coverage"" >> $GITHUB_STEP_SUMMARY\necho \'```\' >> $GITHUB_STEP_SUMMARY\necho $RUST_COVERAGE_OUTPUT >> $GITHUB_STEP_SUMMARY\necho \'```\' >> $GITHUB_STEP_SUMMARY\nif ! echo ""$RUST_COVERAGE_OUTPUT"" | grep ""100.0%""; then\n  echo ""ðŸš¨ Rust Coverage failed. Under 100"" | tee -a $GITHUB_STEP_SUMMARY\n  exit 1\nfi\n', 'genhtml combined.lcov -o rust-coverage', 'python -m coverage html', 'python -m pip install -c ci-constraints-requirements.txt nox', 'nox -v --install-only -s docs-linkcheck\n', 'nox --no-install -s docs-linkcheck -- --color=yes', 'pip install twine requests sigstore', 'echo ""OIDC_AUDIENCE=pypi"" >> $GITHUB_ENV\necho ""PYPI_DOMAIN=pypi.org"" >> $GITHUB_ENV\necho ""TWINE_REPOSITORY=pypi"" >> $GITHUB_ENV\necho ""TWINE_USERNAME=__token__"" >> $GITHUB_ENV\n', 'echo ""OIDC_AUDIENCE=testpypi"" >> $GITHUB_ENV\necho ""PYPI_DOMAIN=test.pypi.org"" >> $GITHUB_ENV\necho ""TWINE_REPOSITORY=testpypi"" >> $GITHUB_ENV\necho ""TWINE_USERNAME=__token__"" >> $GITHUB_ENV\n', 'import os\n\nimport requests\n\nresponse = requests.get(\n    os.environ[""ACTIONS_ID_TOKEN_REQUEST_URL""],\n    params={""audience"": os.environ[""OIDC_AUDIENCE""]},\n    headers={""Authorization"": f""bearer {os.environ[\'ACTIONS_ID_TOKEN_REQUEST_TOKEN\']}""}\n)\nresponse.raise_for_status()\ntoken = response.json()[""value""]\n\nresponse = requests.post(f""https://{os.environ[\'PYPI_DOMAIN\']}/_/oidc/github/mint-token"", json={""token"": token})\nresponse.raise_for_status()\npypi_token = response.json()[""token""]\n\nwith open(os.environ[""GITHUB_ENV""], ""a"") as f:\n    print(f""::add-mask::{pypi_token}"")\n    f.write(f""TWINE_PASSWORD={pypi_token}\\n"")\n', ""twine upload --skip-existing $(find dist/ -type f -name 'cryptography*')"", ""sigstore sign $(find dist/ -type f -name 'cryptography*')"", 'python -m venv .venv', '.venv/bin/pip install -U pip build', '.venv/bin/python -m build --sdist', 'cd vectors/ && ../.venv/bin/python -m build', '# This modifies /etc/os-release so the JS actions\n# from GH can\'t detect that it\'s on alpine:aarch64. It will\n# then use a glibc nodejs, which works fine when gcompat\n# is installed in the container (which it is)\nsed -i ""s:ID=alpine:ID=NotpineForGHA:"" /etc/os-release\n', '/opt/python/${{ matrix.PYTHON.VERSION }}/bin/python -m venv .venv', '.venv/bin/pip install -U pip wheel cffi setuptools-rust', 'mkdir tmpwheelhouse', 'if [ -n ""${{ matrix.PYTHON.ABI_VERSION }}"" ]; then\n    PY_LIMITED_API=""--config-settings=--build-option=--py-limited-api=${{ matrix.PYTHON.ABI_VERSION }} --no-build-isolation""\nfi\nOPENSSL_DIR=""/opt/pyca/cryptography/openssl"" \\\n    OPENSSL_STATIC=1 \\\n    .venv/bin/python -m pip wheel -v $PY_LIMITED_API cryptograph*.tar.gz -w dist/ && mv dist/cryptography*.whl tmpwheelhouse\n', 'auditwheel repair --plat ${{ matrix.MANYLINUX.NAME }} tmpwheelhouse/cryptograph*.whl -w wheelhouse/', 'unzip wheelhouse/*.whl -d execstack.check', 'results=$(readelf -lW execstack.check/cryptography/hazmat/bindings/*.so)\ncount=$(echo ""$results"" | grep -c \'GNU_STACK.*[R ][W ]E\' || true)\nif [ ""$count"" -ne 0 ]; then\n  exit 1\nelse\n  exit 0\nfi\n', '.venv/bin/pip install cryptography --no-index -f wheelhouse/', '.venv/bin/python -c ""from cryptography.hazmat.backends.openssl.backend import backend;print(\'Loaded: \' + backend.openssl_version_text());print(\'Linked Against: \' + backend._ffi.string(backend._lib.OPENSSL_VERSION_TEXT).decode(\'ascii\'))""\n', 'mkdir cryptography-wheelhouse', 'mv wheelhouse/cryptography*.whl cryptography-wheelhouse/', 'curl ""$PYTHON_DOWNLOAD_URL"" -o python.pkg\nsudo installer -pkg python.pkg -target /\n', '${{ matrix.PYTHON.BIN_PATH }} -m venv venv', 'venv/bin/pip install -U pip wheel cffi setuptools-rust', 'mkdir wheelhouse', 'if [ -n ""${{ matrix.PYTHON.ABI_VERSION }}"" ]; then\n    PY_LIMITED_API=""--config-settings=--build-option=--py-limited-api=${{ matrix.PYTHON.ABI_VERSION }} --no-build-isolation""\nfi\n\nOPENSSL_DIR=""$(readlink -f ../openssl-macos-universal2/)"" \\\n    OPENSSL_STATIC=1 \\\n    venv/bin/python -m pip wheel -v $PY_LIMITED_API cryptograph*.tar.gz -w dist/ && mv dist/cryptography*.whl wheelhouse\n', 'venv/bin/pip install -f wheelhouse/ --no-index cryptography', ""find venv/lib/*/site-packages/cryptography/hazmat/bindings -name '*.so' -exec vtool -show {} \\;\n"", 'venv/bin/python -c ""from cryptography.hazmat.backends.openssl.backend import backend;print(\'Loaded: \' + backend.openssl_version_text());print(\'Linked Against: \' + backend._ffi.string(backend._lib.OPENSSL_VERSION_TEXT).decode(\'ascii\'))""\n', 'mkdir cryptography-wheelhouse', 'mv wheelhouse/cryptography*.whl cryptography-wheelhouse/', 'echo ""CRYPTOGRAPHY_WHEEL_NAME=$(basename $(ls cryptography-wheelhouse/cryptography*.whl))"" >> $GITHUB_ENV\n', 'echo ""OPENSSL_DIR=C:/openssl-${{ matrix.WINDOWS.WINDOWS }}"" >> $GITHUB_ENV\necho ""OPENSSL_STATIC=1"" >> $GITHUB_ENV\n', 'python -m pip install -U pip wheel', 'python -m pip install cffi setuptools-rust', 'mkdir wheelhouse', 'if [ -n ""${{ matrix.PYTHON.ABI_VERSION }}"" ]; then\n    PY_LIMITED_API=""--config-settings=--build-option=--py-limited-api=${{ matrix.PYTHON.ABI_VERSION }} --no-build-isolation""\nfi\n\npython -m pip wheel -v cryptography*.tar.gz $PY_LIMITED_API -w dist/ && mv dist/cryptography*.whl wheelhouse/\n', 'pip install -f wheelhouse --no-index cryptography', 'python -c ""from cryptography.hazmat.backends.openssl.backend import backend;print(\'Loaded: \' + backend.openssl_version_text());print(\'Linked Against: \' + backend._ffi.string(backend._lib.OPENSSL_VERSION_TEXT).decode(\'ascii\'))""\n', 'mkdir cryptography-wheelhouse', 'move wheelhouse\\cryptography*.whl cryptography-wheelhouse\\']"
"['mkdir ~/.ssh\necho ""${{ secrets.DEMO_COOKIECUTTER_FLASK_PRIVATE_DEPLOY_KEY }}"" > ~/.ssh/id_rsa\nchmod 600 ~/.ssh/id_rsa\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'invoke build', 'git clone --no-checkout git@github.com:jamescurtin/demo-cookiecutter-flask.git /tmp/demo-cookiecutter-flask', 'mv /tmp/demo-cookiecutter-flask/.git my_flask_app', 'cd my_flask_app\ngit config --global user.email ""jameswcurtin@gmail.com""\ngit config --global user.name ""James Curtin""\ngit add .\ngit commit -m ""Autobuilding example project"" || true\ngit push -u --force origin master || true\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'invoke test', 'invoke lint', 'invoke no-placeholders', 'invoke test-image-build', 'git checkout HEAD^2', 'python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'invoke build']"
"['echo ""PUBLISH=$(echo true)"" >> $GITHUB_ENV', 'pipx install ""poetry==1.5.1""', 'poetry install --extras ""docs lint""', 'python -V\npoetry run python -V\n', ""pushd docs; make SPHINXBUILD='poetry run sphinx-build' html; popd\n"", 'pipx install ""poetry==1.5.1""', 'sudo apt install libevent-dev libncurses5-dev libtinfo-dev libutempter-dev bison\nmkdir ~/tmux-builds\nmkdir ~/tmux-src\ngit clone https://github.com/tmux/tmux.git ~/tmux-src/tmux-${{ matrix.tmux-version }}\ncd ~/tmux-src/tmux-${{ matrix.tmux-version }}\ngit checkout ${{ matrix.tmux-version }}\nsh autogen.sh\n./configure --prefix=$HOME/tmux-builds/tmux-${{ matrix.tmux-version }} && make && make install\nexport PATH=$HOME/tmux-builds/tmux-${{ matrix.tmux-version }}/bin:$PATH\ncd ~\ntmux -V\n', 'poetry install -E ""test coverage lint""\n', 'poetry run ruff .', 'poetry run mypy .', 'python -V\npoetry run python -V\n', 'sudo apt install libevent-2.1-7\nexport PATH=$HOME/tmux-builds/tmux-${{ matrix.tmux-version }}/bin:$PATH\nls $HOME/tmux-builds/tmux-${{ matrix.tmux-version }}/bin\ntmux -V\npoetry run py.test --cov=./ --cov-report=xml\n', 'pipx install ""poetry==1.5.1""', 'poetry build']"
"['pip install tox', 'tox -e docset', ""tar --exclude='.DS_Store' -cvzf structlog.tgz structlog.docset"", 'python -Im pip install build', 'python -Im build --sdist', 'python -Im pip install --upgrade wheel tox', 'python -Im tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -Im pip install --upgrade coverage[toml]', ""python -Im coverage combine\npython -Im coverage html --skip-covered --skip-empty\n\n# Report and write to summary.\npython -Im coverage report | sed 's/^/    /' >> $GITHUB_STEP_SUMMARY\n\n# Report again and fail if under 100%.\npython -Im coverage report --fail-under=100\n"", 'python -Im pip install --upgrade wheel tox', 'python -Im tox -e mypy', 'python -Im pip install --upgrade wheel tox', 'python -Im tox -e docs', 'python -Im pip install -e .[dev]', ""python -Ic 'import structlog; print(structlog.__version__)'"", 'python -Im pip install tox wheel', 'tox -f color']"
"[""print('::set-output name=is-untagged-devel::true')"", ""print('::set-output name=release-requested::true')"", ""from hashlib import sha512\nfrom sys import version\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'print(\n  ""::set-output name=files-hash-key::${{\n      hashFiles(\n        \'setup.cfg\', \'tox.ini\', \'pyproject.toml\',\n        \'.pre-commit-config.yaml\', \'pytest.ini\'\n      )\n  }}"",\n)\n', 'echo ""::set-output name=dir::$(python -m pip cache dir)""', 'git tag --points-at HEAD | xargs git tag --delete', 'python -m pip install --user setuptools-scm', 'import setuptools_scm\nver = setuptools_scm.get_version(\n  ${{\n      steps.untagged-check.outputs.is-untagged-devel == \'true\'\n      && \'local_scheme=""no-local-version""\' || \'\'\n  }}\n)\nprint(\'::set-output name=dist-version::{ver}\'.format(ver=ver))\n', ""print('::set-output name=tag::v${{\n    steps.request-check.outputs.release-requested == 'true'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}')"", ""print('::set-output name=sdist::proxy.py-${{\n    steps.request-check.outputs.release-requested == 'true'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}.tar.gz')\nprint('::set-output name=wheel::proxy.py-${{\n    steps.request-check.outputs.release-requested == 'true'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}-py3-none-any.whl')\n"", 'VER=$(echo \'${{\n    steps.request-check.outputs.release-requested == \'true\'\n    && github.event.inputs.release-version\n    || steps.scm-version.outputs.dist-version\n}}\' | tr + .); PLATFORMS=""linux/386,linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x""; echo ""::set-output name=version::$VER""; echo ""::set-output name=platforms::$PLATFORMS""', ""from hashlib import sha512\nfrom sys import version\n\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'echo ""::set-output name=dir::$(pip cache dir)""', 'python -m pip install --user tox', 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --notest', ""git tag -m '${{ needs.pre-setup.outputs.git-tag }}' '${{ needs.pre-setup.outputs.git-tag }}' -- ${{ github.event.inputs.release-commitish }}"", 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --skip-pkg-install', ""ls -1 'dist/${{ needs.pre-setup.outputs.sdist-artifact-name }}' 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}'"", ""from hashlib import sha512\nfrom sys import version\n\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'echo ""::set-output name=dir::$(pip cache dir)""', 'python -m pip install --user tox', 'shopt -s extglob\nrm -rf !tox.ini\n', 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --notest', 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --skip-pkg-install', ""from hashlib import sha512\nfrom sys import version\n\nhash = sha512(version.encode()).hexdigest()\nprint(f'::set-output name=py-hash-key::{hash}')\n"", 'echo ""::set-output name=dir::$(pip cache dir)""', 'python -m pip install --user tox', ""python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --installpkg 'dist/${{ needs.pre-setup.outputs.wheel-artifact-name }}' --notest"", 'python -m tox --parallel auto --parallel-live --skip-missing-interpreters false --skip-pkg-install', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker buildx create --name proxypybuilder\ndocker buildx use proxypybuilder\ndocker buildx inspect\ndocker buildx ls\n', 'CONTAINER_TAG=""abhinavsingh/proxy.py:${{\n  needs.pre-setup.outputs.container-version\n}}""; docker buildx build --load --build-arg PROXYPY_PKG_PATH=\'dist/${{\n  needs.pre-setup.outputs.wheel-artifact-name\n}}\' -t $CONTAINER_TAG . && docker run -d -p 8899:8899 $CONTAINER_TAG --hostname 0.0.0.0 --enable-web-server --enable-reverse-proxy --plugin proxy.plugin.ReverseProxyPlugin && ./tests/integration/test_integration.sh 8899', 'cd dashboard\nnpm install\ncd ..\n', 'cd dashboard\nnpm run build\ncd ..\n', 'cd dashboard\nnpm run devtools\ncd ..\n', 'make lib-dep\n', './write-scm-version.sh\npython3 check.py\nmake https-certificates\nmake sign-https-certificates\nmake ca-certificates\npython3 -m proxy --version\n', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker buildx create --name proxypybuilder\ndocker buildx use proxypybuilder\ndocker buildx inspect\ndocker buildx ls\n', 'REGISTRY_URL=""ghcr.io/abhinavsingh/proxy.py""; CONTAINER_TAG=$REGISTRY_URL:${{\n  needs.pre-setup.outputs.container-version\n}}; docker buildx build --push --platform ${{\n  needs.pre-setup.outputs.container-platforms\n}} --build-arg SKIP_OPENSSL=1 --build-arg PROXYPY_PKG_PATH=\'dist/${{\n  needs.pre-setup.outputs.wheel-artifact-name\n}}\' -t $CONTAINER_TAG .', 'REGISTRY_URL=""ghcr.io/abhinavsingh/proxy.py""; LATEST_TAG=$REGISTRY_URL:latest; docker buildx build --push --platform ${{\n  needs.pre-setup.outputs.container-platforms\n}} --build-arg SKIP_OPENSSL=1 --build-arg PROXYPY_PKG_PATH=\'dist/${{\n  needs.pre-setup.outputs.wheel-artifact-name\n}}\' -t $LATEST_TAG .', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker buildx create --name proxypybuilder\ndocker buildx use proxypybuilder\ndocker buildx inspect\ndocker buildx ls\n', 'REGISTRY_URL=""ghcr.io/abhinavsingh/proxy.py""; CONTAINER_TAG=$REGISTRY_URL:${{\n  needs.pre-setup.outputs.container-version\n}}-openssl; docker buildx build --push --platform ${{\n  needs.pre-setup.outputs.container-platforms\n}} --build-arg PROXYPY_PKG_PATH=\'dist/${{\n  needs.pre-setup.outputs.wheel-artifact-name\n}}\' -t $CONTAINER_TAG .', 'REGISTRY_URL=""ghcr.io/abhinavsingh/proxy.py""; LATEST_TAG=$REGISTRY_URL:openssl; docker buildx build --push --platform ${{\n  needs.pre-setup.outputs.container-platforms\n}} --build-arg PROXYPY_PKG_PATH=\'dist/${{\n  needs.pre-setup.outputs.wheel-artifact-name\n}}\' -t $LATEST_TAG .', 'docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n', 'docker buildx create --name proxypybuilder\ndocker buildx use proxypybuilder\ndocker buildx inspect\ndocker buildx ls\n', 'REGISTRY_URL=""abhinavsingh/proxy.py""; CONTAINER_TAG=$REGISTRY_URL:${{\n  needs.pre-setup.outputs.container-version\n}}; docker buildx build --push --platform ${{\n  needs.pre-setup.outputs.container-platforms\n}} --build-arg SKIP_OPENSSL=1 --build-arg PROXYPY_PKG_PATH=\'dist/${{\n  needs.pre-setup.outputs.wheel-artifact-name\n}}\' -t $CONTAINER_TAG .', ""git tag -m '${{ needs.pre-setup.outputs.git-tag }}' '${{ needs.pre-setup.outputs.git-tag }}' -- ${{ github.event.inputs.release-commitish }}"", ""git push --atomic origin '${{ needs.pre-setup.outputs.git-tag }}'""]"
"[""python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade 'tox>=4.0.0rc3'\n"", 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -m pip install --upgrade coverage[toml]', 'python -m coverage combine\npython -m coverage html --skip-covered --skip-empty\npython -m coverage report --fail-under=95\n']"
"['echo ""de_DE.UTF-8 UTF-8"" | sudo tee -a /etc/locale.gen\necho ""cs_CZ.UTF-8 UTF-8"" | sudo tee -a /etc/locale.gen\necho ""el_GR.UTF-8 UTF-8"" | sudo tee -a /etc/locale.gen\necho ""fr_FR.UTF-8 UTF-8"" | sudo tee -a /etc/locale.gen\nsudo locale-gen\n', 'set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade coverage[toml] virtualenv tox tox-gh-actions\n', 'python -m tox']"
"['pip install -e .\npip install -r reqs/dev-requirements.txt\n', 'pytest lifelines/tests/ -vv', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f qa/requirements.txt ]; then pip install -r qa/requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --exclude src/exabgp/vendoring/ --exclude build/ --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n# flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', './qa/bin/functional parsing\n', './qa/bin/functional encoding\n', './qa/bin/functional decoding\n', 'env PYTHONPATH=src exabgp_log_enable=false pytest --cov --cov-reset ./tests/*_test.py\n']"
""
""
"['python -m pip install tox', 'python -m tox -e lint', 'python -m pip install tox', 'python -m tox -e types', 'python -m tox -e py', 'python -m pip install tox', 'python -m tox -e integration', 'python -m pip install tox', 'python -m tox -e docs', 'python -m pip install tox', 'tox -e release']"
"['poetry install --with ci', 'poetry run tox', 'poetry run coverage xml', 'poetry install --with docs', 'git config user.name ""dj-stripe commit bot""\ngit config user.email ""admin@djstripe.dev""\n', 'git fetch origin gh-pages --depth=1', 'poetry run mike deploy --push --rebase ""${GITHUB_REF##*/}""', 'poetry run mike set-default --push --rebase ""${LATEST_STABLE_BRANCH##*/}""', 'poetry install --with dev -E postgres', 'poetry run pre-commit run --all-files --show-diff-on-failure']"
"['python -m pip install -U pip wheel\npython -m pip install -r requirements.txt -r requirements-bundle.txt\n', 'sudo apt-get install libportaudio2', '.\\reproducible_build.ps1', './reproducible_build.sh']"
"['python -m pip install --upgrade pip\npython -m pip install -e .[dev]\npython -m pip install coveralls\n', 'pytest --cov\n', 'coveralls --service=github\n']"
"['powershell -command ""Set-ExecutionPolicy -Force -ExecutionPolicy RemoteSigned -Scope CurrentUser""', 'cd ~\necho ${{ secrets.GITHUB_TOKEN }} > "".autopkg_gh_token""\n', 'pip install --requirement gh_actions_requirements.txt', 'if not exist ""%USERPROFILE%/AppData/Local/Autopkg"" mkdir ""%USERPROFILE%/AppData/Local/Autopkg""\nif not exist ""%USERPROFILE%/AppData/Local/Autopkg/config.json"" echo {} > ""%USERPROFILE%/AppData/Local/Autopkg/config.json""\n', 'cd ~ && mkdir -p .config/Autopkg\ncd ~ && echo {} > .config/Autopkg/config.json\n', 'cat <<\'EOFEOF\' > GitHubReleasesInfoProvider.test.recipe.yaml\n---\nDescription: Test GitHubReleasesInfoProvider Processor\nIdentifier: com.github.autopkg.test.GitHubReleasesInfoProvider\nInput:\n  NAME: GitHubReleasesInfoProviderTest\nMinimumVersion: ""2.3""\nProcess:\n    - Processor: GitHubReleasesInfoProvider\n      Arguments:\n        github_repo: audacity/audacity\n        asset_regex: .*64bit\\.exe$\n        include_prereleases: False\nEOFEOF\n', 'cat GitHubReleasesInfoProvider.test.recipe.yaml', 'python Code/autopkg run -vvvv GitHubReleasesInfoProvider.test.recipe.yaml', '.github/workflows/linting.sh', 'powershell -command ""Set-ExecutionPolicy -Force -ExecutionPolicy RemoteSigned -Scope CurrentUser""', 'cd ~\necho ${{ secrets.GITHUB_TOKEN }} > "".autopkg_gh_token""\n', 'pip install --requirement gh_actions_requirements.txt', 'if not exist ""%USERPROFILE%/AppData/Local/Autopkg"" mkdir ""%USERPROFILE%/AppData/Local/Autopkg""\nif not exist ""%USERPROFILE%/AppData/Local/Autopkg/config.json"" echo {} > ""%USERPROFILE%/AppData/Local/Autopkg/config.json""\n', 'cd ~ && mkdir -p .config/Autopkg\ncd ~ && echo {} > .config/Autopkg/config.json\n', 'python Scripts/run_tests.py', 'echo ""**List-processors:""\npython Code/autopkg list-processors --prefs tests/preferences.plist\necho ""**Processor-info:""\npython Code/autopkg processor-info URLDownloader --prefs tests/preferences.plist\necho ""**Repo-add:""\npython Code/autopkg repo-add recipes --prefs tests/preferences.plist\necho ""**Repo-list:""\npython Code/autopkg repo-list --prefs tests/preferences.plist\necho ""**Repo-update:""\npython Code/autopkg repo-update all --prefs tests/preferences.plist\necho ""**Audit:""\npython Code/autopkg audit Firefox.munki --prefs tests/preferences.plist\necho ""**Info:""\npython Code/autopkg info Firefox.munki --prefs tests/preferences.plist\necho ""**List-recipes:""\npython Code/autopkg list-recipes --prefs tests/preferences.plist\necho ""**Make-override:""\npython Code/autopkg make-override Firefox.munki --force --prefs tests/preferences.plist\necho ""**New-recipe:""\npython Code/autopkg new-recipe TestRecipe.check --prefs tests/preferences.plist\necho ""**Verify-trust-info:""\npython Code/autopkg verify-trust-info Firefox.munki --prefs tests/preferences.plist\necho ""**Update-trust-info:""\npython Code/autopkg update-trust-info Firefox.munki --prefs tests/preferences.plist\necho ""**Version:""\npython Code/autopkg version\necho ""**Repo-delete:""\npython Code/autopkg repo-delete recipes --prefs tests/preferences.plist\n', 'python Code/autopkg run -vv Code/tests/Test-Recipes/AutopkgCore.test.recipe.yaml', 'python3 -m pip install appdirs', 'git config --global user.name github-actions\ngit config --global user.email github-actions@github.com\n', 'python3 generate_processor_docs.py -d ../../wiki -y ""${GITHUB_REF##*/}""']"
"[""sudo xcode-select -s '/Applications/Xcode_13.0.app/Contents/Developer'"", 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', 'make', 'make test', 'nuget=""$(vcpkg fetch nuget | tail -n 1)""\n""${nuget}"" \\\n  sources add \\\n  -source ""https://nuget.pkg.github.com/${GITHUB_REPOSITORY_OWNER}/index.json"" \\\n  -storepasswordincleartext \\\n  -name ""GitHub"" \\\n  -username ""${GITHUB_REPOSITORY_OWNER}"" \\\n  -password ""${{ secrets.GITHUB_TOKEN }}""\n""${nuget}"" \\\n  setapikey ""${{ secrets.GITHUB_TOKEN }}"" \\\n  -source ""https://nuget.pkg.github.com/${GITHUB_REPOSITORY_OWNER}/index.json""\n', 'vcpkg install curl[core,http2,non-http,openssl,ssh]:x86-windows', 'python -m pip install --upgrade pip\npip install wheel delvewheel\npip install flake8 pytest -r requirements-dev.txt\n', 'python setup.py bdist_wheel --with-openssl --curl-dir=$env:VCPKG_INSTALLATION_ROOT/packages/curl_x86-windows --openssl-dir=$env:VCPKG_INSTALLATION_ROOT/packages/openssl_x86-windows --openssl-lib-name=libssl.lib --link-arg=libcrypto.lib', 'delvewheel repair --add-path $VCPKG_INSTALLATION_ROOT/installed/x86-windows/bin dist/*.whl\npip install wheelhouse/*.whl\n', 'pytest -v', 'sudo apt-get update\nsudo apt-get install libcurl4-gnutls-dev libgnutls28-dev\n', 'python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings.\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics\n', 'make', 'make test', 'python -m pip install --upgrade pip setuptools\npip install cibuildwheel\ncibuildwheel --output-dir dist\n']"
"['sudo apt-get update && sudo apt-get install -y graphviz', 'python -m pip install -U pip\npython -m pip install -U setuptools wheel\npython -m pip install -U coverage coverage-python-version coveralls\npython -m pip install -e .[test]\n', 'coverage run tests.py', 'coverage report -m --fail-under=100', 'coveralls', 'python -m pip install -U pip\npython -m pip install -U setuptools wheel\npython -m pip install -U tox\n', 'python -m tox -e ${{ matrix.toxenv }}']"
"['python -m pip install --upgrade pip\npip install flake8 black isort mypy pytest numpy\n', 'isort pyvisa -c;\n', 'black pyvisa --check;\n', 'flake8 pyvisa;\n', 'mypy pyvisa;\n', 'python -m pip install --upgrade pip\npip install numpy\n', 'pip install -e .\n', 'pip install pytest-cov\npython -X dev -m pytest --pyargs pyvisa --cov --cov-report xml -v\n', 'exit 0', 'exit 1', 'python -m pip install --upgrade pip\npip install -r docs/requirements.txt\n', 'pip install .\n', 'mkdir docs_output;\nsphinx-build docs/source docs_output -W -b html;\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'pip install --upgrade pip\npip install wheel build\npython -m build . -s\n', 'pip install pytest\npip install dist/*.tar.gz\npython -X dev -m pytest --pyargs pyvisa\n', 'git fetch --prune --unshallow\ngit fetch --depth=1 origin +refs/tags/*:refs/tags/*\n', 'pip install --upgrade pip\npip install wheel build\npython -m build . -w\n', 'pip install pytest\npip install dist/*.whl\npython -X dev -m pytest --pyargs pyvisa\n']"
"['sudo apt-get install libsndfile1', 'pip install numpy pytest', 'pip install --editable . --verbose', 'python -m pytest']"
"['echo ""${{ github.sha }};${{ github.ref }};${{ github.event_name }};${{ github.actor }}"" > OFFICIAL_IMAGE\n', 'python3 -m script.translations download', '# Remove dist, build, and homeassistant.egg-info\n# when build locally for testing!\npip install twine build\npython -m build\n', 'export TWINE_USERNAME=""__token__""\nexport TWINE_PASSWORD=""${{ secrets.TWINE_TOKEN }}""\n\ntwine upload dist/* --skip-existing\n', 'python3 -m pip install packaging tomli\npython3 -m pip install .\nversion=""$(python3 script/version_bump.py nightly)""\n\nif [[ ""$(ls home_assistant_frontend*.whl)"" =~ ^home_assistant_frontend-(.*)-py3-none-any.whl$ ]]; then\n  echo ""Found frontend wheel, setting version to: ${BASH_REMATCH[1]}""\n  frontend_version=""${BASH_REMATCH[1]}"" yq \\\n    --inplace e -o json \\\n    \'.requirements = [""home-assistant-frontend==""+env(frontend_version)]\' \\\n    homeassistant/components/frontend/manifest.json\n\n  sed -i ""s|home-assistant-frontend==.*|home-assistant-frontend==${BASH_REMATCH[1]}|"" \\\n    homeassistant/package_constraints.txt\n\n  python -m script.gen_requirements_all\nfi\n\nif [[ ""$(ls home_assistant_intents*.whl)"" =~ ^home_assistant_intents-(.*)-py3-none-any.whl$ ]]; then\n  echo ""Found intents wheel, setting version to: ${BASH_REMATCH[1]}""\n  yq \\\n    --inplace e -o json \\\n    \'del(.requirements[] | select(contains(""home-assistant-intents"")))\' \\\n    homeassistant/components/conversation/manifest.json\n\n  intents_version=""${BASH_REMATCH[1]}"" yq \\\n    --inplace e -o json \\\n    \'.requirements += [""home-assistant-intents==""+env(intents_version)]\' \\\n    homeassistant/components/conversation/manifest.json\n\n  sed -i ""s|home-assistant-intents==.*|home-assistant-intents==${BASH_REMATCH[1]}|"" \\\n    homeassistant/package_constraints.txt\n\n  python -m script.gen_requirements_all\nfi\n', '# Pandas has issues building on armhf, it is expected they\n# will drop the platform in the near future (they consider it\n# ""flimsy"" on 386). The following packages depend on pandas,\n# so we comment them out.\nsed -i ""s|env_canada|# env_canada|g"" requirements_all.txt\nsed -i ""s|noaa-coops|# noaa-coops|g"" requirements_all.txt\nsed -i ""s|pyezviz|# pyezviz|g"" requirements_all.txt\nsed -i ""s|pykrakenapi|# pykrakenapi|g"" requirements_all.txt\n', 'python3 -m script.translations download', 'echo ""${{ github.sha }};${{ github.ref }};${{ github.event_name }};${{ github.actor }}"" > rootfs/OFFICIAL_IMAGE\n', 'find ./homeassistant/components/*/translations -name ""*.json"" | tar zcvf translations.tar.gz -T -', '# Create general tags\nif [[ ""${{ needs.init.outputs.version }}"" =~ d ]]; then\n  echo ""BUILD_ARGS=--additional-tag dev"" >> $GITHUB_ENV\nelif [[ ""${{ needs.init.outputs.version }}"" =~ b ]]; then\n  echo ""BUILD_ARGS=--additional-tag beta"" >> $GITHUB_ENV\nelse\n  echo ""BUILD_ARGS=--additional-tag stable"" >> $GITHUB_ENV\nfi\n', 'export DOCKER_CLI_EXPERIMENTAL=enabled\n\nfunction create_manifest() {\n  local tag_l=${1}\n  local tag_r=${2}\n\n  docker manifest create ""${{ matrix.registry }}/home-assistant:${tag_l}"" \\\n    ""${{ matrix.registry }}/amd64-homeassistant:${tag_r}"" \\\n    ""${{ matrix.registry }}/i386-homeassistant:${tag_r}"" \\\n    ""${{ matrix.registry }}/armhf-homeassistant:${tag_r}"" \\\n    ""${{ matrix.registry }}/armv7-homeassistant:${tag_r}"" \\\n    ""${{ matrix.registry }}/aarch64-homeassistant:${tag_r}""\n\n  docker manifest annotate ""${{ matrix.registry }}/home-assistant:${tag_l}"" \\\n    ""${{ matrix.registry }}/amd64-homeassistant:${tag_r}"" \\\n    --os linux --arch amd64\n\n  docker manifest annotate ""${{ matrix.registry }}/home-assistant:${tag_l}"" \\\n    ""${{ matrix.registry }}/i386-homeassistant:${tag_r}"" \\\n    --os linux --arch 386\n\n  docker manifest annotate ""${{ matrix.registry }}/home-assistant:${tag_l}"" \\\n    ""${{ matrix.registry }}/armhf-homeassistant:${tag_r}"" \\\n    --os linux --arch arm --variant=v6\n\n  docker manifest annotate ""${{ matrix.registry }}/home-assistant:${tag_l}"" \\\n    ""${{ matrix.registry }}/armv7-homeassistant:${tag_r}"" \\\n    --os linux --arch arm --variant=v7\n\n  docker manifest annotate ""${{ matrix.registry }}/home-assistant:${tag_l}"" \\\n    ""${{ matrix.registry }}/aarch64-homeassistant:${tag_r}"" \\\n    --os linux --arch arm64 --variant=v8\n\n  docker manifest push --purge ""${{ matrix.registry }}/home-assistant:${tag_l}""\n}\n\nfunction validate_image() {\n  local image=${1}\n  if ! cas authenticate --signerID notary@home-assistant.io ""docker://${image}""; then\n    echo ""Invalid signature!""\n    exit 1\n  fi\n}\n\ndocker pull ""${{ matrix.registry }}/amd64-homeassistant:${{ needs.init.outputs.version }}""\ndocker pull ""${{ matrix.registry }}/i386-homeassistant:${{ needs.init.outputs.version }}""\ndocker pull ""${{ matrix.registry }}/armhf-homeassistant:${{ needs.init.outputs.version }}""\ndocker pull ""${{ matrix.registry }}/armv7-homeassistant:${{ needs.init.outputs.version }}""\ndocker pull ""${{ matrix.registry }}/aarch64-homeassistant:${{ needs.init.outputs.version }}""\n\nvalidate_image ""${{ matrix.registry }}/amd64-homeassistant:${{ needs.init.outputs.version }}""\nvalidate_image ""${{ matrix.registry }}/i386-homeassistant:${{ needs.init.outputs.version }}""\nvalidate_image ""${{ matrix.registry }}/armhf-homeassistant:${{ needs.init.outputs.version }}""\nvalidate_image ""${{ matrix.registry }}/armv7-homeassistant:${{ needs.init.outputs.version }}""\nvalidate_image ""${{ matrix.registry }}/aarch64-homeassistant:${{ needs.init.outputs.version }}""\n\n# Create version tag\ncreate_manifest ""${{ needs.init.outputs.version }}"" ""${{ needs.init.outputs.version }}""\n\n# Create general tags\nif [[ ""${{ needs.init.outputs.version }}"" =~ d ]]; then\n  create_manifest ""dev"" ""${{ needs.init.outputs.version }}""\nelif [[ ""${{ needs.init.outputs.version }}"" =~ b ]]; then\n  create_manifest ""beta"" ""${{ needs.init.outputs.version }}""\n  create_manifest ""rc"" ""${{ needs.init.outputs.version }}""\nelse\n  create_manifest ""stable"" ""${{ needs.init.outputs.version }}""\n  create_manifest ""latest"" ""${{ needs.init.outputs.version }}""\n  create_manifest ""beta"" ""${{ needs.init.outputs.version }}""\n  create_manifest ""rc"" ""${{ needs.init.outputs.version }}""\n\n  # Create series version tag (e.g. 2021.6)\n  v=""${{ needs.init.outputs.version }}""\n  create_manifest ""${v%.*}"" ""${{ needs.init.outputs.version }}""\nfi\n', 'echo ""key=venv-${{ env.CACHE_VERSION }}-${{\n  hashFiles(\'requirements_test.txt\') }}-${{\n  hashFiles(\'requirements_all.txt\') }}-${{\n  hashFiles(\'homeassistant/package_constraints.txt\') }}"" >> $GITHUB_OUTPUT', 'echo ""key=pre-commit-${{ env.CACHE_VERSION }}-${{\n  hashFiles(\'.pre-commit-config.yaml\') }}""  >> $GITHUB_OUTPUT', 'integrations=$(ls -Ad ./homeassistant/components/[!_]*  | xargs -n 1 basename)\ntouch .integration_paths.yaml\nfor integration in $integrations; do\n  echo ""${integration}: [homeassistant/components/${integration}/**, tests/components/${integration}/**]"" \\\n    >> .integration_paths.yaml;\ndone\necho ""Result:""\ncat .integration_paths.yaml\n', '# Defaults\nintegrations_glob=""""\nmariadb_groups=${MARIADB_VERSIONS}\npostgresql_groups=${POSTGRESQL_VERSIONS}\ntest_full_suite=""true""\ntest_groups=""[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]""\ntest_group_count=10\ntests=""[]""\ntests_glob=""""\n\nif [[ ""${{ steps.integrations.outputs.changes }}"" != ""[]"" ]];\nthen\n  # Create a file glob for the integrations\n  integrations_glob=$(echo \'${{ steps.integrations.outputs.changes }}\' | jq -cSr \'. | join("","")\')\n  [[ ""${integrations_glob}"" == *"",""* ]] && integrations_glob=""{${integrations_glob}}""\n\n  # Create list of testable integrations\n  possible_integrations=$(echo \'${{ steps.integrations.outputs.changes }}\' | jq -cSr \'.[]\')\n  tests=$(\n    for integration in ${possible_integrations};\n    do\n      if [[ -d ""tests/components/${integration}"" ]]; then\n        echo -n ""\\""${integration}\\"","";\n      fi;\n    done\n  )\n\n  [[ ! -z ""${tests}"" ]] && tests=""${tests::-1}""\n  tests=""[${tests}]""\n  test_groups=""${tests}""\n  # Test group count should be 1, we don\'t split partial tests\n  test_group_count=1\n\n  # Create a file glob for the integrations tests\n  tests_glob=$(echo ""${tests}"" | jq -cSr \'. | join("","")\')\n  [[ ""${tests_glob}"" == *"",""* ]] && tests_glob=""{${tests_glob}}""\n\n  mariadb_groups=""[]""\n  postgresql_groups=""[]""\n  test_full_suite=""false""\nfi\n\n# We need to run the full suite on certain branches.\n# Or, in case core files are touched, for the full suite as well.\nif [[ ""${{ github.ref }}"" == ""refs/heads/dev"" ]] \\\n  || [[ ""${{ github.ref }}"" == ""refs/heads/master"" ]] \\\n  || [[ ""${{ github.ref }}"" == ""refs/heads/rc"" ]] \\\n  || [[ ""${{ steps.core.outputs.any }}"" == ""true"" ]] \\\n  || [[ ""${{ github.event.inputs.full }}"" == ""true"" ]] \\\n  || [[ ""${{ contains(github.event.pull_request.labels.*.name, \'ci-full-run\') }}"" == ""true"" ]];\nthen\n  mariadb_groups=${MARIADB_VERSIONS}\n  postgresql_groups=${POSTGRESQL_VERSIONS}\n  test_groups=""[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]""\n  test_group_count=10\n  test_full_suite=""true""\nfi\n\n# Output & sent to GitHub Actions\necho ""mariadb_groups: ${mariadb_groups}""\necho ""mariadb_groups=${mariadb_groups}"" >> $GITHUB_OUTPUT\necho ""postgresql_groups: ${postgresql_groups}""\necho ""postgresql_groups=${postgresql_groups}"" >> $GITHUB_OUTPUT\necho ""python_versions: ${ALL_PYTHON_VERSIONS}""\necho ""python_versions=${ALL_PYTHON_VERSIONS}"" >> $GITHUB_OUTPUT\necho ""test_full_suite: ${test_full_suite}""\necho ""test_full_suite=${test_full_suite}"" >> $GITHUB_OUTPUT\necho ""integrations_glob: ${integrations_glob}""\necho ""integrations_glob=${integrations_glob}"" >> $GITHUB_OUTPUT\necho ""test_group_count: ${test_group_count}""\necho ""test_group_count=${test_group_count}"" >> $GITHUB_OUTPUT\necho ""test_groups: ${test_groups}""\necho ""test_groups=${test_groups}"" >> $GITHUB_OUTPUT\necho ""tests: ${tests}""\necho ""tests=${tests}"" >> $GITHUB_OUTPUT\necho ""tests_glob: ${tests_glob}""\necho ""tests_glob=${tests_glob}"" >> $GITHUB_OUTPUT\n', 'python -m venv venv\n. venv/bin/activate\npython --version\npip install ""$(cat requirements_test.txt | grep pre-commit)""\n', '. venv/bin/activate\npre-commit install-hooks\n', '. venv/bin/activate\npre-commit run --hook-stage manual black --all-files --show-diff-on-failure\n', '. venv/bin/activate\nshopt -s globstar\npre-commit run --hook-stage manual black --files {homeassistant,tests}/components/${{ needs.info.outputs.integrations_glob }}/{*,**/*} --show-diff-on-failure\n', 'echo ""::add-matcher::.github/workflows/matchers/ruff.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual ruff --all-files --show-diff-on-failure\n', '. venv/bin/activate\nshopt -s globstar\npre-commit run --hook-stage manual ruff --files {homeassistant,tests}/components/${{ needs.info.outputs.integrations_glob }}/{*,**/*} --show-diff-on-failure\n', '. venv/bin/activate\npre-commit run --hook-stage manual isort --all-files --show-diff-on-failure\n', 'echo ""::add-matcher::.github/workflows/matchers/yamllint.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual yamllint --all-files --show-diff-on-failure\n', 'echo ""::add-matcher::.github/workflows/matchers/check-json.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual check-json --all-files\n', '. venv/bin/activate\npre-commit run --hook-stage manual prettier --all-files\n', '. venv/bin/activate\nshopt -s globstar\npre-commit run --hook-stage manual prettier --files {homeassistant,tests}/components/${{ needs.info.outputs.integrations_glob }}/{*,**/*}\n', 'echo ""::add-matcher::.github/workflows/matchers/check-executables-have-shebangs.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual check-executables-have-shebangs --all-files\n', 'echo ""::add-matcher::.github/workflows/matchers/codespell.json""\n', '. venv/bin/activate\npre-commit run --show-diff-on-failure --hook-stage manual codespell --all-files\n', 'echo ""::add-matcher::.github/workflows/matchers/hadolint.json""\n', '. venv/bin/activate\npre-commit run --hook-stage manual bandit --all-files --show-diff-on-failure\n', '. venv/bin/activate\nshopt -s globstar\npre-commit run --hook-stage manual bandit --files {homeassistant,tests}/components/${{ needs.info.outputs.integrations_glob }}/{*,**/*} --show-diff-on-failure\n', 'echo ""key=pip-${{ env.PIP_CACHE_VERSION }}-${{\n  env.HA_SHORT_VERSION }}-$(date -u \'+%Y-%m-%dT%H:%M:%s\')"" >> $GITHUB_OUTPUT', 'sudo apt-get update\nsudo apt-get -y install \\\n  bluez \\\n  ffmpeg \\\n  libavcodec-dev \\\n  libavdevice-dev \\\n  libavfilter-dev \\\n  libavformat-dev \\\n  libavutil-dev \\\n  libswresample-dev \\\n  libswscale-dev \\\n  libudev-dev\n', 'python -m venv venv\n. venv/bin/activate\npython --version\npip install --cache-dir=$PIP_CACHE -U ""pip>=21.0,<23.2"" setuptools wheel\npip install --cache-dir=$PIP_CACHE -r requirements_all.txt\npip install --cache-dir=$PIP_CACHE -r requirements_test.txt\npip install -e .\n', '. venv/bin/activate\npython -m script.hassfest --requirements --action validate\n', '. venv/bin/activate\npython -m script.gen_requirements_all validate\n', 'echo ""::add-matcher::.github/workflows/matchers/pylint.json""\n', '. venv/bin/activate\npython --version\npylint --ignore-missing-annotations=y homeassistant\n', '. venv/bin/activate\npython --version\npylint --ignore-missing-annotations=y homeassistant/components/${{ needs.info.outputs.integrations_glob }}\n', 'mypy_version=$(cat requirements_test.txt | grep mypy | cut -d \'=\' -f 3)\necho ""version=$mypy_version"" >> $GITHUB_OUTPUT\necho ""key=mypy-${{ env.MYPY_CACHE_VERSION }}-$mypy_version-${{\n  env.HA_SHORT_VERSION }}-$(date -u \'+%Y-%m-%dT%H:%M:%s\')"" >> $GITHUB_OUTPUT\n', 'echo ""::add-matcher::.github/workflows/matchers/mypy.json""\n', '. venv/bin/activate\npython --version\nmypy homeassistant pylint\n', '. venv/bin/activate\npython --version\nmypy homeassistant/components/${{ needs.info.outputs.integrations_glob }}\n', 'sudo apt-get update\nsudo apt-get -y install \\\n  bluez \\\n  ffmpeg\n', 'echo ""::add-matcher::.github/workflows/matchers/python.json""\n', "". venv/bin/activate\n# Ideally this should be part of our dependencies\n# However this plugin is fairly new and doesn't run correctly\n# on a non-GitHub environment.\npip install pytest-github-actions-annotate-failures==0.1.3\n"", 'echo ""::add-matcher::.github/workflows/matchers/pytest-slow.json""\n', '. venv/bin/activate\npython3 -m script.translations develop --all\n', '. venv/bin/activate\npython --version\npython3 -X dev -m pytest \\\n  -qq \\\n  --timeout=9 \\\n  --durations=10 \\\n  -n auto \\\n  --dist=loadfile \\\n  --test-group-count ${{ needs.info.outputs.test_group_count }} \\\n  --test-group=${{ matrix.group }} \\\n  --cov=""homeassistant"" \\\n  --cov-report=xml \\\n  -o console_output_style=count \\\n  -p no:sugar \\\n  tests\n', '. venv/bin/activate\npython --version\n\nif [[ ! -f ""tests/components/${{ matrix.group }}/__init__.py"" ]]; then\n  echo ""::error:: missing file tests/components/${{ matrix.group }}/__init__.py""\n  exit 1\nfi\n\npython3 -X dev -m pytest \\\n  -qq \\\n  --timeout=9 \\\n  -n auto \\\n  --cov=""homeassistant.components.${{ matrix.group }}"" \\\n  --cov-report=xml \\\n  --cov-report=term-missing \\\n  -o console_output_style=count \\\n  --durations=0 \\\n  --durations-min=1 \\\n  -p no:sugar \\\n  tests/components/${{ matrix.group }}\n', './script/check_dirty\n', 'sudo apt-get update\nsudo apt-get -y install \\\n  bluez \\\n  ffmpeg \\\n  libmariadb-dev-compat\n', 'echo ""::add-matcher::.github/workflows/matchers/python.json""\n', "". venv/bin/activate\n# Ideally this should be part of our dependencies\n# However this plugin is fairly new and doesn't run correctly\n# on a non-GitHub environment.\npip install pytest-github-actions-annotate-failures==0.1.3\n"", 'echo ""::add-matcher::.github/workflows/matchers/pytest-slow.json""\n', '. venv/bin/activate\npip install mysqlclient sqlalchemy_utils\n', '. venv/bin/activate\npython3 -m script.translations develop --all\n', '. venv/bin/activate\npython --version\n\npython3 -X dev -m pytest \\\n  -qq \\\n  --timeout=20 \\\n  -n 1 \\\n  --cov=""homeassistant.components.recorder"" \\\n  --cov-report=xml \\\n  --cov-report=term-missing \\\n  -o console_output_style=count \\\n  --durations=10 \\\n  -p no:sugar \\\n  --dburl=mysql://root:password@127.0.0.1/homeassistant-test \\\n  tests/components/history \\\n  tests/components/logbook \\\n  tests/components/recorder \\\n  tests/components/sensor\n', './script/check_dirty\n', 'sudo apt-get update\nsudo apt-get -y install \\\n  bluez \\\n  ffmpeg \\\n  postgresql-server-dev-14\n', 'echo ""::add-matcher::.github/workflows/matchers/python.json""\n', "". venv/bin/activate\n# Ideally this should be part of our dependencies\n# However this plugin is fairly new and doesn't run correctly\n# on a non-GitHub environment.\npip install pytest-github-actions-annotate-failures==0.1.3\n"", 'echo ""::add-matcher::.github/workflows/matchers/pytest-slow.json""\n', '. venv/bin/activate\npip install psycopg2 sqlalchemy_utils\n', '. venv/bin/activate\npython3 -m script.translations develop --all\n', '. venv/bin/activate\npython --version\n\npython3 -X dev -m pytest \\\n  -qq \\\n  --timeout=9 \\\n  -n 1 \\\n  --cov=""homeassistant.components.recorder"" \\\n  --cov-report=xml \\\n  --cov-report=term-missing \\\n  -o console_output_style=count \\\n  --durations=0 \\\n  --durations-min=10 \\\n  -p no:sugar \\\n  --dburl=postgresql://postgres:password@127.0.0.1/homeassistant-test \\\n  tests/components/history \\\n  tests/components/logbook \\\n  tests/components/recorder \\\n  tests/components/sensor\n', './script/check_dirty\n', 'export LOKALISE_TOKEN=""${{ secrets.LOKALISE_TOKEN }}""\npython3 -m script.translations upload\n', 'if [[ ${{ github.event_name }} =~ (schedule|workflow_dispatch) ]]; then\n  touch requirements_diff.txt\nelse\n  curl -s -o requirements_diff.txt https://raw.githubusercontent.com/home-assistant/core/master/requirements.txt\nfi\n', '(\n  echo ""GRPC_BUILD_WITH_BORING_SSL_ASM=false""\n  echo ""GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=true""\n  echo ""GRPC_PYTHON_BUILD_WITH_CYTHON=true""\n  echo ""GRPC_PYTHON_DISABLE_LIBC_COMPATIBILITY=true""\n  # GRPC on armv7 needs -lexecinfo (issue #56669) since home assistant installs\n  # execinfo-dev when building wheels. The setuptools build setup does not have an option for\n  # adding a single LDFLAG so copy all relevant linux flags here (as of 1.43.0)\n  echo ""GRPC_PYTHON_LDFLAGS=-lpthread -Wl,-wrap,memcpy -static-libgcc -lexecinfo""\n\n  # Fix out of memory issues with rust\n  echo ""CARGO_NET_GIT_FETCH_WITH_CLI=true""\n\n  # OpenCV headless installation\n  echo ""CI_BUILD=1""\n  echo ""ENABLE_HEADLESS=1""\n\n  # Use C-Extension for sqlalchemy\n  echo ""REQUIRE_SQLALCHEMY_CEXT=1""\n) > .env_file\n', 'requirement_files=""requirements_all.txt requirements_diff.txt""\nfor requirement_file in ${requirement_files}; do\n  sed -i ""s|# pybluez|pybluez|g"" ${requirement_file}\n  sed -i ""s|# beacontools|beacontools|g"" ${requirement_file}\n  sed -i ""s|# fritzconnection|fritzconnection|g"" ${requirement_file}\n  sed -i ""s|# pyuserinput|pyuserinput|g"" ${requirement_file}\n  sed -i ""s|# evdev|evdev|g"" ${requirement_file}\n  sed -i ""s|# pycups|pycups|g"" ${requirement_file}\n  sed -i ""s|# homekit|homekit|g"" ${requirement_file}\n  sed -i ""s|# decora_wifi|decora_wifi|g"" ${requirement_file}\n  sed -i ""s|# python-gammu|python-gammu|g"" ${requirement_file}\n  sed -i ""s|# opencv-python-headless|opencv-python-headless|g"" ${requirement_file}\ndone\n', '# We split requirements all into two different files.\n# This is to prevent the build from running out of memory when\n# resolving packages on 32-bits systems (like armhf, armv7).\n\nsplit -l $(expr $(expr $(cat requirements_all.txt | wc -l) + 1) / 2) requirements_all.txt requirements_all.txt\n', 'if [ ""${{ matrix.arch }}"" = ""i386"" ]; then\n  echo ""NPY_DISABLE_SVML=1"" >> .env_file\nfi\n\n(\n  # cmake > 3.22.2 have issue on arm\n  # Tested until 3.22.5\n  echo ""cmake==3.22.2""\n) >> homeassistant/package_constraints.txt\n\n# Do not pin numpy in wheels building\nsed -i ""/numpy/d"" homeassistant/package_constraints.txt\n', '(\n  echo ""GRPC_BUILD_WITH_BORING_SSL_ASM=false""\n  echo ""GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=true""\n  echo ""GRPC_PYTHON_BUILD_WITH_CYTHON=true""\n  echo ""GRPC_PYTHON_DISABLE_LIBC_COMPATIBILITY=true""\n\n  # GRPC on armv7 needed -lexecinfo (issue #56669) since home assistant installed\n  # execinfo-dev when building wheels. However, this package is no longer available\n  # Alpine 3.17, which we use for the cp311 ABI, so the flag should no longer be needed.\n  echo ""GRPC_PYTHON_LDFLAGS=-lpthread -Wl,-wrap,memcpy -static-libgcc"" # -lexecinfo\n\n  # Fix out of memory issues with rust\n  echo ""CARGO_NET_GIT_FETCH_WITH_CLI=true""\n\n  # OpenCV headless installation\n  echo ""CI_BUILD=1""\n  echo ""ENABLE_HEADLESS=1""\n\n  # Use C-Extension for sqlalchemy\n  echo ""REQUIRE_SQLALCHEMY_CEXT=1""\n) > .env_file\n', 'requirement_files=""requirements_all.txt requirements_diff.txt""\nfor requirement_file in ${requirement_files}; do\n\n  # PyBluez no longer compiles. Commented it out for now.\n  # It need further cleanup down the line, as all machine images\n  # try to install it.\n  # sed -i ""s|# pybluez|pybluez|g"" ${requirement_file}\n\n  # beacontools requires PyBluez.\n  # sed -i ""s|# beacontools|beacontools|g"" ${requirement_file}\n\n  # It doesn\'t build for some reason, so we skip it for now.\n  # Bumping to the latest version (4.7.0.72) supporting Python 3.11\n  # doesn\'t help. Reverted bump in #91871. There are 8 registered\n  # instances using this integration according to analytics.\n  # sed -i ""s|# opencv-python-headless|opencv-python-headless|g"" ${requirement_file}\n\n  sed -i ""s|# fritzconnection|fritzconnection|g"" ${requirement_file}\n  sed -i ""s|# pyuserinput|pyuserinput|g"" ${requirement_file}\n  sed -i ""s|# evdev|evdev|g"" ${requirement_file}\n  sed -i ""s|# pycups|pycups|g"" ${requirement_file}\n  sed -i ""s|# homekit|homekit|g"" ${requirement_file}\n  sed -i ""s|# decora_wifi|decora_wifi|g"" ${requirement_file}\n  sed -i ""s|# python-gammu|python-gammu|g"" ${requirement_file}\n\n  # Some packages are not buildable on armhf anymore\n  if [ ""${{ matrix.arch }}"" = ""armhf"" ]; then\n\n    # Pandas has issues building on armhf, it is expected they\n    # will drop the platform in the near future (they consider it\n    # ""flimsy"" on 386). The following packages depend on pandas,\n    # so we comment them out.\n    sed -i ""s|env_canada|# env_canada|g"" ${requirement_file}\n    sed -i ""s|noaa-coops|# noaa-coops|g"" ${requirement_file}\n    sed -i ""s|pyezviz|# pyezviz|g"" ${requirement_file}\n    sed -i ""s|pykrakenapi|# pykrakenapi|g"" ${requirement_file}\n  fi\ndone\n', '# We split requirements all into two different files.\n# This is to prevent the build from running out of memory when\n# resolving packages on 32-bits systems (like armhf, armv7).\n\nsplit -l $(expr $(expr $(cat requirements_all.txt | wc -l) + 1) / 2) requirements_all.txt requirements_all.txt\n', 'if [ ""${{ matrix.arch }}"" = ""i386"" ]; then\n  echo ""NPY_DISABLE_SVML=1"" >> .env_file\nfi\n\n# Probably not an issue anymore. Removing for now.\n# (\n#   # cmake > 3.22.2 have issue on arm\n#   # Tested until 3.22.5\n#   echo ""cmake==3.22.2""\n# ) >> homeassistant/package_constraints.txt\n\n# Do not pin numpy in wheels building\nsed -i ""/numpy/d"" homeassistant/package_constraints.txt\n']"
"['pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pipx install --pip-args=--constraint=.github/workflows/poetry-constraints.txt poetry\npoetry --version\n', 'poetry install', './scripts/test_integration.sh', 'pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pip install --constraint=.github/workflows/poetry-constraints.txt poetry\npoetry --version\n', 'poetry install', './scripts/lint.sh', 'pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pip install --constraint=.github/workflows/poetry-constraints.txt poetry\npoetry --version\n', 'echo ""::set-output name=sha::$(git rev-parse --verify --quiet HEAD^)""\n', ""poetry version patch &&\nversion=$(poetry version | awk '{ print $2 }') &&\npoetry version $version.dev.$(date +%s)\n"", 'poetry build --ansi\n', 'pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pipx install --pip-args=--constraint=.github/workflows/poetry-constraints.txt poetry\npoetry --version\n', 'poetry install', './scripts/test.sh']"
"['python -m pip install setuptools wheel', 'python setup.py sdist bdist_wheel', 'python -m pip install --upgrade pip setuptools wheel\npip install coverage pep8 pytest\npip install -e .\n', 'coverage run -m pytest --doctest-modules toolz/\npytest bench/\npep8 --ignore=""E731,W503,E402"" --exclude=conf.py,tests,examples,bench -r --show-source .\n', 'coverage report --show-missing --fail-under=100\npip install coveralls\ncoverage report --show-missing\ncoveralls --service=github\n']"
""
"['set -x\n# ID this runner\nasv machine --yes\necho ""Baseline: ${{ steps.previoustag.outputs.tag }} ""\necho ""Contender: ${{ github.sha }}""\n# Use mamba for env creation\n# export CONDA_EXE=$(which mamba)\nexport CONDA_EXE=$(which conda)\n# Run benchmarks for current commit against base\nASV_OPTIONS=""--split --show-stderr --factor $ASV_FACTOR""\nasv continuous $ASV_OPTIONS ${{ steps.previoustag.outputs.tag }}  ${{ github.sha }} \\\n    | sed ""/Traceback \\|failed$\\|PERFORMANCE DECREASED/ s/^/::error::/"" \\\n    | tee benchmarks.log\n# Report and export results for subsequent steps\nif grep ""Traceback \\|failed\\|PERFORMANCE DECREASED"" benchmarks.log > /dev/null ; then\n    exit 1\nfi\n', 'cp benchmarks/README_CI.md benchmarks.log .asv/results/\n', 'set -x\n# ID this runner\nasv machine --yes\necho ""Baseline:  ${{ github.event.pull_request.base.sha }} (${{ github.event.pull_request.base.label }})""\necho ""Contender: ${GITHUB_SHA} (${{ github.event.pull_request.head.label }})""\n# Use mamba for env creation\n# export CONDA_EXE=$(which mamba)\nexport CONDA_EXE=$(which conda)\n# Run benchmarks for current commit against base\nASV_OPTIONS=""--split --show-stderr --factor $ASV_FACTOR""\nasv continuous $ASV_OPTIONS ${{ github.event.pull_request.base.sha }} ${GITHUB_SHA} \\\n    | sed ""/Traceback \\|failed$\\|PERFORMANCE DECREASED/ s/^/::error::/"" \\\n    | tee benchmarks.log\n# Report and export results for subsequent steps\nif grep ""Traceback \\|failed\\|PERFORMANCE DECREASED"" benchmarks.log > /dev/null ; then\n    exit 1\nfi\n', 'cp benchmarks/README_CI.md benchmarks.log .asv/results/\n', 'echo ""TODAY=$(date  +\'%Y-%m-%d\')"" >> $GITHUB_ENV\n', 'python -m pip install --no-deps -e .\n', 'conda info -a\nconda list\npython xarray/util/print_versions.py\n', '# Raise an error if there are warnings in the doctests, with `-Werror`.\n# This is a trial; if it presents an problem, feel free to remove.\n# See https://github.com/pydata/xarray/issues/7164 for more info.\npython -m pytest --doctest-modules xarray --ignore xarray/tests -Werror\n', 'echo ""TODAY=$(date +\'%Y-%m-%d\')"" >> $GITHUB_ENV\n', 'python -m pip install --no-deps -e .\n', 'conda info -a\nconda list\npython xarray/util/print_versions.py\n', 'python -m pip install mypy --force-reinstall\n', 'python -m mypy --install-types --non-interactive --cobertura-xml-report mypy_report\n', 'echo ""TODAY=$(date +\'%Y-%m-%d\')"" >> $GITHUB_ENV\n', 'python -m pip install --no-deps -e .\n', 'conda info -a\nconda list\npython xarray/util/print_versions.py\n', 'python -m pip install mypy --force-reinstall\n', 'python -m mypy --install-types --non-interactive --cobertura-xml-report mypy_report\n', 'python ci/min_deps_check.py ci/requirements/${{ matrix.environment-file }}.yml\n', 'echo ""TODAY=$(date +\'%Y-%m-%d\')"" >> $GITHUB_ENV\n\nif [[ ""${{matrix.python-version}}"" == ""3.11"" ]]; then\n  if [[ ${{matrix.os}} == windows* ]]; then\n    echo ""CONDA_ENV_FILE=ci/requirements/environment-windows-py311.yml"" >> $GITHUB_ENV\n  else\n    echo ""CONDA_ENV_FILE=ci/requirements/environment-py311.yml"" >> $GITHUB_ENV\n  fi\nelif [[ ${{ matrix.os }} == windows* ]] ;\nthen\n  echo ""CONDA_ENV_FILE=ci/requirements/environment-windows.yml"" >> $GITHUB_ENV\nelif [[ ""${{ matrix.env }}"" != """" ]] ;\nthen\n  if [[ ""${{ matrix.env }}"" == ""flaky"" ]] ;\n  then\n    echo ""CONDA_ENV_FILE=ci/requirements/environment.yml"" >> $GITHUB_ENV\n    echo ""PYTEST_EXTRA_FLAGS=--run-flaky --run-network-tests"" >> $GITHUB_ENV\n  else\n    echo ""CONDA_ENV_FILE=ci/requirements/${{ matrix.env }}.yml"" >> $GITHUB_ENV\n  fi\nelse\n  echo ""CONDA_ENV_FILE=ci/requirements/environment.yml"" >> $GITHUB_ENV\nfi\n\necho ""PYTHON_VERSION=${{ matrix.python-version }}"" >> $GITHUB_ENV\n', 'python -m pip install pytest-github-actions-annotate-failures\n', 'python -m pip install --no-deps -e .\n', 'conda info -a\nconda list\npython xarray/util/print_versions.py\n', 'python -c ""import xarray""\n', 'python -m pytest -n 4 --timeout 180 --cov=xarray --cov-report=xml --junitxml=pytest.xml $PYTEST_EXTRA_FLAGS', 'python -m pip install --upgrade pip\npython -m pip install build twine\n', 'git clean -xdf\ngit restore -SW .\npython -m build\n', 'python -m twine check --strict dist/*\npwd\nif [ -f dist/xarray-0.0.0.tar.gz ]; then\n    echo ""âŒ INVALID VERSION NUMBER""\n    exit 1\nelse\n    echo ""âœ… Looks good""\nfi\n', 'mkdir artifacts && cd artifacts\n\nartifacts_url=${{ github.event.workflow_run.artifacts_url }}\n\ngh api ""$artifacts_url"" -q \'.artifacts[] | [.name, .archive_download_url] | @tsv\' | while read artifact\ndo\n  IFS=$\'\\t\' read name url <<< ""$artifact""\n  gh api $url > ""$name.zip""\n  unzip -d ""$name"" ""$name.zip""\ndone\n', 'python -m pip install --upgrade pip\npython -m pip install build twine\n', 'git clean -xdf\ngit restore -SW .\npython -m build\n', 'python -m twine check --strict dist/*\npwd\nif [ -f dist/xarray-0.0.0.tar.gz ]; then\n  echo ""âŒ INVALID VERSION NUMBER""\n  exit 1\nelse\n  echo ""âœ… Looks good""\nfi\n', 'ls -ltrh\nls -ltrh dist\n', 'python -m pip install --upgrade pip\npython -m pip install dist/xarray*.whl\npython -m xarray.util.print_versions\n', 'bash ci/install-upstream-wheels.sh\n', 'python -m pip install --no-deps -e .\n', 'conda info -a\nconda list\npython xarray/util/print_versions.py\n', ""python -c 'import xarray'\n"", 'export ZARR_V3_EXPERIMENTAL_API=1\npython -m pytest --timeout=60 -rf \\\n  --report-log output-${{ matrix.python-version }}-log.jsonl\n', 'bash ci/install-upstream-wheels.sh\n', 'python -m pip install --no-deps -e .\n', 'conda info -a\nconda list\npython xarray/util/print_versions.py\n', 'python -m pip install mypy --force-reinstall\n', 'python -m mypy --install-types --non-interactive --cobertura-xml-report mypy_report\n']"
""
"['export PYTHONIOENCODING=UTF8\npip install coveralls pytest-cov ptyprocess\n', './tools/display-sighandlers.py\n./tools/display-terminalinfo.py\npy.test --cov pexpect --cov-config .coveragerc\n', 'coveralls --service=github\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""', 'python -m pip install --upgrade pip\npip install wheel\npip install -r requirements-dev.txt\npip install -e .\n', 'flaskbb translations compile\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest --cov=./ --cov-report=xml\n']"
"['python -m pip install --upgrade pip\npython -m pip install setuptools==59.8.0\npython -m pip install tox\ntox --notest\n', ""tox -- coverage run --source='viewflow' ./manage.py test --exclude-tag=selenium --exclude-tag=integration\ntox -- coverage lcov\n""]"
"['which python\npython --version\n', 'pip install pip -U\npip install black -U\npip install -r requirements.txt -U\n', 'black --diff .\nblack --check .\n', 'pip freeze\n', 'pytest --cov-config .coveragerc --cov-report html --cov-report term --cov=pgmpy --verbose']"
"['python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox -eflake8', 'tox -edocs', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'SQLALCHEMY_VERSION=""${{ matrix.sqla }}"" tox']"
"['docker-compose pull', 'docker-compose build', 'docker-compose run web ls -l build/html']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'cd $GITHUB_WORKSPACE\necho\nls -F\necho\nenv\n', 'tox']"
"['pip install tox', 'tox -e py', 'pip install tox', 'tox -e style', 'pip install tox', 'tox -e docs']"
"['pip install .\npip install pytest\nsudo apt-get update\nsudo apt-get install -y bats\n', 'pytest\n', 'bats tests/shell\n']"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade tox tox-py\n', 'tox --py current']"
"['sudo apt-get update', 'sudo apt-get install -y -qq --no-install-recommends automake autoconf libtool autopoint gettext cython3 libglib2.0-dev python3-dev python-gi-dev libbluetooth-dev', 'CYTHONEXEC=cython3 ./autogen.sh', 'make', 'CYTHONEXEC=cython3 make distcheck', 'sudo make install', 'sudo apt-get update', 'sudo apt-get install -y -qq --no-install-recommends meson gettext cython3 libglib2.0-dev python3-dev python-gi-dev libbluetooth-dev', 'meson --warnlevel 3 --buildtype debug -Druntime_deps_check=false builddebug', 'ninja -v -C builddebug/', 'python3 -m pip install --user flake8', 'python3 -m flake8', 'apt-get update', 'apt-get install -y -qq --no-install-recommends automake autoconf libtool autopoint gettext libglib2.0-dev python-gi-dev libbluetooth-dev iproute2', 'python3 -m pip install cython mypy pycairo>=1.22.0', './autogen.sh', 'python3 -m mypy -p blueman --strict', 'apt-get update', 'apt-get install -y -qq --no-install-recommends automake autoconf libtool autopoint gettext libglib2.0-dev python-gi-dev libbluetooth-dev iproute2 libgirepository1.0-dev gir1.2-gtk-3.0 gir1.2-nm-1.0 libpulse0 libpulse-mainloop-glib0 libdbus-1-dev', 'python3 -m pip install cython pygobject python-dbusmock dbus-python', './autogen.sh', 'make -C module', 'touch /dev/rfkill', 'PYTHONPATH=module/.libs python3 -m unittest', 'sudo apt-get update', 'sudo apt-get install -y -qq --no-install-recommends autopoint gettext cython3 libglib2.0-dev python-gi-dev libbluetooth-dev', 'CYTHONEXEC=cython3 ./autogen.sh', 'make -C po blueman.pot-update', 'echo ""::set-output name=VERSION::${GITHUB_REF#refs/tags/}""', 'notes=""$(sed --quiet \'/^## ${{ steps.version.outputs.VERSION }}/,/^## /{/^## /b;p}\' CHANGELOG.md)""\nnotes=""${notes//\'%\'/\'%25\'}""\nnotes=""${notes//$\'\\n\'/\'%0A\'}""\nnotes=""${notes//$\'\\r\'/\'%0D\'}""\necho ::set-output name=NOTES::$notes\n', '[ -n ""${{ steps.notes.outputs.NOTES }}"" ] || (echo ""Failed to parse changelog"" && exit 1)', 'grep --quiet \'^AC_INIT(\\[blueman\\], \\[${{ steps.version.outputs.VERSION }}\\]\' configure.ac || (echo ""Did not find expected version in configure.ac"" && exit 1)', 'grep --quiet ""version: \'${{ steps.version.outputs.VERSION }}\'"" meson.build || (echo \'Did not find expected version in meson.build\' && exit 1)', 'sudo apt-get update', 'sudo apt-get install -y -qq --no-install-recommends automake autoconf libtool autopoint gettext cython3 libglib2.0-dev python3-dev python-gi-dev libbluetooth-dev', 'CYTHONEXEC=cython3 ./autogen.sh', 'CYTHONEXEC=cython3 make distcheck', 'curl -F file=@po/blueman.pot -F method=source -H ""Authorization: Token $TOKEN"" https://hosted.weblate.org/api/translations/blueman/blueman/en/file/']"
"['pip install nose2 numpy scipy scikit-learn\n', 'nose2 -v --pretty-assert']"
""
"['sudo apt-get install -qy \\\n  gettext \\\n  libkeyutils-dev \\\n  libgirepository1.0-dev\n', 'pip install -U pip', 'pip install setuptools wheel', 'python setup.py sdist bdist_wheel', 'pip install dist/*.whl keyutils', 'pip install twine flake8', 'twine check dist/*', 'flake8', 'python test/test_cache.py', 'python test/test_match.py', 'pip install twine', 'twine upload dist/*.whl dist/*.tar.gz', 'sudo apt-get install -qy gettext', './lang/report.sh | tee before.tsv | column -t', 'make -BC lang', './lang/report.sh | tee after.tsv | column -t', '.github/pr_text.py | tee summary.md', '.github/set-output summary <summary.md', 'rm before.tsv after.tsv summary.md']"
"['sudo apt-get update\nsudo apt-get install libssl-dev libkrb5-dev\npython -m pip install --upgrade pip\npip install -r dev/requirements-dev.txt\npython dev/build.py \\\n    --ws-dir=./freetds \\\n    --dist-dir=./dist \\\n    --freetds-version=""1.3.13"" \\\n    --with-openssl=yes \\\n    --enable-krb5 \\\n    --sdist \\\n    --static-freetds\npip install pymssql --no-index -f dist\npython -c ""import pymssql; print(pymssql.version_info())""\n', 'pytest -sv\n', 'docker pull $DOCKER_IMAGE\ndocker run --rm --net=""host"" -w=/io -v `pwd`:/io -e MANYLINUX=${{ matrix.manylinux }} $DOCKER_IMAGE /io/dev/build_manylinux_wheels.sh\nls -la dist\n', 'pip install twine --upgrade\ntwine upload --skip-existing -u __token__ -p ${{secrets.PYMSSQL_PYPI_TOKEN}} dist/*\n', 'pip install twine --upgrade\ntwine upload --skip-existing -u __token__ -p ${{secrets.PYMSSQL_TEST_PYPI_TOKEN}} --repository-url=https://test.pypi.org/legacy/ dist/*\n', 'pip install --upgrade pip\npip install -r dev/requirements-dev.txt\npip install delocate\npython setup.py sdist\nbrew install openssl@1.1\nbrew install libiconv\npython dev/build.py \\\n    --ws-dir=./freetds \\\n    --dist-dir=./dist \\\n    --freetds-version=""1.3.13"" \\\n    --with-openssl=yes \\\n    --sdist \\\n    --static-freetds\ndelocate-listdeps --all dist/*.whl\ndelocate-wheel -v dist/*.whl\ndelocate-listdeps --all dist/*.whl\npip install pymssql --no-index -f dist\npython -c ""import pymssql; print(pymssql.version_info())""\n', 'pytest -sv\n', 'pip install twine --upgrade\ntwine upload --skip-existing -u __token__ -p ${{secrets.PYMSSQL_PYPI_TOKEN}} dist/*.whl\n', 'pip install twine --upgrade\ntwine upload --skip-existing -u __token__ -p ${{secrets.PYMSSQL_TEST_PYPI_TOKEN}} --repository-url=https://test.pypi.org/legacy/ dist/*.whl\n', 'choco install openssl --forcex86', 'choco install openssl', 'choco install gperf\npython -m pip install --upgrade pip\npip install -r dev/requirements-dev.txt\npython dev/build.py --ws-dir=freetds --dist-dir=dist --sdist --freetds-version=""1.3.13""\npip install pymssql --no-index -f dist\npython -c ""import pymssql; print(pymssql.version_info())""\n', 'pytest -sv\n', 'pip install twine --upgrade\ntwine upload --skip-existing -u __token__ -p ${{secrets.PYMSSQL_PYPI_TOKEN}} dist/*.whl\n', 'pip install twine --upgrade\ntwine upload --skip-existing -u __token__ -p ${{secrets.PYMSSQL_TEST_PYPI_TOKEN}} --repository-url=https://test.pypi.org/legacy/ dist/*.whl\n']"
"['git config user.name github-actions\ngit config user.email github-actions@github.com\ngit merge origin/3.0 -X theirs\n', 'python -m pip install --upgrade pip\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'python setup.py install\n', 'cd scripts\npython3 publish_api_documentation.py\n', 'git add docs\ngit status\ngit commit -m ""Publish API documentation""\ngit push\n', 'python -m pip install --upgrade pip\nif [ -f dev-requirements.txt ]; then pip install -r dev-requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest -vv\n']"
""
"['sudo apt-get install -y gettext', 'python -m pip install -U pip\npython -m pip install ""Django~=${{ matrix.django }}"" codecov -e .[tests]\n', 'echo ""Python ${{ matrix.python }} / Django ${{ matrix.django }}""\ncoverage run --rcfile=.coveragerc runtests.py\ncodecov\n']"
""
"['gh pr merge --auto --squash ""$PR_URL""', 'python -m pip install -U pip wheel setuptools build twine\n', 'python -m pip install -r requirements/lint.txt -c requirements/constraints.txt\n', 'python -m pip install .\n', 'make mypy\n', '# Some extra requirements are needed to ensure all modules\n# can be scanned by slotscheck.\npip install -r requirements/base.txt -c requirements/constraints.txt\nslotscheck -v -m aiohttp\n', 'sudo apt install libenchant-2-dev\n', 'pip install -r requirements/doc-spelling.txt -c requirements/constraints.txt\n', '# towncrier --yes  # uncomment me after publishing a release\nmake doc-spelling\n', 'python -m build\n', 'twine check --strict dist/*\n', 'LC_ALL=C sort --check --ignore-case CONTRIBUTORS.txt\n', 'make generate-llhttp\n', 'echo ""::set-output name=dir::$(pip cache dir)""    # - name: Cache\n', 'python -m pip install -U pip wheel setuptools build twine\n', 'python -m pip install -r requirements/test.txt -c requirements/constraints.txt\n', 'make cythonize\n', 'PATH=""${HOME}/Library/Python/3.11/bin:${HOME}/.local/bin:${PATH}"" make test', 'pytest --no-cov -vvvvv --lf && exit 1', 'python -m coverage xml\n', 'echo ""Predeploy step""\n', 'python -m pip install -U pip wheel setuptools build twine\n', 'python -m pip install -r requirements/cython.txt -c requirements/constraints.txt\n', 'make cythonize\n', 'python -m build --sdist\n', 'if [[ -n ""${{ matrix.qemu }}"" ]]; then\n  # Build emulated architectures only if QEMU is set,\n  # use default ""auto"" otherwise\n  echo ""CIBW_ARCHS_LINUX=${{ matrix.qemu }}"" >> $GITHUB_ENV\nfi\n', 'python -m pip install -U pip wheel setuptools build twine\n', 'python -m pip install -r requirements/cython.txt -c requirements/constraints.txt\n', 'make cythonize\n', 'python -m pip install -U pip wheel setuptools build twine\n', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | gh auth login --with-token\n', 'tree dist\n', 'echo ""${{ secrets.GITHUB_TOKEN }}"" | gh auth login --with-token\n', 'gh pr checkout ${{ github.event.pull_request.number }}\n', 'python -m pip install -U pip-tools\n', 'make compile-deps\n', 'pre-commit autoupdate']"
"['sudo ./utils/searx.sh install packages\n', 'make V=1 install\n', 'V=1 ./manage pyenv.cmd python ""$FETCH_SCRIPT""\n', 'echo ""Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}""\necho ""Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}""\n', 'sudo ./utils/searx.sh install packages\nsudo apt install firefox\n', 'make V=1 install\nmake V=1 gecko.driver\n', 'make V=1 ci.test', 'make V=1 test.coverage', 'sudo ./utils/searx.sh install packages', 'make V=1 node.env', 'make V=1 themes.all', 'sudo ./utils/searx.sh install buildhost', 'make V=1 docs.clean docs.html\n', 'make -e GIT_URL=$(git remote get-url origin) docker.push']"
""
"['python -m pip install --upgrade pip\n', 'make dirhtml JOBS=$(nproc)', 'rm -r build/.doctrees/', 'curl -H ""Accept: application/json"" -H ""Fastly-Key: $FASTLY_TOKEN"" -X POST ""https://api.fastly.com/service/$FASTLY_SERVICE_ID/purge_all""\n', 'python -m pip install -U pip\npython -m pip install -U wheel\npython -m pip install -U tox\n', 'tox -e py -- -v --cov-report term\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install -r requirements.txt\npip install -r requirements-test.txt\n', 'flake8 --exclude cast_channel_pb2.py,authority_keys_pb2.py,logging_pb2.py examples pychromecast\n', 'pylint examples pychromecast\n', 'black examples pychromecast --check --diff\n']"
"['echo CROSSBAR_BUILD_DATE=`date -u +""%Y-%m-%d""` >> $GITHUB_ENV\necho CROSSBAR_VCS_REF=`git rev-parse --short ${GITHUB_SHA}` >> $GITHUB_ENV\necho CROSSBAR_BUILD_ID=$(date --utc +%Y%m%d)-$(git rev-parse --short ${GITHUB_SHA}) >> $GITHUB_ENV\necho CROSSBAR_VERSION=$(grep -E \'^(__version__)\' ./crossbar/_version.py | cut -d \' \' -f3 | sed -e \'s|[u""\'\\\'\']||g\') >> $GITHUB_ENV\n', 'echo """"\necho ""Build environment configured:""\necho """"\necho ""  CROSSBAR_BUILD_DATE = ${CROSSBAR_BUILD_DATE}""\necho ""  CROSSBAR_BUILD_ID   = ${CROSSBAR_BUILD_ID}""\necho ""  CROSSBAR_VCS_REF    = ${CROSSBAR_VCS_REF}""\necho ""  CROSSBAR_VERSION    = ${CROSSBAR_VERSION}""\necho """"\necho ""Wheels (publish):""\necho ""  AWS_DEFAULT_REGION  = ${AWS_DEFAULT_REGION}""\necho ""  AWS_S3_BUCKET_NAME  = ${AWS_S3_BUCKET_NAME}""\necho """"\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', './deploy-docs.sh\n', 'echo CROSSBAR_BUILD_DATE=`date -u +""%Y-%m-%d""` >> $GITHUB_ENV\necho CROSSBAR_VCS_REF=`git rev-parse --short ${GITHUB_SHA}` >> $GITHUB_ENV\necho CROSSBAR_BUILD_ID=$(date --utc +%Y%m%d)-$(git rev-parse --short ${GITHUB_SHA}) >> $GITHUB_ENV\necho CROSSBAR_VERSION=$(grep -E \'^(__version__)\' ./crossbar/_version.py | cut -d \' \' -f3 | sed -e \'s|[u""\'\\\'\']||g\') >> $GITHUB_ENV\necho WAMP_PRIVATE_KEY=${WAMP_PRIVATE_KEY} >> $GITHUB_ENV\necho ACCESS_TOKEN=${ACCESS_TOKEN} >> $GITHUB_ENV\n', 'echo """"\necho ""Build environment configured:""\necho """"\necho ""  CROSSBAR_BUILD_DATE = ${CROSSBAR_BUILD_DATE}""\necho ""  CROSSBAR_BUILD_ID   = ${CROSSBAR_BUILD_ID}""\necho ""  CROSSBAR_VCS_REF    = ${CROSSBAR_VCS_REF}""\necho ""  CROSSBAR_VERSION    = ${CROSSBAR_VERSION}""\necho """"\necho ""Wheels (publish):""\necho ""  AWS_DEFAULT_REGION  = ${AWS_DEFAULT_REGION}""\necho ""  AWS_S3_BUCKET_NAME  = ${AWS_S3_BUCKET_NAME}""\necho """"\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', './deploy-wheels.sh\n', 'curl -v -X POST -u ""${ACCESS_TOKEN}"" \\\n  -H ""Accept: application/vnd.github.everest-preview+json"" \\\n  -H ""Content-Type: application/json"" \\\n  --data \'{""event_type"":""build"", ""client_payload"": {""CROSSBAR_BUILD_DATE"": ""${CROSSBAR_BUILD_DATE}"", ""CROSSBAR_BUILD_ID"": ""${CROSSBAR_BUILD_ID}"", ""CROSSBAR_VCS_REF"": ""${CROSSBAR_VCS_REF}"", ""CROSSBAR_VERSION"": ""${CROSSBAR_VERSION}""}}\' \\\n  https://api.github.com/repos/crossbario/crossbar-binaries/dispatches\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'tox -c tox.ini -e flake8', 'tox -c tox.ini -e yapf', 'tox -c tox.ini -e mypy', 'tox -c tox.ini -e bandit', 'pip install -r requirements-dev.txt\npip install -U pip setuptools\n', 'pip install -U --force-reinstall -r requirements-latest.txt\npip install .\n', 'tox -c tox.ini -e sphinx', 'pip install -r requirements-dev.txt\npip install -U pip setuptools\n', 'pip install -U --force-reinstall -r requirements-latest.txt\npip install .\n', 'crossbar version\n', 'python ./test/test_imports.py\n', 'python -m twisted.trial crossbar\n', 'pytest -sv crossbar\n', 'pip install -r requirements-dev.txt\npip install -U pip setuptools\n', 'pip install -U --force-reinstall -r requirements-latest.txt\npip install .\n', 'crossbar version\n', './test/test_ab_examples.sh\n', 'pytest -sv --no-install ./test/functests/cbtests\n', 'python -m pip install --upgrade pip setuptools\npip install -r requirements-dev.txt\n', 'pip install -U -r requirements-latest.txt\npip install .\n', 'python -c ""import crossbar; print(crossbar.__version__)""\ncrossbar version\n', './test/test_setup.sh\npytest -sv --no-install ./test/functests/cfctests\n']"
"['pip install ""flake8<6.0.0""', 'pip install mypy\n', 'mypy ../SublimeLinter --platform=${{ matrix.platform }}\n', ""python -m pip install --upgrade pip\npip install Sphinx sphinx_rtd_theme\necho '::add-matcher::'$PWD/.github/python-matcher.json\n"", 'sphinx-build -j auto -W --keep-going docs docs-out\n', ""python -m pip install --upgrade pip\npip install Sphinx sphinx_rtd_theme\necho '::add-matcher::'$PWD/.github/python-matcher.json\n"", 'sphinx-build -j auto -b linkcheck -W --keep-going docs docs-out\n']"
"['# echo ""starting run""\nBLENDER_VERSION=2.93\nBLENDER_URL=https://ftp.nluug.nl/pub/graphics/blender/release/Blender2.93/blender-2.93.1-linux-x64.tar.xz\nSVERCHOK_DIR=scripts/addons/sverchok\nBLENDER_TAR=$(basename $BLENDER_URL)\nBLENDER_DIR=$(basename $BLENDER_URL .tar.xz)\nRELEASE=blender293_LTS\n\nif [ ! -f installation/blender/blender ]\nthen\n    mkdir -p installation\n    cd installation\n\n    # not verbose, but not quiet.\n    wget -nv $BLENDER_URL\n\n    # unpack this tar to a known name\n    mkdir $RELEASE\n    tar xf $BLENDER_TAR -C $RELEASE --strip-components 1\n\n    rm $BLENDER_TAR\n    mv $RELEASE blender\n\n    pushd blender/\n    PYTHON=${BLENDER_VERSION}/python/bin/python3.9\n    $PYTHON -m ensurepip\n    $PYTHON -m pip install --upgrade pip setuptools wheel\n    $PYTHON -m pip install --upgrade scipy geomdl scikit-image\n    $PYTHON -m pip install --upgrade pdoc3\n    popd\n\n    cd ..\nfi\n\n#mkdir -p ${PWD}/installation/blender/${BLENDER_VERSION}/${SVERCHOK_DIR}\nln -s ${PWD} ${PWD}/installation/blender/${BLENDER_VERSION}/${SVERCHOK_DIR}\nmkdir -p ~/.config/blender/${BLENDER_VERSION}/config/\nln -s ${PWD}/tests/references/userpref.blend ~/.config/blender/${BLENDER_VERSION}/config/\n', 'bash ./generate_api_documentation.sh -o ""${GITHUB_WORKSPACE}/gh_pages/apidocs""\n', 'cd ""${GITHUB_WORKSPACE}/gh_pages""\ndate > generated.txt\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""update API documentation bot""\ngit push\n', 'pip install -U Sphinx', 'pip install sphinx-rtd-theme', 'sphinx-build -b html ""${GITHUB_WORKSPACE}/master/docs"" ""${GITHUB_WORKSPACE}/gh_pages/docs""', 'cd ""${GITHUB_WORKSPACE}/gh_pages""\ndate > generated.txt\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit add .\ngit commit -m ""update user documentation bot""\ngit push\n', '# echo ""starting run""\nBLENDER_VERSION=2.93\nBLENDER_URL=https://ftp.nluug.nl/pub/graphics/blender/release/Blender2.93/blender-2.93.1-linux-x64.tar.xz\n# BLENDER_URL=https://ftp.nluug.nl/pub/graphics/blender/release/Blender2.93/blender-2.93.0-linux-x64.tar.xz  # OLDER\nSVERCHOK_DIR=scripts/addons/sverchok\nBLENDER_TAR=$(basename $BLENDER_URL)\nBLENDER_DIR=$(basename $BLENDER_URL .tar.xz)\nRELEASE=blender293_LTS\n\nif [ ! -f installation/blender/blender ]\nthen\n    mkdir -p installation\n    cd installation\n\n    # not verbose, but not quiet.\n    wget -nv $BLENDER_URL\n\n    # unpack this tar to a known name\n    mkdir $RELEASE\n    tar xf $BLENDER_TAR -C $RELEASE --strip-components 1\n\n    rm $BLENDER_TAR\n    mv $RELEASE blender\n\n    pushd blender/\n    PYTHON=${BLENDER_VERSION}/python/bin/python3.9\n    $PYTHON -m ensurepip\n    $PYTHON -m pip install --upgrade pip setuptools wheel\n    $PYTHON -m pip install --upgrade scipy geomdl scikit-image\n    popd\n\n    cd ..\nfi\n\n#mkdir -p ${PWD}/installation/blender/${BLENDER_VERSION}/${SVERCHOK_DIR}\nln -s ${PWD} ${PWD}/installation/blender/${BLENDER_VERSION}/${SVERCHOK_DIR}\nmkdir -p ~/.config/blender/${BLENDER_VERSION}/config/\nln -s ${PWD}/tests/references/userpref.blend ~/.config/blender/${BLENDER_VERSION}/config/\n', 'bash ./run_tests.sh\n']"
""
"['sudo apt install -y gettext aspell libenchant-2-dev', 'python -m pip install -r requirements.txt', 'msgcheck -n hijack/locale/*/LC_MESSAGES/*.po', 'python -m pip install -r requirements.txt', '${{ matrix.lint-command }}', 'sudo apt install -y gettext', 'python -m pip install --upgrade pip build wheel twine readme-renderer', 'python -m build --sdist --wheel', 'python -m twine check dist/*', 'npm ci', 'npm run lint:scss', 'npm ci', 'npm run lint:js', 'sudo apt install -y gettext', 'python -m pip install -e .[test]', 'python -m pip install django~=${{ matrix.django-version }}.0', 'python -m pytest', 'sudo apt install -y gettext', 'python -m pip install --upgrade pip build wheel twine', 'python -m build --sdist --wheel', 'python -m twine upload dist/*']"
"['which python ; python --version ; which pip ; pip --version', 'get-command python ; python --version ; get-command pip ; pip --version', 'git --version', 'pip install wheel && pip install mercurial', 'pip install -r ./requirements-dev.txt', 'python test.py -v']"
"['pip install flake8\nflake8 src scripts conftest.py\n', 'bash scripts/ci/install-bind.sh\n', 'python -m pip install --upgrade pip setuptools wheel tox\n', 'tox --skip-missing-interpreters\n']"
"['chown -R root /github/home', 'python -m pip install pygobject tox', 'python -m tox -e ${{ matrix.tox }}', 'python -m tox -e ${{ matrix.tox }} -- --cov-report=xml', 'python3 -m pip install build', 'python3 -m build']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade setuptools pip wheel build twine\npip install -e .[test]\n', 'flake8 pyocd test --count --select=E9,F63,F7,F82 --show-source --statistics\n', 'pytest --junitxml=test-results-${{ matrix.os }}-${{ matrix.python-version }}.xml --cov=pyocd\n', 'python test/import_all.py\n', 'python -mbuild\n', 'twine check dist/*\n', 'echo ""Skipped due to path filter.""', 'git config --global user.email ""pyocd-bot-noreply@pyocd.io""\ngit config --global user.name ""pyocd-bot""\n', 'cd pyocd\ngit fetch origin\ngit switch --detach ${{ github.event.after }}\n', 'git add pyocd\ngit commit -m ""submodule: pyocd: sync to commit ${{ github.event.after }}""\ngit push origin\n']"
"['poetry lock\npoetry install --with docs\n', 'mkdocs build --clean', 'pip install --upgrade pip', 'pip install ruff', 'ruff check ${{ steps.changed-files.outputs.all_changed_files }}', 'poetry config pypi-token.pypi ${{secrets.PYPI_TOKEN}}\npoetry --build publish\n', 'curl -sSL https://install.python-poetry.org | python3 -', 'poetry lock\npoetry install --with dev            \n', 'git clone https://github.com/sir-kokabi/resources.git resources\n', 'poetry run poe test']"
"['pip install pyodide-build==0.23.0\necho EMSCRIPTEN_VERSION=$(pyodide config get emscripten_version) >> $GITHUB_ENV\n', 'pyodide build']"
"['echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['poetry install --no-interaction --no-ansi --only sphinx', 'source .venv/bin/activate\npython build_pkg.py\n', 'poetry install --no-interaction --no-ansi --no-root --only tooling', 'source .venv/bin/activate\ntox\n', 'poetry install --no-interaction --no-ansi --no-root --only docs', 'source .venv/bin/activate\npython docs/gen_docs.py\n']"
""
"['python -m pip install --upgrade pip\npip install pylint\npip install -r requirements.txt\npip install -r development.txt\npip install ntc_templates==1.4.1\npip install textfsm==0.4.1\npip install .\n', 'pip install -U black;\nblack --check --diff --exclude=""docs|build|tests|samples"" . \n', 'nosetests -v --with-coverage --cover-package=jnpr.junos --cover-inclusive -a unit\n']"
"['sudo apt-get -qq update && sudo apt-get -qq install python3-sphinx -y\npip install --upgrade pip && pip install build twine\n', 'cd docs\nmake html\ncd ..\n', 'python -m build\ntwine check dist/*\nls -l dist\n', 'pip install --upgrade pip\npip install -U wheel setuptools ""tox<4"" tox-gh-actions\n', 'tox', 'pip install --upgrade pip && pip install build twine', 'echo ""TAG=$(eval \'python setup.py version\')"" >> $GITHUB_ENV', 'python -m build\ntwine check dist/*\n', 'twine upload dist/*']"
"['string(TIMESTAMP current_date ""%Y-%m-%d-%H:%M:%S"" UTC)\nmessage(""::set-output name=timestamp::${current_date}"")\n', 'brew update-reset', 'brew update', 'set -x\nbrew install wget\nmkdir -p /tmp/clang\ncd /tmp/clang\nwget https://github.com/llvm/llvm-project/releases/download/llvmorg-15.0.0/clang+llvm-15.0.0-x86_64-apple-darwin.tar.xz -O clang-15.0.0.tar.xz\nls\ntar -xvf clang-15.0.0.tar.xz -C ~\ncd ~\nmv clang+llvm-15.0.0-x86_64-apple-darwin clang-15.0.0\n~/clang-15.0.0/bin/clang++ --version\n', 'brew tap homebrew/cask-fonts && brew install font-dejavu', ""rm '/usr/local/bin/2to3'"", 'brew install ccache', 'brew install libepoxy freetype fontconfig harfbuzz sdl2 sdl2_image opus opusfile qt6 libogg libpng eigen', 'brew install flex make', 'pip3 install --upgrade cython numpy jinja2 lz4 pillow pygments toml', 'CLANG_PATH=""$HOME/clang-15.0.0/bin/clang++""\n./configure --compiler=""$CLANG_PATH"" --mode=debug --ccache --download-nyan\n', 'make -j$(sysctl -n hw.logicalcpu) VERBOSE=1', 'make test', 'sudo DOCKER_BUILDKIT=1 docker build ./packaging/docker/devenv --file ./packaging/docker/devenv/Dockerfile.ubuntu.2204 --tag openage-devenv:latest', 'mkdir -p /tmp/staging\nsudo docker save openage-devenv:latest | gzip > /tmp/staging/devenv.tar.gz\n', 'mkdir -p /tmp/image', 'sudo docker load --input /tmp/image/devenv.tar.gz', 'sudo docker run --rm -v ""$(pwd)"":/mnt/openage -w /mnt/openage openage-devenv:latest \\\n  bash -c \'mkdir build && cd build && cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER=$(which gcc-11) -DCMAKE_CXX_COMPILER=$(which g++-11) -DCMAKE_CXX_FLAGS=\'\' -DCMAKE_EXE_LINKER_FLAGS=\'\' -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_MODULE_LINKER_FLAGS=\'\' -DCMAKE_SHARED_LINKER_FLAGS=\'\' -DDOWNLOAD_NYAN=YES -DCXX_OPTIMIZATION_LEVEL=auto -DCXX_SANITIZE_FATAL=False -DCXX_SANITIZE_MODE=none -DWANT_BACKTRACE=if_available -DWANT_GPERFTOOLS_PROFILER=if_available -DWANT_GPERFTOOLS_TCMALLOC=False -DWANT_INOTIFY=if_available -DWANT_NCURSES=if_available -DWANT_OPENGL=if_available -DWANT_VULKAN=if_available -G Ninja .. && cmake --build . --parallel $(nproc) -- -k1\'\n', 'mkdir -p /tmp/openage\ntar -czvf /tmp/openage/openage-build.tar.gz ./build\n', 'vswhere -latest\n', 'mkdir download\ncd download\n$zipfile = ""openage-dep-x64-windows.zip""\nInvoke-WebRequest https://github.com/duanqn/openage-windows-dep/releases/download/v0.0.1-alpha-ba157a25-eigenfix/openage-dep-x64-windows.zip -OutFile $zipfile\nExpand-Archive -Path $zipfile -DestinationPath . -Force\nRemove-Item $zipfile\n(Get-ChildItem . -Recurse -File).FullName\n', 'cd download\n$zipfile = ""winflexbison-2.5.24.zip""\nInvoke-WebRequest https://github.com/lexxmark/winflexbison/releases/download/v2.5.24/win_flex_bison-2.5.24.zip -OutFile $zipfile\nmkdir winflexbison\nExpand-Archive -Path $zipfile -DestinationPath ./winflexbison -Force\nRemove-Item $zipfile\n(Get-ChildItem ./winflexbison -Recurse -File).FullName\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade Cython wheel numpy lz4 toml pillow pygments pyreadline3 Jinja2\n', '$TOOLCHAIN_FILE = Join-Path download merge-x64-windows scripts buildsystems vcpkg.cmake | Resolve-Path\n$FLEX_PATH = (Get-ChildItem ./download -Recurse -Force -Filter \'win_flex.exe\')[0].FullName\nmkdir build\ncd build\ncmake -DCMAKE_TOOLCHAIN_FILE=""$TOOLCHAIN_FILE"" -DCMAKE_BUILD_TYPE=Release -DCMAKE_TRY_COMPILE_CONFIGURATION=Release -DCMAKE_CXX_FLAGS=\'/Zc:__cplusplus /permissive- /EHsc\' -DCMAKE_EXE_LINKER_FLAGS=\'\' -DCMAKE_MODULE_LINKER_FLAGS=\'\' -DCMAKE_SHARED_LINKER_FLAGS=\'\' -DDOWNLOAD_NYAN=YES -DCXX_OPTIMIZATION_LEVEL=auto -DCXX_SANITIZE_FATAL=False -DCXX_SANITIZE_MODE=none -DWANT_BACKTRACE=if_available -DWANT_GPERFTOOLS_PROFILER=if_available -DWANT_GPERFTOOLS_TCMALLOC=False -DWANT_INOTIFY=if_available -DWANT_NCURSES=if_available -DWANT_OPENGL=if_available -DWANT_VULKAN=if_available -DFLEX_EXECUTABLE=""$FLEX_PATH"" -G ""Visual Studio 16 2019"" -A x64 ../source\ncmake --build . --config RelWithDebInfo -- -nologo -maxCpuCount\n', ""mkdir package\ncd package\nmkdir dll\ncd ..\n$STAGING_PATH = Resolve-Path package\n$DLL_PATH = Join-Path package dll | Resolve-Path\n\ncd build\n$NYAN_DLL = (Get-ChildItem . -Recurse -Force -Filter 'nyan.dll')[0].FullName\n$OPENAGE_DLL = (Get-ChildItem . -Recurse -Force -Filter 'openage.dll')[0].FullName\n$NATIVE_OUTPUT = Split-Path -Path $OPENAGE_DLL -Parent\nCopy-Item -Path ./openage -Destination $STAGING_PATH -Recurse\nCopy-Item -Path $NYAN_DLL -Destination $DLL_PATH\nCopy-Item -Path (Join-Path $NATIVE_OUTPUT *.dll) -Destination $DLL_PATH\nCopy-Item -Path run.* -Destination $STAGING_PATH\n"", '$DLL_PATH = Join-Path package dll | Resolve-Path\ncd package\npython -m openage --add-dll-search-path $DLL_PATH --version\n', 'vswhere -latest\n', 'mkdir download\ncd download\n$zipfile = ""openage-dep-x64-windows.zip""\nInvoke-WebRequest https://github.com/duanqn/openage-windows-dep/releases/download/v0.0.1-alpha-ba157a25-eigenfix/openage-dep-x64-windows.zip -OutFile $zipfile\nExpand-Archive -Path $zipfile -DestinationPath . -Force\nRemove-Item $zipfile\n(Get-ChildItem . -Recurse -File).FullName\n', 'cd download\n$zipfile = ""winflexbison-2.5.24.zip""\nInvoke-WebRequest https://github.com/lexxmark/winflexbison/releases/download/v2.5.24/win_flex_bison-2.5.24.zip -OutFile $zipfile\nmkdir winflexbison\nExpand-Archive -Path $zipfile -DestinationPath ./winflexbison -Force\nRemove-Item $zipfile\n(Get-ChildItem ./winflexbison -Recurse -File).FullName\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade Cython wheel numpy lz4 toml pillow pygments pyreadline3 Jinja2\n', '$TOOLCHAIN_FILE = Join-Path download merge-x64-windows scripts buildsystems vcpkg.cmake | Resolve-Path\n$FLEX_PATH = (Get-ChildItem ./download -Recurse -Force -Filter \'win_flex.exe\')[0].FullName\nmkdir build\ncd build\ncmake -DCMAKE_TOOLCHAIN_FILE=""$TOOLCHAIN_FILE"" -DCMAKE_BUILD_TYPE=Release -DCMAKE_TRY_COMPILE_CONFIGURATION=Release -DCMAKE_CXX_FLAGS=\'/Zc:__cplusplus /permissive- /EHsc\' -DCMAKE_EXE_LINKER_FLAGS=\'\' -DCMAKE_MODULE_LINKER_FLAGS=\'\' -DCMAKE_SHARED_LINKER_FLAGS=\'\' -DDOWNLOAD_NYAN=YES -DCXX_OPTIMIZATION_LEVEL=auto -DCXX_SANITIZE_FATAL=False -DCXX_SANITIZE_MODE=none -DWANT_BACKTRACE=if_available -DWANT_GPERFTOOLS_PROFILER=if_available -DWANT_GPERFTOOLS_TCMALLOC=False -DWANT_INOTIFY=if_available -DWANT_NCURSES=if_available -DWANT_OPENGL=if_available -DWANT_VULKAN=if_available -DFLEX_EXECUTABLE=""$FLEX_PATH"" -G ""Visual Studio 17 2022"" -A x64 ../source\ncmake --build . --config RelWithDebInfo -- -nologo -maxCpuCount\n', ""mkdir package\ncd package\nmkdir dll\ncd ..\n$STAGING_PATH = Resolve-Path package\n$DLL_PATH = Join-Path package dll | Resolve-Path\n\ncd build\n$NYAN_DLL = (Get-ChildItem . -Recurse -Force -Filter 'nyan.dll')[0].FullName\n$OPENAGE_DLL = (Get-ChildItem . -Recurse -Force -Filter 'openage.dll')[0].FullName\n$NATIVE_OUTPUT = Split-Path -Path $OPENAGE_DLL -Parent\nCopy-Item -Path ./openage -Destination $STAGING_PATH -Recurse\nCopy-Item -Path $NYAN_DLL -Destination $DLL_PATH\nCopy-Item -Path (Join-Path $NATIVE_OUTPUT *.dll) -Destination $DLL_PATH\nCopy-Item -Path run.* -Destination $STAGING_PATH\n"", '$DLL_PATH = Join-Path package dll | Resolve-Path\ncd package\npython -m openage --add-dll-search-path $DLL_PATH --version\n']"
"['python -m pip install --upgrade pip', 'python -m pip install tox', 'python -m tox -e mypy-test', 'python -m pip install --upgrade pip wheel', 'pip install tox', 'tox -edocs', 'python -m pip install --upgrade pip', 'python -m pip install tox', 'python -m tox -e ${{ matrix.tox }}', 'python -m pip install --upgrade pip', 'python -m pip install tox', 'python -m tox -e lint', 'python -m pip install build twine', 'python -m build', 'twine check dist/*', 'twine upload -u __token__ -p ${{ secrets.PYPI_API_TOKEN }} dist/*']"
""
"['sudo apt-get update\nsudo apt-get install -y libldap2-dev libsasl2-dev libssl-dev\npip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n', 'black --check setup.py flask_appbuilder', 'flake8 flask_appbuilder', 'mypy flask_appbuilder', ""docker run -d \\\n  -v '${{ github.workspace }}/docker/openldap/ldifs:/ldifs' \\\n  -v '${{ github.workspace }}/docker/openldap/schemas/memberof.ldif:/opt/bitnami/openldap/etc/schema/memberof.ldif' \\\n  -e LDAP_URI=ldap://openldap:1389 \\\n  -e LDAP_BASE=dc=example,dc=org \\\n  -e LDAP_ADMIN_USERNAME=admin \\\n  -e LDAP_ADMIN_PASSWORD=admin_password \\\n  -e LDAP_EXTRA_SCHEMAS=cosine,inetorgperson,nis,memberof \\\n  -p 1389:1389 \\\n  bitnami/openldap:2.6.4\n"", 'sudo apt-get update\nsudo apt-get install -y libldap2-dev libsasl2-dev libssl-dev\npip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-extra.txt\n', 'nosetests --stop -v --with-coverage --cover-package=flask_appbuilder flask_appbuilder.tests --ignore-files=""test_mongoengine\\.py""\n', 'bash <(curl -s https://codecov.io/bash) -cF python\n', ""docker run -d \\\n  -v '${{ github.workspace }}/docker/openldap/ldifs:/ldifs' \\\n  -v '${{ github.workspace }}/docker/openldap/schemas/memberof.ldif:/opt/bitnami/openldap/etc/schema/memberof.ldif' \\\n  -e LDAP_URI=ldap://openldap:1389 \\\n  -e LDAP_BASE=dc=example,dc=org \\\n  -e LDAP_ADMIN_USERNAME=admin \\\n  -e LDAP_ADMIN_PASSWORD=admin_password \\\n  -e LDAP_EXTRA_SCHEMAS=cosine,inetorgperson,nis,memberof \\\n  -p 1389:1389 \\\n  bitnami/openldap:2.6.4\n"", 'sudo apt-get update\nsudo apt-get install -y libldap2-dev libsasl2-dev libssl-dev\npip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-extra.txt\n', 'nosetests --stop -v --with-coverage --cover-package=flask_appbuilder flask_appbuilder.tests --ignore-files=""test_mongoengine\\.py""\n', 'bash <(curl -s https://codecov.io/bash) -cF python\n', ""docker run -d \\\n  -v '${{ github.workspace }}/docker/openldap/ldifs:/ldifs' \\\n  -v '${{ github.workspace }}/docker/openldap/schemas/memberof.ldif:/opt/bitnami/openldap/etc/schema/memberof.ldif' \\\n  -e LDAP_URI=ldap://openldap:1389 \\\n  -e LDAP_BASE=dc=example,dc=org \\\n  -e LDAP_ADMIN_USERNAME=admin \\\n  -e LDAP_ADMIN_PASSWORD=admin_password \\\n  -e LDAP_EXTRA_SCHEMAS=cosine,inetorgperson,nis,memberof \\\n  -p 1389:1389 \\\n  bitnami/openldap:2.6.4\n"", 'sudo apt-get update\nsudo apt-get install -y libldap2-dev libsasl2-dev libssl-dev freetds-bin unixodbc-dev tdsodbc\npip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-extra.txt\nsudo cp .github/workflows/odbcinst.ini /etc/odbcinst.ini\n', 'nosetests --stop -v --with-coverage --cover-package=flask_appbuilder flask_appbuilder.tests --ignore-files=""test_mongoengine\\.py""\n', 'bash <(curl -s https://codecov.io/bash) -cF python\n', ""docker run -d \\\n  -v '${{ github.workspace }}/docker/openldap/ldifs:/ldifs' \\\n  -v '${{ github.workspace }}/docker/openldap/schemas/memberof.ldif:/opt/bitnami/openldap/etc/schema/memberof.ldif' \\\n  -e LDAP_URI=ldap://openldap:1389 \\\n  -e LDAP_BASE=dc=example,dc=org \\\n  -e LDAP_ADMIN_USERNAME=admin \\\n  -e LDAP_ADMIN_PASSWORD=admin_password \\\n  -e LDAP_EXTRA_SCHEMAS=cosine,inetorgperson,nis,memberof \\\n  -p 1389:1389 \\\n  bitnami/openldap:2.6.4\n"", 'sudo apt-get update\nsudo apt-get install -y libldap2-dev libsasl2-dev libssl-dev\npip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-extra.txt\npip install -r requirements-mongodb.txt\n', 'nosetests --stop -v --with-coverage --cover-package=flask_appbuilder flask_appbuilder/tests/test_mongoengine.py\n', 'bash <(curl -s https://codecov.io/bash) -cF python\n']"
"['pip install wheel\npython setup.py bdist_wheel sdist --formats=gztar\n', 'python -m pip install tox\n', 'tox -e precom\n', 'tox -e mypy\n', 'python -m pip install --upgrade pip\npip install ${{ matrix.requests-version }}\nmake install-deps\n', '# Run test\npytest . --asyncio-mode=auto --cov-report term-missing --cov-report xml --cov responses\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\n', 'flake8 --exclude env --ignore E402,E501 .\nblack --exclude=env --check .\n', 'python -m pytest\n']"
"['python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'python -m pip install build\npython -m build --sdist --outdir wheelhouse .\n']"
"['apt-get update\napt-get -y install software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\napt-get update\n', 'apt-get install -y --no-install-recommends \\\n  python3.10 \\\n  python3.10-dev \\\n  python3.10-venv \\\n  python3-pip \\\n  g++\n', 'apt-get update\napt-get install -qq \\\n  libcurl4-openssl-dev \\\n  libtiff-dev \\\n  libgeos-dev \\\n  libjpeg-dev \\\n  libnetcdf-dev \\\n  libhdf4-alt-dev \\\n  libhdf5-serial-dev \\\n  libssl-dev \\\n  libsqlite3-dev \\\n  libexpat-dev \\\n  libxerces-c-dev \\\n  libpng-dev \\\n  libopenjp2-7-dev \\\n  libzstd-dev \\\n  libwebp-dev \\\n  cmake \\\n  curl \\\n  git\nbash ci/gdal-compile.sh git\n', 'export PATH=""${GDAL_DIR}/bin/:${PATH}""\npython3.10 -m venv testenv\n. testenv/bin/activate\npython -m pip install --upgrade pip\npython -m pip wheel -r requirements-dev.txt\npython -m pip install -r requirements-dev.txt\npython setup.py clean\npython -m pip install --no-deps --force-reinstall --no-use-pep517 -e .[test]\n', 'export PATH=""${GDAL_DIR}/bin/:${PATH}""\n. testenv/bin/activate\npython -m pytest -v -m ""not wheel"" -rxXs --cov rasterio --cov-report term-missing\n', 'python -m pip install pre-commit\npre-commit run --show-diff-on-failure --all-files\n', 'apt-get update\napt-get -y install software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\napt-get update\n', 'apt-get install -y --no-install-recommends \\\n  python${{ matrix.python-version }} \\\n  python${{ matrix.python-version }}-dev \\\n  python${{ matrix.python-version }}-venv \\\n  python3-pip \\\n  g++\n', 'python${{ matrix.python-version }} -m venv testenv\n. testenv/bin/activate\npython -m pip install --upgrade pip\npython -m pip wheel -r requirements-dev.txt\npython -m pip install -r requirements-dev.txt\npython setup.py clean\npython -m pip install --no-deps --force-reinstall --no-use-pep517 -e .[test]\n', '. testenv/bin/activate\npython -m pytest -v -m ""not wheel"" -rxXs --cov rasterio --cov-report term-missing\n', 'conda config --prepend channels conda-forge\nconda config --set channel_priority strict\nconda create -n test python=${{ matrix.python-version }} libgdal geos=3.11 cython=0.29 numpy\nsource activate test\npython -m pip install -e . --no-use-pep517 || python -m pip install -e .\npython -m pip install -r requirements-dev.txt\n', 'source activate test\npython -V\nconda info\n', 'source activate test\npython -m pytest -v -m ""not wheel"" -rxXs --cov rasterio --cov-report term-missing -k ""not issue2353""\n', 'source activate test\npython -m pytest -v -m ""not wheel"" -rxXs  --cov rasterio --cov-report term-missing -k ""not test_target_aligned_pixels and not test_reproject_error_propagation and not test_outer_boundless_pixel_fidelity and not issue2353""\n']"
"['./.github/scripts/install_ubuntu_deps.sh', 'python -c \'from music21 import environment; environment.UserSettings()[""lilypondPath""] = ""/home/runner/bin/lilypond""\'', ""python -c 'from music21.test.testSingleCoreAll import ciMain as ci; ci()'"", 'coveralls', 'python -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install -r requirements_dev.txt\n', 'python -m pip install -e .\n', 'pylint -j0 music21\npylint -j0 documentation\n', 'python -m pip install --upgrade pip\npip install wheel\npip install -r requirements.txt\npip install -r requirements_dev.txt\n', 'flake8 music21\nflake8 documentation\n', 'pip install wheel\npython -m pip install -r requirements.txt\npython -m pip install -r requirements_dev.txt\n', 'mypy music21\n']"
"['python -m pip install -e .[dev]\n', 'python -m pytest -v\n']"
"['sudo apt-get install liblapack-dev\npip install --upgrade pip pytest\npip install wheel cython numpy scipy codecov pytest-cov\npip install scikit-learn==0.21.3\npip install git+https://github.com/skggm/skggm.git@${SKGGM_VERSION}\npytest test --cov\nbash <(curl -s https://codecov.io/bash)\n', 'sudo apt-get install liblapack-dev\npip install --upgrade pip pytest\npip install wheel cython numpy scipy codecov pytest-cov scikit-learn\npytest test --cov\nbash <(curl -s https://codecov.io/bash)\n', 'pip install git+https://github.com/skggm/skggm.git@${SKGGM_VERSION}\npytest test --cov\nbash <(curl -s https://codecov.io/bash)\n', 'pip install flake8\nflake8 --extend-ignore=E111,E114 --show-source;\n']"
"['gh pr review --approve ""$PR_URL""', 'gh pr merge --auto --squash --delete-branch ""$PR_URL""', '# set DEFAULT BRANCH\necho ""DEFAULT_BRANCH=$(gh repo view --json defaultBranchRef --jq \'.defaultBranchRef.name\')"" >> ""$GITHUB_ENV"";\n\n# set HAS_MASTER_BRANCH\nif [ -n ""$(git ls-remote --heads origin master)"" ]; then\n  echo ""HAS_MASTER_BRANCH=true"" >> ""$GITHUB_ENV""\nelse\n  echo ""HAS_MASTER_BRANCH=false"" >> ""$GITHUB_ENV""\nfi\n', ""git config --global user.name 'googleworkspace-bot'\ngit config --global user.email 'googleworkspace-bot@google.com'\n"", 'git checkout -B master\ngit reset --hard origin/main\ngit push origin master\n', 'git checkout -B main\ngit reset --hard origin/master\ngit push origin main\n', 'git fetch origin master ""${{ github.event.pull_request.base.sha }}""\ngit diff  --diff-filter=ACM --name-only ""${{ github.event.pull_request.base.sha }}"" ""${{ github.sha }}"" > ""${HOME}/changed_files.txt""\n', 'python -m pip install --upgrade pip\npip install pipenv\n', 'mkdir ""${HOME}/secrets""\necho ""${DEFAULT_CREDENTIALS}"" > ""${HOME}/secrets/default_credentials.json""\necho ""${SERVICE_ACCOUNT_CREDENTIALS}"" > ""${HOME}/secrets/service_account.json""\necho ""${CLIENT_ID_FILE}"" > ""${HOME}/secrets/client_id.json""\n', './.github/scripts/test.sh', 'echo ""Test matrix finished""']"
"['pip install --upgrade build\n', 'python -m build\n', 'pip install -r requirements-dev.txt\n', 'black --check .\n', 'flake8 . --count --show-source --statistics\n', 'mkdir /tmp/dynamodb_local\nwget -O - https://s3-us-west-2.amazonaws.com/dynamodb-local/dynamodb_local_latest.tar.gz \\\n  | tar xz --directory /tmp/dynamodb_local\njava -Djava.library.path=/tmp/dynamodb_local/DynamoDBLocal_lib -jar /tmp/dynamodb_local/DynamoDBLocal.jar \\\n  -sharedDb -inMemory &\n', 'pip install -r requirements.txt\n', 'mkdir dump && cp -a tests/testTable dump\npython dynamodump/dynamodump.py -m restore --noConfirm -r local -s testTable -d testRestoredTable \\\n  --host localhost --port 8000 --accessKey a --secretKey a\npython dynamodump/dynamodump.py -m backup -r local -s testRestoredTable --host localhost --port 8000 \\\n  --accessKey a --secretKey a\npython tests/test.py\n', 'python dynamodump/dynamodump.py -m restore --noConfirm -r local -s ""*"" --host localhost --port 8000 \\\n  --accessKey a --secretKey a\nrm -rf dump/test*\npython dynamodump/dynamodump.py -m backup -r local -s ""*"" --host localhost --port 8000 --accessKey a \\\n  --secretKey a\npython tests/test.py\n', 'python dynamodump/dynamodump.py -m restore --noConfirm -r local -s ""test*"" --host localhost --port 8000 \\\n  --accessKey a --secretKey a --prefixSeparator """"\nrm -rf dump/test*\npython dynamodump/dynamodump.py -m backup -r local -s ""test*"" --host localhost --port 8000 --accessKey a \\\n  --secretKey a --prefixSeparator """"\npython tests/test.py\n', 'mkdir /tmp/dynamodb_local\nwget -O - https://s3-us-west-2.amazonaws.com/dynamodb-local/dynamodb_local_latest.tar.gz \\\n  | tar xz --directory /tmp/dynamodb_local\njava -Djava.library.path=/tmp/dynamodb_local/DynamoDBLocal_lib -jar /tmp/dynamodb_local/DynamoDBLocal.jar \\\n  -sharedDb -inMemory &\n', 'pip install -r requirements.txt\n', 'mkdir ${{ env.DUMP_PATH }} && cp -a tests/testTable ${{ env.DUMP_PATH }}\npython dynamodump/dynamodump.py -m restore --noConfirm -r local -s testTable -d testRestoredTable \\\n  --host localhost --port 8000 --accessKey a --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nrm -rf ${{ env.DUMP_PATH }}\npython dynamodump/dynamodump.py -m backup -r local -s testRestoredTable --host localhost --port 8000 \\\n  --accessKey a --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nDUMP_DATA_DIR=${{ env.DUMP_PATH }} python tests/test.py\n', 'python dynamodump/dynamodump.py -m restore --noConfirm -r local -s ""*"" --host localhost --port 8000 \\\n  --accessKey a --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nrm -rf ${{ env.DUMP_PATH }}/test*\npython dynamodump/dynamodump.py -m backup -r local -s ""*"" --host localhost --port 8000 --accessKey a \\\n  --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nDUMP_DATA_DIR=${{ env.DUMP_PATH }} python tests/test.py\n', 'python dynamodump/dynamodump.py -m restore --noConfirm -r local -s ""test*"" --host localhost --port 8000 \\\n  --accessKey a --secretKey a --prefixSeparator """" --dumpPath ${{ env.DUMP_PATH }}\nrm -rf ${{ env.DUMP_PATH }}/test*\npython dynamodump/dynamodump.py -m backup -r local -s ""test*"" --host localhost --port 8000 --accessKey a \\\n  --secretKey a --prefixSeparator """" --dumpPath ${{ env.DUMP_PATH }}\nDUMP_DATA_DIR=${{ env.DUMP_PATH }} python tests/test.py\n', 'mkdir /tmp/dynamodb_local\nwget -O - https://s3-us-west-2.amazonaws.com/dynamodb-local/dynamodb_local_latest.tar.gz \\\n  | tar xz --directory /tmp/dynamodb_local\njava -Djava.library.path=/tmp/dynamodb_local/DynamoDBLocal_lib -jar /tmp/dynamodb_local/DynamoDBLocal.jar \\\n  -sharedDb -inMemory &\n', 'pip install -r requirements.txt\n', 'mkdir ${{ env.DUMP_PATH }} && cp -a tests/testTable ${{ env.DUMP_PATH }}\npython dynamodump/dynamodump.py -m restore --noConfirm -r local -s testTable -d testRestoredTable \\\n  --host localhost --port 8000 --accessKey a --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nrm -rf ${{ env.DUMP_PATH }}\npython dynamodump/dynamodump.py -m backup -r local -s testRestoredTable --host localhost --port 8000 \\\n  --accessKey a --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nDUMP_DATA_DIR=${{ env.DUMP_PATH }} python tests/test.py\n', 'python dynamodump/dynamodump.py -m restore --noConfirm -r local -s ""*"" --host localhost --port 8000 \\\n  --accessKey a --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nrm -rf ${{ env.DUMP_PATH }}/test*\npython dynamodump/dynamodump.py -m backup -r local -s ""*"" --host localhost --port 8000 --accessKey a \\\n  --secretKey a --dumpPath ${{ env.DUMP_PATH }}\nDUMP_DATA_DIR=${{ env.DUMP_PATH }} python tests/test.py\n', 'python dynamodump/dynamodump.py -m restore --noConfirm -r local -s ""test*"" --host localhost --port 8000 \\\n  --accessKey a --secretKey a --prefixSeparator """" --dumpPath ${{ env.DUMP_PATH }}\nrm -rf ${{ env.DUMP_PATH }}/test*\npython dynamodump/dynamodump.py -m backup -r local -s ""test*"" --host localhost --port 8000 --accessKey a \\\n  --secretKey a --prefixSeparator """" --dumpPath ${{ env.DUMP_PATH }}\nDUMP_DATA_DIR=${{ env.DUMP_PATH }} python tests/test.py\n']"
"['set -xeu\npython --version\npip install tox\n', 'tox -e ${{ matrix.tox-env }}', 'python --version\npip install tox coverage\n', 'tox -e ${{ matrix.tox-env }}', ""echo 'Success !'""]"
"['python src/julia/find_libpython.py --list-all --verbose', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox\n', 'python ci/install_pycall.py', 'python -m tox -- --verbose --cov=julia', 'cat .tox/py/log/pytest.log', 'python -m pip install --upgrade tox', 'python -m tox -e ${{ matrix.toxenv }}', 'python -m pip install --user setuptools wheel\n', 'python setup.py bdist_wheel\npython setup.py sdist\n', 'python -m pip install --user tox', 'cd ci/test-upload && python -m tox', 'pip download --dest dist --no-deps --index-url https://test.pypi.org/simple/ ${{ matrix.package }}\npip download --dest dist --no-deps --index-url https://test.pypi.org/simple/ ${{ matrix.package }} --no-binary :all:\n', 'ls -lh dist', 'python -m pip install --upgrade tox', 'python -m tox -- --no-julia -k test__using_default_setup']"
"['./ci/install_dependencies.sh docs\n', 'cd docs\nmake html\n', './ci/install_dependencies.sh pypi\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', './ci/install_dependencies.sh\n', './ci/lint.sh docstring\n', './ci/run_tests.sh\n']"
"['pip3 install flake8', 'find . -name ""*.py"" | xargs flake8 $1', 'pip3 install jsonschema', 'python3 ./tests/schema.py\npython3 ./tests/db_entry.py\npython3 ./tests/icons_entry.py\npython3 ./tests/ordered_db.py\npython3 ./tests/icon_cache.py\npython3 ./tests/duplicates.py\n']"
"['python -m pip install "".[test,hg]""', 'pip install .', 'python -m pytest -v --timeout=300 --webdriver=ChromeHeadless --durations=100 test', 'pip install "".[doc]""', 'sphinx-build -W docs/source html']"
"['python -m pip install --upgrade pip\npython -m pip install codespell flake8\npython -m pip install -e .\n', 'codespell --ignore-words-list=""ba,claus,vas"" --quiet-level=2', 'flake8 . --count --ignore=E122,E226,E265,E741,E742 --max-complexity=22 --max-line-length=124 --show-source --statistics', 'python src/tests/test.py -v']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npip install tox tox-gh-actions coveralls\n', 'tox -p auto']"
"['sudo /etc/init.d/mysql start\n', 'python -m pip install --upgrade pip\npip install -U pip poetry\npoetry install\npoetry run pip install psycopg2 mysqlclient\npoetry run pip install --pre -U ""${{ matrix.django-version }}"" ""git+https://github.com/django-haystack/django-haystack.git""\n', 'make qa', 'poetry run pytest']"
""
"['python -m pip install --upgrade pip\npip install tox flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox\n', 'tox\n', 'pip install codecov\ncodecov\n']"
"['sudo add-apt-repository ppa:alex-p/tesseract-ocr-devel\n', 'sudo apt-get update\nsudo apt-get install -y --no-install-recommends \\\n  curl \\\n  ghostscript \\\n  img2pdf \\\n  libffi-dev \\\n  libsm6 libxext6 libxrender-dev \\\n  pngquant \\\n  poppler-utils \\\n  tesseract-ocr \\\n  tesseract-ocr-deu \\\n  tesseract-ocr-eng \\\n  tesseract-ocr-osd \\\n  unpaper \\\n  zlib1g\n', 'sudo apt-get install -y --no-install-recommends \\\n  libexempi8\n', 'sudo apt-get install -y --no-install-recommends \\\n  libxml2-dev \\\n  libxslt1-dev \\\n  pypy3-dev\n', 'python -m pip install --upgrade pip wheel\npython -m pip install --prefer-binary .[test]\n', 'tesseract --version\ngs --version\npngquant --version\nunpaper --version\nimg2pdf --version\n', 'python -m pytest --cov-report xml --cov=ocrmypdf --cov=tests/ -n0 tests/\n', 'brew update\nbrew install \\\n  exempi \\\n  ghostscript \\\n  jbig2enc \\\n  openjpeg \\\n  pngquant \\\n  tesseract\n', 'python -m pip install --upgrade pip wheel\npython -m pip install --prefer-binary .[test]\n', 'tesseract --version\ngs --version\npngquant --version\nimg2pdf --version\n', 'python -m pytest --cov-report xml --cov=ocrmypdf --cov=tests/ -n0 tests/\n', 'choco install --yes --no-progress --pre tesseract\nchoco install --yes --no-progress --ignore-checksums ghostscript\n', 'python -m pip install --upgrade pip wheel\npython -m pip install --prefer-binary .[test]\n', 'python -m pytest --cov-report xml --cov=ocrmypdf --cov=tests/ -n0 tests/\n', 'python -m pip install --upgrade pip wheel build\npython -m build --sdist --wheel\n', 'echo ""DOCKER_IMAGE_TAG=${GITHUB_REF##*/}"" >> $GITHUB_ENV', ""echo 'DOCKER_IMAGE_TAG=latest' >> $GITHUB_ENV"", 'echo ""DOCKER_REPOSITORY=jbarlow83"" >> $GITHUB_ENV', 'echo ""DOCKER_IMAGE_NAME=ocrmypdf"" >> $GITHUB_ENV', 'echo ""Building image ${DOCKER_REPOSITORY}/${DOCKER_IMAGE_NAME}:${DOCKER_IMAGE_TAG}""', 'docker buildx build \\\n  --push \\\n  --platform linux/arm64/v8,linux/amd64  \\\n  --tag ""${DOCKER_REPOSITORY}/${DOCKER_IMAGE_NAME}:${DOCKER_IMAGE_TAG}"" \\\n  --file .docker/Dockerfile .\n']"
"['python -m pip install -U setuptools wheel build', 'python -m build .', 'python -m pip install -U tox', 'python -m tox -e py']"
"['free -hmw\nlscpu\nsudo apt install -y python3-dev python3-pip python3-venv python3-mysqldb\nif [[ -z ""$(type google-chrome 2>/dev/null)"" ]]; then\n  wget ""https://dl.google.com/linux/direct/${CHROME_DEB}"" && sudo apt install -y ""./${CHROME_DEB}"";\nfi\npython3 -m venv --system-site-packages ""${HOME}/INSTALL""\ntravis/install.sh\n', 'source ""${HOME}/INSTALL/bin/activate""\npip install pytest-xdist==2.2.1 pytest==6.2.5\n# We have 4 vCPUs available, but only use 3 here to avoid timeouts like\n# https://ci.appveyor.com/project/grr/grr-ia94e/builds/20483467/messages ,\n# which happen when tests stall.\npytest --verbose -n 3 grr/ --ignore grr/server/grr_response_server/gui/selenium_tests/ --ignore grr/client/grr_response_client/client_actions/windows/\n# jsTree tests seem to fail on Chrome 71 headless due to https://github.com/GoogleChrome/puppeteer/issues/3463\nif [ $(google-chrome --version | grep -Eo "" [0-9]{1,3}"") != ""71"" ]; then (cd grr/server/grr_response_server/gui/static/ && npm run gulp test); fi\n', 'sudo apt install -y python3-dev python3-pip python3-venv python3-mysqldb\npython3 -m venv --system-site-packages ""${HOME}/INSTALL""\ntravis/install.sh\n', 'source ""${HOME}/INSTALL/bin/activate""\nmkdir -p _openapi_artifacts/openapi_description\nmkdir -p _openapi_artifacts/documentation\ntravis/build_api_documentation.sh ""_openapi_artifacts/openapi_description/openapi_description.json"" ""_openapi_artifacts/documentation/openapi_documentation.html""\nls -la _openapi_artifacts/*\n', 'sudo apt install fakeroot debhelper libffi-dev libssl-dev\npip install virtualenv\nvirtualenv ""${HOME}/INSTALL""\n', 'travis/install_client_builder.sh\ntravis/build_templates.sh\nls -la gcs_upload_dir\n', 'pip install --upgrade setuptools virtualenv\nvirtualenv ""${HOME}/INSTALL""\n', 'travis/install_client_builder.sh\ntravis/build_templates.sh\nls -la gcs_upload_dir\n', 'docker run -dit \\\n  --volume ""${PWD}:/mnt/grr"" \\\n  --workdir /mnt/grr \\\n  --env DOCKER_USER=""${DOCKER_USER}"" \\\n  --env TRAVIS_OS_NAME=""linux"" \\\n  --name ""${DOCKER_CONTAINER}"" \\\n  ""${DOCKER_IMG}""\n# Using `bash -l` here and below to make sure devtools\n# (including the C++ 14 compatible compiler) are properly\n# registered in the environment variables.\ndocker exec ""${DOCKER_CONTAINER}"" bash -l travis/set_up_test_user.sh\ndocker exec --user ""${DOCKER_USER}"" ""${DOCKER_CONTAINER}"" bash -l -c \'/usr/local/bin/python3.9 -m venv ""/home/${DOCKER_USER}/INSTALL""\'\ndocker exec --user ""${DOCKER_USER}"" ""${DOCKER_CONTAINER}"" bash -l travis/install_client_builder.sh\ndocker exec --user ""${DOCKER_USER}"" ""${DOCKER_CONTAINER}"" bash -l travis/build_templates.sh\ndocker exec ""${DOCKER_CONTAINER}"" rpm -vih gcs_upload_dir/*.rpm\nls -la gcs_upload_dir\n', 'set -ex\npip install virtualenv wheel\npython -u appveyor/windows_templates/build_windows_templates.py --grr_src=$GITHUB_WORKSPACE --output_dir=$GITHUB_WORKSPACE/output --test_repack_install\nmkdir -p gcs_upload_dir\nmv -v output*/* gcs_upload_dir\nls -la gcs_upload_dir\n', 'sudo apt-get update\nsudo apt-get install -y fakeroot debhelper libffi-dev libssl-dev python3-dev python3-pip python3-venv python3-mysqldb wget openjdk-8-jdk zip git devscripts libmysqlclient-dev dh-virtualenv dh-make libc6-i386 lib32z1\npython3 -m venv --system-site-packages ""${HOME}/INSTALL""\n""${HOME}/INSTALL/bin/python3"" -m pip install --upgrade pip \'setuptools<58.3.1\' wheel\n', 'travis/install.sh\nmkdir -p grr/config/grr_response_templates/templates\nmv -v ~/_artifacts/windows-installers/GRR_*_amd64.msi.zip grr/config/grr_response_templates/templates\nmv -v ~/_artifacts/ubuntu-installers/grr_*_amd64.deb.zip grr/config/grr_response_templates/templates\nmv -v ~/_artifacts/centos-installers/grr_*_amd64.rpm.zip grr/config/grr_response_templates/templates\nmv -v ~/_artifacts/osx-installers/grr_*_amd64.xar.zip grr/config/grr_response_templates/templates\ntravis/build_local_pyindex.sh\ntravis/build_server_deb.sh\nls -la gcs_upload_dir\n', 'printf ""\\n[mysqld]\\nmax_allowed_packet=42M\\nlog_bin_trust_function_creators=1\\n"" | sudo tee -a /etc/mysql/my.cnf\nsudo /etc/init.d/mysql start\n', 'free -hmw\nlscpu\nsudo -EH ./appveyor/e2e_tests/install_mem_usage_cron.sh\nsudo -EH ./appveyor/e2e_tests/install_latest_server_deb.sh\n', 'sudo -EH ./appveyor/e2e_tests/run_e2e_tests.sh\nsudo -EH ./appveyor/e2e_tests/test_repack.sh\n', ""export BRANCH=$(echo $GITHUB_REF | cut -d'/' -f 3)\n./appveyor/docker_build/build_docker_image.sh\n"", 'docker push -a ${{ env.DOCKER_REPOSITORY }}\n', 'ls -la _artifacts/*/\nCOMMIT_TIME=$(git show -s --date=\'format-local:%Y-%m-%dT%H:%M:%SZ\' --format=""%cd"" $GITHUB_SHA)\nOUTPUT_DIR=gcs_upload_dir/${COMMIT_TIME}_${GITHUB_SHA}/\necho ""OUTPUT_DIR=$OUTPUT_DIR"" >> $GITHUB_ENV\nmkdir -p $OUTPUT_DIR/centos/\nmv -v _artifacts/centos-installers/* $OUTPUT_DIR/centos\nmkdir -p $OUTPUT_DIR/ubuntu/\nmv -v _artifacts/ubuntu-installers/* $OUTPUT_DIR/ubuntu\nmkdir -p $OUTPUT_DIR/osx/\nmv -v _artifacts/osx-installers/* $OUTPUT_DIR/osx\nmkdir -p $OUTPUT_DIR/windows/\nmv -v _artifacts/windows-installers/* $OUTPUT_DIR/windows\nmkdir -p $OUTPUT_DIR/server_deb/\nmv -v _artifacts/server-deb/* $OUTPUT_DIR/server_deb\n', 'gsutil rm gs://${{ env.GCS_BUCKET }}/${{ env.GCS_LATEST_PATH }}/** || true\ngsutil cp -r $OUTPUT_DIR/server_deb/* gs://${{ env.GCS_BUCKET }}/${{ env.GCS_LATEST_PATH }}/\n']"
"['docker build -t crypto101 docker/', 'set -x\nset -e\ndocker run --rm -it -v ${{ github.workspace }}:/repo crypto101 ./build_release ./_release/\n']"
"['git checkout HEAD^2', 'python -m pip install -r docs/requirements.txt', 'cd docs\nsphinx-build -b html -n -d _build/doctrees . _build/html\n', 'pip install --upgrade flake8', 'python -m pip install isort', 'python -m pip install --upgrade pip\npip install -r requirements/base.txt\npip install Django==${{ matrix.django-version }}\npython setup.py install\n', 'coverage run --source=explorer manage.py test', 'python -m pip install --upgrade pip\npip install -r requirements/base.txt\npip install -r requirements/optional.txt\npip install Django==${{ matrix.django-version }}\npython setup.py install\n', 'coverage run --source=explorer manage.py test', 'python -m pip install --upgrade pip\npip install ${{ matrix.django-version }} coverage\npython setup.py install\n', 'coverage run --source=explorer manage.py test']"
"['sudo apt install -y --no-install-recommends libnettle8 libsodium-dev libssl-dev libkrb5-dev ssh cmake ninja-build', 'brew install nettle liboqs libsodium openssl', 'echo ""/usr/local/opt/openssl@3/bin"" >> $GITHUB_PATH', 'curl -fLO https://github.com/ShiftMediaProject/nettle/releases/download/${{ env.nettle_version }}/libnettle_${{ env.nettle_version }}_msvc17.zip\nExpand-Archive libnettle_${{ env.nettle_version }}_msvc17.zip nettle\ncp nettle\\bin\\x64\\*.dll ""$env:Python_ROOT_DIR""\n', 'cmake -GNinja -Bbuild . -DCMAKE_INSTALL_PREFIX=/usr -DBUILD_SHARED_LIBS=ON -DOQS_BUILD_ONLY_LIB=ON -DOQS_DIST_BUILD=ON -DCMAKE_C_COMPILER_LAUNCHER=ccache\ncmake --build build\nsudo cmake --install build\n', 'cmake -GNinja -Bbuild . -DBUILD_SHARED_LIBS=ON -DOQS_BUILD_ONLY_LIB=ON -DOQS_DIST_BUILD=ON\ncmake --build build\ncp build\\bin\\oqs.dll ""$env:Python_ROOT_DIR""\n', 'pip install tox', ""import os, sys, platform, subprocess\nV = sys.version_info\np = platform.system().lower()\nsubprocess.run(\n    ['tox', '-e', f'py{V.major}{V.minor}-{p}', '--', '-ra'],\n    check=True)\n"", 'sudo apt install -y sqlite3\npip install tox\n', 'shopt -s nullglob\nfor f in .coverage.*-windows; do\n  sqlite3 ""$f"" ""update file set path = replace(path, \'\\\\\', \'/\');""\ndone\ntox -e report\n']"
"['pipx run towncrier check --compare-with origin/${{ github.base_ref }}\n', 'echo tag=${GITHUB_REF#refs/tags/} >> $GITHUB_OUTPUT\n', 'curl -sL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'poetry build -vvv\n', 'echo tag=${GITHUB_REF#refs/tags/} >> $GITHUB_OUTPUT\n', 'curl -sL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'ls -la dist\n', 'poetry publish\n', 'echo version=$(python -c ""import sys; print(\'-\'.join(str(v) for v in sys.version_info))"") >> $GITHUB_OUTPUT\n', 'curl -sL https://install.python-poetry.org | python - -y\n', 'echo ""$HOME/.local/bin"" >> $GITHUB_PATH', 'echo ""$APPDATA\\Python\\Scripts"" >> $GITHUB_PATH', 'poetry config virtualenvs.in-project true\n', '# `timeout` is not available on macOS, so we define a custom function.\n[ ""$(command -v timeout)"" ] || function timeout() { perl -e \'alarm shift; exec @ARGV\' ""$@""; }\n# Using `timeout` is a safeguard against the Poetry command hanging for some reason.\ntimeout 10s poetry run pip --version || rm -rf .venv\n', 'poetry install\n', 'poetry run mypy\n', 'poetry run pytest -q tests\npoetry install\n']"
"['python -m pip install -U setuptools wheel twine\npython setup.py sdist bdist_wheel\npython -m twine check --strict dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\n', 'tox']"
"['echo ""latest_python=$latest_python"" >> $GITHUB_OUTPUT\necho ""supported_pythons=$supported_pythons"" >> $GITHUB_OUTPUT\n', 'pip install -r ci/requirements.lint.txt\nconda list\n', 'make lint', 'pip install -r ci/requirements.doc.txt\npip install .\nconda list\n', 'make doc', 'pip install .\nconda list\n', 'make test', 'conda install -q --yes -c conda-forge --file ci/conda_requirements.txt\npip install . --no-deps\nconda list\n', 'pip install .\nconda list\n', 'make test', ""# set version from '${{ github.ref_name }}'\nexport RELEASE_VERSION=${{ github.ref_name }}\npip install numpy cython\npython setup.py sdist\n""]"
[]
"['pip install tox', 'make README', 'tox -e py']"
"['pip install --upgrade pip\n', 'python setup.py sdist --for-pypi', 'mkdir -p dist\nmv -v wheelhouse/* dist/\nls -l dist/\n', 'pip install --upgrade pip\n', 'python setup.py sdist --for-pypi', 'mkdir -p dist\nmv -v wheelhouse/* dist/\nls -l dist/\n']"
"['sudo apt-get update -qq\nsudo apt-get install -y blender wget python3 python3-pip zip\n', 'pip3 install -r requirements.txt', 'bash tests/lint/pylint.sh src/magic_uv', 'bash tests/lint/pep8.sh src/magic_uv', 'wget http://mirror.cs.umn.edu/blender.org/release/Blender2.77/blender-2.77-linux-glibc211-x86_64.tar.bz2\ntar jxf blender-2.77-linux-glibc211-x86_64.tar.bz2\n', 'cp -r src/magic_uv blender-2.77-linux-glibc211-x86_64/2.77/scripts/addons', 'blender-2.77-linux-glibc211-x86_64/blender --factory-startup --background -noaudio --python tests/python/run_tests.py', 'wget https://download.blender.org/release/Blender2.83/blender-2.83.3-linux64.tar.xz\ntar xf blender-2.83.3-linux64.tar.xz\n', 'cp -r src/magic_uv blender-2.83.3-linux64/2.83/scripts/addons', 'blender-2.83.3-linux64/blender --factory-startup --background -noaudio --python tests/python/run_tests.py', 'mkdir release\ncd src\nzip -r magic_uv.zip magic_uv\ncd ..\nmv src/magic_uv.zip release\n']"
"['echo ""Optional): Review Draft Release: ${{ steps.prep-release.outputs.release_url }}""\n', 'echo ""Verify the final release""\necho ${{ steps.finalize-release.outputs.release_url }}\n', 'echo ""Failed to Publish the Draft Release Url:""\necho ${{ steps.populate-release.outputs.release_url }}\n', 'hatch run cov:test', 'hatch run test:nowarn || hatch run test:nowarn --lf\n', ""hatch run typing:test\nhatch run lint:style\npipx run 'validate-pyproject[all]' pyproject.toml\npipx run doc8 --max-line-length=200\n"", 'hatch run docs:build', 'hatch run test:nowarn || hatch run test:nowarn --lf\n', 'pip install -e .\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox']"
"['python -m pip install --upgrade hatch\n', 'hatch run +py=${{ matrix.py }} +type= test:with-coverage\n', 'hatch run +py=${{ matrix.py }} +type= integration:test\n', 'python -m pip install --upgrade hatch\n', 'hatch run style:format && git diff --exit-code', 'hatch run style:lint', 'hatch run types:check', 'hatch run lint:markdown', 'hatch run lint:js', 'hatch run lint:css', 'hatch run lint:spelling', 'python -m pip install -U hatch\n', 'hatch build\n', 'expected_wheel=(-emkdocs/{templates/sitemap.xml,config/base.py,py.typed,contrib/search/lunr-language/lunr.nl.js,themes/{mkdocs,readthedocs}/{base.html,locales/{de,es}/LC_MESSAGES/messages.mo}})\nexpected_sdist=(""${expected_wheel[@]}"" -e{pyproject.toml,hatch_build.py})\ntest ""$(tar -ztf dist/mkdocs-*.tar.gz | grep -F ""${expected_sdist[@]}"" | tee /dev/stderr | wc -l)"" -eq ""${#expected_sdist[@]}""\ntest ""$(unzip -l dist/mkdocs-*any.whl | grep -F ""${expected_wheel[@]}"" | tee /dev/stderr | wc -l)"" -eq ""${#expected_wheel[@]}""\n', 'python -m pip install -U hatch\n', 'hatch build\n', 'hatch publish\n']"
"['sudo apt remove python3-pip\npython -m pip install --upgrade pip\npython -m pip install . black coverage codecov flake8 isort==5.6.4 mypy pytest readme_renderer types-contextvars asyncssh\npip list\n', 'flake8 .\ncoverage run -m pytest\n', 'mypy --strict src/prompt_toolkit --platform win32\nmypy --strict src/prompt_toolkit --platform linux\nmypy --strict src/prompt_toolkit --platform darwin\nisort -c --profile black src examples tests setup.py\nblack --check src examples tests setup.py\n', 'python -m readme_renderer README.rst > /dev/null\n', 'codecov\n']"
"['echo ""::set-output name=current_date::$(date +\'%Y-%m-%d\')""', 'git checkout -b update-discovery-artifacts-${{ steps.date.outputs.current_date }}\n', 'pip3 install -e .', 'pip3 install -r requirements.txt', ""sed -i -e 's/if credentials is None/if False/g' googleapiclient/discovery.py"", 'python3 updatediscoveryartifacts.py', './createcommits.sh', 'git push -f --set-upstream origin update-discovery-artifacts-${{ steps.date.outputs.current_date }}', 'python3 buildprbody.py\noutput=$(cat temp/allapis.summary)\noutput=""${output//\'%\'/\'%25\'}""\noutput=""${output//$\'\\n\'/\'%0A\'}""\noutput=""${output//$\'\\r\'/\'%0D\'}""\necho ""::set-output name=change_summary::$output""\n']"
"['python -m pip install --upgrade setuptools pip wheel\npython -m pip install nox\n', 'ci/run_conditional_tests.sh\n', 'python -m pip install --upgrade setuptools pip wheel\npython -m pip install nox\n', 'ci/run_conditional_tests.sh\n', 'python -m pip install --upgrade setuptools pip wheel\npython -m pip install nox\n', 'ci/run_conditional_tests.sh\n', 'ci/run_conditional_tests.sh\n', 'echo ""::set-output name=current_date::$(date +\'%Y-%m-%d\')""', 'pip3 install -r requirements.txt', 'python3 scripts/updateapilist.py', 'python -m pip install --upgrade setuptools pip wheel\npython -m pip install nox\n', 'ci/run_conditional_tests.sh\n', 'python -m pip install --upgrade setuptools pip wheel\npython -m pip install nox\n', 'ci/run_conditional_tests.sh\n', 'echo ""::set-output name=num_files_changed::$(git diff HEAD~1 -- packages | wc -l)""', 'python -m pip install --upgrade setuptools pip wheel\npython -m pip install coverage\n', 'coverage combine .coverage-results/.coverage*\ncoverage report --show-missing --fail-under=100\n']"
""
"['sudo apt-get update\nsudo apt-get install $(cat apt-requirements.txt)\npython -m pip install --upgrade pip\npip install flake8 pytest pytest-cov wheel\npip install -r pip-requirements.txt\n', 'debian/build.sh\nmkdir debs\nmv ../*.deb debs\n', 'sudo apt-get update\nsudo apt-get install $(cat apt-requirements.txt)\npython -m pip install --upgrade pip\npip install build\n', 'python -m build --sdist --wheel --outdir dist/', 'git fetch --tags --force\norigin=""$(git config --get remote.origin.url)""\n# Will get tag name only if a tag triggered this workflow.\n# tagname=""${GITHUB_REF#refs/tags/}""\n# Use this to get the most recent tag on this branch rather than the tag triggering the current build.\ntagname=""$(git describe --tags --abbrev=0 --match ""v*.*.*"")""\nprerelease=""--prerelease""\nnotes=""[See here for changelog for this release]($origin/blob/$tagname/CHANGELOG.rst)""\n# If we are a tag and on master, should be a full release not a prerelease\nif git branch --all --contains tags/$tagname | grep --silent ""master""; then\n  prerelease=""""\nfi\n# Include other files by adding them as additional arguments\ngh release create ""$tagname"" ../debs/*.deb --title ""$tagname"" --notes ""$notes"" $prerelease\n', 'sudo apt-get update\nsudo apt-get install $(cat apt-requirements.txt)\npython -m pip install --upgrade pip\npip install flake8 wheel\npip install -r pip-requirements.txt\n', 'sudo apt-get update\nsudo apt-get install $(cat apt-requirements.txt)\npython -m pip install --upgrade pip\npip install flake8 tox pytest pytest-cov wheel\npip install -r pip-requirements.txt\n', 'tox -e clean,coverage,report', 'tar -cvzf test_coverage_report-${{ matrix.python-version }}.tar.gz test_coverage_report_html/', 'sudo apt-get update\nsudo apt-get install $(cat apt-requirements.txt)\npython -m pip install --upgrade pip\npip install dbus-python gobject pygobject PyQt5 qscintilla\n', 'pip install ""${GITHUB_WORKSPACE}""\nautokey-gtk --help\n# qt xcb module requires a display to connect to, so won\'t work.\n# export QT_DEBUG_PLUGINS=1\n# autokey-qt --help\n']"
"['python -m pip install --upgrade pip\npython -m pip install --requirement requirements.txt\npython -m pip install pycodestyle==2.6.0 pytest\nsudo apt install -y build-essential libpoppler-cpp-dev pkg-config python3-dev tesseract-ocr\npython -m pip install pdftotext docutils pygments pytesseract pillow jq\n', 'pytest -v']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\npython -m pip install -e .[signals] -r requirements-dev.txt\n', 'python setup.py sdist bdist_wheel\n', 'python -m pip install --upgrade pip wheel\npython -m pip install -e .[signals] -r requirements-dev.txt\n', 'wget --quiet http://dynamodb-local.s3-website-us-west-2.amazonaws.com/dynamodb_local_latest.tar.gz -O /tmp/dynamodb_local_latest.tar.gz\ntar -xzf /tmp/dynamodb_local_latest.tar.gz -C /tmp\njava -Djava.library.path=/tmp/DynamoDBLocal_lib -jar /tmp/DynamoDBLocal.jar -inMemory -port 8000 &\n', 'pytest --cov-report term-missing --cov=pynamodb tests\n', 'coveralls --service=github\n', 'python -m pip install --upgrade pip\npython -m pip install -e .[signals] -r requirements-dev.txt\n', 'mypy .\n', 'python -m pip install --upgrade pip\npython -m pip install -r docs/requirements.txt\n', 'sphinx-build -W docs /tmp/docs-build\n']"
"['python -m pip install --upgrade pip\ncd docs/\nmake install\n', 'pip install .\npip install .[extra]\ncd ..\nevennia --init gamedir\ncd gamedir\nevennia migrate\n', 'git config --global user.email ""docbuilder@evennia.com""\ngit config --global user.name ""Evennia docbuilder action""\ngit branch\ncd docs\nmake release\n', 'python -m pip install --upgrade pip\npip install wheel\npip install psycopg2-binary==2.9.5   # req by postgresql\npip install mysqlclient\npip install coveralls\npip install tblib\npip install .\npip install .[extra]\n', 'evennia --init testing_mygame\ncp .github/workflows/${{ matrix.TESTING_DB }}_settings.py testing_mygame/server/conf/settings.py\ncd testing_mygame\nevennia migrate\nevennia collectstatic --noinput\n', 'evennia test ${{ env.UNIT_TEST_SETTINGS }} evennia\n', 'coverage run --rcfile=../pyproject.toml ../bin/unix/evennia test ${{ env.COVERAGE_TEST_SETTINGS }} evennia\ncoverage combine\ncoverage xml\ncoverage --version\ncoverage report | grep TOTAL\n', 'cd testing_mygame\ncoveralls\n']"
"['echo ""::group::Run sudo apt-get update""\nsudo apt-get update\necho ""::endgroup::""\necho ""::group::Run sudo apt-get install graphviz""\nsudo apt-get install graphviz\necho ""::endgroup::""\necho ""::group::Run dot -V""\nDOT_VERSION=""$(dot -V 2>&1)""\necho $DOT_VERSION;\necho ""::endgroup::""\necho ""::notice::Apt installed ${DOT_VERSION#dot - }""\n', 'echo ""::group::Run brew update --preinstall""\nbrew update --preinstall\necho ""::endgroup::""\necho ""::group::Run brew install graphviz""\nbrew install graphviz\necho ""::endgroup::""\necho ""::group::Run dot -V""\nDOT_VERSION=""$(dot -V 2>&1)""\necho $DOT_VERSION\necho ""::endgroup::""\necho ""::notice::Brew installed ${DOT_VERSION#dot - }""\n', 'echo ""::group::Run choco install --no-progress graphviz""\nchoco install --no-progress graphviz\necho ""::endgroup::""\necho ""::group::Run dot -V""\nDOT_VERSION=""$(dot -V 2>&1)""\necho $DOT_VERSION\necho ""::endgroup::""\necho ""::notice::Choco installed ${DOT_VERSION#dot - }""\n', 'echo ""::group::Run python -m pip install --upgrade pip setuptools wheel""\npython -m pip install --upgrade pip setuptools wheel\necho ""::endgroup::""\necho ""::group::Run pip install .[test] flake8""\npip install .[test] flake8\necho ""::endgroup::""\n', 'python run-tests.py', 'python run-tests.py --only-exe --cov-append', 'python run-tests.py --skip-exe --cov-append', 'echo ""::group::Run coverage report --fail-under=100 --show-missing --skip-covered""\nFAILED=0\ncoverage report --fail-under=100 --show-missing --skip-covered || FAILED=$?\necho ""::endgroup::""\n[ $FAILED -eq 0 ] || echo ""::warning::coverage report --fail-under=100 failed with exit code $FAILED""\n', 'echo ""::group::Run pip install .[docs]""\npip install .[docs]\necho ""::endgroup::""\necho ""::group::Run build-docs.py -b doctest""\nFAILED=0\npython build-docs.py -b doctest || FAILED=$?\necho ::endgroup::\n[ $FAILED -eq 0 ] || echo ""::warning::build-docs.py -b doctest failed with exit code $FAILED""\n', 'echo ""::group::Run try-examples.py""\nFAILED=0\npython try-examples.py || FAILED=$?\necho ""::endgroup::""\nSEVERITY=warning\n${{ startsWith(matrix.name, \'windows-\') }} && SEVERITY=notice\n[ $FAILED -eq 0 ] || echo ""::$SEVERITY::try-examples.py failed with exit code $FAILED (XFAIL on Windows cbuild)""\n', 'echo ""::group::Run lint-code.py""\nFAILED=0\npython lint-code.py || FAILED=$?\necho ""::endgroup::""\n[ $FAILED -eq 0 ] || echo ""::warning::lint-code.py failed with exit code $FAILED""\n', 'echo ""::group::Run update-help.py""\nFAILED=0\npython update-help.py || FAILED=$?\necho ""::endgroup::""\n[ $FAILED -eq 0 ] || echo ""::warning::update-help.py failed with exit code $FAILED""\n', 'echo ""::group::Run pip install pytype""\npip install pytype\necho ""::endgroup::""\necho ""::group::Run pytype""\nFAILED=0\npytype || FAILED=$?\necho ""::endgroup::""\n[ $FAILED -eq 0 ] || echo ""::warning::pytype failed with exit code $FAILED""\n']"
""
"['brew install nss', 'cd tests\npython show_encodings\npython run_all -v\n']"
"['python -m pip install tox coverage', 'tox -v', 'curl -o codecov.sh -f https://codecov.io/bash || curl -o codecov.sh -f https://codecov.io/bash || curl -o codecov.sh -f https://codecov.io/bash\nbash codecov.sh -n ""tox -e ${{ matrix.PYTHON.TOXENV }}""\n', '/venv/bin/pip install tox', '/venv/bin/tox -v', 'curl -o codecov.sh -f https://codecov.io/bash || curl -o codecov.sh -f https://codecov.io/bash || curl -o codecov.sh -f https://codecov.io/bash\nbash codecov.sh -n ""tox -e ${{ matrix.TEST.TOXENV }} on ${{ matrix.TEST.CONTAINER }}""\n']"
"['pip install -r dev-requirements.txt', 'pip install .', 'pytest']"
"['python -m pip install flake8 pytest pytest-cov', 'python -m flake8', 'python -m pytest --cov', 'coverage lcov']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install tox tox-gh-actions\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', 'tox\n']"
"['pip install -r docs/requirements.txt', 'cd docs\nsphinx-build -b html -n -W --keep-going -j auto . _build\n', 'pip install -r tests/requirements.txt\npip install coveralls\n', 'pytest --cov-config=.coveragerc --cov=redminelib', 'coveralls --service=github', 'pip install coveralls\ncoveralls --service=github --finish\n']"
"['python -m pip install --upgrade pip\npip install -e .[testing]\npip install ""${{ matrix.django }}""\n', 'coverage run --parallel-mode --source wagtail runtests.py\n', 'python -m pip install --upgrade pip\npip install ""psycopg2>=2.6""\npip install -e .[testing]\npip install ""${{ matrix.django }}""\n${{ matrix.install_extras }}\n', 'coverage run --parallel-mode --source wagtail runtests.py\n', 'python -m pip install --upgrade pip\npip install ""mysqlclient>=1.4,<2""\npip install -e .[testing]\npip install ""${{ matrix.django }}""\n', 'coverage run --parallel-mode --source wagtail runtests.py\n', 'sudo swapoff -a\nsudo sysctl -w vm.swappiness=1\nsudo sysctl -w fs.file-max=262144\nsudo sysctl -w vm.max_map_count=262144\n', 'python -m pip install --upgrade pip\npip install -e .[testing]\npip install ""${{ matrix.django }}""\npip install ""elasticsearch>=5,<6""\npip install certifi\n', 'coverage run --parallel-mode --source wagtail runtests.py wagtail.search wagtail.documents wagtail.images --elasticsearch5\n', 'sudo swapoff -a\nsudo sysctl -w vm.swappiness=1\nsudo sysctl -w fs.file-max=262144\nsudo sysctl -w vm.max_map_count=262144\n', 'python -m pip install --upgrade pip\npip install -e .[testing]\npip install ""${{ matrix.django }}""\npip install ""elasticsearch>=7,<8""\npip install certifi\n', 'coverage run --parallel-mode --source wagtail runtests.py wagtail.search wagtail.documents wagtail.images --elasticsearch7\n', 'sudo swapoff -a\nsudo sysctl -w vm.swappiness=1\nsudo sysctl -w fs.file-max=262144\nsudo sysctl -w vm.max_map_count=262144\n', 'python -m pip install --upgrade pip\npip install ""psycopg2>=2.6""\npip install -e .[testing]\npip install ""${{ matrix.django }}""\npip install ""elasticsearch>=6,<7""\npip install certifi\n', 'coverage run --parallel-mode --source wagtail runtests.py wagtail.search wagtail.documents wagtail.images --elasticsearch6\n', 'sudo swapoff -a\nsudo sysctl -w vm.swappiness=1\nsudo sysctl -w fs.file-max=262144\nsudo sysctl -w vm.max_map_count=262144\n', 'python -m pip install --upgrade pip\npip install ""psycopg2>=2.6""\npip install -e .[testing]\npip install ""${{ matrix.django }}""\npip install ""elasticsearch>=7,<8""\npip install certifi\n', 'coverage run --parallel-mode --source wagtail runtests.py wagtail.search wagtail.documents wagtail.images --elasticsearch7\n', 'python -m pip install --upgrade pip\npip install coverage\n', 'coverage combine\ncoverage report -m --skip-covered\n']"
[]
"['python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox -eflake8', 'tox -edocs', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox', 'python -m pip install --upgrade pip wheel', 'pip install tox tox-gh-actions', 'tox']"
"['pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.check }}\n', 'pip install --upgrade pip\npip install tox\n', 'tox -e py', 'pip install --upgrade pip\npip install build\n', 'python -m build .', 'pip install --upgrade pip\npip install build\n', 'python -m build .']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'bash tools/quick_tests.sh ./sunfish.py\n']"
"['pip install --user --upgrade tox', '""$HOME/.local/bin/tox""\n', 'pip install --user --upgrade tox', '""$HOME/.local/bin/tox""\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['for i in 1 2 3; do\n  echo ""try $i"" && sudo apt-get update -y && sudo apt-get install graphviz -y && exit 0;\ndone\nexit 1\n', 'python -m pip install tox', 'import codecs; import os; import sys\nenv = ""TOXENV=py3{}\\n"".format(sys.version_info[1])\nprint(""Picked:\\n{}for{}"".format(env, sys.version))\nwith codecs.open(os.environ[""GITHUB_ENV""], ""a"", ""utf-8"") as file_handler:\n     file_handler.write(env)\n', 'tox -vv --notest', 'tox --skip-pkg-install', 'python -m pip install tox', 'python -m tox -e ${{ matrix.tox_env }}', 'python -m pip install build', 'python -m build --sdist --wheel . -o dist', 'python -m pip install build', 'pyproject-build -s -w . -o dist']"
"['python -m pip install --upgrade build', 'python -m build', 'npx changelogithub --no-group', 'python -m pip install --upgrade pip\npip install pytest pytest-cov\n', 'pytest', 'pytest --cov=mistune --cov-report=xml']"
"['echo ""CI_PYTHON_VERSION=${{ matrix.python-version }}"" >> $GITHUB_ENV\necho ""CI_PACKAGE=colour"" >> $GITHUB_ENV\necho ""CI_SHA=${{ github.sha }}"" >> $GITHUB_ENV\necho ""MPLBACKEND=AGG"" >> $GITHUB_ENV\necho ""COLOUR_SCIENCE__DOCUMENTATION_BUILD=True"" >> $GITHUB_ENV\n', 'sudo apt-get update\nsudo apt-get --yes install graphviz graphviz-dev latexmk texlive-full\n', 'curl -sSL https://install.python-poetry.org | POETRY_HOME=$HOME/.poetry python3 -\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry run python -m pip install --upgrade pip\npoetry install --extras ""read-the-docs""\npoetry run python -c ""import imageio;imageio.plugins.freeimage.download()""\n', 'poetry run invoke docs\n', 'echo ""CI_PYTHON_VERSION=${{ matrix.python-version }}"" >> $GITHUB_ENV\necho ""CI_PACKAGE=colour"" >> $GITHUB_ENV\necho ""CI_SHA=${{ github.sha }}"" >> $GITHUB_ENV\necho ""CI_OPENIMAGEIO_VERSION=2.4.5.0"" >> $GITHUB_ENV\necho ""CI_OPENIMAGEIO_ARTIFACT=https://github.com/colour-science/artifacts/releases/download/OpenImageIO-Release-2.4.5.0/OpenImageIO-Release-2.4.5.0.zip"" >> $GITHUB_ENV\necho ""COVERALLS_REPO_TOKEN=${{ secrets.COVERALLS_REPO_TOKEN }}"" >> $GITHUB_ENV\necho ""MPLBACKEND=AGG"" >> $GITHUB_ENV\n', 'sudo apt-get update\nsudo apt-get --yes install graphviz graphviz-dev libboost-all-dev libilmbase-dev libopenexr-dev libpng-dev libtiff5-dev\n', 'curl -sSL https://install.python-poetry.org | POETRY_HOME=$HOME/.poetry python3 -\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'poetry run python -m pip install --upgrade pip\npoetry install --extras ""graphviz meshing optional plotting""\npoetry run python -c ""import imageio;imageio.plugins.freeimage.download()""\n', 'poetry run python -m pip install --upgrade pip\npoetry install --extras ""meshing optional plotting""\npoetry run python -c ""import imageio;imageio.plugins.freeimage.download()""\n', 'curl -L $CI_OPENIMAGEIO_ARTIFACT -o OpenImageIO-Release-$CI_OPENIMAGEIO_VERSION.zip\nunzip OpenImageIO-Release-$CI_OPENIMAGEIO_VERSION.zip -d OpenImageIO-Release-$CI_OPENIMAGEIO_VERSION\nsudo cp OpenImageIO-Release-$CI_OPENIMAGEIO_VERSION/bin/* /usr/bin/\nsudo cp -r OpenImageIO-Release-$CI_OPENIMAGEIO_VERSION/lib/* /usr/lib/\nsudo rm -rf /usr/lib/python$CI_PYTHON_VERSION\ncp -r OpenImageIO-Release-$CI_OPENIMAGEIO_VERSION/lib/python$CI_PYTHON_VERSION/site-packages/OpenImageIO $(poetry env info -p)/lib/python$CI_PYTHON_VERSION/site-packages/\n', 'poetry run pre-commit run --all-files\n', 'poetry run python -OO -c ""import $CI_PACKAGE""\n', 'poetry run python -W ignore -m pytest --disable-warnings --doctest-modules --ignore=$CI_PACKAGE/examples --cov=$CI_PACKAGE $CI_PACKAGE\n', 'if [ -z ""$COVERALLS_REPO_TOKEN"" ]; then echo \\""COVERALLS_REPO_TOKEN\\"" secret is undefined!; else poetry run coveralls; fi\n', 'echo ""CI_PACKAGE=colour"" >> $GITHUB_ENV\n', 'brew install graphviz\nexport GRAPHVIZ_DIR=""/usr/local/Cellar/graphviz/<VERSION>""\npip install pygraphviz --global-option=build_ext --global-option=""-I$GRAPHVIZ_DIR/include"" --global-option=""-L$GRAPHVIZ_DIR/lib""\n', 'pip install -r requirements.txt\n', 'mypy --install-types --non-interactive --show-error-codes --warn-unused-ignores --warn-redundant-casts $CI_PACKAGE\n']"
"['sudo apt-get install strace', 'echo ""dir=$(pip cache dir)"" >> $GITHUB_OUTPUT', 'pip install . -r dev_requirements.txt', 'pip freeze', 'pip check', 'doit pyflakes', 'doit codestyle', 'py.test -vv ${{ matrix.pytest-args }}', 'pip install codecov\ndoit coverage\ncodecov\n', 'sudo apt-get install hunspell hunspell-en-us', 'pip install . -r doc_requirements.txt -r dev_requirements.txt', 'pip freeze', 'doit -v2 website']"
""
"['python -m pip install wheel', '$WebClient = New-Object System.Net.WebClient\n$WebClient.DownloadFile(""https://github.com/deluge-torrent/gvsbuild-release/releases/download/latest/gvsbuild-py${{ matrix.python }}-vs16-${{ matrix.arch }}.zip"",""C:\\GTK.zip"")\n7z x C:\\GTK.zip -oc:\\GTK\necho ""C:\\GTK\\release\\lib"" | Out-File -FilePath $env:GITHUB_PATH -Append\necho ""C:\\GTK\\release\\bin"" | Out-File -FilePath $env:GITHUB_PATH -Append\necho ""C:\\GTK\\release"" | Out-File -FilePath $env:GITHUB_PATH -Append\npython -m pip install --no-index --find-links=""C:\\GTK\\release\\python"" pycairo PyGObject\n', 'python -m pip install twisted[tls]==22.4.0 libtorrent==${{ matrix.libtorrent }} pyinstaller==4.10 pygame -r requirements.txt\n', 'python -m pip install .\npython setup.py install_scripts\n', 'pyinstaller --clean delugewin.spec --distpath freeze\n', 'cp libssl-1_1.dll libssl-1_1-x64.dll\ncp libcrypto-1_1.dll libcrypto-1_1-x64.dll\n', 'python setup_nsis.py\nmakensis /Darch=${{ matrix.arch }} deluge-win-installer.nsi\n', 'echo ""SECURITY_TESTS=True"" >> $GITHUB_ENV', 'pip install --upgrade pip wheel\npip install -r requirements.txt -r requirements-tests.txt\npip install -e .\n', 'wget -O- $TESTSSL_URL$TESTSSL_VER | tar xz\nmv -t deluge/tests/data testssl.sh-$TESTSSL_VER/testssl.sh testssl.sh-$TESTSSL_VER/etc/;\n', 'sudo mkdir /cores/ && sudo chmod 777 /cores/\necho ""/cores/%E.%p"" | sudo tee /proc/sys/kernel/core_pattern\n', 'ulimit -c unlimited  # Enable core dumps to be captured\npython -c \'from deluge._libtorrent import lt; print(lt.__version__)\';\ncatchsegv python -X dev -m pytest -v -m ""not (todo or gtkui)"" deluge\n', 'pip install --upgrade pip wheel\npip install -r requirements.txt -r requirements-tests.txt\npip install -e .\n', 'python -c \'import libtorrent as lt; print(lt.__version__)\';\npytest -v -m ""not (todo or gtkui or security)"" deluge\n', 'pip install --upgrade pip wheel\npip install tox\nsudo apt-get install enchant\n', 'tox -e $TOX_ENV\n']"
"['python -m pip install tox', 'python -m tox -e mypy', 'python -m pip install tox', 'python -m tox -e py${{ matrix.tox-factor }}', 'python -m pip install tox', 'python -m tox -e lint', 'python -m pip install build twine', 'python -m build', 'twine check dist/*', 'twine upload -u __token__ -p ${{ secrets.PYPI_API_TOKEN }} dist/*']"
""
"['python -m pip install --upgrade pip setuptools wheel\npip install -r requirements/ci.txt\n', 'ruff flask_limiter tests\n', 'black tests flask_limiter\n', 'isort -c --profile=black limits tests\n', 'mypy flask_limiter\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -r requirements/ci.txt\n', 'pip uninstall -y flask werkzeug\npip install ""flask${{ matrix.flask-version }}""\n', 'pytest --cov-report=xml\n', ""coverage report --fail-under=100 || (echo 'Insufficient coverage' && $(exit 1))\n"", 'python -m pip install build\npython -m build --wheel\n', 'pipx run build --sdist\n', './scripts/github_release_notes.sh > release_notes.md\n']"
"['python -c ""import sys; print(sys.version)""', 'pip install tox tox-gh-actions', 'tox -e py3-ci']"
""
['python setup.py bdist_wheel']
"['echo ""::set-output name=value::$(/bin/find bin .github -type f -exec md5sum {} \\; | sort -k 2 | md5sum)""\n', 'bin/install-on-ubuntu\nmkdir -p podman\nsudo podman export mlr > podman/mlr.tar\n', 'cat << EOF | sudo podman exec -i -w /opt/mailur mlr /bin/bash\nset -exuo pipefail\n\n. bin/activate\nbin/manage.py lint --ci\nEOF\n', 'cat << EOF | sudo podman exec -i -w /opt/mailur mlr /bin/bash\nset -exuo pipefail\n\n. bin/activate\nbin/manage.py test\nEOF\n']"
"['python -m pip install -U setuptools wheel pip\n', 'pip install -U poetry\npip install -U poetry-core\n', 'pip install -U pytest\n', 'pip install -U black\npip install -U -r requirements.txt\n', 'pip uninstall -y passlib dnspython loguru\npip install .\n', 'make test', 'echo ""ðŸ’¡ The ${{ github.repository }} repository has been cloned to the runner.""', 'cd tests && pytest -v -x -s\n# ${{ github.workspace }}\n', 'echo ""ðŸ This demo job\'s status is ${{ job.status }}.""']"
"['python -m pip install tox', 'import os; import platform; import sys; from pathlib import Path\nenv = f\'TOXENV=py{"""" if platform.python_implementation() == ""CPython"" else ""py""}3{sys.version_info.minor}\'\nprint(f""Picked: {env} for {sys.version} based of {sys.executable}"")\nwith Path(os.environ[""GITHUB_ENV""]).open(""ta"") as file_handler:\n    file_handler.write(env)\n', 'tox r -vv --notest', 'tox r --skip-pkg-install --', 'tox r --skip-pkg-install', 'import os; import sys\nos.rename(f"".tox/.coverage.{os.environ[\'TOXENV\']}"", f"".tox/.coverage.{os.environ[\'TOXENV\']}-{sys.platform}"")\n', 'python -m pip install tox', 'tox -e coverage --notest', 'python -m pip install build', 'pyproject-build --wheel .', 'tox -e coverage', 'python -m pip install tox', 'tox -vv --notest -e ${{ matrix.tox_env }}', 'tox --skip-pkg-install -e ${{ matrix.tox_env }}', 'python -m pip install build', 'pyproject-build -s -w . -o dist']"
""
"['python -m pip install --upgrade pip setuptools\npython -m pip install tox tox-gh-actions\n', 'python -m tox\npython -m tox -e coverage_report\n']"
"['pip install -r requirements-dev.txt\n', 'python -m pip install --upgrade pip\npip install -r requirements-dev.txt\n', 'pytest\npytest --only-ipython-magic\n', ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n${{ env.img }} \\\nbash -exc '${{ env.py }} -m venv .env && \\\nsource .env/bin/activate && \\\npip install --upgrade pip && \\\npip install -r requirements-dev.txt && \\\npytest && \\\npytest --only-ipython-magic && \\\ndeactivate'\n"", 'pipx run check-manifest', 'python setup.py sdist']"
"['python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install tox\n', 'python -m tox -e ${{ matrix.env.tox }}', 'bash ./tests/check_pre_commit.sh\n', 'python -m pip install tox\n', 'python -m tox -e coverage', 'python -m pip install tox\n', 'python -m tox -e setup']"
""
"['python3 -m pip install setuptools wheel twine\n', 'python3 utils/build-dists.py\n', ""set -exo pipefail;\nif [ $(python3 -m twine check dist/* | grep -c 'warning') != 0 ]; then exit 1; fi\n"", 'python3 -m pip install nox\n', 'nox -s lint', 'python3 -m pip install nox\n', 'nox -s docs', 'mkdir /tmp/elasticsearch\nwget -O - https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-${{ matrix.es-version }}-linux-x86_64.tar.gz | tar xz --directory=/tmp/elasticsearch --strip-components=1\n/tmp/elasticsearch/bin/elasticsearch -d\n', 'python3 -m pip install nox\n', 'nox -rs test-${{ matrix.python-version }}\n']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npip install -r requirements.txt\n', 'python setup.py install\n', 'seleniumbase\nsbase\n', 'seleniumbase install chromedriver\n', 'echo ""def test_1(): pass"" > nothing.py\npytest nothing.py\n', 'echo ""def test_2(): pass"" > nothing2.py\nnosetests nothing2.py\n', 'pytest examples/unit_tests/verify_framework.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/offline_examples --rs --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/boilerplates/boilerplate_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_window_switching.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pip install seleniumbase -U --no-deps --force-reinstall --no-cache-dir\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npip install -r requirements.txt\n', 'python setup.py install\n', 'seleniumbase\nsbase\n', 'seleniumbase install chromedriver\n', 'echo ""def test_1(): pass"" > nothing.py\npytest nothing.py\n', 'echo ""def test_2(): pass"" > nothing2.py\nnosetests nothing2.py\n', 'pytest examples/unit_tests/verify_framework.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/offline_examples --rs --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/boilerplates/boilerplate_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_window_switching.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pip install seleniumbase -U --no-deps --force-reinstall --no-cache-dir\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npip install -r requirements.txt\n', 'python setup.py install\n', 'seleniumbase\nsbase\n', 'seleniumbase install chromedriver\n', 'echo ""def test_1(): pass"" > nothing.py\npytest nothing.py\n', 'echo ""def test_2(): pass"" > nothing2.py\nnosetests nothing2.py\n', 'pytest examples/unit_tests/verify_framework.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/offline_examples --rs --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/boilerplates/boilerplate_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_window_switching.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pip install seleniumbase -U --no-deps --force-reinstall --no-cache-dir\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npip install -r requirements.txt\n', 'python setup.py install\n', 'pip install flake8\n# Stop the build if there are flake8 issues\nflake8 . --count --show-source --statistics --exclude=temp\n', 'sudo apt install google-chrome-stable\n', 'seleniumbase\nsbase\n', 'seleniumbase install chromedriver\n', 'echo ""def test_1(): pass"" > nothing.py\npytest nothing.py\n', 'echo ""def test_2(): pass"" > nothing2.py\nnosetests nothing2.py\n', 'pytest examples/unit_tests/verify_framework.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/boilerplates/boilerplate_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_demo_site.py --browser=chrome --xvfb -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/iframe_tests.py --browser=chrome --xvfb --rs --crumbs -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_mfa_login.py --browser=chrome --xvfb -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/iframe_tests.py --browser=chrome --xvfb --rs -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_window_switching.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/my_first_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_inspect_html.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'behave examples/behave_bdd/features/calculator.feature -D rs -D crumbs -D xvfb -T -k\n', 'behave examples/behave_bdd/features/realworld.feature -D rs -D crumbs -D xvfb -T -k\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npip install -r requirements.txt\n', 'python setup.py install\n', 'pip install flake8\n# Stop the build if there are flake8 issues\nflake8 . --count --show-source --statistics --exclude=temp\n', 'sudo apt install google-chrome-stable\n', 'seleniumbase\nsbase\n', 'seleniumbase install chromedriver\n', 'echo ""def test_1(): pass"" > nothing.py\npytest nothing.py\n', 'echo ""def test_2(): pass"" > nothing2.py\nnosetests nothing2.py\n', 'pytest examples/unit_tests/verify_framework.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/boilerplates/boilerplate_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_demo_site.py --browser=chrome --xvfb -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/iframe_tests.py --browser=chrome --xvfb --rs --crumbs -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_mfa_login.py --browser=chrome --xvfb -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/iframe_tests.py --browser=chrome --xvfb --rs -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_window_switching.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/my_first_test.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'pytest examples/test_inspect_html.py --browser=chrome --headless -v -s --junit-xml=junit/test-results.xml\n', 'behave examples/behave_bdd/features/calculator.feature -D rs -D crumbs -D xvfb -T -k\n', 'behave examples/behave_bdd/features/realworld.feature -D rs -D crumbs -D xvfb -T -k\n']"
"[""shopt -s globstar &&\n! grep -E '.{80}' **/*.py\n""]"
"['sudo apt-get update -q\nsudo apt-get install -qqy nasm\n', 'cd drivers/linux\nmake\n', 'apk --no-cache --update add linux${{ matrix.distro.variant }} linux${{ matrix.distro.variant }}-dev nasm\n\n# DKMS is not yet packaged in Alpine\napk --no-cache --update add bash gcc git make\ngit clone --depth=1 --branch=v3.0.5 https://github.com/dell/dkms /opt/dkms\nmake -C /opt/dkms install\n', 'pacman -Syu --noconfirm dkms linux${{ matrix.distro.variant }}-headers nasm\n', 'if [ ""${{ matrix.distro.tag }}"" = 7 ] ; then\n    yum install -y kernel kernel-devel nasm\n    yum install -y elfutils-libelf-devel gcc git make\nelif [ ""${{ matrix.distro.tag }}"" = stream8 ] ; then\n    dnf install -y --enablerepo=powertools kernel kernel-devel nasm\n    dnf install -y elfutils-libelf-devel gcc git make\nelif [ ""${{ matrix.distro.tag }}"" = stream9 ] ; then\n    dnf install -y --enablerepo=crb kernel kernel-devel nasm\n    dnf install -y elfutils-libelf-devel gcc git make\nfi\n\n# DKMS is not longer packaged in CentOS Stream\nif ! command -v dkms > /dev/null 2>&1 ; then\n    git clone --depth=1 --branch=v3.0.5 https://github.com/dell/dkms /opt/dkms\n    make -C /opt/dkms install\nfi\n', 'apt-get update -q\napt-get install -qqy dkms nasm\n', 'apt-get update -q\napt-get install -qqy dkms linux-headers-generic nasm\n', 'KERNEL_VER=\'\'\nif [ ""${{ matrix.distro.name }}"" = alpine ] ; then\n    # Parse ""lib/modules/5.15.53-0-lts/build""\n    KERNEL_VER=""$(apk info --contents ""linux${{ matrix.distro.variant }}-dev"" | sed -n \'s:^lib/modules/\\([^/][^/]*\\)/.*:\\1:p\' | head -n 1)""\nelif [ ""${{ matrix.distro.name }}"" = archlinux ] ; then\n    # Parse ""/usr/lib/modules/5.18.0-arch1-1/build/""\n    KERNEL_VER=""$(pacman -Qql ""linux${{ matrix.distro.variant }}-headers"" | sed -n \'s:^/usr/lib/modules/\\([^/]\\+\\)/.*:\\1:p\' | head -n 1)""\nelif [ ""${{ matrix.distro.name }}"" = centos ] ; then\n    # Parse ""Source RPM  : kernel-3.10.0-1160.71.1.el7.src.rpm""\n    KERNEL_VER=""$(LANG=C rpm -qi kernel-devel | sed -n \'s/^Source RPM *: kernel-\\(.*\\).src.rpm$/\\1.x86_64/p\' | tail -n 1)""\nelif [ ""${{ matrix.distro.name }}"" = debian ] ; then\n    # Parse ""Depends: linux-headers-5.10.0-15-amd64 (= 5.10.120-1)""\n    KERNEL_VER=""$(LANG=C dpkg --status linux-headers-amd64 | sed -n \'s/^Depends: linux-headers-\\(\\S*\\)\\( .*\\)\\?$/\\1/p\' | head -n 1)""\nelif [ ""${{ matrix.distro.name }}"" = ubuntu ] ; then\n    # Parse ""Depends: linux-headers-5.15.0-40-generic""\n    KERNEL_VER=""$(LANG=C dpkg --status linux-headers-generic | sed -n \'s/^Depends: linux-headers-\\(\\S*\\)\\( .*\\)\\?$/\\1/p\' | head -n 1)""\nfi\nif [ -z ""${KERNEL_VER}"" ] ; then\n    echo >&2 ""Error: no kernel package found""\n    exit 1\nfi\necho ""Found packaged kernel ${KERNEL_VER}""\necho ""KERNEL_VER=${KERNEL_VER}"" >> ""$GITHUB_ENV""\n\nCHIPSEC_MODULE_VER=""$(cat chipsec/VERSION)""\necho ""CHIPSEC_MODULE_VER=${CHIPSEC_MODULE_VER}"" >> ""$GITHUB_ENV""\n\necho ""kernel=${KERNEL_VER}"" >> ""$GITHUB_OUTPUT""\necho ""chipsec=${CHIPSEC_MODULE_VER}"" >> ""$GITHUB_OUTPUT""\necho ""uname_m=$(uname -m)"" >> ""$GITHUB_OUTPUT""\n', 'echo ""Building chipsec ${CHIPSEC_MODULE_VER} for Linux kernel ${KERNEL_VER}""\ndkms add drivers/linux\ndkms install -m chipsec -v ""${CHIPSEC_MODULE_VER}"" -k ""${KERNEL_VER}""\n', 'dkms status', 'MODULE=""$(ls -1 ""/var/lib/dkms/chipsec/${CHIPSEC_MODULE_VER}/${KERNEL_VER}/$(uname -m)/module/chipsec.ko""* | head -n1)""\necho ""module_path=${MODULE}"" >> ""$GITHUB_OUTPUT""\nmodinfo ""${MODULE}""\n', 'pip install -r windows_requirements.txt', 'python3 setup.py build_ext -i', 'python setup.py test', ""sed 's/^        return modules_failed$/        return 0/' -i chipsec_main.py\n"", 'sudo apt-get update -q\nsudo apt-get install -qqy dkms nasm python3-setuptools\npip install distro\npip install -r linux_requirements.txt\n', 'python3 setup.py build_ext -i', 'KERNEL_VER=""$(uname -r)""\nCHIPSEC_MODULE_VER=""$(cat chipsec/VERSION)""\necho ""Building chipsec ${CHIPSEC_MODULE_VER} for Linux kernel ${KERNEL_VER}""\nsudo dkms add drivers/linux\nsudo dkms install -m chipsec -v ""${CHIPSEC_MODULE_VER}"" -k ""${KERNEL_VER}""\n', 'python3 setup.py test', 'sudo python3 setup.py install', 'PYTHONEXE=""$(which python)""\nsudo ${PYTHONEXE} chipsec_main.py --ignore_platform\n']"
"['pip install flake8', 'flake8 .', 'python xlwingsjs/check_version.py', 'python -m pip install pythonnet wheel setuptools-rust\n', 'mkdir ./aspose\ncurl -L https://www.nuget.org/api/v2/package/Aspose.Cells/17.12.0 -o ./aspose/aspose.cells.nupkg\nunzip ./aspose/aspose.cells.nupkg -d ./aspose/\n', 'msbuild $Env:GITHUB_WORKSPACE\\xlwingsdll\\xlwings.sln /p:Configuration=Release -maxcpucount\nmsbuild $Env:GITHUB_WORKSPACE\\xlwingsdll\\xlwings.sln /p:Configuration=Release /p:Platform=x64 -maxcpucount\n', 'echo ""C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64"" >> $GITHUB_PATH', 'signtool sign /f ${{ steps.write_file.outputs.filePath }} /p ${{ secrets.CODESIGN_PASSWORD }} /tr http://timestamp.sectigo.com /td sha256 /fd SHA256 xlwings32.dll xlwings64.dll\n', 'python ./scripts/build_excel_files.py\n', '# Microsoft Office Subject Interface Packages for Digitally Signing VBA Projects: https://www.microsoft.com/en-us/download/details.aspx?id=56617\n# NOTE: see README in officesips.exe: the whole stack needs to be x86, including signtool, regsvr32.exe and C++ Redistribution Runtime 2010\nmkdir ./officesips\ncurl -L https://download.microsoft.com/download/F/B/4/FB46F8CA-6A6F-4CB0-B8F4-06BF3D44DA48/officesips.exe -o ./officesips.exe\n7z x ./officesips.exe -o./officesips\n', '# https://vcredist.com\n# Must be 2010: https://stackoverflow.com/questions/60239192/signtool-exe-error-signersign-failed-2147220492-0x800403f4-when-signing-ex\nInstall-Module -Name VcRedist -Force\nNew-Item -Path .\\VcRedist -ItemType Directory\n$VcRedists = Get-VcList -Export Unsupported | Where-Object { $_.Release -eq ""2010"" -and $_.Architecture -eq ""x86"" }\nSave-VcRedist -VcList $VcRedists -Path .\\VcRedist\nInstall-VcRedist -VcList $VcRedists -Path .\\VcRedist\n', '# Make sure it uses the 32bit version of regsvr32.exe\nC:\\Windows\\system32\\regsvr32.exe officesips\\msosipx.dll\n', '# Office 365 supports three different signatures on VBA projects, which is the reason why OffSign.bat signs and verifies the files 3x\n# Note that signtools is in ...\\bin\\x86\n.\\officesips\\OffSign.bat ""C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x86\\"" ""sign /f ${{ steps.write_file.outputs.filePath }} /p ${{ secrets.CODESIGN_PASSWORD }} /tr http://timestamp.sectigo.com /td sha256 /fd SHA256"" ""verify /pa"" "".\\xlwings\\addin\\xlwings.xlam""\n', 'python -m pip install wheel setuptools-rust pytest', 'rm pyproject.toml\npython setup.py sdist bdist_wheel\n', 'python -m pip install xlwings --no-index --find-links dist --force-reinstall  --no-deps\ncd ..\npython -c ""import xlwings;print(xlwings.__version__)""\npython -c ""import xlwings;print(xlwings.__path__)""\nXLWINGS_ENGINE=remote pytest xlwings/tests/test_engines/test_engines.py\nXLWINGS_ENGINE=calamine XLWINGS_FILE_EXTENSION=xlsm pytest xlwings/tests/test_engines/test_engines.py\nXLWINGS_ENGINE=calamine XLWINGS_FILE_EXTENSION=xlsb pytest xlwings/tests/test_engines/test_engines.py\nXLWINGS_ENGINE=calamine XLWINGS_FILE_EXTENSION=xls pytest xlwings/tests/test_engines/test_engines.py\nxlwings quickstart testproject1\n', 'echo ""C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64"" >> $GITHUB_PATH', 'pip install pyinstaller pywin32 msal watchgod', 'pyinstaller ./xlwings/cli.py --onefile --name xlwings --icon ./resources/xlwings.ico\ncp ./dist/xlwings.exe ./xlwings.exe\n', 'signtool sign /f ${{ steps.write_file.outputs.filePath }} /p ${{ secrets.CODESIGN_PASSWORD }} /tr http://timestamp.sectigo.com /td sha256 /fd SHA256 xlwings.exe\n', 'python -m pip install wheel twine\n', 'twine upload Package/*.tar.gz\ntwine upload Package/*.whl\ncurl -X POST -d {} https://api.netlify.com/build_hooks/$NETLIFY_BUILD_HOOK\n']"
"['python -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade coveralls tox tox-py tox-venv\n', 'tox --py current', 'tox -e ${{ matrix.toxenv }}', 'tox -e pypy3-django22,pypy3-django32', 'coveralls --service=github', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build\n']"
"['python -m pip install coverage tox', 'python -m tox']"
"[""pip install -r <(grep ansible tests/requirements.txt) ansible-lint==4.3.7 'rich>=9.5.1,<11.0.0' netaddr"", 'ansible-galaxy install -r requirements.yml', 'ansible-lint -x 106,204,205,208 -v --force-color ./roles/*/ ./infrastructure-playbooks/*.yml site-container.yml.sample site-container.yml.sample dashboard.yml', 'ansible-playbook -i ./tests/functional/all_daemons/hosts site.yml.sample --syntax-check --list-tasks -vv', 'ansible-playbook -i ./tests/functional/all_daemons/hosts site-container.yml.sample --syntax-check --list-tasks -vv', 'ansible-playbook -i ./tests/functional/all_daemons/hosts dashboard.yml --syntax-check --list-tasks -vv', 'ansible-playbook -i ./tests/functional/all_daemons/hosts infrastructure-playbooks/*.yml --syntax-check --list-tasks -vv', 'if [[ -n $(grep --exclude-dir=.git -P ""\\xa0"" -r .) ]]; then echo \'NBSP characters found\'; exit 1; fi', '${GITHUB_WORKSPACE}/tests/scripts/workflows/defaults.sh', 'pip install flake8', 'flake8 --max-line-length 160 ./library/ ./module_utils/ ./plugins/filter/ ./tests/library/ ./tests/module_utils/ ./tests/plugins/filter/ ./tests/conftest.py ./tests/functional/tests/', 'pip install -r tests/requirements.txt', 'pytest --cov=library/ --cov=module_utils/ --cov=plugins/filter/ -vvvv tests/library/ tests/module_utils/ tests/plugins/filter/', '${GITHUB_WORKSPACE}/tests/scripts/workflows/signed-off.sh']"
"['python -m pip install --upgrade pip\npython -m pip install pytest\nif [ -f requirements.test.txt ]; then pip install -r requirements.test.txt; fi\n', 'python shapefile.py\n', 'pytest\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
""
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions codecov\n', 'tox -- -v', 'codecov']"
""
"['pip install black==22.3 flake8==5.0.4 flake8-future-import flake8-logging-format flake8-import-order flake8-quotes flake8-black\n', 'black --version\nflake8 --version\nflake8\n', 'wget -O clang-format https://github.com/DMOJ/clang-tools-static-binaries/releases/download/master-5ea3d18c/clang-format-12_linux-amd64\nchmod a+x ./clang-format\n', ""find dmoj/ \\( -name '*.h' -or -name '*.cpp' -or -name '*.c' \\) -not -name _cptbox.cpp -print0 | xargs -0 ./clang-format --dry-run -Werror --color"", 'pip install cython mypy types-termcolor types-requests types-PyYAML\npip install -r requirements.txt\n', 'mypy --version\nmypy dmoj\n', 'pip install cython', 'python setup.py sdist', 'sudo apt-get install -y libseccomp-dev', 'pip install dist/*.tar.gz\ndmoj-autoconf\n', 'docker pull dmoj/runtimes-tier3', 'curl -L ""https://github.com/DMOJ/runtimes-python/releases/latest/download/python${{ matrix.python-version }}-amd64.tar.gz"" |\ntar -xz\n', 'cat > run <<\'EOF\'\n#!/bin/bash -e\nexport PYTHONUNBUFFERED=1\nexport LANG=C.UTF-8\nexport PYTHONIOENCODING=utf8\ncd /code\nexport PYTHON=""/code/python${{ matrix.python-version }}/bin/python${{ matrix.python-version }}""\n""$PYTHON"" -m pip install --upgrade pip wheel\n""$PYTHON"" -m pip install cython coverage\n""$PYTHON"" -m pip install -e .[test]\nchmod o+w .\nrunuser -u judge -w PATH /code/run-su\nEOF\n\ncat > run-su <<\'EOF\'\n#!/bin/bash -e\n. ~/.profile\ncd /code\n""$PYTHON"" -m coverage run -m unittest discover dmoj/tests/\n""$PYTHON"" -m coverage run --append .docker.test.py\n""$PYTHON"" -m coverage combine\n""$PYTHON"" -m coverage xml\nEOF\n\nchmod a+x run run-su\n', 'mkdir -p ""$HOME/docker-cache/pip""\nsudo chown root:root -R ""$HOME/docker-cache""\n', 'docker run -e PYTHON_VERSION=""${{ matrix.python-version }}"" -v ""$(pwd):/code"" -v ""$HOME/docker-cache:/root/.cache"" --cap-add=SYS_PTRACE dmoj/runtimes-tier3', 'sudo chown -R ""$USER:$USER"" ""$HOME/docker-cache""', 'docker pull dmoj/runtimes-tier3', 'cat > run <<\'EOF\'\n#!/bin/bash -e\ncp -r /source /code\ncd /code\ncurl -L ""https://github.com/DMOJ/runtimes-python/releases/latest/download/python${{ matrix.python-version }}-aarch64.tar.gz"" | tar -xz\n\nexport PYTHONUNBUFFERED=1\nexport LANG=C.UTF-8\nexport PYTHONIOENCODING=utf8\nexport PYTHON=""/code/python${{ matrix.python-version }}/bin/python${{ matrix.python-version }}""\n\n""$PYTHON"" -m pip install --upgrade pip wheel\n""$PYTHON"" -m pip install cython coverage\n""$PYTHON"" -m pip install -e .[test]\nchmod o+w .\n\ncode=0\nrunuser -u judge -w PATH /source/run-su || code=$?\ncp /code/coverage.xml /source || true\nexit ""$code""\nEOF\n\ncat > run-su <<\'EOF\'\n#!/bin/bash -e\n. ~/.profile\ncd /code\n""$PYTHON"" -m coverage run -m unittest discover dmoj/tests/\n""$PYTHON"" -m coverage run --append .docker.test.py\n""$PYTHON"" -m coverage combine\n""$PYTHON"" -m coverage xml\nEOF\n\nchmod a+x run run-su\n', 'docker run -e PYTHON_VERSION=""${{ matrix.python-version }}"" -v ""$(pwd):/source"" -v ""$HOME/docker-cache:/root/.cache"" \\\n  --entrypoint=/usr/bin/tini --cap-add=SYS_PTRACE dmoj/runtimes-tier3 /source/run\n', 'cd dmoj/cptbox/syscalls\npython generate.py\n', 'pip install cython', 'python setup.py sdist', 'sudo apt-get install -y libseccomp-dev', 'pip install dist/*.tar.gz\ndmoj-autoconf\n']"
"['pip install flake8 flake8-import-order flake8-future-import flake8-commas flake8-logging-format flake8-quotes', 'flake8 --version\nflake8\n', 'pip install wheel\npip install -r requirements.txt\npip install mysqlclient coverage\ncp .ci.settings.py dmoj/local_settings.py\n', 'sudo systemctl start mysql.service', 'coverage run --source=. manage.py test judge', 'coverage xml', 'npm install -g sass postcss-cli postcss autoprefixer', './make_style.sh', 'curl -s https://raw.githubusercontent.com/Fyrd/caniuse/master/data.json | python3 -m json.tool > resources/caniuse.json\n', 'git submodule init\ngit submodule update\n', 'sudo apt-get install gettext\npip install -r requirements.txt\npip install pymysql\n', 'fail=0\nwhile read -r file; do\n    if ! msgfmt --check-format ""$file""; then\n        fail=$((fail + 1))\n    fi\ndone < <(find locale -name \'*.po\')\nexit ""$fail""\n', 'echo ""STATIC_ROOT = \'/tmp\'"" > dmoj/local_settings.py\npython manage.py compilemessages\n', 'git submodule init\ngit submodule update\n', 'sudo apt-get install gettext\ncurl -O https://artifacts.crowdin.com/repo/deb/crowdin.deb\nsudo dpkg -i crowdin.deb\npip install -r requirements.txt\npip install pymysql\n', 'echo ""STATIC_ROOT = \'/tmp\'"" > dmoj/local_settings.py\npython manage.py makemessages -l en -e py,html,txt\npython manage.py makemessages -l en -d djangojs\n', 'cat > crowdin.yaml <<EOF\nproject_identifier: dmoj\n\nfiles:\n  - source: /locale/en/LC_MESSAGES/django.po\n    translation: /locale/%two_letters_code%/LC_MESSAGES/django.po\n    languages_mapping:\n      two_letters_code:\n        zh-CN: zh_Hans\n        sr-CS: sr_Latn\n  - source: /locale/en/LC_MESSAGES/djangojs.po\n    translation: /locale/%two_letters_code%/LC_MESSAGES/djangojs.po\n    languages_mapping:\n      two_letters_code:\n        zh-CN: zh_Hans\n        sr-CS: sr_Latn\nEOF\necho ""api_key: ${CROWDIN_API_TOKEN}"" >> crowdin.yaml\ncrowdin upload sources\n', 'git submodule init\ngit submodule update\n', 'sudo apt-get install gettext\ncurl -O https://artifacts.crowdin.com/repo/deb/crowdin.deb\nsudo dpkg -i crowdin.deb\npip install -r requirements.txt\npip install pymysql\n', 'echo ""STATIC_ROOT = \'/tmp\'"" > dmoj/local_settings.py\npython manage.py makemessages -l en -e py,html,txt\npython manage.py makemessages -l en -d djangojs\n', 'cat > crowdin.yaml <<EOF\nproject_identifier: dmoj\n\nfiles:\n  - source: /locale/en/LC_MESSAGES/django.po\n    translation: /locale/%two_letters_code%/LC_MESSAGES/django.po\n    languages_mapping:\n      two_letters_code:\n        zh-CN: zh_Hans\n        zh-TW: zh_Hant\n        sr-CS: sr_Latn\n  - source: /locale/en/LC_MESSAGES/djangojs.po\n    translation: /locale/%two_letters_code%/LC_MESSAGES/djangojs.po\n    languages_mapping:\n      two_letters_code:\n        zh-CN: zh_Hans\n        zh-TW: zh_Hant\n        sr-CS: sr_Latn\nEOF\necho ""api_key: ${CROWDIN_API_TOKEN}"" >> crowdin.yaml\ncrowdin download\nrm crowdin.yaml\n', 'rm -rf src/\ngit add locale\ngit checkout .\ngit clean -fd\ngit reset\n', 'set -eu\ni18n_head=$(git rev-parse --verify -q origin/update-i18n || echo ""HEAD"")\necho ""Comparing against: $i18n_head""\nchanged=0\nwhile IFS= read -r -d $\'\\0\' file; do\n  if git diff -s --exit-code ""$file""; then\n    continue\n  fi\n\n  if git diff ""$i18n_head"" -- ""$file"" | tail -n +5 2>/dev/null | grep -qP \'^[-+](?!""PO-Revision-Date:|""POT-Creation-Date:)\'; then\n    echo ""Changed: $file""\n    changed=1\n  else\n    echo ""No material change: $file""\n    git checkout ""$i18n_head"" -- ""$file"" > /dev/null 2>&1\n  fi\ndone < <(git ls-files -z \'locale/*/LC_MESSAGES/*.po\')\n\nif [ ""$changed"" = 0 ]; then\n  git reset --hard ""$i18n_head""\nfi\n']"
"['python -m pip install --upgrade pip\nsudo apt-get update\nsudo apt install -y libgirepository1.0-dev gir1.2-gtk-3.0 libgtksourceview-4-dev\npip install --user -e git+https://github.com/getting-things-gnome/liblarch.git#egg=liblarch\npip install --user pytest pycairo PyGObject caldav mock lxml\n', './run-tests\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest pytest-cov\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python setup.py install\npy.test --cov-report=xml --cov=plantcv tests/\n', 'python -m pip install --upgrade pip\npip install pytest\npip install -r requirements.txt\n', 'python setup.py install\npy.test tests/\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
""
"['sudo apt-get update\nsudo apt-get install --no-install-recommends -y libpulse-dev libasound2-dev\nsudo apt-get install --no-install-recommends -y ffmpeg\n', ""python -m pip install 'pocketsphinx<5'\npython -m pip install git+https://github.com/openai/whisper.git soundfile\npython -m pip install .\n"", 'python -m unittest discover --verbose\n']"
"['tests/${{ matrix.test_script }}.sh', 'wget -O tests/test.sh https://gist.githubusercontent.com/geerlingguy/73ef1e5ee45d8694570f334be385e181/raw/\nchmod +x tests/test.sh\n', 'tests/test.sh', 'pip3 install ansible molecule[docker] yamllint ansible-lint', 'molecule test']"
"['./scripts/github/setup-environment.sh\n', '# install dev dependencies for Python YAML and LDAP packages\n# https://github.com/StackStorm/st2-auth-ldap\n./scripts/github/install-apt-packages-use-cache.sh\n', './scripts/github/install-virtualenv.sh\n', './scripts/ci/install-requirements.sh\n', './scripts/ci/print-versions.sh\n', 'script -e -c ""make ${TASK}""\n', './scripts/ci/run-nightly-make-task-if-exists.sh ""${TASK}""\n', './scripts/github/setup-environment.sh\n', '# install dev dependencies for Python YAML and LDAP packages\n# https://github.com/StackStorm/st2-auth-ldap\n./scripts/github/install-apt-packages-use-cache.sh\n', './scripts/github/install-virtualenv.sh\n', './scripts/ci/install-requirements.sh\n', '# prep a ci-specific dev conf file that uses runner instead of stanley\n# this user is the username of the user in GitHub actions, used for SSH, etc during\n# integration tests (important)\ncp conf/st2.dev.conf ""${ST2_CONF}"" ; sed -i -e ""s/stanley/${ST2_CI_USER}/"" ""${ST2_CONF}""\n\nsudo -E ./scripts/ci/add-itest-user-key.sh\n', 'echo ""$ST2_CI_REPO_PATH""\nsudo ST2_CI_REPO_PATH=""${ST2_CI_REPO_PATH}"" scripts/ci/permissions-workaround.sh\n', './scripts/github/configure-rabbitmq.sh\n', './scripts/ci/print-versions.sh\n', 'script -e -c ""make ${TASK}""\n', './scripts/ci/run-nightly-make-task-if-exists.sh ""${TASK}""\n', './scripts/ci/submit-codecov-coverage.sh\n', './scripts/github/setup-environment.sh\n', '# install dev dependencies for Python YAML and LDAP packages\n# https://github.com/StackStorm/st2-auth-ldap\n./scripts/github/install-apt-packages-use-cache.sh\n', './scripts/github/install-virtualenv.sh\n', './scripts/ci/install-requirements.sh\n', '# prep a ci-specific dev conf file that uses runner instead of stanley\n# this user is the username of the user in GitHub actions, used for SSH, etc during\n# integration tests (important)\ncp conf/st2.dev.conf ""${ST2_CONF}"" ; sed -i -e ""s/stanley/${ST2_CI_USER}/"" ""${ST2_CONF}""\n\nsudo -E ./scripts/ci/add-itest-user-key.sh\n', 'docker run --rm --detach -p 127.0.0.1:6379:6379/tcp --name redis redis:latest\nuntil [ ""$(docker inspect -f {{.State.Running}} redis)"" == ""true"" ]; do sleep 0.1; done\n', 'echo ""$ST2_CI_REPO_PATH""\nsudo ST2_CI_REPO_PATH=""${ST2_CI_REPO_PATH}"" scripts/ci/permissions-workaround.sh\n', './scripts/github/configure-rabbitmq.sh\n', '\n./scripts/ci/print-versions.sh\n', 'script -e -c ""make ${TASK}"" && exit 0\n', './scripts/ci/submit-codecov-coverage.sh\n', 'tar cvzpf logs.tar.gz logs/*\n', 'docker rm --force redis || true', '# install dev dependencies for Python YAML and LDAP packages\n# https://github.com/StackStorm/st2-auth-ldap\n./scripts/github/install-apt-packages-use-cache.sh\n', './pants lint ::\n', './scripts/github/install-apt-packages-use-cache.sh\n', './scripts/github/install-virtualenv.sh\n', './scripts/ci/install-requirements.sh\n', './scripts/ci/print-versions.sh\n', 'script -e -c ""make ${TASK}"" && exit 0\n', './scripts/github/setup-environment.sh\n', '# install dev dependencies for Python YAML and LDAP packages\n# https://github.com/StackStorm/st2-auth-ldap\n./scripts/github/install-apt-packages-use-cache.sh\n', './scripts/github/install-virtualenv.sh\n', './scripts/ci/install-requirements.sh\n', '# prep a ci-specific dev conf file that uses runner instead of stanley\n# this user is the username of the user in GitHub actions, used for SSH, etc during\n# integration tests (important)\ncp conf/st2.dev.conf ""${ST2_CONF}"" ; sed -i -e ""s/stanley/${ST2_CI_USER}/"" ""${ST2_CONF}""\n\nsudo -E ./scripts/ci/add-itest-user-key.sh\n', 'docker run --rm --detach -p 127.0.0.1:6379:6379/tcp --name redis redis:latest\nuntil [ ""$(docker inspect -f {{.State.Running}} redis)"" == ""true"" ]; do sleep 0.1; done\n', 'echo ""$ST2_CI_REPO_PATH""\nsudo ST2_CI_REPO_PATH=""${ST2_CI_REPO_PATH}"" scripts/ci/permissions-workaround.sh\n', './scripts/ci/print-versions.sh\n', '# There is a race in some orequesta integration tests so they tend to fail quite often.\n# To avoid needed to re-run whole workflow in such case, we should try to retry this\n# specific step. This saves us a bunch of time manually re-running the whole workflow.\n# TODO: Try to identify problematic tests (iirc mostly orquesta ones) and only retry /\n# re-run those.\nset +e\nfor i in $(seq 1 ${MAX_ATTEMPTS}); do\n  echo ""Attempt: ${i}/${MAX_ATTEMPTS}""\n  script -e -c ""timeout 10m make ${TASK}"" && exit 0\n  exit_code=$?\n  echo ""Command failed / timed out (exit_code=${exit_code}), will retry in ${RETRY_DELAY} seconds...""\n  sleep ${RETRY_DELAY}\ndone\nset -e\necho ""Failed after ${MAX_ATTEMPTS} attempts, failing the job.""\nexit 1\n', './scripts/ci/submit-codecov-coverage.sh\n', 'tar cvzpf logs.tar.gz logs/*\n', 'docker rm --force redis || true', './pants tailor --check update-build-files --check ::\n', '# install dev dependencies for Python YAML and LDAP packages\n# https://github.com/StackStorm/st2-auth-ldap\n./scripts/github/install-apt-packages-use-cache.sh\n', './pants test pylint_plugins/:: pants-plugins/::\n']"
"['python -m pip install --upgrade pip\npip install .[test]\n', 'python -m unittest discover -v tests/integration/non_user_endpoints\n', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/ .', 'python -m pip install --upgrade pip\npip install .[test]\n', 'pip install -Iv enum34==1.1.6 # https://bitbucket.org/stoneleaf/enum34/issues/27/enum34-118-broken\npip install flake8\nflake8 . --count --show-source --statistics\n', 'python -m unittest discover -v tests/unit\n']"
"['pip install --upgrade pip wheel', 'pip install bandit black codespell flake8 flake8-bugbear flake8-comprehensions isort mypy pytest pyupgrade safety', 'bandit --recursive --skip B311 .', 'black --check . || true', 'codespell', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics', 'flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --show-source --statistics', 'isort --check-only --profile black . || true', 'pip install -r requirements.txt', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive . || true', 'pytest .', 'shopt -s globstar && pyupgrade --py36-plus **/*.py || true', 'safety check', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install tox\n', 'tox -e ${{ matrix.toxenv }}\n']"
"['pip install --upgrade pipenv\n', 'pipenv install --dev --skip-lock\n', '# stop the build if there are Python syntax errors or undefined names\npipenv run flake8 .\n\n# exit-zero treats all errors as warnings.\npipenv run flake8 . --exit-zero --select=C,E,F,W\n', 'pipenv run black --check .\n', 'pip install --upgrade pipenv\n', 'pipenv install --dev --skip-lock\n', 'pipenv run pytest\n']"
"['python setup.py build_ext -if\n', 'docker pull mariadb:10.11\ndocker run -d -e MARIADB_ROOT_PASSWORD=root -p 3306:3306 --rm --name mariadb mariadb:10.11\nsudo apt-get -y install libmariadb-dev\nmysql --version\nmysql -uroot -proot -h127.0.0.1 -e ""CREATE DATABASE mysqldb_test""\n', 'sudo systemctl start mysql.service\nmysql --version\nmysql -uroot -proot -e ""CREATE DATABASE mysqldb_test""\n', 'pip install -v .\n', 'pip install -r requirements.txt\n', 'pytest --cov=MySQLdb tests\n', 'sudo systemctl start mysql.service\nmysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -uroot -proot mysql\nmysql -uroot -proot -e ""set global innodb_flush_log_at_trx_commit=0;""\nmysql -uroot -proot -e ""CREATE USER \'scott\'@\'%\' IDENTIFIED BY \'tiger\'; GRANT ALL ON *.* TO scott;""\nmysql -uroot -proot -e ""CREATE DATABASE django_default; CREATE DATABASE django_other;""\n', '#pip install -r requirements.txt\n#pip install mysqlclient  # Use stable version\npip install .\n', 'sudo apt-get install libmemcached-dev\nwget https://github.com/django/django/archive/${DJANGO_VERSION}.tar.gz\ntar xf ${DJANGO_VERSION}.tar.gz\ncp ci/test_mysql.py django-${DJANGO_VERSION}/tests/\ncd django-${DJANGO_VERSION}\npip install . -r tests/requirements/py3.txt\n', 'cd django-${DJANGO_VERSION}/tests/\nPYTHONPATH=.. python3 ./runtests.py --settings=test_mysql\n', 'curl -LO ""https://downloads.mariadb.com/Connectors/c/connector-c-${CONNECTOR_VERSION}/mariadb-connector-c-${CONNECTOR_VERSION}-src.zip""\nunzip ""mariadb-connector-c-${CONNECTOR_VERSION}-src.zip"" -d c:/\nmv ""c:/mariadb-connector-c-${CONNECTOR_VERSION}-src"" c:/mariadb-connector-src\n', 'mkdir build\ncd build\ncmake -A x64 .. -DCMAKE_BUILD_TYPE=Release -DCLIENT_PLUGIN_DIALOG=static -DCLIENT_PLUGIN_SHA256_PASSWORD=static -DCLIENT_PLUGIN_CACHING_SHA2_PASSWORD=static\ncmake --build . -j 8 --config Release\ncmake -DCMAKE_INSTALL_PREFIX=c:/mariadb-connector -DCMAKE_INSTALL_COMPONENT=Development -DCMAKE_BUILD_TYPE=Release -P cmake_install.cmake\n', 'pwd\nfind .\ncat <<EOF >site.cfg\n[options]\nstatic = True\nconnector = C:/mariadb-connector\nEOF\ncat site.cfg\n', 'python -m pip install cibuildwheel==2.12.3', 'python -m cibuildwheel --prerelease-pythons --output-dir dist', 'python -m pip install build\npython -m build -s -o dist\n']"
"['wget https://kapeli.com/feeds/zzz/docsetcontrib.tgz; tar -xzf docsetcontrib.tgz;\n./docsetcontrib --verify | tee output.txt;\npassOrFail=$([ ${PIPESTATUS[0]} == 0  ] && echo ""**PASS** ðŸŸ¢"" || echo ""**FAIL** ðŸ”´"");\nresult=""$(cat output.txt)"";\nresult=""${result//\'%\'/%25}"";\nresult=""${result//$\'\\n\'/%0A}"";\nresult=""${result//$\'\\r\'/%0D}"";\necho ""::set-output name=result_code::$result"";\necho ""::set-output name=passOrFail_code::$passOrFail"";\n']"
"['source tools/ci.sh && ci_build_packages_setup', 'source tools/ci.sh && ci_build_packages_check_manifest', 'source tools/ci.sh && ci_build_packages_compile_index', 'source tools/ci.sh && ci_push_package_index', 'source tools/ci.sh && ci_cleanup_package_index ${{ github.event.ref }}', 'source tools/ci.sh && ci_code_formatting_setup', 'source tools/ci.sh && ci_code_formatting_run', 'git diff --exit-code']"
"['pip install -e . # otherwise templates are not packaged\ngit config --global user.email ""github.action@example.com""\ngit config --global user.name ""Github Action""\n', 'putup demo-project -d ""Demonstration of a project generated with PyScaffold"" -u https://pyscaffold.org/ -l MIT --pre-commit\ncd demo-project\necho -e "".. image:: https://readthedocs.org/projects/pyscaffold-demo/badge/?version=latest\\n    :alt: ReadTheDocs\\n    :target: https://pyscaffold-demo.readthedocs.io/\\n\\n$(cat README.rst)"" > README.rst\nrm -rf .git # for github-action-push-to-another-repository to work\n', 'python -m pip install --upgrade pip setuptools tox\npython -m tox -e clean,build\npython -m tox -e publish -- --verbose --repository pypi\n']"
"['pip install tox', 'tox -e py']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade wheel\npython -m pip install --progress-bar=off tox -rci/requirements.txt\n', 'cd examples/${{ matrix.target }}\ntox -v -e ${{ matrix.tox-python-version }}\n', 'python -mpip install --progress-bar=off -r ci/requirements.txt\nvirtualenv --version\npip --version\ntox --version\npip list --format=freeze\n', 'tox -e ${{ matrix.tox_env }} -v\n']"
"['pip install -e .\npip install -r requirements.txt\npip install codecov\n', 'make cov-ci\npython setup.py check -rms\ncodecov\n', 'pip install -e .\npip install -r requirements.txt\n', 'make lint\n', 'python -m pip install -U pip wheel twine', 'python setup.py sdist bdist_wheel', 'twine check dist/*', 'twine upload dist/*\n']"
"['python -m pip install -U tox', 'tox -e py']"
"['sudo apt install libxml2-dev libxslt-dev\nsudo locale-gen de_DE.UTF-8\nsudo locale-gen en_US.UTF-8\nsudo locale-gen ko_KR.UTF-8\nsudo update-locale\n', 'pip install .[test]', 'pytest --cov agate', 'python charts.py', 'pip install --upgrade check-manifest flake8 isort setuptools', 'check-manifest', 'flake8 .', 'isort . --check-only']"
"['set -xe\npython -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade virtualenv tox tox-gh-actions          \n', 'python -m tox']"
"['pip install -r test-requirements.txt\n', 'make lint && tox\n']"
"[""python -m pip install --upgrade pip\npip install flake8 pytest\npip install .[meka,keras,gpl,test]\npip install 'openne @ git+https://github.com/thunlp/OpenNE.git@master#subdirectory=src'\n"", '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['python -m pip install --upgrade setuptools pip wheel\npython -m pip install ""tox<4"" ""tox-gh-actions~=2.8""\n', 'tox -vv']"
"['python -m pip install tox', 'python -m tox -e ${{ matrix.tox-env }}', 'python -m pip install build', 'python -m build .', 'python -m pip install build', 'python -m build .']"
"['python -m pip install --upgrade pip\npip install --upgrade -r requirements-optional.txt\npip install --upgrade -e .\n', 'black --check --diff green example', 'mypy .', 'green -tvvvv green\ncd example && green -tvvvv proj\n', 'pip install --upgrade coveralls\ngreen -tvvvvr green\n', 'coveralls --service=github']"
"['python -m pip install --upgrade cython', './update_cpp.sh', 'pipx run build --sdist', 'python -VV\npython -m site\npython -m pip install --upgrade pip setuptools wheel\npython -m pip install --upgrade virtualenv tox tox-gh-actions\npython -m pip install --upgrade cython\n', './update_cpp.sh', 'python -m tox']"
"['python -m pip install --upgrade pip\npython -m pip install twine\npython -m pip install sphinx\nif [ -f dev-requirements.txt ]; then pip install -r dev-requirements.txt; fi\n', 'python tests.py\npython setup.py sdist\ntwine check dist/*\nsphinx-build -b html docs dist/docs\n', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['echo ""VERSION=$(git describe --tags)"" >> ""$GITHUB_OUTPUT""', 'sed -n -e \'s;^FROM nginxproxy/docker-gen:\\([0-9.]*\\).*;VERSION=\\1;p\' Dockerfile >> ""$GITHUB_OUTPUT""', 'echo ${{ steps.docker_build_debian.outputs.digest }}', 'echo ""VERSION=$(git describe --tags)"" >> ""$GITHUB_OUTPUT""', 'sed -n -e \'s;^FROM nginxproxy/docker-gen:\\([0-9.]*\\).*;VERSION=\\1;p\' Dockerfile >> ""$GITHUB_OUTPUT""', 'echo ${{ steps.docker_build_alpine.outputs.digest }}', 'python -m pip install --upgrade pip\npip install -r python-requirements.txt\n', 'make build-webserver', 'make build-nginx-proxy-test-${{ matrix.base_docker_image }}', 'pytest']"
"['make generate-manifest\npython setup.py sdist\nmv dist/psutil*.tar.gz wheelhouse/\n', 'make generate-manifest\npython setup.py sdist\nmv dist/psutil*.tar.gz wheelhouse/\n', '# py3\npython3 -m pip install flake8 isort\npython3 -m flake8 .\npython3 -m isort .\n# clinter\nfind . -type f \\( -iname ""*.c"" -o -iname ""*.h"" \\) | xargs python3 scripts/internal/clinter.py\n', 'python scripts/internal/print_hashes.py wheelhouse/\npipx run twine check --strict wheelhouse/*\npipx run abi3audit --verbose --strict wheelhouse/*-abi3-*.whl\n', 'python3 -m pip install PyGithub', 'PYTHONUNBUFFERED=1 PYTHONWARNINGS=always python3 .github/workflows/issues.py\n']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox -e lint\n', 'tox -e testcore\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e testcore\n', 'python setup.py sdist', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e docs\n', 'tar -czvf docs.tar.gz -C docs/_build html rtdpage\n', 'mkdir ./${{ env.LATEST_DOCS_DIR }}\ntar -xzf ./docs.tar.gz -C ./${{ env.LATEST_DOCS_DIR }}\n', 'if [[ ${{ env.RELEASE_BUILD }} == true ]]; then\n  echo ""::set-output name=dst_dir::stable""\nelse\n  echo ""::set-output name=dst_dir::latest""\nfi\n', 'rm -rf ${{ env.DOCS_DIR }}/.git\nrm -rf ${{ env.DOCS_DIR }}/en/${{ steps.get-destination-dir.outputs.dst_dir }}\nmkdir -p ${{ env.DOCS_DIR }}/en/${{ steps.get-destination-dir.outputs.dst_dir }}\ncp -rf ${{ env.LATEST_DOCS_DIR }}/html/* ${{ env.DOCS_DIR }}/en/${{ steps.get-destination-dir.outputs.dst_dir }}\nif [[ ${{ env.RELEASE_BUILD }} == false ]]; then\n  rm -rf ${{ env.DOCS_DIR }}/page\n  mkdir -p ${{ env.DOCS_DIR }}/page\n  cp -rf ${{ env.LATEST_DOCS_DIR }}/rtdpage/* ${{ env.DOCS_DIR }}/page\nfi\n', 'if [ -z ""$(ls -A ${{ env.DOCS_DIR }})"" ]; then\n   echo ""Docs folder is empty. Aborting!""\n   exit 1\nfi\n', 'python -m pip install --upgrade pip\npip install tox\n', '# Free space\nsudo apt clean\ndocker rmi $(docker image ls -aq)\ndf -h\ntox -e testexamples\n', 'df -h\ntox -e testexamples\n', 'tox -e testexamples\n', 'pip install -U .', 'pip install esptool==3.*', 'pio run -d ${{ matrix.project.config_dir }} -e ${{ matrix.project.env_name }}']"
"['C:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\python -m pip install poetry', 'cd desktop\nC:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\Scripts\\poetry install\nC:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\Scripts\\poetry env list --full-path\n', 'cd desktop && C:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\Scripts\\poetry run python .\\scripts\\get-tor.py win64', 'if ((Test-Path -Path \'desktop\\onionshare\\resources\\tor\\obfs4proxy.exe\') -eq $True) {\n  Write-Output ""obfs4proxy already built""\n} else {\n  cd desktop\n  .\\scripts\\build-pt-obfs4proxy.ps1\n}\n', 'if ((Test-Path -Path \'desktop\\onionshare\\resources\\tor\\snowflake-client.exe\') -eq $True) {\n  Write-Output ""snowflake already built""\n} else {\n  cd desktop\n  .\\scripts\\build-pt-snowflake.ps1\n}\n', 'if ((Test-Path -Path \'desktop\\onionshare\\resources\\tor\\meek-client.exe\') -eq $True) {\n  Write-Output ""meek already built""\n} else {\n  cd desktop\n  .\\scripts\\build-pt-meek.ps1\n}\n', 'cd desktop\nC:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\Scripts\\poetry run python .\\setup-freeze.py build\nC:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\Scripts\\poetry run python .\\scripts\\build-windows.py cleanup-build\n', 'mv desktop\\build\\exe.win-amd64-3.10\\ ~\\onionshare-win64\nCompress-Archive -LiteralPath ~\\onionshare-win64 -DestinationPath ~\\onionshare-win64.zip\n', 'curl -L https://www.python.org/ftp/python/3.10.9/python-3.10.9-macos11.pkg --output ~/Downloads/python.pkg\nsudo installer -pkg ~/Downloads/python.pkg -target /\n', 'python3 -m pip install poetry\n', 'cd desktop\n/Library/Frameworks/Python.framework/Versions/3.10/bin/poetry install\n', 'cd desktop\n/Library/Frameworks/Python.framework/Versions/3.10/bin/poetry run python ./scripts/get-tor.py macos\n', 'if [[ -f ""desktop/onionshare/resources/tor/obfs4proxy"" ]]; then\n  echo ""obfs4proxy already built""\nelse\n  cd desktop\n  ./scripts/build-pt-obfs4proxy.sh\nfi\n', 'if [[ -f ""desktop/onionshare/resources/tor/snowflake-client"" ]]; then\n  echo ""snowflake already built""\nelse\n  cd desktop\n  ./scripts/build-pt-snowflake.sh\nfi\n', 'if [[ -f ""desktop/onionshare/resources/tor/meek-client"" ]]; then\n  echo ""meek already built""\nelse\n  cd desktop\n  ./scripts/build-pt-meek.sh\nfi\n', 'brew install libiodbc\ncd ~/Downloads\ncurl -O -L https://github.com/PostgresApp/PostgresApp/releases/download/v2.5.12/Postgres-2.5.12-14.dmg\nhdiutil attach Postgres-2.5.12-14.dmg\ncp -r /Volumes/Postgres-2.5.12-14/Postgres.app /Applications/\nhdiutil detach /Volumes/Postgres-2.5.12-14\n', 'cd desktop\n/Library/Frameworks/Python.framework/Versions/3.10/bin/poetry run python ./setup-freeze.py build\n/Library/Frameworks/Python.framework/Versions/3.10/bin/poetry run python ./setup-freeze.py bdist_mac\n/Library/Frameworks/Python.framework/Versions/3.10/bin/poetry run python ./scripts/build-macos.py cleanup-build\n', 'cd desktop/build\ntar -czvf ~/onionshare-macos-intel.tar.gz OnionShare.app\n', 'sudo apt update\nsudo apt install -y flatpak flatpak-builder\nflatpak remote-add --if-not-exists --user flathub https://flathub.org/repo/flathub.flatpakrepo\n', 'cd flatpak\nflatpak-builder build --force-clean --install-deps-from=flathub --install --user org.onionshare.OnionShare.yaml\nflatpak build-bundle ~/.local/share/flatpak/repo ~/OnionShare.flatpak org.onionshare.OnionShare --runtime-repo=https://flathub.org/repo/flathub.flatpakrepo\n', 'sudo lxd init --auto\nsudo snap install snapcraft --classic\nsudo ufw disable\n', 'sudo snapcraft --use-lxd', 'sudo apt-get update\nsudo apt-get -y install tor obfs4proxy\npip install --upgrade pip poetry\n', 'cd cli && poetry install', 'cd cli\npoetry run pytest -v ./tests\npoetry run onionshare-cli --local-only ./tests --auto-stop-timer 2 --verbose\npoetry run onionshare-cli --local-only --receive --auto-stop-timer 2 --verbose\npoetry run onionshare-cli --local-only --website ../docs --auto-stop-timer 2 --verbose\npoetry run onionshare-cli --local-only --chat --auto-stop-timer 5 --verbose\n', 'sudo apt-get update\nsudo apt-get install -y tor obfs4proxy gcc python3-dev python3-pyside2.qtcore python3-pyside2.qtwidgets python3-pyside2.qtgui\nsudo apt-get install -y xvfb x11-utils libxkbcommon-x11-0 libxcb-randr0-dev libxcb-xtest0-dev libxcb-xinerama0-dev libxcb-shape0-dev libxcb-xkb-dev libxcb-render-util0 libxcb-icccm4 libxcb-keysyms1 libxcb-image0\npip install --upgrade pip poetry\n', 'cd desktop && poetry install', 'cd desktop\nQT_QPA_PLATFORM=offscreen QT_DEBUG_PLUGINS=1 xvfb-run poetry run pytest -v ./tests/test_gui_*.py\n']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\npython -m pip install -r tests/requirements.txt\n', ""pytest -o 'flake8-ignore=*.py ALL'"", 'pytest', 'coverage xml', 'python -m pip install --upgrade pip\npip install build\n', 'python -m build']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
""
"['tar xvvf bin.tar.xz\n', 'curl_version=$(curl --version | head -n 1 | awk \'{ print $2 }\')\necho ""cURL is ${curl_version}""\nif [ ""$curl_version"" == ""7.68.0"" ]; then\n  export curl_retry=""--retry 5 --retry-connrefused""\nelse\n  export curl_retry=""--retry 5 --retry-all-errors""\nfi\necho ""curl_retry=${curl_retry}"" >> $GITHUB_ENV\n', 'export PYTHON=$(which python3)\nexport PIP=$(which pip3)\nexport gam=""${PYTHON} -m gam""\nexport gampath=""$(readlink -e .)""\necho -e ""PYTHON: ${PYTHON}\\nPIP: ${PIP}\\gam: ${gam}\\ngampath: ${gampath}""\necho ""PYTHON=${PYTHON}"" >> $GITHUB_ENV\necho ""PIP=${PIP}"" >> $GITHUB_ENV\necho ""gam=${gam}"" >> $GITHUB_ENV\necho ""gampath=${gampath}"" >> $GITHUB_ENV\necho ""JID=${JID}"" >> $GITHUB_ENV\necho ""ACTIONS_CACHE=${ACTIONS_CACHE}"" >> $GITHUB_ENV\necho ""ACTIONS_GOAL=${ACTIONS_GOAL}"" >> $GITHUB_ENV\n', 'echo ""RUNNING: apt update...""\nsudo apt-get -qq --yes update\nsudo apt-get -qq --yes install swig libpcsclite-dev\n', '# Install latest Rust\ncurl $curl_retry -fsS -o rust.sh https://sh.rustup.rs\nbash ./rust.sh -y\nsource $HOME/.cargo/env\n# needed for Rust to compile cryptography Python package for universal2\nrustup target add aarch64-apple-darwin\n', 'echo ""We are running on ${RUNNER_OS}""\nLD_LIBRARY_PATH=""${OPENSSL_INSTALL_PATH}/lib:${PYTHON_INSTALL_PATH}/lib:/usr/local/lib""\nif [[ ""${arch}"" == ""Win64"" ]]; then\n  PYEXTERNALS_PATH=""amd64""\n  PYBUILDRELEASE_ARCH=""x64""\n  GAM_ARCHIVE_ARCH=""x86_64""\n  WIX_ARCH=""x64""\n  CHOC_OPS=""""\nelif [[ ""${arch}"" == ""Win32"" ]]; then\n  PYEXTERNALS_PATH=""win32""\n  PYBUILDRELEASE_ARCH=""Win32""\n  GAM_ARCHIVE_ARCH=""x86""\n  WIX_ARCH=""x86""\n  CHOC_OPS=""--forcex86""\nfi\nif [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  #brew install coreutils\n  #brew install bash\n  MAKE=make\n  MAKEOPT=""-j$(sysctl -n hw.logicalcpu)""\n  PERL=perl\n  echo ""MACOSX_DEPLOYMENT_TARGET=10.15"" >> $GITHUB_ENV\n  echo ""PYTHON=${PYTHON_INSTALL_PATH}/bin/python3"" >> $GITHUB_ENV\n  #echo ""PIP_ARGS=--no-binary=:all:"" >> $GITHUB_ENV\nelif [[ ""${RUNNER_OS}"" == ""Linux"" ]]; then\n  MAKE=make\n  MAKEOPT=""-j$(nproc)""\n  PERL=perl\n  echo ""PYTHON=${PYTHON_INSTALL_PATH}/bin/python3"" >> $GITHUB_ENV\nelif [[ ""${RUNNER_OS}"" == ""Windows"" ]]; then\n  MAKE=nmake\n  MAKEOPT=""""\n  PERL=""c:\\strawberry\\perl\\bin\\perl.exe""\n  LD_LIBRARY_PATH=""${LD_LIBRARY_PATH}:${PYTHON_SOURCE_PATH}/PCbuild/${PYEXTERNALS_PATH}""\n  echo ""PYTHON=${PYTHON_SOURCE_PATH}/PCbuild/${PYEXTERNALS_PATH}/python.exe"" >> $GITHUB_ENV\n  echo ""GAM_ARCHIVE_ARCH=${GAM_ARCHIVE_ARCH}"" >> $GITHUB_ENV\n  echo ""WIX_ARCH=${WIX_ARCH}"" >> $GITHUB_ENV\nfi\necho ""We\'ll run make with: ${MAKEOPT}""\necho ""JID=${jid}"" >> $GITHUB_ENV\necho ""staticx=${staticx}"" >> $GITHUB_ENV\necho ""arch=${arch}"" >> $GITHUB_ENV\necho ""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" >> $GITHUB_ENV\necho ""MAKE=${MAKE}"" >> $GITHUB_ENV\necho ""MAKEOPT=${MAKEOPT}"" >> $GITHUB_ENV\necho ""PERL=${PERL}"" >> $GITHUB_ENV\necho ""PYEXTERNALS_PATH=${PYEXTERNALS_PATH}"" >> $GITHUB_ENV\necho ""PYBUILDRELEASE_ARCH=${PYBUILDRELEASE_ARCH}"" >> $GITHUB_ENV\necho ""openssl_archs=${openssl_archs}"" >> $GITHUB_ENV\n', 'mkdir -vp ""${GITHUB_WORKSPACE}/src""\ncd ""${GITHUB_WORKSPACE}/src""\ngit clone https://github.com/openssl/openssl.git\ncd ""${OPENSSL_SOURCE_PATH}""\nexport LATEST_STABLE_TAG=$(git tag --list openssl-* | grep -v alpha | grep -v beta | sort -Vr | head -n1)\necho ""Checking out version ${LATEST_STABLE_TAG}""\ngit checkout ""${LATEST_STABLE_TAG}""\nexport COMPILED_OPENSSL_VERSION=${LATEST_STABLE_TAG:8} # Trim the openssl- prefix\necho ""COMPILED_OPENSSL_VERSION=${COMPILED_OPENSSL_VERSION}"" >> $GITHUB_ENV\nif [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  for openssl_arch in $openssl_archs; do\n    ssldir=""${OPENSSL_SOURCE_PATH}-${openssl_arch}""\n    mkdir -v ""${ssldir}""\n    cp -vrf ${OPENSSL_SOURCE_PATH}/* ""${ssldir}/""\n  done\n  rm -vrf ""${OPENSSL_SOURCE_PATH}""\nelse\n  mv -v ""${OPENSSL_SOURCE_PATH}"" ""${OPENSSL_SOURCE_PATH}-${openssl_archs}""\nfi\n', 'for openssl_arch in $openssl_archs; do\n  cd ""${GITHUB_WORKSPACE}/src/openssl-${openssl_arch}""\n  # --libdir=lib is needed so Python can find OpenSSL libraries\n  ""${PERL}"" ./Configure ""${openssl_arch}"" --libdir=lib --prefix=""${OPENSSL_INSTALL_PATH}"" $OPENSSL_CONFIG_OPTS\ndone\n', 'mv /usr/bin/link /usr/bin/gnulink', 'for openssl_arch in $openssl_archs; do\n  cd ""${GITHUB_WORKSPACE}/src/openssl-${openssl_arch}""\n  $MAKE ""${MAKEOPT}""\ndone\n', 'if [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  for openssl_arch in $openssl_archs; do\n    cd ""${GITHUB_WORKSPACE}/src/openssl-${openssl_arch}""\n    # install_sw saves us ages processing man pages :-)\n    $MAKE install_sw\n    mv ""${OPENSSL_INSTALL_PATH}"" ""${GITHUB_WORKSPACE}/bin/ssl-${openssl_arch}""\n  done\n  mkdir -vp ""${OPENSSL_INSTALL_PATH}/lib""\n  mkdir -vp ""${OPENSSL_INSTALL_PATH}/bin""\n  for archlib in libcrypto.3.dylib libssl.3.dylib libcrypto.a libssl.a; do\n    lipo -create ""${GITHUB_WORKSPACE}/bin/ssl-darwin64-x86_64/lib/${archlib}"" \\\n                 ""${GITHUB_WORKSPACE}/bin/ssl-darwin64-arm64/lib/${archlib}"" \\\n         -output ""${GITHUB_WORKSPACE}/bin/ssl/lib/${archlib}""\n  done\n  mv ${GITHUB_WORKSPACE}/bin/ssl-darwin64-x86_64/include ${GITHUB_WORKSPACE}/bin/ssl/\n  lipo -create ""${GITHUB_WORKSPACE}/bin/ssl-darwin64-x86_64/bin/openssl"" \\\n               ""${GITHUB_WORKSPACE}/bin/ssl-darwin64-arm64/bin/openssl"" \\\n       -output ""${GITHUB_WORKSPACE}/bin/ssl/bin/openssl""\n  rm -rf ${GITHUB_WORKSPACE}/bin/ssl-darwin64-x86_64\n  rm -rf ${GITHUB_WORKSPACE}/bin/ssl-darwin64-arm64\n  echo ""LDFLAGS=-L${OPENSSL_INSTALL_PATH}/lib"" >> $GITHUB_ENV\n  echo ""CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS=1"" >> $GITHUB_ENV\n  echo ""CFLAGS=-I${OPENSSL_INSTALL_PATH}/include -arch arm64 -arch x86_64 ${CFLAGS}"" >> $GITHUB_ENV\n  echo ""ARCHFLAGS=-arch x86_64 -arch arm64"" >> $GITHUB_ENV\nelse\n  cd ""${GITHUB_WORKSPACE}/src/openssl-${openssl_archs}""\n  # install_sw saves us ages processing man pages :-)\n  $MAKE install_sw\nfi\n', '""${OPENSSL_INSTALL_PATH}/bin/openssl"" version\n""${OPENSSL_INSTALL_PATH}/bin/openssl"" version -f\nfile ""${OPENSSL_INSTALL_PATH}/bin/openssl""\n', 'cd ""${GITHUB_WORKSPACE}/src""\ngit clone https://github.com/python/cpython.git\ncd ""${PYTHON_SOURCE_PATH}""\nexport LATEST_STABLE_TAG=$(git tag --list | grep -v a | grep -v rc | grep -v b | sort -Vr | head -n1)\ngit checkout ""${LATEST_STABLE_TAG}""\nexport COMPILED_PYTHON_VERSION=${LATEST_STABLE_TAG:1} # Trim the ""v"" prefix\necho ""COMPILED_PYTHON_VERSION=${COMPILED_PYTHON_VERSION}"" >> $GITHUB_ENV\n', 'cd ""${PYTHON_SOURCE_PATH}""\nif [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  extra_args=( ""--enable-universalsdk"" ""--with-universal-archs=universal2"" )\nelse\n  extra_args=( )\nfi\n./configure --with-openssl=""${OPENSSL_INSTALL_PATH}"" \\\n            --prefix=""${PYTHON_INSTALL_PATH}"" \\\n            --enable-shared \\\n            --with-ensurepip=upgrade \\\n            --enable-optimizations \\\n            --with-lto \\\n            ""${extra_args[@]}""\n', 'cd ""${env:PYTHON_SOURCE_PATH}""\nPCBuild\\get_externals.bat\n', 'cd ""${env:PYTHON_SOURCE_PATH}""\n$env:OPENSSL_EXT_PATH = ""$(Get-Item externals\\openssl-bin-* | Select -exp FullName)\\""\necho ""External OpenSSL was downloaded to ${env:OPENSSL_EXT_PATH}""\nRemove-Item -recurse -force ""${env:OPENSSL_EXT_PATH}*""\n# Emulate what this script does:\n# https://github.com/python/cpython/blob/main/PCbuild/openssl.vcxproj\n$env:OPENSSL_EXT_TARGET_PATH = ""${env:OPENSSL_EXT_PATH}${env:PYEXTERNALS_PATH}""\necho ""Copying our OpenSSL to ${env:OPENSSL_EXT_TARGET_PATH}""\nmkdir ""${env:OPENSSL_EXT_TARGET_PATH}\\include\\openssl\\""\nCopy-Item -Path ""${env:GITHUB_WORKSPACE}/src/openssl-${env:openssl_archs}\\LICENSE.txt"" -Destination ""${env:OPENSSL_EXT_TARGET_PATH}\\LICENSE"" -Verbose\ncp -v ""$env:OPENSSL_INSTALL_PATH\\lib\\*"" ""${env:OPENSSL_EXT_TARGET_PATH}""\ncp -v ""$env:OPENSSL_INSTALL_PATH\\bin\\*"" ""${env:OPENSSL_EXT_TARGET_PATH}""\ncp -v ""$env:OPENSSL_INSTALL_PATH\\include\\openssl\\*"" ""${env:OPENSSL_EXT_TARGET_PATH}\\include\\openssl\\""\ncp -v ""$env:OPENSSL_INSTALL_PATH\\include\\openssl\\applink.c"" ""${env:OPENSSL_EXT_TARGET_PATH}\\include\\""\n', 'pip install --upgrade pip\npip install --upgrade sphinx\nsphinx-build --version\n', 'cd ""${env:PYTHON_SOURCE_PATH}""\n# We need out custom openssl.props which uses OpenSSL 3 DLL names\nCopy-Item -Path ""${env:GITHUB_WORKSPACE}\\src\\tools\\openssl.props"" -Destination PCBuild\\ -Verbose\necho ""Building for ${env:PYBUILDRELEASE_ARCH}...""\nPCBuild\\build.bat -m --pgo -c Release -p ""${env:PYBUILDRELEASE_ARCH}""\n', 'cd ""${PYTHON_SOURCE_PATH}""\necho ""Running: ${MAKE} ${MAKEOPT}""\n$MAKE $MAKEOPT\n', 'cd ""${PYTHON_SOURCE_PATH}""\n$MAKE altinstall\n$MAKE bininstall\nexport PATH=""${PATH}:${PYTHON_INSTALL_PATH}/bin""\necho ""PATH=${PATH}"" >> $GITHUB_ENV\necho ""PATH: ${PATH}""\n', '""${PYTHON}"" -V\n', 'curl $curl_retry -O https://bootstrap.pypa.io/get-pip.py\n""${PYTHON}"" get-pip.py\n""${PYTHON}"" -m pip install --upgrade pip\n""${PYTHON}"" -m pip install --upgrade wheel\n""${PYTHON}"" -m pip install --upgrade setuptools\n', 'echo ""before anything...""\n""${PYTHON}"" -m pip list\nif [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  # cffi is a dep of cryptography and doesn\'t ship\n  # a universal2 wheel so we must build one ourself :-/\n  export CFLAGS=""-arch x86_64 -arch arm64""\n  export ARCHFLAGS=""-arch x86_64 -arch arm64""\n  ""${PYTHON}"" -m pip install --upgrade --force-reinstall --no-binary :all: \\\n                    --no-cache-dir --no-deps --use-pep517 \\\n                    --use-feature=no-binary-enable-wheel-cache \\\n                    cffi \n  echo ""before cryptography...""\n  ""${PYTHON}"" -m pip list\n  # cryptography has a universal2 wheel but getting it installed\n  # on x86-64 MacOS is a royal pain in the keester.\n  ""${PYTHON}"" -m pip download --only-binary :all: \\\n                                --dest . \\\n                                --no-cache \\\n                                --no-deps \\\n                                --platform macosx_10_15_universal2 \\\n                                cryptography\n  ""${PYTHON}"" -m pip install --force-reinstall --no-deps cryptography*.whl\n  echo ""after cryptography...""\n  ""${PYTHON}"" -m pip list\n  ""${PYTHON}"" -m pip install --upgrade --no-binary :all: -r requirements.txt\nelse\n  ""${PYTHON}"" -m pip install --upgrade -r requirements.txt\n  echo ""after requirements...""\n  ""${PYTHON}"" -m pip list\n  ""${PYTHON}"" -m pip install --force-reinstall --no-deps --upgrade cryptography\nfi\necho ""after everything...""\n""${PYTHON}"" -m pip list\n', 'git clone https://github.com/pyinstaller/pyinstaller.git\ncd pyinstaller\nexport latest_release=$(git tag --list | grep -v dev | grep -v rc | sort -Vr | head -n1)\ngit checkout ""${latest_release}""\n# remove pre-compiled bootloaders so we fail if bootloader compile fails\nrm -rvf PyInstaller/bootloader/*-*/*\ncd bootloader\ncase ""${arch}"" in\n  ""Win32"")\n    export PYINSTALLER_BUILD_ARGS=""--target-arch=32bit""\n    ;;\n  ""Win64"")\n    export PYINSTALLER_BUILD_ARGS=""--target-arch=64bit""\n    ;;\nesac\necho ""PyInstaller build arguments: ${PYINSTALLER_BUILD_ARGS}""\n""${PYTHON}"" ./waf all $PYINSTALLER_BUILD_ARGS\ncd ..\necho ""---- Installing PyInstaller ----""\n""${PYTHON}"" -m pip install .\n', 'if [[ ""${staticx}"" == ""yes"" ]]; then\n  export distpath=""./dist/gam""\n  export gampath=""${distpath}""\nelse\n  export distpath=""./dist""\n  export gampath=""${distpath}/gam""\nfi\nmkdir -p -v ""${gampath}""\nif [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  export gampath=$($PYTHON -c ""import os; print(os.path.realpath(\'$gampath\'))"")\nelif [[ ""${RUNNER_OS}"" == ""Windows"" ]]; then\n  # Work around issue where PyInstaller picks up python3.dll from other Python versions\n  # https://github.com/pyinstaller/pyinstaller/issues/7102\n  export PATH=""/usr/bin""\nelse\n  export gampath=$(realpath ""${gampath}"")\nfi\nexport gam=""${gampath}/gam""\necho ""gampath=${gampath}"" >> $GITHUB_ENV\necho ""gam=${gam}"" >> $GITHUB_ENV\necho -e ""GAM: ${gam}\\nGAMPATH: ${gampath}""\n# TEMP force everything back to one file.\nexport PYINSTALLER_BUILD_ONEFILE=""yes""\nexport distpath=""./dist/gam""\nexport gampath=""${distpath}""\n""${PYTHON}"" -m PyInstaller --clean --noconfirm --distpath=""${distpath}"" gam.spec\n', 'cp -v roots.pem $gampath\ncp -v LICENSE $gampath\ncp -v GamCommands.txt $gampath\nif [[ ""${RUNNER_OS}"" == ""Windows"" ]]; then\n    cp -v gam-setup.bat $gampath\nfi\n', '""${PYTHON}"" -m pip install --upgrade patchelf-wrapper\n""${PYTHON}"" -m pip install --upgrade staticx\n', 'case $RUNNER_ARCH in\n  X64)\n    ldlib=/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n    ;;\n  ARM64)\n    ldlib=/lib/aarch64-linux-gnu/ld-linux-aarch64.so.1\n    ;;\nesac\necho ""ldlib=${ldlib}""\n$PYTHON -m staticx -l ""${ldlib}"" ""${gam}"" ""${gam}-staticx""\nrm -v ""${gam}""\nmv -v ""${gam}-staticx"" ""${gam}""\n', '$PYTHON -m unittest discover --start-directory ./ --pattern ""*_test.py"" --buffer\n$gam version extended\nexport GAMVERSION=$($gam version simple)\necho ""GAM Version ${GAMVERSION}""\necho ""GAMVERSION=${GAMVERSION}"" >> $GITHUB_ENV\n', 'if [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n    GAM_ARCHIVE=""gam-${GAMVERSION}-macos-universal2.tar.xz""\nelif [[ ""${RUNNER_OS}"" == ""Linux"" ]]; then\n    if [[ ""${staticx}"" == ""yes"" ]]; then\n      libver=""legacy""\n    else\n      libver=""glibc$(ldd --version | awk \'/ldd/{print $NF}\')""\n    fi\n    GAM_ARCHIVE=""gam-${GAMVERSION}-linux-$(arch)-$libver}.tar.xz""\nfi\ntar -C dist/ --create --verbose --exclude-from ""${GITHUB_WORKSPACE}/.github/actions/package_exclusions.txt"" --file $GAM_ARCHIVE --xz gam\n', 'cd dist/\nGAM_ARCHIVE=""../gam-${GAMVERSION}-windows-${GAM_ARCHIVE_ARCH}.zip""\n/c/Program\\ Files/7-Zip/7z.exe a -tzip $GAM_ARCHIVE gam ""-xr@${GITHUB_WORKSPACE}/.github/actions/package_exclusions.txt"" -bb3\ncd ..\n/c/Program\\ Files\\ \\(x86\\)/WiX\\ Toolset\\ v3.11/bin/candle.exe -arch ""${WIX_ARCH}"" gam.wxs\n/c/Program\\ Files\\ \\(x86\\)/WiX\\ Toolset\\ v3.11/bin/light.exe -ext /c/Program\\ Files\\ \\(x86\\)/WiX\\ Toolset\\ v3.11/bin/WixUIExtension.dll gam.wixobj -o ""gam-${GAMVERSION}-windows-${GAM_ARCHIVE_ARCH}.msi"" || true;\nrm -v -f *.wixpdb\n', 'export voutput=$($gam version extended)\nexport python_line=$(echo -e ""${voutput}"" | grep ""Python "")\nexport python_arr=($python_line)\nexport this_python=${python_arr[1]}\nif [[ ""${this_python}"" != ""${COMPILED_PYTHON_VERSION}"" ]]; then\n  echo ""ERROR: Tried to compile Python ${COMPILED_PYTHON_VERSION} but ended up with ${this_python}""\n  exit 1\nfi\nexport openssl_line=$(echo -e ""${voutput}"" | grep ""OpenSSL "")\nexport openssl_arr=($openssl_line)\nexport this_openssl=""${openssl_arr[1]}""\nif [[ ""${this_openssl}"" != ""${COMPILED_OPENSSL_VERSION}"" ]]; then\n  echo ""ERROR: Tried to compile OpenSSL ${COMPILED_OPENSSL_VERSION} but ended up with ${this_openssl}""\n  exit 1\nfi\necho ""We successfully compiled Python ${this_python} and OpenSSL ${this_openssl}""\n', 'if [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  brew install gnupg\nfi\nsource ../.github/actions/decrypt.sh ../.github/actions/creds.tar.xz.gpg creds.tar.xz\nrm $gampath/oauth2service.json\nexport OAUTHFILE=""oauth2.txt-gam-gha-${JID}""\necho ""OAUTHFILE=${OAUTHFILE}"" >> $GITHUB_ENV\nexport gam_user=""gam-gha-${JID}@pdl.jaylee.us""\necho ""gam_user=${gam_user}"" >> $GITHUB_ENV\n$gam checkconn\n$gam create signjwtserviceaccount\nexport CUSTOMER_ID=""C03uzfv2s""\nexport GA_DOMAIN=""pdl.jaylee.us""\nexport GA_ADMIN_EMAIL=""$gam_user""\ntouch ""${gampath}/enabledasa.txt""\n#$gam oauth info\n$gam info domain\n#$gam oauth refresh\n$gam info user\nexport tstamp=$($PYTHON -c ""import time; print(time.time_ns())"")\nexport newbase=""gha_test_${JID}_${tstamp}""\nexport newuser=""${newbase}@pdl.jaylee.us""\nexport newgroup=""${newbase}-group@pdl.jaylee.us""\nexport newalias=""${newbase}-alias@pdl.jaylee.us""\nexport newbuilding=""${newbase}-building""\nexport newresource=""${newbase}-resource""\nexport newou=""aaaGithub Actions/${newbase}""\n\n# cleanup old runs\nrm ""${gampath}/enabledasa.txt""\nGAM_CSV_ROW_FILTER=""name:regex:gha_test_${JID}_"" $gam print vaultholds | $gam csv - gam delete vaulthold ""id:~~holdId~~"" matter ""id:~~matterId~~""\ntouch ""${gampath}/enabledasa.txt""\nGAM_CSV_ROW_FILTER=""name:regex:gha_test_${JID}_"" $gam print features | $gam csv - gam delete feature ~name\nGAM_CSV_ROW_FILTER=""name:regex:^gha_test_${JID}_"" $gam user $gam_user print shareddrives asadmin | $gam csv - gam user $gam_user delete shareddrive ~id nukefromorbit\n$gam print users query ""gha.jid=$JID"" | $gam csv - gam delete user ~primaryEmail\nGAM_CSV_ROW_FILTER=""name:regex:^gha_test_${JID}_"" $gam print ous fromparent ""aaaGithub Actions"" | $gam csv - gam delete ou ~orgUnitId\nGAM_CSV_ROW_FILTER=""groupKey.id:regex:^gha_test_${JID}_"" $gam print cigroups | $gam csv - gam delete cigroup ~groupKey.id\nGAM_CSV_ROW_FILTER=""resourceId:regex:^gha_test_${JID}_"" $gam print resources | $gam csv - gam delete resource ~resourceId\nGAM_CSV_ROW_FILTER=""buildingId:regex:^gha_test_${JID}_"" $gam print buildings | $gam csv - gam delete building ~buildingId\n\necho ""Creating OrgUnit ${newou}""\n$gam create ou ""${newou}""\nexport GAM_THREADS=5\necho email > sample.csv;\nfor i in {1..10}; do\n  echo ""${newbase}-bulkuser-$i"" >> sample.csv;\ndone\n$gam create user $newuser firstname GHA lastname $JID displayname ""Github Actions ${JID}"" password random ou ""${newou}"" recoveryphone 12125121110 recoveryemail jay0lee@gmail.com gha.jid $JID languages en+,en-GB-\n$gam user $newuser update photo https://dummyimage.com/400x600/000/fff\n$gam user $newuser get photo\n$gam user $newuser delete photo\n$gam create alias $newalias user $newuser\n$gam create group $newgroup name ""GHA $JID group"" description ""This is a description"" isarchived true\n$gam user $gam_user sendemail recipient $newuser subject ""test message $newbase"" message ""GHA test message""\n$gam user $gam_user sendemail recipient exchange@pdl.jaylee.us subject ""test ${tstamp}"" message ""test message""\nrm ""${gampath}/enabledasa.txt""\n$gam user $newuser add license workspaceenterpriseplus\n$gam print privileges\ntouch ""${gampath}/enabledasa.txt""\n$gam update cigroup $newgroup security memberrestriction \'member.type == 1 || member.customer_id == groupCustomerId()\'\n$gam info cigroup $newgroup\n$gam update group $newgroup add owner $gam_user\n$gam update group $newgroup add member $newuser\nrm ""${gampath}/enabledasa.txt""\n$gam create admin $newuser _GROUPS_EDITOR_ROLE CUSTOMER # condition nonsecuritygroup\n$gam create admin $newgroup _HELP_DESK_ADMIN_ROLE org_unit ""${newou}""\nGAM_CSV_ROW_FILTER=""assignedToUser:regex:${newuser}"" $gam print admins | $gam csv - gam delete admin ""~roleAssignmentId""\nGAM_CSV_ROW_FILTER=""assignedToGroup:regex:${newgroup}"" $gam print admins | $gam csv - gam delete admin ""~roleAssignmentId""\ntouch ""${gampath}/enabledasa.txt""\n$gam csv sample.csv gam create user ~~email~~ firstname ""GHA Bulk"" lastname ~~email~~ gha.jid $JID ou ""${newou}""\n$gam csv sample.csv gam update user ~~email~~ recoveryphone 12125121110 recoveryemail jay0lee@gmail.com password random displayname ""GitHub Actions Bulk ${JID}""\n$gam csv sample.csv gam update user ~~email~~ recoveryphone """" recoveryemail """"\nrm ""${gampath}/enabledasa.txt""\n$gam csv sample.csv gam user ~email add license workspaceenterpriseplus\ntouch ""${gampath}/enabledasa.txt""\n$gam csv sample.csv gam user $gam_user sendemail recipient ~~email~~@pdl.jaylee.us subject ""test message $newbase"" message ""GHA test message""\n$gam csv sample.csv gam update group $newgroup add member ~email\n$gam info group $newgroup\n$gam info cigroup $newgroup membertree\n$gam user $gam_user check serviceaccount\n# confirm mailbox is provisoned before continuing\n$gam user $newuser waitformailbox\n$gam user $newuser imap on\n$gam user $newuser show imap\n$gam user $newuser show delegates\n#$gam user $newuser add contactdelegate ""${newbase}-bulkuser-1""\n#$gam user $newuser print contactdelegates\nexport biohazard=$(echo -e \'\\xe2\\x98\\xa3\')\n$gam user $newuser label ""$biohazard unicode biohazard $biohazard""\n$gam user $newuser show labels\n$gam user $newuser show labels > labels.txt\n$gam user $gam_user importemail subject ""GHA import $newbase"" message ""This is a test import"" labels IMPORTANT,UNREAD,INBOX,STARRED\n$gam user $gam_user insertemail subject ""GHA insert $newbase"" file gam.py labels INBOX,UNREAD # yep body is gam code\n$gam user $gam_user sendemail subject ""GHA send $gam_user $newbase"" file gam.py recipient admin@pdl.jaylee.us\n$gam user $gam_user draftemail subject ""GHA draft $newbase"" message ""Draft message test""\n$gam csvfile sample.csv:email waitformailbox\n$gam user $newuser delegate to ""${newbase}-bulkuser-1""\n$gam users ""$gam_user $newbase-bulkuser-1 $newbase-bulkuser-2 $newbase-bulkuser-3"" delete messages query in:anywhere maxtodelete 99999 doit\n$gam users ""$newbase-bulkuser-4 $newbase-bulkuser-5 $newbase-bulkuser-6"" trash messages query in:anywhere maxtotrash 99999 doit\n$gam users ""$newbase-bulkuser-7 $newbase-bulkuser-8 $newbase-bulkuser-9"" modify messages query in:anywhere maxtomodify 99999 addlabel IMPORTANT addlabel STARRED doit\n$gam user $newuser delete label --ALL_LABELS--\nGAM_CSV_ROW_FILTER=""name:regex:gha-test-${JID}"" $gam print features | $gam csv - gam delete feature ~name\n$gam create feature name VC-$newbase\n$gam create feature name Whiteboard-$newbase\n$gam create building ""My Building - $newbase"" id $newbuilding floors 1,2,3,4,5,6,7,8,9,10,11,12,14,15 description ""No 13th floor here...""\n$gam create resource $newresource ""Resource Calendar $tstamp"" capacity 25 features Whiteboard-$newbase,VC-$newbase building $newbuilding floor 15 type Room\n$gam info resource $newresource\n$gam user $newuser show filelist\n$gam calendar $gam_user printacl | $gam csv - gam calendar $gam_user delete id ~id # clear ACLs\n$gam calendar $gam_user update read domain\n$gam calendar $gam_user update freebusy default\n$gam calendar $gam_user add editor $newuser\n$gam calendar $gam_user showacl\n$gam calendar $gam_user printacl | $gam csv - gam calendar $gam_user delete id ~id\nstarttime=$($PYTHON -c ""import datetime; print((datetime.datetime.now() + datetime.timedelta(hours=1)).strftime(\'%Y-%m-%dT%H:%M:%S.%f+00:00\'))"")\nendtime=$($PYTHON -c ""import datetime; print((datetime.datetime.now() + datetime.timedelta(hours=2)).strftime(\'%Y-%m-%dT%H:%M:%S.%f+00:00\'))"")\n$gam calendar $gam_user addevent summary ""GHA test event"" start ""${starttime}"" end ""${endtime}"" attendee $newgroup hangoutsmeet guestscanmodify true sendupdates all\n$gam calendar $gam_user printevents after -0d\nrm ""${gampath}/enabledasa.txt""\nmatterid=uid:$($gam create vaultmatter name ""GHA matter $newbase"" description ""test matter"" collaborators $newuser | head -1 | cut -d \' \' -f 3)\n$gam create vaulthold matter $matterid name ""GHA hold $newbase"" corpus mail accounts $newuser\n$gam print vaultmatters matterstate open\n$gam print vaultholds matter $matterid\n$gam print vaultcount matter $matterid corpus mail everyone todrive\n$gam create vaultexport matter $matterid name ""GHA export $newbase"" corpus mail accounts $newuser\n$gam print exports matter $matterid | $gam csv - gam info export $matterid id:~~id~~\ntouch ""${gampath}/enabledasa.txt""\n$gam csv sample.csv gam user ~email add calendar id:$newresource\n$gam delete resource $newresource\n$gam delete feature Whiteboard-$newbase\n$gam delete feature VC-$newbase\n$gam delete building $newbuilding\n$gam delete group $newgroup\nrm ""${gampath}/enabledasa.txt""\necho start\n$gam user $newuser delete license workspaceenterpriseplus\necho finish\ntouch ""${gampath}/enabledasa.txt""\n$gam whatis $newuser\n$gam user $gam_user show tokens\nrm ""${gampath}/enabledasa.txt""\n$gam print exports matter $matterid | $gam csv - gam download export $matterid id:~~id~~\n$gam delete hold ""GHA hold $newbase"" matter $matterid\n$gam update matter $matterid action close\n$gam update matter $matterid action delete\n# shakes off vault hold on user so we can delete\n$gam print users query ""email:${newuser}"" orgunitpath | $gam csv - gam update user ~primaryEmail ou ~orgUnitPath\n$gam user $newuser show holds\nexport sn=""$JID$JID$JID$JID-$(openssl rand -base64 32 | sed \'s/[^a-zA-Z0-9]//g\')""\n$gam create device serialnumber $sn devicetype android\ntouch ""${gampath}/enabledasa.txt""\n$gam delete user $newuser\n$gam print users query ""gha.jid=$JID"" | $gam csv - gam delete user ~primaryEmail\n$gam print mobile\n$gam print devices\n$gam print browsers\n$gam print cros allfields orderby serialnumber\n$gam show crostelemetry storagepercentonly\n$gam report usageparameters customer\n$gam report usage customer parameters gmail:num_emails_sent,accounts:num_1day_logins\n$gam report customer todrive\n$gam report users fields accounts:is_less_secure_apps_access_allowed,gmail:last_imap_time,gmail:last_pop_time filters ""accounts:last_login_time>2019-01-01T00:00:00.000Z"" todrive\n$gam report admin start -3d todrive\n$gam print devices nopersonaldevices nodeviceusers filter ""serial:$JID$JID$JID$JID-"" | $gam csv - gam delete device id ~name\nrm ""${gampath}/enabledasa.txt""\n$gam print userinvitations\n$gam print userinvitations | $gam csv - gam send userinvitation ~name\ntouch ""${gampath}/enabledasa.txt""\n$gam create caalevel ""zzz_${newbase}"" basic condition ipsubnetworks 1.1.1.1/32,2.2.2.2/32 endcondition\n$gam print caalevels\n$gam delete caalevel ""zzz_${newbase}""\ndriveid=$($gam user $gam_user add shareddrive ""${newbase}"" | awk \'{print $NF}\')\necho ""Created shared drive ${driveid}""\n$gam user $gam_user add drivefile localfile gam.py parentid ""${driveid}""\n$gam user $gam_user update shareddrive ""${driveid}"" ou ""${newou}""\n$gam user $gam_user show shareddrives asadmin\n$gam user $gam_user update shareddrive ""${driveid}"" ou ""aaaGithub Actions"" # so we can delete our OU...\n$gam user $gam_user delete shareddrive ""${driveid}"" nukefromorbit\necho ""printer model count:""\nssoprofile=$($gam create inboundssoprofile name ""El Goog ${newbase}"" loginurl https://www.google.com logouturl https://www.google.com changepasswordurl https://www.google.com entityid ElGoog return_name_only)\n$gam create inboundssocredential profile ""id:${ssoprofile}"" generate_key\n$gam create inboundssoassignment profile ""id:${ssoprofile}"" orgunit ""${newou}"" mode SAML_SSO\n$gam delete inboundssoassignment ""orgunit:${newou}""\n$gam delete inboundssoprofile ""id:${ssoprofile}""\n$gam print printermodels | wc -l\n$gam print printers\nprinterid=$($gam create printer displayname ""${newbase}"" uri ipp://localhost:631 driverless description ""made by $(gam_user)"" ou ""${newou}"" | grep \'id: [a-z,0-9]*$\' | cut -d\' \' -f3)\n$gam info printer ""$printerid""\n$gam delete printer ""$printerid""\n$gam delete ou ""${newou}""\n', 'tar cJvvf bin.tar.xz bin/\n', 'export dateversion=""$(date  +\'%Y%m%d.%H%M%S\')""\necho ""Date version: ${dateversion}""\necho ""dateversion=${dateversion}"" >> $GITHUB_OUTPUT\n', 'curl -o ./roots.pem -vvvv https://pki.goog/roots.pem', 'git config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add roots.pem\ngit diff --quiet && git diff --staged --quiet || git commit -am \'[ci skip] Updated roots.pem\'\n']"
""
"['vagrant --version', 'vagrant up centos6 --provision-with dependencies', 'vagrant --version', 'vagrant up ubuntu1604 --provision-with dependencies', 'cd /opt/\nsudo git clone https://github.com/pwnlandia/mhn.git\n', 'sudo apt update && sudo apt upgrade -y\nsudo apt install -y python-pip\nsudo pip install --upgrade pip\nsudo apt install apt-transport-https -y\nsudo apt install build-essential -y\nsudo apt remove mongo* -y\n', 'cd /opt/mhn/scripts/\nsudo ./install_hpfeeds.sh\n', 'cd /opt/mhn/scripts/\nsudo ./install_mnemosyne.sh\n', 'cd /opt/mhn/scripts/\nsudo ./install_honeymap.sh', 'cd /opt/\nsudo git clone https://github.com/pwnlandia/mhn.git\n', 'sudo apt update && sudo apt upgrade -y\nsudo apt install -y python-pip\nsudo pip install --upgrade pip\nsudo apt install apt-transport-https -y\nsudo apt install build-essential -y\nsudo apt remove mongo* -y\n', 'cd /opt/mhn/scripts/\nsudo ./install_hpfeeds.sh\n', 'cd /opt/mhn/scripts/\nsudo ./install_mnemosyne.sh\n', 'cd /opt/mhn/scripts/\nsudo ./install_honeymap.sh\n']"
"['pip install -U setuptools\npip install -r tests/requirements-mypy.txt\nmypy\n', 'make lint\n', 'pip install -U twine wheel\npython setup.py sdist bdist_wheel\n', 'twine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'pip install uvloop', 'pip install -e . -c tests/requirements.txt\n', 'make ci-test\npython -m coverage xml\n', 'python -m pip install -U pip wheel twine', 'python setup.py sdist bdist_wheel', 'twine upload dist/*\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install pre-commit', 'pre-commit autoupdate']"
['git checkout HEAD^2']
"['brew install coreutils', 'echo ""TERM=linux"" >> $GITHUB_ENV\n', 'pip3 install coverage', 'pip3 install -r requirements.txt', 'stdbuf -oL -eL ./test/RUN.sh 2>&1']"
"['mv ./dockerfiles/* ./', ""sed -i -e 's|FROM archlinux:base-devel|FROM actionless/pikaur|' Dockerfile_${PYVER}"", 'docker build . -f Dockerfile_${PYVER} -t oomox', 'docker run -e SKIP_SHELLCHECK=${SKIP_SHELLCHECK} -e SKIP_MYPY=${SKIP_MYPY} -e SKIP_VULTURE=${SKIP_VULTURE} -e SKIP_RUFF=${SKIP_RUFF} oomox:latest ./maintenance_scripts/lint.sh']"
"['python -m pip install --upgrade pip\npip install .[tests]\n', 'pytest -v --cov=gtts --cov-report=xml', 'pip install --upgrade pip\npip install build\n', 'python -m build', 'python -m pip install --upgrade pip\npip install .[tests]\n', 'pytest -v --cov=gtts --cov-report=xml', 'python -m pip install --upgrade pip\npip install .\n', 'python scripts/gen_langs.py ${TEMP_LANG_FILE_PATH}', 'python scripts/check_langs.py', 'cp ""${TEMP_LANG_FILE_PATH}"" ""${CURR_LANG_FILE_PATH}""']"
"['make venv\n', 'make test-all\n']"
"['sudo apt-get update\nsudo apt-get install gcc-8 g++-8\nsudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc-8 100\nsudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++-8 100\n', 'sudo apt-get update\nsudo apt-get install clang-7\nsudo update-alternatives --install /usr/bin/cc cc /usr/bin/clang-7 100\nsudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/clang++-7 100\n', 'python3 -m pip install -r test_requirements.txt\necho -e ""import coverage\\ncoverage.process_startup()"" > $(python -c ""print(__import__(\'sysconfig\').get_path(\'purelib\'))"")/sitecustomize.py\n', 'python3 run_tests.py --quiet', 'python3 benchmark.py --quiet', 'coverage combine && coverage xml', 'sudo apt-get update\nsudo apt-get install gcc-8 g++-8\nsudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc-8 100\nsudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++-8 100\n', 'wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install -y clang-tidy libc6-dbg build-essential\nsudo update-alternatives --install /usr/bin/clang-tidy clang-tidy /usr/bin/clang-tidy-10 100\n', 'wget https://sourceware.org/pub/valgrind/valgrind-3.16.1.tar.bz2\ntar xf valgrind-3.16.1.tar.bz2\npushd valgrind-3.16.1\n./configure && make -j3 && sudo make install\npopd\n', 'python3 -m pip install -r test_requirements.txt', 'YCM_TESTRUN=1 python3 build.py --clang-completer --clang-tidy --valgrind\npython3 run_tests.py --valgrind --skip-build --no-flake8 --quiet\n', 'python3 -m pip install -r test_requirements.txt\necho -e ""import coverage\\ncoverage.process_startup()"" > $(python -c ""print(__import__(\'sysconfig\').get_path(\'purelib\'))"")/sitecustomize.py\n', 'python3 benchmark.py --msvc ${{ matrix.msvc }} --quiet', 'python3 run_tests.py --msvc ${{ matrix.msvc }} --quiet', 'coverage combine && coverage xml']"
"['mkdir -p $PIP_CACHE_DIR\nmkdir -p $LINTDIR\n', 'pip install flake8', 'flake8 pydeps/** --max-line-length=199 --exit-zero --exclude mf', 'mkdir -p build\n', 'pip install sphinx', 'pip install -e .', 'sphinx-build -W -b html docs build/sphinx/html', 'sphinx-build -W -n -T -b man docs build/sphinx/man', 'mkdir -p $PIP_CACHE_DIR', 'pip install -r requirements.txt', 'pip list', 'pytest -vv --cov=pydeps --cov-report=xml tests', 'mkdir -p $PIP_CACHE_DIR', 'rm -rf dist\nrm -rf build\n', 'pip install -U wheel packaging twine', 'pip install -r requirements.txt', 'python setup.py sdist bdist_wheel', 'twine upload -u __token__ -p ${{ secrets.PYPI_API_TOKEN }} dist/*      \n', 'docker build --no-cache \\\n--progress=plain \\\n--tag $DOCKER_USERNAME/pydeps-devcontainer:$IMAGE_TAG \\\n--file docker-images/devcontainer/Dockerfile .\n', 'docker push $DOCKER_USERNAME/pydeps-devcontainer:$IMAGE_TAG\n']"
"['python -m pip install --upgrade pip\npip install .[files,huey,celery]\npip uninstall django-spirit -y\npip install --upgrade Django==${{ matrix.django }}\npip install --upgrade coveralls\n', 'python setup.py -q build\npython ./spirit/extra/bin/spirit.py startproject project\nexport PYTHONWARNINGS=""default""\ncoverage run --source=. runtests.py\n', 'coveralls --service=github\n', 'python -m pip install --upgrade pip\npip install --upgrade coveralls\ncoveralls --finish\n', 'npm install -g yarn\nnpm install -g jasmine@3.6.1\nyarn\n', 'make testjs\n', 'python -m pip install --upgrade pip\npip install .\npip uninstall django-spirit -y\npip install --upgrade Django==${{ matrix.django }}\n', 'python ./spirit/extra/bin/spirit.py startproject project\nexport PYTHONWARNINGS=""default""\nexport ST_UPLOAD_FILE_ENABLED=0\nexport ST_INSTALL_HUEY=0\nmake test\n', 'python -m pip install --upgrade pip\npip install .[huey]\npip uninstall django-spirit -y\npip install --upgrade Django==${{ matrix.django }}\n', 'python ./spirit/extra/bin/spirit.py startproject project\nexport PYTHONWARNINGS=""default""\nexport ST_UPLOAD_FILE_ENABLED=0\nexport ST_TASK_MANAGER=""huey""\nmake test\n']"
"['make docker-qa-build PYTHON_VERSION=${{matrix.python-version}}\n', '.github/bump_version ./ minor > atlassian/VERSION\nmake docker-qa PYTHON_VERSION=${{matrix.python-version}}\n']"
[]
"['pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pipx install --pip-args=--constraint=.github/workflows/python-constraints.txt poetry\npoetry --version\n', 'pipx install --pip-args=--constraint=.github/workflows/constraints.txt nox\npipx inject --pip-args=--constraint=.github/workflows/constraints.txt nox nox-poetry\nnox --version\n', 'nox --force-color --session=docs-build', 'pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pip install --constraint=.github/workflows/python-constraints.txt poetry\npoetry --version\n', 'echo ""::set-output name=sha::$(git rev-parse --verify --quiet HEAD^)""\n', ""poetry version patch &&\nversion=$(poetry version | awk '{ print $2 }') &&\npoetry version $version.dev.$(date +%s)\n"", 'poetry build --ansi\n', 'pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'import os\nimport pip\nwith open(os.environ[""GITHUB_ENV""], mode=""a"") as io:\n    print(f""VIRTUALENV_PIP={pip.__version__}"", file=io)\n', 'pipx install --pip-args=--constraint=.github/workflows/python-constraints.txt poetry\npoetry --version\n', 'pipx install --pip-args=--constraint=.github/workflows/constraints.txt nox\npipx inject --pip-args=--constraint=.github/workflows/constraints.txt nox nox-poetry\nnox --version\n', 'import hashlib\nimport sys\npython = ""py{}.{}"".format(*sys.version_info[:2])\npayload = sys.version.encode() + sys.executable.encode()\ndigest = hashlib.sha256(payload).hexdigest()\nresult = ""${{ runner.os }}-{}-{}-pre-commit"".format(python, digest[:8])\nprint(""::set-output result={}"".format(result))\n', 'nox --force-color --python=${{ matrix.python }}\n', 'pip install --constraint=.github/workflows/constraints.txt pip\npip --version\n', 'pipx install --pip-args=--constraint=.github/workflows/python-constraints.txt poetry\npoetry --version\n', 'pipx install --pip-args=--constraint=.github/workflows/constraints.txt nox\npipx inject --pip-args=--constraint=.github/workflows/constraints.txt nox nox-poetry\nnox --version\n', 'nox --force-color --session=coverage\n', 'nox --force-color --session=coverage -- xml\n']"
""
"['python -m pip install --upgrade pip\npip install flake8 pytest\npip install -r requirements.txt\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest\n']"
"['python -m pip install --upgrade pip\npython -m pip install pytest\npython -m pip install -e .\n', 'cd utest\npython -m pytest\n']"
"['pip install tox', 'tox -e ${{ matrix.python }}-${{ matrix.httplib }}', 'pip install tox', 'tox -e ${{ matrix.tox }}']"
""
""
"['# Change all uppercase to lowercase\nOWNER=$(echo ${{ github.repository_owner }} | tr \'[A-Z]\' \'[a-z]\')\n\n# Strip git ref prefix from version\nVERSION=$(echo ""${{ github.ref }}"" | sed -e \'s,.*/\\(.*\\),\\1,\')\n\n# Strip ""v"" prefix from tag name\n[[ ""${{ github.ref }}"" == ""refs/tags/""* ]] && VERSION=$(echo $VERSION | sed -e \'s/^v//\')\n\n# Use Docker `latest` tag convention\n[ ""$VERSION"" == ""master"" ] && LATEST=""latest""\n\necho ""OWNER=$OWNER"" >> $GITHUB_ENV\necho ""VERSION=$VERSION"" >> $GITHUB_ENV\n\necho ""OWNER=$OWNER""\necho ""VERSION=$VERSION""\n']"
"['python scripts/dev/ci/problemmatchers.py py3 ${{ runner.temp }}', 'dbus-run-session tox -e bleeding', 'python scripts/dev/ci/problemmatchers.py ${{ matrix.testenv }} ${{ runner.temp }}', '[[ ${{ matrix.testenv }} == eslint ]] && npm install -g eslint\n[[ ${{ matrix.testenv }} == docs ]] && sudo apt-get update && sudo apt-get install --no-install-recommends asciidoc\nif [[ ${{ matrix.testenv }} == shellcheck ]]; then\n    scversion=""stable""\n    bindir=""$HOME/.local/bin""\n    mkdir -p ""$bindir""\n    wget -qO- ""https://github.com/koalaman/shellcheck/releases/download/$scversion/shellcheck-$scversion.linux.x86_64.tar.xz"" | tar -xJv --strip-components 1 -C ""$bindir"" shellcheck-$scversion/shellcheck\n    echo ""$bindir"" >> ""$GITHUB_PATH""\nfi\nif [[ ${{ matrix.testenv }} == actionlint ]]; then\n    bindir=""$HOME/.local/bin""\n    mkdir -p ""$bindir""\n    wget -q https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash\n    bash download-actionlint.bash latest ""$bindir""\n    echo ""$bindir"" >> ""$GITHUB_PATH""\nfi\npython -m pip install -U pip\npython -m pip install -U -r misc/requirements/requirements-tox.txt\n', 'if [[ -z ""${{ matrix.args }}"" ]]; then\n    dbus-run-session -- tox -e ${{ matrix.testenv }}\nelse\n    dbus-run-session -- tox -e ${{ matrix.testenv }} -- ${{ matrix.args }}\nfi\n', 'python scripts/dev/ci/problemmatchers.py py38 ${{ runner.temp }}', 'dbus-run-session -- tox -e ${{ matrix.testenv }} -- ${{ matrix.args }}', 'python scripts/dev/ci/problemmatchers.py ${{ matrix.testenv }} ${{ runner.temp }}', 'sudo apt-get update\nsudo apt-get install --no-install-recommends libyaml-dev libegl1-mesa libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-shape0 libxcb-cursor0\n', 'python -m pip install -U pip\npython -m pip install -U -r misc/requirements/requirements-tox.txt\n', 'dbus-run-session -- tox -e ${{ matrix.testenv }} -- ${{ matrix.args }}', 'tox -e ${{ matrix.testenv }} -- ${{ matrix.args }}', 'bash scripts/dev/ci/backtrace.sh ${{ matrix.testenv }}', 'pip install jinja2', 'python3 generate.py ${{ matrix.image }}', 'python -m pip install -U pip\npython -m pip install -U -r misc/requirements/requirements-tox.txt\n', ""sed -i '' '/.-d., .--debug.,/s/$/ default=True,/' qutebrowser/qutebrowser.py\n"", 'tox -e ${{ matrix.toxenv }} -- --gh-token ${{ secrets.GITHUB_TOKEN }} ${{ matrix.args }}', 'echo ""date=$(date +\'%Y-%m-%d\')"" >> ""$GITHUB_OUTPUT""\necho ""sha_short=$(git rev-parse --short HEAD)"" >> ""$GITHUB_OUTPUT""\n', 'python3 scripts/dev/recompile_requirements.py ${{ github.event.input.environments }}', 'sudo apt-get update\nsudo apt-get install --no-install-recommends libyaml-dev libegl1-mesa libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 libxcb-render-util0 libxcb-xinerama0 libxcb-shape0 asciidoc python3-venv xvfb\n', 'python -m pip install -U pip\npython -m pip install -U -r misc/requirements/requirements-tox.txt\n', 'xvfb-run python3 scripts/mkvenv.py --dev', ""xvfb-run .venv/bin/python3 -m qutebrowser --no-err-windows --nowindow --temp-basedir about:blank ':later 500 quit'""]"
"['echo ""ATEST_PYTHON=$((get-command python.exe).Path)"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append', 'echo ""ATEST_PYTHON=$(which python)"" >> $GITHUB_ENV', 'echo ""BASE_PYTHON=$((get-command python.exe).Path)"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append', 'echo ""BASE_PYTHON=$(which python)"" >> $GITHUB_ENV', 'choco install curl -y --no-progress\nchoco install zip -y --no-progress\n', 'sudo apt-get update\nsudo apt-get -y -q install libxslt-dev libxml2-dev\n', 'sudo apt-get update\nsudo apt-get -y -q install xvfb scrot zip curl libxml2-dev libxslt1-dev\n', 'python -m pip install -r atest/requirements.txt\n${{ env.ATEST_PYTHON }} -m pip install -r atest/requirements-run.txt\n${{ matrix.set_codepage }}\n${{ matrix.set_display }}\n${{ env.ATEST_PYTHON }} atest/run.py --interpreter ${{ env.BASE_PYTHON }} --exclude no-ci ${{ matrix.atest_args }} atest/robot\n', 'Get-ChildItem atest/results -Include output.xml -Recurse | Remove-Item\n', ""find atest/results -type f -name 'output.xml' -exec rm {} +\n"", 'echo \'<html><head><meta http-equiv = ""refresh"" content ="" 0 ; url = /report.html""></head></html>\' > atest/results/index.html\nzip -r -j site.zip atest/results > no_output 2>&1\ncurl -s -H ""Content-Type: application/zip"" -H ""Authorization: Bearer ${{ secrets.NETLIFY_TOKEN }}"" --data-binary ""@site.zip"" https://api.netlify.com/api/v1/sites > response.json\necho ""REPORT_URL=$(cat response.json|python -c ""import sys, json; print(\'https://\' + json.load(sys.stdin)[\'subdomain\'] + \'.netlify.com\')"")"" >> $GITHUB_ENV\necho ""JOB_STATUS=$(python -c ""print(\'${{ job.status }}\'.lower())"")"" >> $GITHUB_ENV\n', 'echo \'<html><head><meta http-equiv = ""refresh"" content ="" 0 ; url = /report.html""></head></html>\' > atest/results/index.html\nzip -r -j site.zip atest/results > no_output 2>&1\ncurl -s -H ""Content-Type: application/zip"" -H ""Authorization: Bearer ${{ secrets.NETLIFY_TOKEN }}"" --data-binary ""@site.zip"" https://api.netlify.com/api/v1/sites > response.json\necho ""REPORT_URL=$(cat response.json|python -c ""import sys, json; print(\'https://\' + json.load(sys.stdin)[\'subdomain\'] + \'.netlify.com\')"")"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\necho ""JOB_STATUS=$(python -c ""print(\'${{ job.status }}\'.lower())"")"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'echo ""ATEST_PYTHON=$((get-command python.exe).Path)"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append', 'echo ""ATEST_PYTHON=$(which python)"" >> $GITHUB_ENV', 'echo ""BASE_PYTHON=$((get-command python.exe).Path)"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append', 'echo ""BASE_PYTHON=$(which python)"" >> $GITHUB_ENV', 'choco install curl -y --no-progress\nchoco install zip -y --no-progress\n', 'sudo apt-get update\nsudo apt-get -y -q install xvfb scrot zip curl libxml2-dev libxslt1-dev\n', 'python -m pip install -r atest/requirements.txt\n${{ env.ATEST_PYTHON }} -m pip install -r atest/requirements-run.txt\n${{ matrix.set_codepage }}\n${{ matrix.set_display }}\n${{ env.ATEST_PYTHON }} atest/run.py --interpreter ${{ env.BASE_PYTHON }} --exclude no-ci ${{ matrix.atest_args }} atest/robot\n', 'Get-ChildItem atest/results -Include output.xml -Recurse | Remove-Item\n', ""find atest/results -type f -name 'output.xml' -exec rm {} +\n"", 'echo \'<html><head><meta http-equiv = ""refresh"" content ="" 0 ; url = /report.html""></head></html>\' > atest/results/index.html\nzip -r -j site.zip atest/results > no_output 2>&1\ncurl -s -H ""Content-Type: application/zip"" -H ""Authorization: Bearer ${{ secrets.NETLIFY_TOKEN }}"" --data-binary ""@site.zip"" https://api.netlify.com/api/v1/sites > response.json\necho ""REPORT_URL=$(cat response.json|python -c ""import sys, json; print(\'https://\' + json.load(sys.stdin)[\'subdomain\'] + \'.netlify.com\')"")"" >> $GITHUB_ENV\necho ""JOB_STATUS=$(python -c ""print(\'${{ job.status }}\'.lower())"")"" >> $GITHUB_ENV\n', 'echo \'<html><head><meta http-equiv = ""refresh"" content ="" 0 ; url = /report.html""></head></html>\' > atest/results/index.html\nzip -r -j site.zip atest/results > no_output 2>&1\ncurl -s -H ""Content-Type: application/zip"" -H ""Authorization: Bearer ${{ secrets.NETLIFY_TOKEN }}"" --data-binary ""@site.zip"" https://api.netlify.com/api/v1/sites > response.json\necho ""REPORT_URL=$(cat response.json|python -c ""import sys, json; print(\'https://\' + json.load(sys.stdin)[\'subdomain\'] + \'.netlify.com\')"")"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\necho ""JOB_STATUS=$(python -c ""print(\'${{ job.status }}\'.lower())"")"" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n', 'python -m pip install -r utest/requirements.txt\npython utest/run.py -v\n', 'python -m pip install -r utest/requirements.txt\npython utest/run.py -v\n', 'echo ${{ github.event }}']"
"['npm install -g yarn\npip install --upgrade pip\npip install build\npip freeze\n', 'python -m build --sdist --wheel .\nls -l dist\n', './ci/check_sdist.py dist/jupyterhub-*.tar.gz\n', 'pip install dist/*.whl\n./ci/check_installed_data.py\n', ""docker run --rm -v $PWD/dist:/dist:ro docker.io/library/python:3.9-slim-bullseye bash -c 'pip install /dist/jupyterhub-*.tar.gz'\n"", 'pip install twine\ntwine upload --skip-existing dist/*\n', 'if [ ""${{ startsWith(github.ref, \'refs/tags/\') || (github.ref == \'refs/heads/main\') }}"" = ""true"" ]; then\n    # Empty => Docker Hub\n    echo ""REGISTRY="" >> $GITHUB_ENV\nelse\n    echo ""REGISTRY=localhost:5000/"" >> $GITHUB_ENV\nfi\n', 'docker login -u ""${{ secrets.DOCKERHUB_USERNAME }}"" -p ""${{ secrets.DOCKERHUB_TOKEN }}""\n', 'pip install -r docs/requirements.txt pytest\n', 'pytest docs/\n', 'cd docs\nmake html\n', 'cd docs\nmake linkcheck\n', 'cd docs\nexport REDIRAFFE_BRANCH=origin/${{ github.base_ref }}\nmake rediraffecheckdiff\n', 'cd docs\nexport REDIRAFFE_BRANCH=$(git describe --tags --abbrev=0)\nmake rediraffecheckdiff\n', 'cd docs\nexport REDIRAFFE_BRANCH=3.0.0\nmake rediraffecheckdiff\n', 'npm install -g yarn\n', 'cd jsx\nyarn\n', 'cd jsx\nyarn test\n', 'if [ ""${{ matrix.subdomain }}"" != """" ]; then\n    echo ""JUPYTERHUB_TEST_SUBDOMAIN_HOST=http://localhost.jovyan.org:8000"" >> $GITHUB_ENV\nfi\nif [ ""${{ matrix.db }}"" == ""mysql"" ]; then\n    echo ""MYSQL_HOST=127.0.0.1"" >> $GITHUB_ENV\n    echo ""JUPYTERHUB_TEST_DB_URL=mysql+mysqldb://root@127.0.0.1:3306/jupyterhub"" >> $GITHUB_ENV\nfi\nif [ ""${{ matrix.ssl }}"" == ""ssl"" ]; then\n    echo ""SSL_ENABLED=1"" >> $GITHUB_ENV\nfi\nif [ ""${{ matrix.db }}"" == ""postgres"" ]; then\n    echo ""PGHOST=127.0.0.1"" >> $GITHUB_ENV\n    echo ""PGUSER=test_user"" >> $GITHUB_ENV\n    echo ""PGPASSWORD=hub[test/:?"" >> $GITHUB_ENV\n    echo ""JUPYTERHUB_TEST_DB_URL=postgresql://test_user:hub%5Btest%2F%3A%3F@127.0.0.1:5432/jupyterhub"" >> $GITHUB_ENV\nfi\nif [ ""${{ matrix.serverextension }}"" != """" ]; then\n    echo ""JUPYTERHUB_SINGLEUSER_EXTENSION=1"" >> $GITHUB_ENV\nelif [ ""${{ matrix.noextension }}"" != """" ]; then\n    echo ""JUPYTERHUB_SINGLEUSER_EXTENSION=0"" >> $GITHUB_ENV\nfi\n', 'npm install\nnpm install -g configurable-http-proxy yarn\nnpm list\n', 'pip install --upgrade pip\npip install -e "".[test]""\n\nif [ ""${{ matrix.oldest_dependencies }}"" != """" ]; then\n  # take any dependencies in requirements.txt such as tornado>=5.0\n  # and transform them to tornado==5.0 so we can run tests with\n  # the earliest-supported versions\n  cat requirements.txt | grep \'>=\' | sed -e \'s@>=@==@g\' > oldest-requirements.txt\n  pip install -r oldest-requirements.txt\nfi\n\nif [ ""${{ matrix.main_dependencies }}"" != """" ]; then\n    # Tests are broken:\n    # https://github.com/jupyterhub/jupyterhub/issues/4418\n    # pip install git+https://github.com/ipython/traitlets#egg=traitlets --force\n    pip install --upgrade --pre sqlalchemy\nfi\nif [ ""${{ matrix.legacy_notebook }}"" != """" ]; then\n    pip uninstall jupyter_server --yes\n    pip install \'notebook<7\'\nfi\nif [ ""${{ matrix.jupyter_server }}"" != """" ]; then\n    pip install ""jupyter_server==${{ matrix.jupyter_server }}""\nfi\nif [ ""${{ matrix.db }}"" == ""mysql"" ]; then\n    pip install mysqlclient\nfi\nif [ ""${{ matrix.db }}"" == ""postgres"" ]; then\n    pip install psycopg2-binary\nfi\nif [ ""${{ matrix.serverextension }}"" != """" ]; then\n  pip install \'jupyter-server>=2\'\nfi\n\npip freeze\n', 'if [ ""${{ matrix.db }}"" == ""mysql"" ]; then\n  if [[ -z ""$(which mysql)"" ]]; then\n    sudo apt-get update\n    sudo apt-get install -y mysql-client\n  fi\n    DB=mysql bash ci/docker-db.sh\n    DB=mysql bash ci/init-db.sh\nfi\nif [ ""${{ matrix.db }}"" == ""postgres"" ]; then\n  if [[ -z ""$(which psql)"" ]]; then\n    sudo apt-get update\n    sudo apt-get install -y postgresql-client\n  fi\n    DB=postgres bash ci/docker-db.sh\n    DB=postgres bash ci/init-db.sh\nfi\n', 'echo ""PYTEST_ADDOPTS=$PYTEST_ADDOPTS -m browser"" >> ""${GITHUB_ENV}""', 'python -m playwright install --with-deps', 'pytest -k ""${{ matrix.subset }}"" --maxfail=2 --cov=jupyterhub jupyterhub/tests\n', 'DOCKER_BUILDKIT=1 docker build -t jupyterhub/jupyterhub .\ndocker build -t jupyterhub/jupyterhub-onbuild onbuild\ndocker build -t jupyterhub/singleuser singleuser\n', 'docker run --rm -t jupyterhub/jupyterhub jupyterhub --help\n', 'docker run --rm -t -v $PWD/dockerfiles:/io jupyterhub/jupyterhub python3 /io/test.py\n']"
""
""
"['export PYTHON=$(which python3)\nexport gyb=""${PYTHON} -m gyb""\nexport gybpath=""$(readlink -e .)""\necho -e ""PYTHON: ${PYTHON}\\nPIP: ${PIP}\\ngyb: ${gyb}\\ngybpath: ${gybpath}""\necho ""PYTHON=${PYTHON}"" >> $GITHUB_ENV\necho ""gyb=${gyb}"" >> $GITHUB_ENV\necho ""gybpath=${gybpath}"" >> $GITHUB_ENV\necho ""JID=${JID}"" >> $GITHUB_ENV\necho ""ACTIONS_CACHE=${ACTIONS_CACHE}"" >> $GITHUB_ENV\necho ""ACTIONS_GOAL=${ACTIONS_GOAL}"" >> $GITHUB_ENV\n', 'echo ""We are running on ${RUNNER_OS}""\nif [[ ""${arch}"" == ""Win64"" ]]; then\n  PYEXTERNALS_PATH=""amd64""\n  PYBUILDRELEASE_ARCH=""x64""\n  OPENSSL_CONFIG_TARGET=""VC-WIN64A""\n  CHOC_OPS=""""\nelif [[ ""${arch}"" == ""Win32"" ]]; then\n  PYEXTERNALS_PATH=""win32""\n  PYBUILDRELEASE_ARCH=""Win32""\n  OPENSSL_CONFIG_TARGET=""VC-WIN32""\n  CHOC_OPS=""--forcex86""\nfi\nif [[ ""${RUNNER_OS}"" == ""macOS"" ]]; then\n  MAKE=make\n  MAKEOPT=""-j$(sysctl -n hw.logicalcpu)""\n  PERL=perl\n  # We only care about non-deprecated OSes\n  MACOSX_DEPLOYMENT_TARGET=""10.15""\n  echo ""MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET}"" >> $GITHUB_ENV\n  echo ""PYTHON=${PYTHON_INSTALL_PATH}/bin/python3"" >> $GITHUB_ENV\nelif [[ ""${RUNNER_OS}"" == ""Linux"" ]]; then\n  MAKE=make\n  MAKEOPT=""-j$(nproc)""\n  PERL=perl\n  echo ""PYTHON=${PYTHON_INSTALL_PATH}/bin/python3"" >> $GITHUB_ENV\nelif [[ ""${RUNNER_OS}"" == ""Windows"" ]]; then\n  MAKE=nmake\n  MAKEOPT=""""\n  PERL=""c:\\strawberry\\perl\\bin\\perl.exe""\n  echo ""PYTHON=${PYTHON_SOURCE_PATH}/PCbuild/${PYEXTERNALS_PATH}/python.exe"" >> $GITHUB_ENV\nfi\necho ""We\'ll run make with: ${MAKEOPT}""\necho ""JID=${jid}"" >> $GITHUB_ENV\necho ""arch=${arch}"" >> $GITHUB_ENV\necho ""MAKE=${MAKE}"" >> $GITHUB_ENV\necho ""MAKEOPT=${MAKEOPT}"" >> $GITHUB_ENV\necho ""PERL=${PERL}"" >> $GITHUB_ENV\necho ""PYEXTERNALS_PATH=${PYEXTERNALS_PATH}"" >> $GITHUB_ENV\necho ""PYBUILDRELEASE_ARCH=${PYBUILDRELEASE_ARCH}"" >> $GITHUB_ENV\necho ""OPENSSL_CONFIG_TARGET=${OPENSSL_CONFIG_TARGET}"" >> $GITHUB_ENV\necho ""CHOC_OPS=${CHOC_OPS}"" >> $GITHUB_ENV\necho ""LD_LIBRARY_PATH=${OPENSSL_INSTALL_PATH}/lib:${PYTHON_INSTALL_PATH}/lib:/usr/local/lib"" >> $GITHUB_ENV\n', 'git clone https://github.com/openssl/openssl.git\ncd ""${OPENSSL_SOURCE_PATH}""\nexport LATEST_STABLE_TAG=$(git tag --list openssl-* | grep -v alpha | grep -v beta | sort -Vr | head -n1)\necho ""Checking out version ${LATEST_STABLE_TAG}""\ngit checkout ""${LATEST_STABLE_TAG}""\nexport COMPILED_OPENSSL_VERSION=${LATEST_STABLE_TAG:8} # Trim the openssl- prefix\necho ""COMPILED_OPENSSL_VERSION=${COMPILED_OPENSSL_VERSION}"" >> $GITHUB_ENV\n', 'choco install sqlite $CHOC_OPS\n', 'cd ""${OPENSSL_SOURCE_PATH}""\n# --libdir=lib is needed so Python can find OpenSSL libraries\n""${PERL}"" ./Configure ""${OPENSSL_CONFIG_TARGET}"" --libdir=lib --prefix=""${OPENSSL_INSTALL_PATH}"" $OPENSSL_CONFIG_OPTS\n', 'mv /usr/bin/link /usr/bin/gnulink', 'cd ""${OPENSSL_SOURCE_PATH}""\n$MAKE ""${MAKEOPT}""\n', 'cd ""${OPENSSL_SOURCE_PATH}""\n# install_sw saves us ages processing man pages :-)\n$MAKE install_sw\n', '""${OPENSSL_INSTALL_PATH}/bin/openssl"" version\n', 'git clone https://github.com/python/cpython.git\ncd ""${PYTHON_SOURCE_PATH}""\nexport LATEST_STABLE_TAG=$(git tag --list | grep -v a | grep -v rc | grep -v b | sort -Vr | head -n1)\ngit checkout ""${LATEST_STABLE_TAG}""\nexport COMPILED_PYTHON_VERSION=${LATEST_STABLE_TAG:1} # Trim the ""v"" prefix\necho ""COMPILED_PYTHON_VERSION=${COMPILED_PYTHON_VERSION}"" >> $GITHUB_ENV\n', 'cd ""${PYTHON_SOURCE_PATH}""\n./configure --with-openssl=""${OPENSSL_INSTALL_PATH}"" \\\n            --prefix=""${PYTHON_INSTALL_PATH}"" \\\n            --enable-shared \\\n            --with-ensurepip=upgrade \\\n            --enable-optimizations \\\n            --with-lto\n', 'cd ""${env:PYTHON_SOURCE_PATH}""\nPCBuild\\get_externals.bat\n', 'cd ""${env:PYTHON_SOURCE_PATH}""\n$env:OPENSSL_EXT_PATH = ""$(Get-Item externals\\openssl-bin-* | Select -exp FullName)\\""\necho ""External OpenSSL was downloaded to ${env:OPENSSL_EXT_PATH}""\nRemove-Item -recurse -force ""${env:OPENSSL_EXT_PATH}*""\n# Emulate what this script does:\n# https://github.com/python/cpython/blob/main/PCbuild/openssl.vcxproj\n$env:OPENSSL_EXT_TARGET_PATH = ""${env:OPENSSL_EXT_PATH}${env:PYEXTERNALS_PATH}""\necho ""Copying our OpenSSL to ${env:OPENSSL_EXT_TARGET_PATH}""\nmkdir ""${env:OPENSSL_EXT_TARGET_PATH}\\include\\openssl\\""\nCopy-Item -Path ""${env:OPENSSL_SOURCE_PATH}\\LICENSE.txt"" -Destination ""${env:OPENSSL_EXT_TARGET_PATH}\\LICENSE"" -verbose\ncp ""$env:OPENSSL_INSTALL_PATH\\lib\\*"" ""${env:OPENSSL_EXT_TARGET_PATH}""\ncp ""$env:OPENSSL_INSTALL_PATH\\bin\\*"" ""${env:OPENSSL_EXT_TARGET_PATH}""\ncp ""$env:OPENSSL_INSTALL_PATH\\include\\openssl\\*"" ""${env:OPENSSL_EXT_TARGET_PATH}\\include\\openssl\\""\ncp ""$env:OPENSSL_INSTALL_PATH\\include\\openssl\\applink.c"" ""${env:OPENSSL_EXT_TARGET_PATH}\\include\\""\n', 'pip install --upgrade pip\npip install --upgrade sphinx\nsphinx-build --version\n', 'cd ""${env:PYTHON_SOURCE_PATH}""\n# We need out custom openssl.props which uses OpenSSL 3 DLL names\nCopy-Item -Path ""${env:GITHUB_WORKSPACE}\\openssl.props"" -Destination PCBuild\\ -verbose\necho ""Building for ${env:PYBUILDRELEASE_ARCH}...""\nPCBuild\\build.bat -m --pgo -c Release -p ""${env:PYBUILDRELEASE_ARCH}""\n', 'cd ""${PYTHON_SOURCE_PATH}""\necho ""Running: ${MAKE} ${MAKEOPT}""\n$MAKE $MAKEOPT\n', 'cd ""${PYTHON_SOURCE_PATH}""\n$MAKE altinstall\n$MAKE bininstall\n', '""${PYTHON}"" -V\n', 'curl -O https://bootstrap.pypa.io/get-pip.py\n""${PYTHON}"" get-pip.py\n""${PYTHON}"" -m pip install --upgrade pip\n""${PYTHON}"" -m pip install --upgrade wheel\n""${PYTHON}"" -m pip install --upgrade setuptools\n', 'set +e\n""${PYTHON}"" -m pip list --outdated --format=freeze | grep -v \'^\\-e\' | cut -d = -f 1  | xargs -n1 ""${PYTHON}"" -m pip install -U --force-reinstall\n""${PYTHON}"" -m pip install --upgrade -r requirements.txt\n', 'git clone https://github.com/pyinstaller/pyinstaller.git\ncd pyinstaller\nexport latest_release=$(git tag --list | grep -v dev | grep -v rc | sort -Vr | head -n1)\necho $latest_release\ngit checkout ""${latest_release}""\n# remove pre-compiled bootloaders so we fail if bootloader compile fails\nrm -rf PyInstaller/bootloader/*-*/*\ncd bootloader\nif [[ ""${arch}"" == ""Win64"" ]]; then\n  export PYINSTALLER_BUILD_ARGS=""--target-arch=64bit""\nfi\necho ""PyInstaller build arguments: ${PYINSTALLER_BUILD_ARGS}""\n""${PYTHON}"" ./waf all $PYINSTALLER_BUILD_ARGS\ncd ..\n""${PYTHON}"" -m pip install .\n""${PYTHON}"" -m PyInstaller --version\n', '""${PYTHON}"" -m PyInstaller --clean --distpath=gyb gyb.spec\nif [ -x ""$(command -v realpath)"" ]; then\n  realpath=realpath\nelse\n  brew install coreutils\n  realpath=grealpath\nfi\necho ""gybpath=$(${realpath} ./gyb/)"" >> $GITHUB_ENV\necho ""gyb=$(${realpath} ./gyb/gyb)"" >> $GITHUB_ENV\necho -e ""GYB: ${gyb}\\nGYBPATH: ${gybpath}""\n', 'echo -e ""PYTHON: $PYTHON\\ngyb: $gyb\\ngybpath: $gybpath\\n""\ntouch ""${gybpath}/nobrowser.txt""\n$gyb --version\nexport GYBVERSION=$($gyb --short-version )\necho ""GYB Version ${GYBVERSION}""\necho ""GYBVERSION=${GYBVERSION}"" >> $GITHUB_ENV\nif [ -d ""gyb"" ]; then\n  cp LICENSE gyb/\nfi\nrm -f ""${gybpath}/nobrowser.txt""\nrm -f ""${gybpath}/lastcheck.txt""\n', 'this_glibc_ver=$(ldd --version | awk \'/ldd/{print $NF}\')\nGYB_ARCHIVE=""gyb-${GYBVERSION}-linux-$(arch)-glibc${this_glibc_ver}.tar.xz""\ntar cfJ $GYB_ARCHIVE gyb/\n', '""${PYTHON}"" -m pip install --upgrade patchelf-wrapper\n""${PYTHON}"" -m pip install --upgrade staticx\n', 'case $RUNNER_ARCH in\n  X64)\n    ldlib=/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n    ;;\n  ARM64)\n    ldlib=/lib/aarch64-linux-gnu/ld-linux-aarch64.so.1\n    ;;\nesac\necho ""ldlib=${ldlib}""\n$PYTHON -m staticx -l ""${ldlib}"" gyb/gyb gyb/gyb-staticx\n', 'gyb/gyb-staticx --version\nmv gyb/gyb-staticx gyb/gyb\n', 'GYB_ARCHIVE=""gyb-${GYBVERSION}-linux-$(uname -m)-legacy.tar.xz""\ntar cfJ $GYB_ARCHIVE gyb/\n', 'MACOSVERSION=$(defaults read loginwindow SystemVersionStampAsString)\nGYB_ARCHIVE=""gyb-${GYBVERSION}-macos-x86_64.tar.xz""\ntar cfJ $GYB_ARCHIVE gyb/\n', 'cp gyb-setup.bat gyb\nGYB_ARCHIVE=gyb-$GYBVERSION-windows-x86_64.zip\n/c/Program\\ Files/7-Zip/7z.exe a -tzip $GYB_ARCHIVE gyb -xr!.svn\nmkdir gyb-64\ncp -rf gyb/* gyb-64/;\n/c/Program\\ Files\\ \\(x86\\)/WiX\\ Toolset\\ v3.11/bin/candle.exe -arch x64 windows-gyb.wxs\n/c/Program\\ Files\\ \\(x86\\)/WiX\\ Toolset\\ v3.11/bin/light.exe -ext /c/Program\\ Files\\ \\(x86\\)/WiX\\ Toolset\\ v3.11/bin/WixUIExtension.dll windows-gyb.wixobj -o gyb-$GYBVERSION-windows-x86_64.msi || true;\nrm -f *.wixpdb\n', 'export voutput=$($gyb --version)\nexport python_line=$(echo -e ""${voutput}"" | grep ""Python "")\nexport python_arr=($python_line)\nexport this_python=${python_arr[1]}\nif [[ ""${this_python}"" != ""${COMPILED_PYTHON_VERSION}"" ]]; then\n  echo ""ERROR: Tried to compile Python ${COMPILED_PYTHON_VERSION} but ended up with ${this_python}""\n  exit 1\nfi\nexport openssl_line=$(echo -e ""${voutput}"" | grep ""OpenSSL "")\nexport openssl_arr=($openssl_line)\nexport this_openssl=${openssl_arr[1]}\nif [[ ""${this_openssl}"" != ""${COMPILED_OPENSSL_VERSION}"" ]]; then\n  echo ""ERROR: Tried to compile OpenSSL ${COMPILED_OPENSSL_VERSION} but ended up with ${this_openssl}""\n  exit 1\nfi\necho ""We successfully compiled Python ${this_python} and OpenSSL ${this_openssl}""\n', 'export gyb_user=""gyb-gha-${JID}@pdl.jaylee.us""\nexport gyb_group=""gyb-travis-group-${JID}@pdl.jaylee.us""\nsource .github/actions/decrypt.sh .github/actions/creds.tar.gpg creds.tar\n$gyb --action check-service-account --email $gyb_user\n$gyb --action purge --email $gyb_user\n$gyb --action purge-labels --email $gyb_user\n$gyb --action restore --local-folder samples/gyb-format --email $gyb_user --cleanup\n$gyb --action restore --local-folder samples/gyb-format --email $gyb_user --service-account --noresume\n$gyb --action restore-group --local-folder samples/gyb-format --email $gyb_group --use-admin $gyb_user --cleanup --cleanup-from ""God of Thunder <thor@avengers.com>""\n$gyb --action restore-group --local-folder samples/gyb-format --email $gyb_group --use-admin $gyb_user --service-account --noresume\n$gyb --action restore-group --local-folder samples/google-takeout --email $gyb_group --use-admin $gyb_user\n$gyb --action restore-group --local-folder samples/vault-export-mbox --email $gyb_group --use-admin $gyb_user --service-account\n$gyb --action restore-mbox --local-folder samples/historic-public-mbox --email $gyb_user --cleanup\n$gyb --action restore-mbox --local-folder samples/historic-public-mbox --email $gyb_user --service-account --noresume\n$gyb --action restore-mbox --local-folder samples/google-takeout --email $gyb_user --cleanup --cleanup-from  ""God of Thunder <thor@avengers.com>""\n$gyb --action restore-mbox --local-folder samples/vault-export-mbox --email $gyb_user\n$gyb --action count --email $gyb_user\n$gyb --action count --email $gyb_user --service-account\n$gyb --action quota --email $gyb_user\n$gyb --action quota --email $gyb_user --service-account\n$gyb --action estimate --email $gyb_user\n$gyb --action estimate --email $gyb_user --service-account\n$gyb --action print-labels --email $gyb_user\n$gyb --action print-labels --email $gyb_user --service-account\n$gyb --action backup --email $gyb_user\n$gyb --action backup --email $gyb_user --service-account --local-folder sa-backup\n$gyb --action backup-chat --email $gyb_user\n']"
"['pip install mkdocs-material mkdocs-redirects', 'mkdocs gh-deploy --force', 'docker compose -f docker/e2e/docker-compose.yml up -d', 'docker compose -f docker/e2e/docker-compose.yml exec -T timesketch python3 /usr/local/src/timesketch/end_to_end_tests/tools/run_in_container.py', 'docker compose -f docker/e2e/docker-compose.yml up -d', 'docker compose -f docker/e2e/docker-compose.yml exec -T timesketch python3 /usr/local/src/timesketch/end_to_end_tests/tools/run_in_container.py', 'pip install -r test_requirements.txt\npip install -r requirements.txt\npip install -e .\npip install api_client/python\npip install importer_client/python\n', 'git config pull.rebase false && git fetch -p origin master\nfor FILE in `git --no-pager diff origin/master --name-only --diff-filter=ACMR | grep \\.py$`; do echo ""Running pylint against ${FILE}""; pylint --rcfile=.pylintrc ${FILE}; done\n', 'yarn add eslint@5.16.0', 'git config pull.rebase false && git fetch -p origin master\nfor FILE in `git --no-pager diff origin/master --name-only --diff-filter=ACMR | grep -e \\.vue$ -e \\.js$ | grep -v dist\\/js | grep ^timesketch\\/frontend\\/ | sed s/\'^timesketch\\/frontend\\/\'/\'\'/`; do echo ""Running eslint against ${FILE}""; yarn run eslint ${FILE}; done\n', 'yarn add eslint@5.16.0', 'git config pull.rebase false && git fetch -p origin master\nfor FILE in `git --no-pager diff origin/master --name-only --diff-filter=ACMR | grep -e \\.vue$ -e \\.js$ | grep -v dist\\/js | grep ^timesketch\\/frontend-ng\\/ | sed s/\'^timesketch\\/frontend-ng\\/\'/\'\'/`; do echo ""Running eslint against ${FILE}""; yarn run eslint ${FILE}; done\n', 'pip install pipenv\npipenv install -d\npipenv install -r test_requirements.txt\n', 'pipenv run python run_tests.py\n', 'sudo apt install yarn npm --no-install-recommends\nyarn install\n', 'yarn run test:unit']"
"['python -m pip install --upgrade pip', 'python -m pip install --upgrade yapf toml', 'python -m pip install av==9.2 opencv-python-headless --only-binary "":all:""', 'python -m pip install -r requirements_headless.txt', 'python -m yapf --diff --recursive scenedetect/', 'python -m yapf --diff --recursive tests/']"
""
"['make develop\npython -m pip install -U wheel twine setuptools\n', 'make lint\n', 'make test\n', 'python setup.py sdist bdist_wheel\npython -m twine check dist/*\n', 'make develop\npython -m pip install -U wheel twine setuptools\n', 'make lint\n', 'make test\n', 'python setup.py sdist bdist_wheel\npython -m twine check dist/*\n', 'python -m twine upload dist/* --skip-existing || echo ""Failed to deploy""\n']"
"['make develop\npython -m pip install -U wheel twine setuptools\n', 'make lint\n', 'make test\n', 'python setup.py sdist bdist_wheel\npython -m twine check dist/*\n', 'make develop\npython -m pip install -U wheel twine setuptools\n', 'make lint\n', 'make test\n', 'python setup.py sdist bdist_wheel\npython -m twine check dist/*\n', 'python -m twine upload dist/* --skip-existing\n']"
"['mkdir -p dist', 'python -m pip install build --user', 'python -m build --sdist --wheel --outdir dist/', 'python -m pip install --upgrade pip setuptools\npip install -r tests/requirements.txt\n', 'pip freeze\npython -b -m pytest -v --cov=privacyidea --cov-report=xml tests/\n']"
"['DOCKER_IMAGE=openwpm/openwpm\nTAGS=""${DOCKER_IMAGE}:${GITHUB_SHA}""\n# If it\'s a tagged build it should get published as such\nif [[ $GITHUB_REF == refs/tags/v* ]]; then\n  VERSION=${GITHUB_REF#refs/tags/v}\n  TAGS=""$TAGS,${DOCKER_IMAGE}:$VERSION""\nfi\n# If it is on master it should get tagged as latest\nif [[ $GITHUB_REF == refs/heads/master ]]; then\n    TAGS=""$TAGS,${DOCKER_IMAGE}:latest""\nfi\necho ::set-output name=tags::${TAGS}\n', 'echo ${{ steps.docker_build.outputs.digest }}', 'echo ""MAMBA_PATH=$HOME/mamba"" >> $GITHUB_ENV', 'echo ""OPENWPM_MAMBA_PATH=$MAMBA_PATH/envs/openwpm"" >> $GITHUB_ENV', '$GITHUB_WORKSPACE/scripts/install-mamba.sh', 'echo ""$MAMBA_PATH/bin"" >> $GITHUB_PATH', '$GITHUB_WORKSPACE/install.sh', '$GITHUB_WORKSPACE/install.sh --skip-create', 'echo ""$OPENWPM_MAMBA_PATH/bin"" >> $GITHUB_PATH', 'pre-commit run --all', 'echo ""MAMBA_PATH=$HOME/mamba"" >> $GITHUB_ENV', 'echo ""OPENWPM_MAMBA_PATH=$MAMBA_PATH/envs/openwpm"" >> $GITHUB_ENV', '$GITHUB_WORKSPACE/scripts/install-mamba.sh', 'echo ""$MAMBA_PATH/bin"" >> $GITHUB_PATH', '$GITHUB_WORKSPACE/install.sh', '$GITHUB_WORKSPACE/install.sh --skip-create', 'echo ""$OPENWPM_MAMBA_PATH/bin"" >> $GITHUB_PATH', './scripts/ci.sh']"
"['# openssh-server trying to change file permissions to 0600 and we want to do it in /tmp directory\ncp -r ./e2e_tests/ssh-server-config /tmp/ssh\nsed -i ""s#./ssh-server-config#/tmp/ssh#g"" ./e2e_tests/docker-compose.yaml\nchmod 600 ./e2e_tests/ssh-server-config/ssh_host_rsa_key\ncd e2e_tests && docker-compose up -d\n', 'id\nuname -a\nlsb_release -a\n${{ matrix.python }} -V\ncurl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py\n${{ matrix.python }} get-pip.py\n${{ matrix.python }} -m pip install --upgrade pip\n${{ matrix.python }} -m pip install .\n${{ matrix.python }} -m pip install psycopg2-binary>=2.9.6 pymysql>=1.0.3 pymongo>=4.3.3\n${{ matrix.python }} -m pip install --upgrade pyopenssl\n\nssh -o ""StrictHostKeyChecking=no"" linuxserver@127.0.0.1 -p 2223 -i ./e2e_tests/ssh-server-config/ssh_host_rsa_key -vvvvv ""uname -a""\n\n# cd e2e_tests && docker-compose logs ssh; cd ..\n# cd e2e_tests && docker-compose exec ssh cat /config/logs/openssh/current; cd ..\n# docker exec openssh-server tail -f /config/logs/openssh/current\n', '${{ matrix.python }} e2e_tests/run_docker_e2e_db_tests.py', 'timeout 10s ${{ matrix.python }} e2e_tests/run_docker_e2e_hangs_tests.py', 'docker exec openssh-server cat /config/logs/openssh/current > openssh-server.log']"
"['python -m pip install --upgrade pip\npython -m pip install pipenv\npipenv install --dev\n', 'pipenv run codespell -L paket --skip=./textstat/resources/**/easy_words.txt,./build/*,./textstat.egg-info/*,./.git/* --exclude-file=.codespellignorelines\n', 'pipenv run flake8 . --exclude=build/\n', 'pipenv run pytest test.py\n']"
""
"['pip install tox', 'tox']"
"['python -m pip install --upgrade pip setuptools wheel tox', 'tox -e apidocs', ""python -m pip install --upgrade pip setuptools wheel tox \npython -m pip install '.[test]'\n"", 'pytest']"
"['python3 -m pip install tox tox-gh-actions\n', 'npm ci || npm ci || npm ci\n', 'npm run build\n', 'tox -- -vvv --selenosis-driver=chrome-headless --numprocesses=4 --exitfirst || \\\ntox -- -vvv --selenosis-driver=chrome-headless\n', 'tox -e coverage-report\ntox -e codecov\n', 'npm run report && npm run codecov', 'npm ci || npm ci || npm ci\n', 'pip install flake8', 'npm run lint:${{ matrix.tool }}\n', 'echo Test Successful']"
"['sudo apt-get install shellcheck\n./run-tests.sh --check-shellscript\n', 'pip install --upgrade pip\npip install pycodestyle\n./run-tests.sh --check-pycodestyle\n', 'pip install --upgrade pip\npip install pydocstyle\n./run-tests.sh --check-pydocstyle\n', 'pip install --upgrade pip\npip install check-manifest\n./run-tests.sh --check-manifest\n', 'npm install -g jsonlint', './run-tests.sh --check-fixtures', 'pip install --upgrade pip\npip install isort\n./run-tests.sh --check-isort\n', 'docker run -i --rm hadolint/hadolint:v1.18.2 < Dockerfile', './scripts/generate-localhost-certificate.sh', './run-tests.sh --check-docker-build', 'docker-compose run --rm web ./run-tests.sh --check-pytest']"
""
"['pip install packaging\n', 'echo ""release_version=`echo \'${{ github.event.inputs.requested_release_tag }}\' | sed \'s/^v//\'`"" >> $GITHUB_ENV\n', 'echo ""release_tag=v${release_version}"" >> $GITHUB_ENV\n', 'echo ""ciso8601_version=`grep -Po \'VERSION = ""\\K[^""]*\' setup.py`"" >> $GITHUB_ENV\n', 'curl https://pypi.org/pypi/ciso8601/json | python -c \'import json, sys; contents=sys.stdin.read(); parsed = json.loads(contents); print(""pypi_version="" + parsed[""info""][""version""])\' >> $GITHUB_ENV\n', ""echo 'Requested release tag `${{ github.event.inputs.requested_release_tag }}`'\necho 'Release version       `${{ env.release_version }}`'\necho 'Release tag           `${{ env.release_tag }}`'\necho 'VERSION in setup.py   `${{ env.ciso8601_version }}`'\necho 'Version in PyPI       `${{ env.pypi_version }}`'\n"", 'echo ""${{ env.release_version }}"" | sed \'/^[0-9]\\+\\.[0-9]\\+\\.[0-9]\\+$/!{q1}\'\n', 'echo ""${{ env.release_tag }}"" | sed \'/^v[0-9]\\+\\.[0-9]\\+\\.[0-9]\\+$/!{q1}\'\n', '[[ ${{ env.release_version }} == ${{ env.ciso8601_version }} ]]\n', 'python -c \'import sys; from packaging import version; code = 0 if version.parse(""${{ env.pypi_version }}"") < version.parse(""${{ env.release_version }}"") else 1; sys.exit(code)\'\n', 'grep ${{ env.release_version }} CHANGELOG.md\n', 'echo -e ""release_version=${{ env.release_version }}\\nrelease_tag=${{ env.release_tag }}"" > release_values.txt\n', 'python setup.py sdist']"
""
"['TAG=$(basename ${GITHUB_REF})\nVERSION=${TAG/v/}\nwget https://raw.githubusercontent.com/jumpserver/installer/master/quick_start.sh\nsed -i ""s@VERSION=dev@VERSION=v${VERSION}@g"" quick_start.sh\necho ""::set-output name=TAG::$TAG""\necho ""::set-output name=VERSION::$VERSION""\n']"
""
"['python -m pip install --upgrade pip\npip install -r requirements.txt\npip install urwid twisted watchdog ""jedi >=0.16"" babel ""sphinx >=1.5,<7""\npip install pytest pytest-cov numpy\n', 'python setup.py build\n', 'python setup.py build_sphinx\npython setup.py build_sphinx_man\n', 'pytest --cov=bpython --cov-report=xml -v\n', 'python -m pip install --upgrade pip\npip install black codespell\n', 'black --check .', 'python -m pip install --upgrade pip\npip install mypy\npip install -r requirements.txt\npip install urwid twisted watchdog ""jedi >=0.16"" babel ""sphinx >=1.5"" numpy\npip install types-backports types-requests types-setuptools types-toml types-pygments\n', 'mypy']"
"['true', 'skip=""""\nif [[ ""$(uname -s)"" == ""Linux"" ]]; then\n  skip=""${{ env._PEX_TEST_PYENV_VERSIONS }}""\nfi\necho ""skip=${skip}"" >> $GITHUB_OUTPUT\n', 'skip=""""\nif [[ ""$(uname -s)"" == ""Linux"" ]]; then\n  skip=""${{ env._PEX_TEST_PYENV_VERSIONS }}""\nfi\necho ""skip=${skip}"" >> $GITHUB_OUTPUT\n', 'skip=""""\nif [[ ""$(uname -s)"" == ""Linux"" ]]; then\n  skip=""${{ env._PEX_TEST_PYENV_VERSIONS }}""\nfi\necho ""skip=${skip}"" >> $GITHUB_OUTPUT\n', 'skip=""""\nif [[ ""$(uname -s)"" == ""Linux"" ]]; then\n  skip=""${{ env._PEX_TEST_PYENV_VERSIONS }}""\nfi\necho ""skip=${skip}"" >> $GITHUB_OUTPUT\n', '# This is needed for `test_requirement_file_from_url` for building `lxml`.\nsudo apt install --yes libxslt-dev\n', 'false', 'true', 'true', 'if [[ -n ""${{ github.event.inputs.tag }}"" ]]; then\n  RELEASE_TAG=${{ github.event.inputs.tag }}\nelse\n  RELEASE_TAG=${GITHUB_REF#refs/tags/}\nfi\nif [[ ""${RELEASE_TAG}"" =~ ^v[0-9]+.[0-9]+.[0-9]+$ ]]; then\n  echo ""release-tag=${RELEASE_TAG}"" >> $GITHUB_OUTPUT\n  echo ""release-version=${RELEASE_TAG#v}"" >> $GITHUB_OUTPUT\nelse\n  echo ""::error::Release tag \'${RELEASE_TAG}\' must match \'v\\d+.\\d+.\\d+\'.""\n  exit 1\nfi\n']"
""
"['git tag v${{ github.event.inputs.version }} ${{ github.sha }}\ngit push origin v${{ github.event.inputs.version }}\n', 'pip install pypandoc\nsudo apt-get install pandoc\npython scripts/gen-release-notes.py\n', 'python -m pip install --upgrade pip\npip install tox\n', 'tox -e ${{ matrix.tox_env }}\n']"
"['ls -al ${{ env.hugo-image-cache-path }}', 'ls -al ${{ env.hugo-image-cache-path }}', 'python -m pip install --upgrade pip\npip install -r ./ci/requirements.txt\n', 'npm install', 'hugo --gc', 'curl \\\n-H ""Accept: application/vnd.github+json"" \\\n-H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" \\\nhttps://api.github.com/repos/${{ env.repo-owner }}/${{ env.repo-name }}/actions/caches?key=${{ env.hugo-image-cache-name }} \\\n-o cache-list.json\n\necho ""The following caches will be deleted:""\ncat cache-list.json\n', 'for id in $(jq \'.actions_caches[].id\' cache-list.json); do\n  echo ""Deleting cache with id $id""\n  curl \\\n  -X DELETE \\\n  -H ""Accept: application/vnd.github+json"" \\\n  -H ""Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"" \\\n  https://api.github.com/repos/${{ env.repo-owner }}/${{ env.repo-name }}/actions/caches/$id\ndone', 'python -m pip install --upgrade pip\npip install -r ./ci/requirements.txt\n', 'python ./ci/blueberry.py\n', 'sudo apt-get install moreutils', 'docs_dir_files=$(echo $added_modified |  jq -c \'[.[] | select(.|test(""^docs/""))]\')\necho ""::set-output name=added_modified::$docs_dir_files""\necho ""Added or modified files located within docs/ directory:""\necho $docs_dir_files | jq \'.\'\n', 'python -m pip install --upgrade pip\npip install python-frontmatter\n', 'python ./ci/check-links.py ${{ steps.files.outputs.all }}', 'ls -al ${{ env.hugo-image-cache-path }}', 'ls -al ${{ env.hugo-image-cache-path }}', 'python -m pip install --upgrade pip\npip install -r ./ci/requirements.txt\n', 'npm install', '# Start Hugo in background:\nhugo server &\n\n# Wait for server to start\n# (or possibly fail to start):\nwhile true\ndo\n    # Check if the server is responding on 1313,\n    # which means that the Hugo build finished.\n    if nc -z localhost 1313\n    then\n        exit 0\n    fi\n\n    # Check if the Hugo process is still running.\n    # If not, then the server failed to build.\n    if ! pgrep hugo > /dev/null\n    then\n        exit 1\n    fi\n\n    sleep 1\ndone\n', 'python ./ci/docs404.py']"
"['python -m pip install --upgrade pip\npip install tox\n', 'tox run -f py$(echo ${{ matrix.python-version }} | tr -d .)', 'python -m pip install --upgrade pip\npip install tox\n', 'tox']"
""
"['sudo apt update\nsudo apt install ffmpeg\n', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements.dev.txt\npip install .\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-line-length=127 --ignore=W503,W504,C901\n', 'mypy ffmpeg_normalize\n', 'pytest test/test.py\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\nsudo apt-get install gdal-bin\n', 'tox']"
"['pip install flake8', 'flake8 .', 'python runtests.py -v --failfast', 'pip install multiprocess', 'python runtests.py -v --failfast']"
""
"['poetry install\npoetry build\n', 'poetry run pytest -s', 'poetry run musicbox -v']"
[]
"['python -m pip install --upgrade pip\npip install isort black flake8\n', 'isort -p mlxtend --check --diff --line-length 88 --multi-line 3 --py 39 --profile black mlxtend/*\nblack --check --diff mlxtend/*\n# exit-zero treats all errors as warnings.\nflake8 . --config=.flake8 --count --exit-zero --statistics ', '# $CONDA is an environment variable pointing to the root of the miniconda directory\necho $CONDA/bin >> $GITHUB_PATH\n', 'conda env update --file environment.yml --name base\n', 'conda update conda -y -q\nconda config --add channels conda-forge\nconda update conda -y -q\nconda install scikit-learn=1.0.2 pandas=1.3.5  -y -q\nconda install tensorflow joblib pytest -y -q\nconda install imageio scikit-image -y -q\nconda install dlib -y -q\npip install markdown\npip install coverage\npip install -e  .\npython -c ""import numpy; print(\'NumPy:\', numpy.__version__)""\npython -c ""import scipy; print(\'SciPy:\', scipy.__version__)""\npython -c ""import sklearn; print(\'Scikit-learn:\', sklearn.__version__)""\npython -c ""import pandas; print(\'Pandas:\', pandas.__version__)""\ncoverage run --source=mlxtend --branch -m pytest mlxtend\ncoverage xml\n']"
"['gh pr review --approve ""$PR_URL""', 'gh pr merge --auto --squash --delete-branch ""$PR_URL""', './.github/scripts/install.sh', 'python3 -m nox --session docs', 'echo ""Creating tar for generated docs""\ncd ./docs/_build/html && tar cvf ~/docs.tar .\n\necho ""Unpacking tar into gh-pages branch""\ngit fetch --no-tags --prune --depth=1 origin +refs/heads/*:refs/remotes/origin/*\ncd $GITHUB_WORKSPACE && git checkout gh-pages && tar xvf ~/docs.tar\n', './.github/scripts/install.sh', 'python3 -m nox -e distribution', 'rm -rf googlemaps-* dist/', './.github/scripts/install.sh', 'python3 -m nox --session ""tests-${{ matrix.python-version }}""\npython3 -m nox -e distribution\n', 'echo ""Test matrix finished"";\nexit 0;\n']"
""
""
"[""pip install -v './python-control[test]'"", '# Install compilers, libraries, and development environment\nsudo apt-get -y install gfortran cmake --fix-missing\nsudo apt-get -y install libblas-dev liblapack-dev\n\n# Compile and install slycot\npip install -v .\n', 'pytest -v control/tests', 'mamba install cvxopt pandas slycot \n', 'make html\nmake doctest\n', 'echo $CONDA/bin >> $GITHUB_PATH', 'conda create \\\n   --name control-examples-env \\\n   --channel conda-forge \\\n   --strict-channel-priority \\\n   --quiet --yes \\\n   pip setuptools setuptools-scm \\\n   numpy matplotlib scipy \\\n   slycot pmw jupyter\n', 'conda run -n control-examples-env pip install .\n', 'cd examples\nconda run -n control-examples-env ./run_examples.sh\nconda run -n control-examples-env ./run_notebooks.sh\n', 'sudo apt-get -y update\nsudo apt-get -y install gfortran cmake --fix-missing\ncase ${{ matrix.bla_vendor }} in\n  unset | Generic )  sudo apt-get -y install libblas-dev liblapack-dev ;;\n  OpenBLAS        ) sudo apt-get -y install libopenblas-dev ;;\n  *)\n    echo ""bla_vendor option ${{ matrix.bla_vendor }} not supported""\n    exit 1 ;;\nesac\n', 'case ${{ matrix.bla_vendor }} in\n  unset | Generic | Apple ) ;; # Found in system\n  OpenBLAS )\n    brew install openblas\n    echo ""BLAS_ROOT=/usr/local/opt/openblas/"" >> $GITHUB_ENV\n    echo ""LAPACK_ROOT=/usr/local/opt/openblas/"" >> $GITHUB_ENV\n    ;;\n  *)\n    echo ""bla_vendor option ${{ matrix.bla_vendor }} not supported""\n    exit 1 ;;\nesac\necho ""FC=gfortran-11"" >> $GITHUB_ENV\n', 'if [[ $BLA_VENDOR = unset ]]; then unset BLA_VENDOR; fi\npython -m pip install --upgrade pip\npip wheel -v -w . .\nwheeldir=slycot-wheels/${{ matrix.os }}-${{ matrix.python }}-${{ matrix.bla_vendor }}\nmkdir -p ${wheeldir}\ncp ./slycot*.whl ${wheeldir}/\n', 'set -e\nnumpyversion=$(python -c \'import numpy; print(numpy.version.version)\')\nconda mambabuild --python ""${{ matrix.python }}"" --numpy $numpyversion conda-recipe\n# preserve directory structure for custom conda channel\nfind ""${CONDA_PREFIX}/conda-bld"" -maxdepth 2 -name \'slycot*.tar.bz2\' | while read -r conda_pkg; do\n  conda_platform=$(basename $(dirname ""${conda_pkg}""))\n  mkdir -p ""slycot-conda-pkgs/${conda_platform}""\n  cp ""${conda_pkg}"" ""slycot-conda-pkgs/${conda_platform}/""\ndone\n', 'echo ""matrix=$(python3 .github/scripts/set-pip-test-matrix.py)"" >> $GITHUB_OUTPUT', 'echo ""matrix=$(python3 .github/scripts/set-conda-test-matrix.py)"" >> $GITHUB_OUTPUT', 'set -xe\nsudo apt-get -y update\ncase ${{ matrix.blas_lib }} in\n  Generic )  sudo apt-get -y install libblas3 liblapack3 ;;\n  unset | OpenBLAS        ) sudo apt-get -y install libopenblas-base ;;\n  *)\n    echo ""BLAS ${{ matrix.blas_lib }} not supported for wheels on Ubuntu""\n    exit 1 ;;\nesac\nupdate-alternatives --display libblas.so.3-x86_64-linux-gnu\nupdate-alternatives --display liblapack.so.3-x86_64-linux-gnu\n', 'set -xe\nbrew install coreutils\ncase ${{ matrix.blas_lib }} in\n  unset | Generic | Apple ) ;; # system provided (Uses Apple Accelerate Framework)\n  OpenBLAS )\n    brew install openblas\n    echo ""DYLIB_LIBRARY_PATH=/usr/local/opt/openblas/lib"" >> $GITHUB_ENV\n    ;;\n  *)\n    echo ""BLAS option ${{ matrix.blas_lib }} not supported for wheels on MacOS""\n    exit 1 ;;\nesac\n', 'python -m pip install --upgrade pip\npip install matplotlib scipy pytest pytest-cov pytest-timeout coverage coveralls\npip install slycot-wheels/${{ matrix.packagekey }}/slycot*.whl\npip show slycot\n', 'JOBNAME=""$JOBNAME"" pytest control/tests', 'brew install coreutils', 'set -e\ncase ${{ matrix.blas_lib }} in\n  unset        ) # the conda-forge default (os dependent)\n    mamba install libblas libcblas liblapack\n    ;;\n  Generic      )\n    mamba install \'libblas=*=*netlib\' \'libcblas=*=*netlib\' \'liblapack=*=*netlib\'\n    echo ""libblas * *netlib"" >> $CONDA_PREFIX/conda-meta/pinned\n    ;;\n  OpenBLAS     )\n    mamba install \'libblas=*=*openblas\' openblas\n    echo ""libblas * *openblas"" >> $CONDA_PREFIX/conda-meta/pinned\n    ;;\n  Intel10_64lp )\n    mamba install \'libblas=*=*mkl\' mkl\n    echo ""libblas * *mkl"" >> $CONDA_PREFIX/conda-meta/pinned\n    ;;\nesac\nconda index --no-progress ./slycot-conda-pkgs\nmamba install -c ./slycot-conda-pkgs slycot\nconda list\n', 'JOBNAME=""$JOBNAME"" pytest control/tests', ""if [[ '${{matrix.cvxopt}}' == 'conda' ]]; then\n  mamba install cvxopt\nfi\nif [[ '${{matrix.slycot}}' == 'conda' ]]; then\n  mamba install slycot\nfi\nif [[ '${{matrix.pandas}}' == 'conda' ]]; then\n  mamba install pandas\nfi\n"", 'pytest -v --cov=control --cov-config=.coveragerc control/tests']"
"['bash installing_deps.sh', 'pushd bin\nbash LAUNCH.sh -l\npopd\n', 'pushd bin\nbash LAUNCH.sh -t\n']"
"['pip --use-deprecated=legacy-resolver install flake8\npip --use-deprecated=legacy-resolver install -r tests/requirements.txt\nflake8 --exclude=.moban.d,docs,setup.py --builtins=unicode,xrange,long . \npython setup.py checkdocs\n', 'pip install moban gitfs2 pypifs moban-jinja2-github moban-ansible\nmoban\ngit status\ngit diff --exit-code\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip --use-deprecated=legacy-resolver install -r requirements.txt\npip --use-deprecated=legacy-resolver install -r tests/requirements.txt\n', 'pip freeze\nnosetests --verbosity=3 --with-coverage --cover-package pyexcel --cover-package tests tests --with-doctest --doctest-extension=.rst  docs/source pyexcel\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'pip install tox\ntox -e ${{ matrix.tox-env }}\n']"
"['python -m pip install --upgrade pip wheel ""setuptools>=61""\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip list\n', 'python -m pip install -e . --no-build-isolation -v', 'pushd doc\nmake html\nmake html\npopd\n', 'source ci/github-actions/push-docs-gh-pages.sh']"
"['python -m pip install --upgrade pip\npython -m pip install --upgrade --requirement requirements/tests.txt\npython -m pip install .\n', 'make test\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade build polib\npip install .\n', 'make package\n']"
"['python -m pip install --upgrade pip\npip install pytest pytest-cov mock pycodestyle coverage pylint\npip install -e .\n', 'pycodestyle amodem/ scripts/\npylint --extension-pkg-whitelist=numpy --reports=no amodem --rcfile .pylintrc\n', 'pytest -v --cov=amodem\n']"
""
""
"['./install-test.sh\n', 'cd ..\ngit clone --depth=1 --branch=master https://github.com/textX/Arpeggio\npip uninstall -y Arpeggio\ncd Arpeggio && python setup.py install\ncd ../textX\n', './runtests.sh\n', 'coveralls\n']"
"['python -m pip install -r requirements.txt\npython -m pip install flake8 coverage\n', 'flake8\n', 'python setup.py test\n', 'python setup.py --quiet install\ncoverage run --source fredapi.fred fredapi/tests/test_fred.py\n']"
"['python -m pip install --upgrade pip\npip install tox tox-gh-actions\n', 'tox\n']"
"['pip install --upgrade pip ruff setuptools wheel', 'ruff . --select=E9,F63,F7,F82,PLE,YTT --show-source .', 'ruff --exit-zero --select=ALL --statistics --target-version=py37 .', 'ruff --exit-zero --select=ALL --ignore=ANN204,COM812,ERA001,RSE102 --statistics --target-version=py37 . || true | grep ""\\[\\*\\]""', 'pip install black codespell mypy pytest safety', 'black --check .', 'codespell --quiet-level=1', 'pip install -r requirements.txt || pip install --editable . || pip install . || true', 'mkdir --parents --verbose .mypy_cache', 'mypy --ignore-missing-imports --install-types --non-interactive .', 'pytest --doctest-modules . || true', 'pytest . || true', 'safety check', 'pip install pre-commit', 'pre-commit --version', 'pre-commit install', 'pre-commit autoupdate', 'pre-commit run --all-files', 'pip install beautifulsoup4 lxml requests', 'python trending_python.py']"
"['python -m pip install --upgrade pip\npip install -r requirements-tests.txt\n', 'flake8 sshuttle tests --count --show-source --statistics\n', 'PYTHONPATH=$PWD pytest\n']"
"['df -h\n', 'echo ""not needed for now""\n', 'brew install libomp\n', 'touch packages/vaex-core/build/lib*/vaex/vaexfast* || echo ""ok to fail""\ntouch packages/vaex-core/build/lib*/vaex/super* || echo ""ok to fail""\nls -alh packages/vaex-core/build/lib*/vaex/ || echo ""ok to fail""\n', 'pip install myst_parser\npip install -e . -v\n# ./ci/03-install-vaex.sh micromamba\n', 'mkdir dist\n(cd packages/vaex-core && python setup.py bdist_wheel); cp packages/vaex-core/dist/* dist\n(cd packages/vaex-graphql && python setup.py bdist_wheel); cp packages/vaex-graphql/dist/* dist\n(cd packages/vaex-jupyter && python setup.py bdist_wheel); cp packages/vaex-jupyter/dist/* dist\n(cd packages/vaex-ml && python setup.py bdist_wheel); cp packages/vaex-ml/dist/* dist\n(cd packages/vaex-viz && python setup.py bdist_wheel); cp packages/vaex-viz/dist/* dist\n(cd packages/vaex-astro && python setup.py bdist_wheel); cp packages/vaex-astro/dist/* dist\n(cd packages/vaex-hdf5 && python setup.py bdist_wheel); cp packages/vaex-hdf5/dist/* dist\n(cd packages/vaex-server && python setup.py bdist_wheel); cp packages/vaex-server/dist/* dist\n(cd packages/vaex-meta && python setup.py bdist_wheel); cp packages/vaex-server/dist/* dist\n', '# by default on 3.6 we get an old version, so manually upgrade\npip install gcsfs==0.8.0\n', '# 3.10 doesn\'t seem to work\npip install ""pytest-asyncio<0.14""\n', './ci/04-run-test-suite.sh\n', 'VAEX_CACHE_RESULTS=1 ./ci/04-run-test-suite.sh\n', 'python -m vaex.ml.spec packages/vaex-ml/vaex/ml/spec_new.json\ndiff packages/vaex-ml/vaex/ml/spec_new.json packages/vaex-ml/vaex/ml/spec.json\n', './ci/05-run-notebooks.sh\n', ""pip install -e 'packages/vaex-contrib[gcp]'\n./ci/06-run-contrib-tests.sh\n"", 'pip install -vv dist/vaex*core* dist/vaex*hdf5*\n', 'python -c ""import vaex; df = vaex.example()""\n', 'pip install -vv dist/*\n', 'python -c ""import vaex; df = vaex.example()""\n', 'vaex convert ~/.vaex/data/helmi-dezeeuw-2000-FeH-v2-10percent.hdf5 test.parquet\npip install rich\nVAEX_PROGRESS_TYPE=rich vaex convert ~/.vaex/data/helmi-dezeeuw-2000-FeH-v2-10percent.hdf5 test.parquet\nvaex settings yaml\nvaex settings md\n', 'vaex server --add-example --port 9999&\n', 'python -c ""import vaex; df = vaex.open(\'ws://localhost:9999/example\'); df.x.sum()""\ncurl -i http://localhost:9999/histogram/example/x\n', 'python -m pip install twine wheel\n', 'mkdir dist\n', '(cp README.md packages/vaex-meta/ && cd packages/vaex-meta && python setup.py sdist bdist_wheel); cp packages/vaex-meta/dist/* dist\n', 'twine upload dist/vaex* dist/vaex*\n', '(cd packages/vaex-graphql && python setup.py sdist bdist_wheel); cp packages/vaex-graphql/dist/* dist\n', 'twine upload dist/vaex-graphql* dist/vaex_graphql*\n', '(cd packages/vaex-jupyter && python setup.py sdist bdist_wheel); cp packages/vaex-jupyter/dist/* dist\n', 'twine upload dist/vaex-jupyter* dist/vaex_jupyter*\n', '(cd packages/vaex-ml && python setup.py sdist bdist_wheel); cp packages/vaex-ml/dist/* dist\n', 'twine upload dist/vaex-ml* dist/vaex_ml*\n', '(cd packages/vaex-contrib && python setup.py sdist bdist_wheel); cp packages/vaex-contrib/dist/* dist\n', 'twine upload dist/vaex-contrib* dist/vaex_contrib*\n', '(cd packages/vaex-viz && python setup.py sdist bdist_wheel); cp packages/vaex-viz/dist/* dist\n', 'twine upload dist/vaex-viz* dist/vaex_viz*\n', '(cd packages/vaex-astro && python setup.py sdist bdist_wheel); cp packages/vaex-astro/dist/* dist\n', 'twine upload dist/vaex-astro* dist/vaex_astro*\n', '(cd packages/vaex-hdf5 && python setup.py sdist bdist_wheel); cp packages/vaex-hdf5/dist/* dist\n', 'twine upload dist/vaex-hdf5* dist/vaex_hdf5*\n', '(cd packages/vaex-server && python setup.py sdist bdist_wheel); cp packages/vaex-server/dist/* dist\n', 'openssl sha256 dist/*\ntwine upload dist/vaex-server* dist/vaex_server*\n', 'python -m pip install cibuildwheel==2.8.1\n', 'python -m pip install twine wheel\n', 'mkdir dist\nmkdir packages/vaex-core/bin\ncp bin/install_pcre.sh packages/vaex-core/bin/\n', 'python -m cibuildwheel --output-dir dist packages/vaex-core\n', 'python -m cibuildwheel --output-dir dist packages/vaex-core\n', 'cd packages/vaex-core && python setup.py sdist\n', 'cp packages/vaex-core/dist/* dist/\n', 'openssl sha256 dist/*\ntwine upload --skip-existing dist/vaex?core*\n']"
"['python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'make release\n', 'pip install -r requirements-docs.txt\npip install -e .\nmkdocs gh-deploy --force\n', 'python -m pip install --upgrade pip pip install tox tox-gh-actions', 'tox']"
"['python -m pip install --upgrade pip\npip install pyinstaller\npip install --user -e .\n', 'pyinstaller --distpath ./${{ env.DISTPATH }} -F --icon=ci/espressif.ico --add-data="".${{ env.STUBS_DIR }}*.json${{ matrix.SEPARATOR }}${{ env.STUBS_DIR }}"" esptool.py\npyinstaller --distpath ./${{ env.DISTPATH }} -F --icon=ci/espressif.ico --add-data="".${{ env.EFUSE_DIR }}*.yaml${{ matrix.SEPARATOR }}${{ env.EFUSE_DIR }}"" espefuse.py\npyinstaller --distpath ./${{ env.DISTPATH }} -F --icon=ci/espressif.ico espsecure.py\npyinstaller --distpath ./${{ env.DISTPATH }} -F --icon=ci/espressif.ico esp_rfc2217_server.py\n', './ci/Sign-File.ps1 -Path ./${{ env.DISTPATH }}/esptool.exe\n./ci/Sign-File.ps1 -Path ./${{ env.DISTPATH }}/espefuse.exe\n./ci/Sign-File.ps1 -Path ./${{ env.DISTPATH }}/espsecure.exe\n./ci/Sign-File.ps1 -Path ./${{ env.DISTPATH }}/esp_rfc2217_server.exe\n', './${{ env.DISTPATH }}/esptool${{ matrix.EXTEN }} -h\n./${{ env.DISTPATH }}/espefuse${{ matrix.EXTEN }} -h\n./${{ env.DISTPATH }}/espsecure${{ matrix.EXTEN }} -h\n./${{ env.DISTPATH }}/esp_rfc2217_server${{ matrix.EXTEN }} -h\n', 'mv LICENSE README.md ./${{ env.DISTPATH }}', 'echo ::set-output name=VERSION::${GITHUB_REF/refs\\/tags\\//}', 'mv esptool-${{ matrix.TARGET }} ${{ env.DISTPATH }}\nzip -r ${{ env.DISTPATH }}.zip ./${{ env.DISTPATH }}/*\n', 'python -m pip install --upgrade pip\npip install setuptools twine\n', 'PUBLISHED_VERSION=$(curl https://pypi.org/pypi/esptool/json 2>/dev/null | jq -r \'.info.version\')\nCURRENT_VERSION=$(python setup.py --version 2>/dev/null)\n\nif [ ""$PUBLISHED_VERSION"" == ""$CURRENT_VERSION"" ]; then\n  echo ""Version ${PUBLISHED_VERSION} already published, skipping...""\n  exit 1\nelse\n  echo ""Packaging and publishing new esptool version: ${CURRENT_VERSION}""\n  python setup.py sdist\n  tar -ztvf dist/*\n  twine upload dist/*\nfi\n', 'sudo apt-get update\nsudo apt-get install -y softhsm2\nsudo chmod -R a+rx /etc/softhsm\nsudo chmod a+r /etc/softhsm/softhsm2.conf\nsudo chown -R $(whoami) /var/lib/softhsm\n./ci/setup_softhsm2.sh || exit 1\n', 'python setup.py build\npip install --extra-index-url https://dl.espressif.com/pypi -e .[dev,hsm]\npytest test/test_imagegen.py\npytest test/test_espsecure.py\npytest test/test_espsecure_hsm.py\npytest test/test_merge_bin.py\npytest test/test_image_info.py\npytest test/test_modules.py\n', 'esptool.py --help\nespefuse.py --help\nespsecure.py --help\n', 'export TOOLCHAIN_DIR=$HOME/toolchain\n\nexport ESP8266_BINDIR=$TOOLCHAIN_DIR/xtensa-lx106-elf/bin\nexport ESP32_BINDIR=$TOOLCHAIN_DIR/xtensa-esp32-elf/bin\nexport ESP32S2_BINDIR=$TOOLCHAIN_DIR/xtensa-esp32s2-elf/bin\nexport ESP32S3_BINDIR=$TOOLCHAIN_DIR/xtensa-esp32s3-elf/bin\nexport ESP32C3_BINDIR=$TOOLCHAIN_DIR/riscv32-esp-elf/bin\n\nexport PATH=$PATH:$ESP8266_BINDIR:$ESP32_BINDIR:$ESP32S2_BINDIR:$ESP32S3_BINDIR:$ESP32C3_BINDIR\n\n./ci/setup_ci_build_env.sh\nmake -C flasher_stub V=1\ncd flasher_stub && python ./compare_stubs.py\n', 'pip install --extra-index-url https://dl.espressif.com/pypi -e .[dev]\npython -m flake8\n']"
"['sudo apt remove python3-pip\npython -m pip install --upgrade pip\npython -m pip install . black isort mypy pytest readme_renderer\npip list\n', 'mypy ptpython\nisort -c --profile black ptpython examples setup.py\nblack --check ptpython examples setup.py\n', './tests/run_tests.py\n', 'python -m readme_renderer README.rst > /dev/null\n']"
"['docker pull opendronemap/odm\ndocker export $(docker create opendronemap/odm) --output odm-wsl-rootfs-amd64.tar.gz\ngzip odm-wsl-rootfs-amd64.tar.gz\necho ::set-output name=amd64-rootfs::""odm-wsl-rootfs-amd64.tar.gz""\n', 'echo ""Docker image digest: ${{ steps.docker_build.outputs.digest }}""\necho ""WSL AMD64 rootfs URL: ${{ steps.upload-amd64-wsl-rootfs.browser_download_url }}""\n', 'curl -X POST -u ""${{secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}"" -H ""Accept: application/vnd.github.everest-preview+json"" -H ""Content-Type: application/json"" https://api.github.com/repos/OpenDroneMap/NodeODM/actions/workflows/publish-docker.yaml/dispatches --data \'{""ref"": ""master""}\'', 'curl -X POST -u ""${{secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}"" -H ""Accept: application/vnd.github.everest-preview+json"" -H ""Content-Type: application/json"" https://api.github.com/repos/OpenDroneMap/NodeODM/actions/workflows/publish-docker-gpu.yaml/dispatches --data \'{""ref"": ""master""}\'', 'python -m pip install virtualenv\n', 'python configure.py build\n', 'rmdir SuperBuild\\download /s /q\nrmdir SuperBuild\\build /s /q\n', 'python configure.py dist --code-sign-cert-path $env:CODE_SIGN_CERT_PATH\n', 'python -m pip install virtualenv\n', 'python configure.py build\n', 'rmdir SuperBuild\\download /s /q\nrmdir SuperBuild\\build /s /q\n', 'python configure.py dist\n']"
"['import sys\nprint(sys.version)\n', 'pip install poetry\npoetry install\npip list\n', 'pip install black\nblack -l 120 -t py36 -t py37 -t py38 --check .\n', 'poetry run pytest --cov --cov-report=term --cov-report=xml\n', 'pip install poetry\npoetry install\npoetry run pip install mypy\n', '# run mypy, tee output to file\npoetry run mypy --non-interactive --install-types --show-column-numbers --hide-error-context wechatpy | tee /tmp/mypy.out\nexit_code=""${PIPESTATUS[0]}""\n# analyze output\npoetry run python .github/workflows/github.py /tmp/mypy.out\nexit $exit_code\n', 'python -c ""import sys; print(sys.version)""', 'pip install poetry\npoetry install\npoetry build\nls -lrth dist/\n']"
"['python -m pip install --upgrade pip\npip install -U invoke pycodestyle flake8\n', 'invoke test --style;\n', 'python -m pip install --upgrade pip\npip install -U invoke sphinx tornado pscript>=0.5.6 webruntime dialite;\n', ""python -c 'import flexx.ui';\ninvoke docs --clean --build;\n"", 'python -m pip install --upgrade pip\npip install -U tornado pscript>=0.5.6 webruntime dialite;\npip install -U invoke pytest pytest-cov;\n']"
"['pip install -r requirements-mongo.txt', 'pip install -r requirements-elastic.txt', 'curl http://127.0.0.1:9200', 'echo \'DB_VIEW = ""elastic://ivre@localhost:9200/ivre""\' >> ~/.ivre.conf', 'for cli in ipinfo scancli view; do ivre $cli --init < /dev/null; done', 'tar jxf ./tests/mongodb_backup/backup_nmap_passive.tar.bz2', 'cd tests && python tests.py', 'cat /tmp/webserver.log', 'curl -L https://github.com/koalaman/shellcheck/releases/download/v0.9.0/shellcheck-v0.9.0.linux.x86_64.tar.xz | tar -Jxf - -C /usr/local/bin --strip-components 1 shellcheck-v0.9.0/shellcheck', 'pip install -r requirements-linting.txt', 'pip install .', 'mv ivre ivre_bak', 'black -t py36 --check ./doc/conf.py ./setup.py ./bin/ivre ./tests/tests.py ./ivre_bak/ ./pkg/stubs/', 'bandit --severity-level high -r ./doc/conf.py ./setup.py ./bin/ivre ./tests/tests.py ./ivre_bak/ ./pkg/stubs/', ""git ls-files | grep -vE '^web/static/(doc|an|bs|d3|jq|lk)/|^data/|\\.(png|gif|svg)$' | xargs -r codespell --ignore-words=pkg/codespell_ignore"", 'pylint -e all -d abstract-method,arguments-differ,attribute-defined-outside-init,broad-except,broad-exception-raised,duplicate-code,fixme,function-redefined,global-statement,import-error,invalid-name,locally-disabled,missing-docstring,no-member,protected-access,super-init-not-called,suppressed-message,too-few-public-methods,too-many-ancestors,too-many-arguments,too-many-boolean-expressions,too-many-branches,too-many-instance-attributes,too-many-lines,too-many-locals,too-many-nested-blocks,too-many-public-methods,too-many-return-statements,too-many-statements,unsubscriptable-object,unused-argument,line-too-long,consider-using-f-string ivre ./doc/conf.py ./setup.py ./bin/ivre', 'pylint -e all -d unused-argument,too-many-arguments,missing-function-docstring,missing-class-docstring,missing-module-docstring,multiple-statements,invalid-name,too-few-public-methods ./pkg/stubs/*.pyi', 'isort --profile black --check-only ivre ./doc/conf.py ./setup.py ./bin/ivre ./tests/tests.py ./pkg/stubs/*.pyi', 'mv ivre_bak ivre', 'MYPYPATH=./pkg/stubs/ mypy --follow-imports=skip --disallow-untyped-calls --disallow-untyped-decorators --disallow-untyped-defs --disallow-incomplete-defs --no-implicit-optional --warn-redundant-casts --warn-unused-ignores --warn-return-any ./ivre/{active,analyzer,data,parser,tags,tools,types}/*.py ./ivre/{__init__,activecli,agent,config,flow,geoiputils,graphroute,keys,nmapopt,utils,zgrabout}.py', 'MYPYPATH=./pkg/stubs/ mypy --follow-imports=skip --disallow-untyped-calls --disallow-untyped-decorators --disallow-incomplete-defs --no-implicit-optional --warn-redundant-casts --warn-unused-ignores --warn-return-any ./ivre/db/mongo.py', 'mv ivre ivre_bak', 'flake8 --ignore=E402,E501,F401 ./doc/conf.py', 'flake8 --ignore=E501,W503 ./setup.py ./bin/ivre', 'flake8 --ignore=E203,E402,E501,W503 ./tests/tests.py', 'flake8 --ignore=E203,E501,W503 ./ivre_bak/', 'flake8 --ignore=E302,E305,E701,E704 ./pkg/stubs/*.pyi', 'rstcheck -r ./doc/', 'if ! find ./doc/ -type f -name \'*.rst\' -print0 | xargs -0 grep \'[[:space:]]$\'; then echo ""trailing spaces OK""; else echo ""!! trailing spaces !!""; false; fi', 'sphinx-lint -e all -d line-too-long -d default-role ./doc/', 'shellcheck .github/actions/install/install*.sh pkg/builddockers pkg/builddocs pkg/buildrpm pkg/runchecks', 'FNAME=`mktemp`; ivre runscans --output Agent > ""$FNAME""; shellcheck ""$FNAME""; rm -f ""$FNAME""', 'pip install -r requirements-maxmind.txt', 'cd tests && python tests.py', 'cat /tmp/webserver.log', 'pip install -r requirements-mongo.txt', ""mongo --eval 'db.version()'"", 'for cli in ipinfo scancli view flowcli; do ivre $cli --init < /dev/null; done', 'cd tests && python tests.py', 'cat /tmp/webserver.log', 'pip install -r requirements-postgres.txt', 'psql ""postgresql://ivre:ivre@localhost/ivre"" -c ""SELECT version();""', 'echo \'DB = ""postgresql://ivre:ivre@localhost/ivre""\' >> ~/.ivre.conf', 'for cli in ipinfo scancli view; do ivre $cli --init < /dev/null; done', 'cd tests && python tests.py', 'cat /tmp/webserver.log', 'pip install -r requirements-sqlite.txt', 'echo \'DB = ""sqlite:////tmp/ivre.db""\' >> ~/.ivre.conf', 'ivre ipinfo --init < /dev/null', 'cd tests && python tests.py', 'cat /tmp/webserver.log', 'pip install -r requirements-tinydb.txt', 'pip install typing_extensions', 'echo \'DB = ""tinydb:////tmp/ivredb""\' >> ~/.ivre.conf', 'for cli in ipinfo scancli view flowcli; do ivre $cli --init < /dev/null; done', 'cd tests && python tests.py', 'cat /tmp/webserver.log']"
"['python -mpip install --upgrade setuptools pip tox virtualenv', 'tox -e ${{ matrix.toxenv }}']"
"['poetry config pypi-token.pypi ${{ secrets.PYPI_TOKEN }}\npoetry publish --build --no-interaction\n', 'poetry install --only main,test --all-extras', 'poetry run pip install ""Django==${{ matrix.django-version }}""\n', 'poetry run pip install ""djangorestframework==${{ matrix.drf-version }}""\n', ""poetry run pip freeze | grep -E 'djangorestframework|Django'"", 'make test', 'poetry install --only code-quality', 'make run-hooks']"
"['python -m pip install --upgrade pip setuptools wheel\npip install Cython\n', 'python setup.py build --export --exportheaders --noversioncheck', 'echo ::set-output name=branch::${GITHUB_REF#refs/heads/}']"
"['pip install wheel && pip install .[dev]', 'pytest --cov=mesa tests/ --cov-report=xml', 'pip install ruff==0.0.254', ""ruff . --format=github --extend-exclude 'mesa/cookiecutter-mesa/*'"", 'pip install black[jupyter]', 'black --check --exclude=mesa/cookiecutter-mesa/* .', 'pip install -U pip wheel setuptools', 'python setup.py sdist bdist_wheel']"
"['pip install -r requirements-dev-3.7.txt', 'pip install -r requirements-dev.txt', '# stop the build if there are Python syntax errors or undefined names\nflake8 deepdiff --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 deepdiff --count --exit-zero --max-complexity=26 --max-line-lengt=250 --statistics\n', 'pytest --cov-report=xml --cov=deepdiff tests/ --runslow\n', 'pytest\n']"
"['dnf install -y dnf-plugins-core langpacks-en\n', 'dnf copr -y enable @gift/dev\ndnf install -y @development-tools python3 python3-devel libbde-python3 libcaes-python3 libcreg-python3 libesedb-python3 libevt-python3 libevtx-python3 libewf-python3 libfsapfs-python3 libfsext-python3 libfsfat-python3 libfshfs-python3 libfsntfs-python3 libfsxfs-python3 libfvde-python3 libfwnt-python3 libfwsi-python3 liblnk-python3 libluksde-python3 libmodi-python3 libmsiecf-python3 libolecf-python3 libphdi-python3 libqcow-python3 libregf-python3 libscca-python3 libsigscan-python3 libsmdev-python3 libsmraw-python3 libvhdi-python3 libvmdk-python3 libvsgpt-python3 libvshadow-python3 libvslvm-python3 python3-XlsxWriter python3-acstore python3-artifacts python3-bencode python3-certifi python3-cffi python3-chardet python3-cryptography python3-dateutil python3-defusedxml python3-dfdatetime python3-dfvfs python3-dfwinreg python3-dtfabric python3-fakeredis python3-future python3-idna python3-lz4 python3-mock python3-opensearch python3-pefile python3-psutil python3-pyparsing python3-pytsk3 python3-pytz python3-pyxattr python3-pyyaml python3-redis python3-requests python3-setuptools python3-six python3-urllib3 python3-yara python3-zmq\n', 'python3 ./run_tests.py\n', 'if test -f tests/end-to-end.py; then PYTHONPATH=. python3 ./tests/end-to-end.py --debug -c config/end-to-end.ini; fi\n', 'python3 ./setup.py sdist\n', 'python3 ./setup.py bdist\n', 'python3 ./setup.py build\npython3 ./setup.py install\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential python3 python3-dev libbde-python3 libcaes-python3 libcreg-python3 libesedb-python3 libevt-python3 libevtx-python3 libewf-python3 libfsapfs-python3 libfsext-python3 libfsfat-python3 libfshfs-python3 libfsntfs-python3 libfsxfs-python3 libfvde-python3 libfwnt-python3 libfwsi-python3 liblnk-python3 libluksde-python3 libmodi-python3 libmsiecf-python3 libolecf-python3 libphdi-python3 libqcow-python3 libregf-python3 libscca-python3 libsigscan-python3 libsmdev-python3 libsmraw-python3 libvhdi-python3 libvmdk-python3 libvsgpt-python3 libvshadow-python3 libvslvm-python3 python3-acstore python3-artifacts python3-bencode python3-certifi python3-cffi-backend python3-chardet python3-cryptography python3-dateutil python3-defusedxml python3-dfdatetime python3-dfvfs python3-dfwinreg python3-distutils python3-dtfabric python3-fakeredis python3-flor python3-future python3-idna python3-lz4 python3-mock python3-opensearch python3-pefile python3-psutil python3-pyparsing python3-pytsk3 python3-pyxattr python3-redis python3-requests python3-setuptools python3-six python3-tz python3-urllib3 python3-xlsxwriter python3-yaml python3-yara python3-zmq\n', 'python3 ./run_tests.py\n', 'if test -f tests/end-to-end.py; then PYTHONPATH=. python3 ./tests/end-to-end.py --debug -c config/end-to-end.ini; fi\n', 'python3 ./setup.py sdist\n', 'python3 ./setup.py bdist\n', 'python3 ./setup.py build\npython3 ./setup.py install\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv libbde-python3 libcaes-python3 libcreg-python3 libesedb-python3 libevt-python3 libevtx-python3 libewf-python3 libfsapfs-python3 libfsext-python3 libfsfat-python3 libfshfs-python3 libfsntfs-python3 libfsxfs-python3 libfvde-python3 libfwnt-python3 libfwsi-python3 liblnk-python3 libluksde-python3 libmodi-python3 libmsiecf-python3 libolecf-python3 libphdi-python3 libqcow-python3 libregf-python3 libscca-python3 libsigscan-python3 libsmdev-python3 libsmraw-python3 libssl-dev libvhdi-python3 libvmdk-python3 libvsgpt-python3 libvshadow-python3 libvslvm-python3 python3-acstore python3-artifacts python3-bencode python3-certifi python3-cffi-backend python3-chardet python3-cryptography python3-dateutil python3-defusedxml python3-dfdatetime python3-dfvfs python3-dfwinreg python3-distutils python3-dtfabric python3-fakeredis python3-flor python3-future python3-idna python3-lz4 python3-mock python3-opensearch python3-pefile python3-pip python3-psutil python3-pyparsing python3-pytsk3 python3-pyxattr python3-redis python3-requests python3-setuptools python3-six python3-tz python3-urllib3 python3-xlsxwriter python3-yaml python3-yara python3-zmq\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv libbde-python3 libcaes-python3 libcreg-python3 libesedb-python3 libevt-python3 libevtx-python3 libewf-python3 libfsapfs-python3 libfsext-python3 libfsfat-python3 libfshfs-python3 libfsntfs-python3 libfsxfs-python3 libfvde-python3 libfwnt-python3 libfwsi-python3 liblnk-python3 libluksde-python3 libmodi-python3 libmsiecf-python3 libolecf-python3 libphdi-python3 libqcow-python3 libregf-python3 libscca-python3 libsigscan-python3 libsmdev-python3 libsmraw-python3 libssl-dev libvhdi-python3 libvmdk-python3 libvsgpt-python3 libvshadow-python3 libvslvm-python3 python3-acstore python3-artifacts python3-bencode python3-certifi python3-cffi-backend python3-chardet python3-cryptography python3-dateutil python3-defusedxml python3-dfdatetime python3-dfvfs python3-dfwinreg python3-distutils python3-dtfabric python3-fakeredis python3-flor python3-future python3-idna python3-lz4 python3-mock python3-opensearch python3-pefile python3-pip python3-psutil python3-pyparsing python3-pytsk3 python3-pyxattr python3-redis python3-requests python3-setuptools python3-six python3-tz python3-urllib3 python3-xlsxwriter python3-yaml python3-yara python3-zmq\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv libbde-python3 libcaes-python3 libcreg-python3 libesedb-python3 libevt-python3 libevtx-python3 libewf-python3 libfsapfs-python3 libfsext-python3 libfsfat-python3 libfshfs-python3 libfsntfs-python3 libfsxfs-python3 libfvde-python3 libfwnt-python3 libfwsi-python3 liblnk-python3 libluksde-python3 libmodi-python3 libmsiecf-python3 libolecf-python3 libphdi-python3 libqcow-python3 libregf-python3 libscca-python3 libsigscan-python3 libsmdev-python3 libsmraw-python3 libssl-dev libvhdi-python3 libvmdk-python3 libvsgpt-python3 libvshadow-python3 libvslvm-python3 python3-acstore python3-artifacts python3-bencode python3-certifi python3-cffi-backend python3-chardet python3-cryptography python3-dateutil python3-defusedxml python3-dfdatetime python3-dfvfs python3-dfwinreg python3-distutils python3-dtfabric python3-fakeredis python3-flor python3-future python3-idna python3-lz4 python3-mock python3-opensearch python3-pefile python3-pip python3-psutil python3-pyparsing python3-pytsk3 python3-pyxattr python3-redis python3-requests python3-setuptools python3-six python3-tz python3-urllib3 python3-xlsxwriter python3-yaml python3-yara python3-zmq\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv libbde-python3 libcaes-python3 libcreg-python3 libesedb-python3 libevt-python3 libevtx-python3 libewf-python3 libfsapfs-python3 libfsext-python3 libfsfat-python3 libfshfs-python3 libfsntfs-python3 libfsxfs-python3 libfvde-python3 libfwnt-python3 libfwsi-python3 liblnk-python3 libluksde-python3 libmodi-python3 libmsiecf-python3 libolecf-python3 libphdi-python3 libqcow-python3 libregf-python3 libscca-python3 libsigscan-python3 libsmdev-python3 libsmraw-python3 libssl-dev libvhdi-python3 libvmdk-python3 libvsgpt-python3 libvshadow-python3 libvslvm-python3 python3-acstore python3-artifacts python3-bencode python3-certifi python3-cffi-backend python3-chardet python3-cryptography python3-dateutil python3-defusedxml python3-dfdatetime python3-dfvfs python3-dfwinreg python3-distutils python3-dtfabric python3-fakeredis python3-flor python3-future python3-idna python3-lz4 python3-mock python3-opensearch python3-pefile python3-pip python3-psutil python3-pyparsing python3-pytsk3 python3-pyxattr python3-redis python3-requests python3-setuptools python3-six python3-tz python3-urllib3 python3-xlsxwriter python3-yaml python3-yara python3-zmq\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n']"
"['python -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n']"
"['sudo apt update && sudo apt install gettext unzip', 'pip3 install -U setuptools build pip check-manifest', 'check-manifest', 'python -m build', 'unzip -l dist/pretix*whl | grep node_modules || exit 1', 'sudo apt update && sudo apt install enchant-2 hunspell aspell-en', 'pip3 install -Ur requirements.txt', 'make spelling', '[ ! -s _build/spelling/output.txt ]', 'sudo apt update && sudo apt install gettext', 'pip3 install -e "".[dev]""', 'python manage.py compilemessages', 'python manage.py compilejsi18n', 'sudo apt update && sudo apt install enchant-2 hunspell hunspell-de-de aspell-en aspell-de', 'pip3 install -e "".[dev]""', 'potypo', 'pip3 install -e "".[dev]"" psycopg2-binary', 'isort -c .', 'pip3 install -e "".[dev]"" psycopg2-binary', 'flake8 .', 'pip3 install licenseheaders', 'licenseheaders -t ../.licenseheader -E .py -x ""*/migrations/*.py""', 'git diff --exit-code', 'sudo apt update && sudo apt install gettext', 'pip3 install --ignore-requires-python -e "".[dev]"" psycopg2-binary', 'python manage.py check', 'make npminstall', 'make all compress', 'PRETIX_CONFIG_FILE=tests/travis_${{ matrix.database }}.cfg py.test -n 3 -p no:sugar --cov=./ --cov-report=xml --reruns 3 tests --maxfail=100']"
"['pip install --upgrade pip\npip install -r docs/requirements.txt\n', 'make -C docs html', 'pip install --upgrade pip\npip install -r test-requirements.txt\n', 'flake8']"
"['python -m pip install -U pip setuptools wheel', 'pip install -r dev-deps/lint.txt', 'python -m pip install -U pip setuptools build', 'python -m build', 'python -m pip install -U pip setuptools build', 'python -m build', 'echo PYTEST_ADDOPTS=""${PYTEST_ADDOPTS} --hypothesis-profile=more-examples"" >> ""${GITHUB_ENV}""\n', 'python -m pip install -U pip setuptools wheel tox==4.2.0', 'echo PYTEST_ADDOPTS=""${PYTEST_ADDOPTS} --cov=bidict --cov-config=.coveragerc --cov-report=xml"" >> ""${GITHUB_ENV}""\n', 'tox -e py']"
"['if [ ""${{ matrix.group }}"" == ""docs"" ]; then\n    echo ""GROUP=docs"" >> $GITHUB_ENV\n    echo ""TIMEOUT_MINUTES=15"" >> $GITHUB_ENV\nfi\nif [ ""${{ matrix.group }}"" == ""nbextensions"" ]; then\n    echo ""GROUP=nbextensions"" >> $GITHUB_ENV\n    echo ""TIMEOUT_MINUTES=25"" >> $GITHUB_ENV\nfi\nif [ ""${{ matrix.group }}"" == ""python"" ]; then\n    echo ""GROUP=python"" >> $GITHUB_ENV\n    echo ""TIMEOUT_MINUTES=20"" >> $GITHUB_ENV\nfi\n', 'sudo apt-get install pandoc\n# Remove node\nsudo rm -rf $(which node)\nsudo rm -rf $(which node)\n', 'python -m pip install --upgrade pip wheel setuptools\npython tasks.py install --group=""$GROUP""\npip freeze\n', 'npx playwright install\n', 'python tasks.py tests --group=""$GROUP""\n', 'sudo rm -rf $(which node)\nsudo rm -rf $(which node)\n', 'echo ""GROUP=labextensions"" >> $GITHUB_ENV\n', 'python -m pip install --upgrade pip wheel setuptools\npython tasks.py install --group=labextensions\npip freeze\n', 'npx playwright install\n', 'python tasks.py tests --group=labextensions\n']"
"['echo ""::set-output name=dir::$(pip cache dir)""\n', 'sudo apt-get install -y libmemcached-dev zlib1g-dev\npython -m pip install --upgrade pip wheel\npython -m pip install tox tox-gh-actions coveralls\n', 'tox', 'coveralls --service=github', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'sudo apt-get install -y libmemcached-dev zlib1g-dev\npython -m pip install --upgrade pip wheel\npython -m pip install tox tox-gh-actions coveralls\n', 'tox', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['sudo apt-get install -y python3-enchant aspell-en hunspell-en-us', 'pip install -e .[docs]', 'sphinx-build -W -b linkcheck -d _build/doctrees . _build/linkcheck', 'sphinx-build -W -b spelling -d _build/doctrees . _build/spelling', 'sphinx-build -W -b html -d _build/doctrees . _build/html', 'pip install black', 'black --check .', 'pip install -e . isort', 'isort --check .', 'pip install Flask==${{ matrix.flask-version }}', 'pip install -e .[test]', 'pip install psycopg2', 'coverage run -m pytest']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'echo ""::set-output name=dir::$(pip cache dir)""\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['pip install setuptools~=57.5.0', 'pip install -r requirements/dev-requirements.txt', 'make test/flake8', 'make test/yamllint', 'pip3 install --upgrade docker-compose', 'make dev/build', 'make dev/up_detached', 'sleep 360', 'make dev/test']"
"['sudo apt-get install graphviz', 'python -c ""import sys; print(sys.version)""', 'pip install --upgrade setuptools pip wheel tox coveralls', 'python_env=$(echo $TOX_ENV | sed -e s/-dev$//)\npython -m tox -e ${python_env}\n', 'python -m tox -e $TOX_ENV', 'coveralls --service=github', 'pip3 install --upgrade coveralls\ncoveralls --finish\n', 'python -m pip install --upgrade setuptools pip wheel twine\npython -m pip install build --user\n', 'python -m build --sdist --wheel --outdir dist/ .\ntwine check --strict dist/*\n']"
"['python -m pip install --upgrade pip\npip install -e .[develop]\n', 'pytest -s -v']"
""
"['sudo apt-get update\npip install -U pip\npip install ""tox<4.0.0""\n', 'docker version\ndocker info\n', 'PV=${{ matrix.python-version }}\necho ""Running tests for Python version  $PV ( ${PV/./} )""\nmake test-py""${PV/./}""\n']"
"['python setup.py install\nconda list\nconda info --all\n', 'isort . --check-only\nblack . --line-length 100 --check --diff\nflake8 .\npydocstyle .\n', 'make -C ./docs html', 'coverage run --source ./pynamical --module pytest --verbose\ncoverage xml -i\ncoverage report -m\n']"
"['rm -rf AppDir\nmkdir -p AppDir/usr/src\ncp pyfda  AppDir/usr/src -r\nsudo apt-get --quiet install --yes python3 python3-pip python3-setuptools python3-wheel\npython3 -m pip install --ignore-installed --prefix=/usr --root=AppDir -r ./requirements.txt\n', 'ls', 'sed -i \'s@\\&git_tag .*@\\&git_tag ""${{ github.sha }}""@\' com.github.chipmuenk.pyfda/com.github.chipmuenk.pyfda.yaml\n', 'sed -i \'s@\\&git_tag .*@\\&git_tag ""${{ github.ref_name }}""@\' com.github.chipmuenk.pyfda/com.github.chipmuenk.pyfda.yaml\n', 'sed -i ""s@\\&git_url .*pyfda.git@\\&git_url $GITHUB_SERVER_URL/$GITHUB_REPOSITORY@"" com.github.chipmuenk.pyfda/com.github.chipmuenk.pyfda.yaml\n', 'sed -i \'s@config-opts: \\[""\\(.*\\)""\\]@config-opts: [""\\1"", ""-DDEV_BUILD=${{ github.ref_name }}""]@\' com.github.chipmuenk.pyfda/com.github.chipmuenk.pyfda.yaml\n', 'sed -i \'s@desktop-file-name-suffix: """"@desktop-file-name-suffix: "" (Development)""@\' com.github.chipmuenk.pyfda/com.github.chipmuenk.pyfda.yaml\n', 'cat com.github.chipmuenk.pyfda/com.github.chipmuenk.pyfda.yaml', 'sudo apt-get update \nsudo apt install flatpak\nsudo flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\nsudo flatpak install -y org.freedesktop.appstream-glib\nflatpak run --env=G_DEBUG=fatal-criticals org.freedesktop.appstream-glib validate ./ressource/linux/pyfda.appdata.xml\n', 'echo ""Show artifacts folder:""\nls artifacts\necho ""Show artifacts/pyfda Flatpak folder:""\nls ""artifacts/pyfda Flatpak""\necho ""Show artifacts/com.github.chipmuenk.pyfda-x86_64:""\nls ""artifacts/com.github.chipmuenk.pyfda-x86_64""\n', 'echo ""${{ steps.create_release.outputs.upload_url }}"" > release_url.txt', 'python -m pip install --upgrade pip\npip install -r requirements.txt\npip install pyinstaller pyinstaller-hooks-contrib\n', '${{matrix.CMD_BUILD}}', 'value=`cat release_url/release_url.txt`\necho ::set-output name=upload_url::$value\n', 'python3 -m pip install --upgrade build\npython3 -m build\n', 'python3 -m pip install --upgrade pip\n# Install build deps\npython3 -m pip install setuptools wheel twine\n# If requirements.txt exists, install dependencies from it\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n# Install the package (pyfda) from setup.py\n## python3 setup.py install\n', 'python3 setup.py sdist bdist_wheel', 'python3 -m twine upload dist/*']"
"['python -m pip install --upgrade pip\npython -m pip install flake8\npython -m pip install .\n', 'python setup.py test', 'python -m pip install --upgrade pip\npython -m pip install .\npython -m pip install mypy\npython -m pip install types-atomicwrites\n', 'mypy --ignore-missing-imports khard', 'python -m pip install --upgrade pip\npython -m pip install .[doc]\n', 'python setup.py build\nmake -C doc html man\n']"
"['python -m pip install -e plugins/hanlp_trie\npython -m pip install -e plugins/hanlp_common\npython -m pip install -e .\npython -m pip install pytest\n', 'pytest tests\npytest plugins/hanlp_trie/tests\n', 'python -m pip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\npython -m twine upload dist/*\n']"
"['sudo apt install pgbouncer -y\n\nsudo chmod 666 /etc/pgbouncer/*.*\n\ncat <<EOF > /etc/pgbouncer/userlist.txt\n""postgres"" ""postgres""\nEOF\n\ncat <<EOF > /etc/pgbouncer/pgbouncer.ini\n[databases]\n* = host=localhost port=5432\n[pgbouncer]\nlisten_port = 6432\nlisten_addr = localhost\nauth_type = trust\nauth_file = /etc/pgbouncer/userlist.txt\nlogfile = pgbouncer.log\npidfile = pgbouncer.pid\nadmin_users = postgres\nEOF\n\nsudo systemctl stop pgbouncer\n\npgbouncer -d /etc/pgbouncer/pgbouncer.ini\n\npsql -h localhost -U postgres -p 6432 pgbouncer -c \'show help\'\n', 'pip install -U pip setuptools\npip install --no-cache-dir "".[sshtunnel]""\npip install -r requirements-dev.txt\npip install keyrings.alt>=3.1\n', 'coverage run --source pgcli -m pytest', 'behave tests/features --no-capture', 'rst2html.py --halt=warning changelog.rst >/dev/null', 'black --check .', 'coverage combine\ncoverage report\ncodecov\n']"
"['echo ""PRs must be made against the develop branch.""\nexit 1\n', 'python scripts/ci/install\n', 'python scripts/ci/run-tests\n']"
"['pip install requests requests[socks] yt-dlp pyyaml ${{ matrix.python-packages }} pyinstaller\npython scripts/pyinstaller.py\n', 'pip install -r requirements.txt\npip install ""flake8<4"" ""importlib-metadata<5""\npip install youtube-dl\n', 'case ""${{ matrix.python-version }}"" in\n    3.4|3.5)\n        # don\'t install yt-dlp\n        ;;\n    3.6)\n        # install from PyPI\n        pip install yt-dlp\n        ;;\n    *)\n        # install from master\n        pip install https://github.com/yt-dlp/yt-dlp/archive/refs/heads/master.tar.gz\n        ;;\nesac\n', 'flake8 .\n', 'make test\n', 'make\n']"
"['sudo apt-get install libgraphviz-dev graphviz\npython -m pip install --upgrade pip\npython -m pip install -r requirements.txt -r requirements_diagrams.txt \npython -m pip install -U -r requirements_test.txt\npython -c ""print \'hello\'"" > /dev/null 2>&1 || pip install -r requirements_mypy.txt\n', 'coverage run --source=transitions -m pytest --doctest-modules tests/\ncoverage xml --ignore-errors\n']"
"['make install', 'make test-all', 'make build']"
"['pip install black -c requirements.txt', 'black thinc', 'echo modified=$(if git diff-index --quiet HEAD --; then echo ""false""; else echo ""true""; fi) >> $GITHUB_OUTPUT', 'echo ""Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}""\necho ""Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}""\n', 'echo ""$GITHUB_CONTEXT""', 'pip install git+https://${{ secrets.EXPLOSIONBOT_TOKEN }}@github.com/explosion/explosion-bot\npython -m explosionbot\n', 'python -m pip install --upgrade pip setuptools wheel\npip install -r requirements.txt\n', 'python setup.py build_ext --inplace\npython setup.py sdist --formats=gztar\n', 'python -m mypy thinc --no-implicit-reexport', 'rm -rf thinc', 'python -m pip freeze\npip freeze --exclude pywin32 > installed.txt\npip uninstall -y -r installed.txt\n', 'SDIST=$(python -c ""import os;print(os.listdir(\'./dist\')[-1])"" 2>&1)\nPIP_CONSTRAINT=""build-constraints.txt"" pip install dist/$SDIST\n', 'python -c ""import thinc""', 'pip install -r requirements.txt\npip install ipykernel pydot graphviz\npython -m ipykernel install --name thinc-notebook-tests --user\npython -m pytest --pyargs thinc -Werror --cov=thinc --cov-report=term\n', 'pip install ""protobuf~=3.20.0"" ""tensorflow~=2.5.0""\npip install ""mxnet; sys_platform != \'win32\'""\npip install ""torch!=1.13.0"" --extra-index-url https://download.pytorch.org/whl/cpu\npip install ""numpy~=1.23.0; python_version==\'3.10\' and sys_platform==\'win32\'""\npip install ""numpy<1.24.0""\npip install -r requirements.txt\npip uninstall -y mypy\n', 'python -m pytest --pyargs thinc --cov=thinc --cov-report=term', 'pip uninstall -y tensorflow\npip install thinc-apple-ops\npython -m pytest --pyargs thinc_apple_ops\n', 'python -m pytest --pyargs thinc']"
"['pip install -e .\n', 'pip install --upgrade build\npython -m build\n']"
"['python -m pip install --upgrade pip\npython -m pip install build\npython -m build --wheel\npython -m pip install -r requirements-dev.txt\npython -m pip install dist/pytrends-*.whl\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'pytest --block-network --cov-report term\n']"
""
"['git fetch --unshallow\ngit remote add upstream https://git.launchpad.net/cloud-init\n', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -qy update\nsudo DEBIAN_FRONTEND=noninteractive apt-get -qy install tox\n', 'python3 --version', 'tox', 'tox', 'tools/check_json_format.sh cloudinit/config/schemas/schema-cloud-config-v1.json\ntools/check_json_format.sh cloudinit/config/schemas/versions.schema.cloud-config.json\n', 'git fetch --unshallow\ngit remote add upstream https://git.launchpad.net/cloud-init\n', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -qy update\nsudo DEBIAN_FRONTEND=noninteractive apt-get -qy install tox lintian\n', 'make check_spelling\n', 'tox\n', 'tools/check-cla-signers', 'cat > unsigned-cla.txt <<EOF\n  Hello ${{ github.event.pull_request.user.login }},\n\n  Thank you for your contribution to cloud-init.\n\n  In order for us to merge this pull request, you need\n  to have signed the Contributor License Agreement (CLA).\n  Please sign the CLA by following our\n  contribution guide at:\n    https://cloudinit.readthedocs.io/en/latest/topics/contributing.html\n\n  Thanks,\n  Your friendly cloud-init upstream\nEOF\n\nhas_signed() {\n    username=""$1""\n    grep -q "": \\""$username\\"""" ./tools/.lp-to-git-user && return 0\n    grep -q ""^$username$"" ./tools/.github-cla-signers && return 0\n    return 1\n}\n\nif has_signed ""${{ github.event.pull_request.user.login }}""; then\n    echo ""Thanks ${{ github.event.pull_request.user.login }} for signing cloud-init\'s CLA""\nelse\n   cat unsigned-cla.txt\n   exit 1\nfi\n', 'sudo DEBIAN_FRONTEND=noninteractive apt-get update\nsudo DEBIAN_FRONTEND=noninteractive apt-get -y install \\\n  debhelper \\\n  dh-python \\\n  fakeroot \\\n  python3-setuptools \\\n  sbuild \\\n  ubuntu-dev-tools\nsudo sbuild-adduser $USER\ncp /usr/share/doc/sbuild/examples/example.sbuildrc /home/$USER/.sbuildrc\n# Install all build and test dependencies\n./tools/read-dependencies -r requirements.txt -r test-requirements.txt -d ubuntu -s -i\n', ""./packages/bddeb -S -d --release ${{ env.RELEASE }}\nsudo -E su $USER -c 'mk-sbuild ${{ env.RELEASE }}'\nsudo -E su $USER -c 'DEB_BUILD_OPTIONS=nocheck sbuild --nolog --no-run-lintian --no-run-autopkgtest --verbose --dist=${{ env.RELEASE }} --build-dir=${{ runner.temp }} cloud-init_*.dsc'\n"", 'ls -hal cloud-init*.deb\n', 'sudo DEBIAN_FRONTEND=noninteractive apt-get -y update\nsudo DEBIAN_FRONTEND=noninteractive apt-get -y install tox wireguard\n', 'ssh-keygen -P """" -q -f ~/.ssh/id_rsa\necho ""[lxd]"" > /home/$USER/.config/pycloudlib.toml\nsudo adduser $USER lxd\n# Jammy GH Action runners have docker installed, which edits iptables\n# in a way that is incompatible with lxd.\n# https://linuxcontainers.org/lxd/docs/master/howto/network_bridge_firewalld/#prevent-issues-with-lxd-and-docker\nsudo iptables -I DOCKER-USER -j ACCEPT\nsudo lxd init --auto\n', 'sg lxd -c \'CLOUD_INIT_CLOUD_INIT_SOURCE=""$(ls cloud-init*.deb)"" tox -e integration-tests-ci\'\n', 'sudo apt-get -qy update\nsudo apt-get -qy install tox\n', 'tox -e ${{ matrix.toxenv }}', 'pip install tox', 'tox -e ${{ matrix.toxenv }}']"
"['python -mpip install --progress-bar=off -r ci/requirements.txt\nvirtualenv --version\npip --version\ntox --version\npip list --format=freeze\n', 'tox -e ${{ matrix.tox_env }} -v\n']"
"['python -m pip install --upgrade pip wheel', 'pip install tox', 'tox -elint', 'python -m pip install --upgrade pip wheel', 'pip install tox', 'tox -e${{ matrix.tox }}', 'python -m pip install build twine', 'python -m build', 'twine check dist/*', 'twine upload -u __token__ -p ${{ secrets.PYPI_API_TOKEN }} dist/*']"
['pip install -r tests/requirements.txt\npython -m unittest discover tests\n']
"['poetry build\npoetry publish\n', 'poetry install -v\n', 'poetry run pytest -v tests\n', 'poetry install -v\n', 'poetry run pytest -v tests\n', 'poetry install -v\n', 'poetry run pytest -v tests\n']"
"['dnf install -y dnf-plugins-core langpacks-en\n', 'dnf copr -y enable @gift/dev\ndnf install -y @development-tools python3 python3-devel python3-pyyaml python3-setuptools\n', 'python3 ./run_tests.py\n', 'if test -f tests/end-to-end.py; then PYTHONPATH=. python3 ./tests/end-to-end.py --debug -c config/end-to-end.ini; fi\n', 'python3 ./setup.py sdist\n', 'python3 ./setup.py bdist\n', 'python3 ./setup.py build\npython3 ./setup.py install\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential python3 python3-dev python3-distutils python3-setuptools python3-yaml\n', 'python3 ./run_tests.py\n', 'if test -f tests/end-to-end.py; then PYTHONPATH=. python3 ./tests/end-to-end.py --debug -c config/end-to-end.ini; fi\n', 'python3 ./setup.py sdist\n', 'python3 ./setup.py bdist\n', 'python3 ./setup.py build\npython3 ./setup.py install\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv python3-distutils python3-pip python3-setuptools python3-yaml\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv python3-distutils python3-pip python3-setuptools python3-yaml\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv python3-distutils python3-pip python3-setuptools python3-yaml\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n', 'apt-get update -q\napt-get install -y libterm-readline-gnu-perl locales software-properties-common\nlocale-gen en_US.UTF-8\nln -f -s /usr/share/zoneinfo/UTC /etc/localtime\n', 'add-apt-repository -y universe\nadd-apt-repository -y ppa:deadsnakes/ppa\nadd-apt-repository -y ppa:gift/dev\napt-get update -q\napt-get install -y build-essential git libffi-dev python${{ matrix.python-version }} python${{ matrix.python-version }}-dev python${{ matrix.python-version }}-venv python3-distutils python3-pip python3-setuptools python3-yaml\n', 'python3 -m pip install tox\n', 'tox -e${{ matrix.toxenv }}\n']"
"['python -m pip install --upgrade pip\npip install .\npip install .[columns]\npip install .[ical]\npip install .[prompt]\npip install .[test]\npip install pylint\npip install codecov\npip install -U isort\npip install pytest\n', 'python -m pytest\n']"
"['python -m pip install -U pip\npython -m pip install -U setuptools twine wheel\n', 'python setup.py --version\npython setup.py sdist --format=gztar bdist_wheel\ntwine check dist/*\n', 'python -m pip install --upgrade pip\npython -m pip install --upgrade tox tox-gh-actions\n', 'tox -v\n']"
"['pip install --upgrade pip\npip install .\n', 'if [[ $SKIP_DOCS != 1 ]]; then\n  make doc_reqs\n  make html\nfi\n', 'pip install --upgrade pip\npip install wheel numpy requests six python-dateutil pytest pytest-cov mock coverage\nmake install\npip install cython==0.29.23\npython setup.py build_ext -i\npip install -e .\n\nwhich python; python --version\n', 'pip install pre-commit\npre-commit run --from-ref ${{ github.event.pull_request.base.sha }} --to-ref ${{ github.event.pull_request.head.sha }}\n', ""if [[ -n $COVERAGE ]]; then\n  PYTEST_FLAGS='--cov=./'\nfi\nPYTHONPATH=. python -m pytest -v $PYTEST_FLAGS\n"", 'python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n', 'curl -o /tmp/Python38.pkg https://www.python.org/ftp/python/3.8.10/python-3.8.10-macos11.pkg\nsudo installer -pkg /tmp/Python38.pkg -target /\nsh ""/Applications/Python 3.8/Install Certificates.command""\n', 'python -m pip install cibuildwheel\n', 'if [[ ""$CIBW_ARCHS_MACOS"" == arm64 ]]; then\n    # SciPy requires 12.0 on arm to prevent kernel panics\n    # https://github.com/scipy/scipy/issues/14688\n    # so being conservative, we just do the same here\n    export MACOSX_DEPLOYMENT_TARGET=12.0\nelse\n    export MACOSX_DEPLOYMENT_TARGET=10.9\nfi\n\necho MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET}\n\npython -m cibuildwheel --output-dir dist\n', 'python -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir dist\n', 'python -m pip install --upgrade pip\npip install numpy Cython twine setuptools setuptools_scm\n', 'VERSION=$(git describe --tags)\npython setup.py sdist\nls -la ${{ github.workspace }}/dist\n# We prefer to release wheels before source because otherwise there is a\n# small window during which users who pip install cesium will require compilation.\ntwine upload ${{ github.workspace }}/dist/*.whl\ntwine upload ${{ github.workspace }}/dist/cesium-${VERSION:1}.tar.gz\n']"
"[""python -m pip install --upgrade pip\npip install black 'isort[colors]<6'\n"", 'python -m black --check  --diff --color .\n', 'python -m isort --check --diff --color .\n', 'python -m pip install --upgrade pip\npip install build coveralls>=3.0.0\npip install -v --editable .[test]\n', 'python -m pytest --cov pykrige --cov-report term-missing -v tests/\npython -m coveralls --service=github\n', '# PEP 517 package builder from pypa\npython -m build --sdist --outdir dist .\n']"
"['jq --null-input \\\n        --arg number ""$NUMBER"" \\\n        --arg pr_url ""$PR_URL"" \\\n        --arg repo ""$REPO"" \\\n        --arg user ""$USER"" \\\n        --arg title ""$TITLE"" \\\n\'{ ""text"": ""[\\($repo)] | [\\($title) #\\($number)](\\($pr_url)) was merged into master by \\($user)"" }\' > mattermost.json\n', 'DATE=$(date  --date=""7 days ago"" +""%Y-%m-%d"")\nMERGED_URL=""https://github.com/pulls?q=merged%3A%3E${DATE}+org%3Acertbot""\nUPDATED_URL=""https://github.com/pulls?q=updated%3A%3E${DATE}+org%3Acertbot""\necho ""{\\""text\\"":\\""## Updates Across Certbot Repos\\n\\n\n- Certbot team members SHOULD look at: [link]($MERGED_URL)\\n\\n\n- Certbot team members MAY also want to look at: [link]($UPDATED_URL)\\n\\n\n- Want to Discuss something today? Place it [here](https://docs.google.com/document/d/17YMUbtC1yg6MfiTMwT8zVm9LmO-cuGVBom0qFn8XJBM/edit?usp=sharing) and we can meet today on Zoom.\\n\\n \n- The key words SHOULD and MAY in this message are to be interpreted as described in [RFC 8147](https://www.rfc-editor.org/rfc/rfc8174). \\""\n}"" > mattermost.json\n']"
"['python -m pip install --upgrade pip\nif [ -f test/requirements.txt ]; then pip install -r test/requirements.txt; fi\n', 'make --keep-going THEMIS_VIM=${{ steps.vim.outputs.executable }} test lint', 'python -m pip install --upgrade pip\nif [ -f test/requirements.txt ]; then pip install -r test/requirements.txt; fi\n', 'pytest --cov=./rplugin/python3/deoplete --cov=./test --cov-report=xml']"
"['pip install -U pip wheel\npip install -e "".[testing]""\npip install -e "".[optional]""\n', 'python setup.py validate\n', '# Install v1.4 for testing\npip install ""SQLAlchemy>=1.4,<2""\npython setup.py unit_tests --test-target tests/slack_sdk/oauth/installation_store/test_sqlalchemy.py && \\\npython setup.py unit_tests --test-target tests/slack_sdk/oauth/state_store/test_sqlalchemy.py\n', 'pip install -U pip\npip install -e "".[testing]""\npip install -e "".[optional]""\n# As pytype can change its behavior in newer versions, we manually upgrade it\npip install ""pytype==2023.5.24""\n', 'pytype slack_sdk/\n']"
[]
"['poetry install', 'poetry run pylama .\n', 'poetry run black --check .\n', 'poetry run mypy --version\npoetry run mypy ./netmiko/\n', 'poetry install', 'poetry run pytest -v -s tests/test_import_netmiko.py\npoetry run pytest -v -s tests/unit/test_base_connection.py\npoetry run pytest -v -s tests/unit/test_utilities.py\npoetry run pytest -v -s tests/unit/test_ssh_autodetect.py\npoetry run pytest -v -s tests/unit/test_connection.py\npoetry run pytest -v -s tests/unit/test_entry_points.py\n']"
"['docker build . --file Dockerfile --tag mapillary/opensfm:$GITHUB_SHA', 'docker run mapillary/opensfm:$GITHUB_SHA /bin/sh -c ""cd cmake_build && ctest""', 'docker run mapillary/opensfm:$GITHUB_SHA python3 -m pytest', 'docker build . --file Dockerfile.ceres2 --tag mapillary/opensfm.ceres2:$GITHUB_SHA', 'docker run mapillary/opensfm.ceres2:$GITHUB_SHA /bin/sh -c ""cd cmake_build && ctest""', 'docker run mapillary/opensfm.ceres2:$GITHUB_SHA python3 -m pytest']"
"['sudo apt-get update && sudo apt-get install language-pack-fr', 'pip install tox', 'tox -e py', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n']"
"['python -m pip install --upgrade pip\npip install -U tox\n', 'tox -p -e py,build,release --notest\n', 'tox -e py\n', 'tox -e build\n', 'if [[ ""$GITHUB_EVENT_NAME"" == ""push"" || \\\n      ""$GITHUB_EVENT_NAME"" == ""workflow_dispatch"" ]]; then\n  export TWINE_REPOSITORY_URL=""https://test.pypi.org/legacy/""\n  export TWINE_PASSWORD=""${{ secrets.TEST_PYPI_UPLOAD_TOKEN }}""\nelif [[ ""$GITHUB_EVENT_NAME"" == ""release"" ]]; then\n  export TWINE_REPOSITORY=""pypi""\n  export TWINE_PASSWORD=""${{ secrets.PYPI_UPLOAD_TOKEN }}""\nelse\n  echo ""Unknown event name: ${GITHUB_EVENT_NAME}""\n  exit 1\nfi\ntox -e release\n', 'python -m pip install -U tox six', 'curl https://get.enterprisedb.com/postgresql/postgresql-9.5.21-2-windows-x64-binaries.zip --output $env:GITHUB_WORKSPACE\\postgresql9.5.21.zip\nunzip -oq $env:GITHUB_WORKSPACE\\postgresql9.5.21.zip -d .postgresql\n', '$env:Path += "";$env:GITHUB_WORKSPACE\\.postgresql\\pgsql\\bin""\nci_tools/retry.bat python updatezinfo.py\n', './ci_tools/retry.sh python updatezinfo.py', 'python -m tox', 'python -m tox -e coverage', 'python -m pip install --upgrade pip\npython -m pip install -U tox\n', 'tox', 'python -m pip install -U tox', 'python -m tox -e build', 'exactly_one() {\n  value=$(find dist -iname $1 | wc -l)\n  if [ $value -ne 1 ]; then\n    echo ""Found $value instances of $1, not 1""\n    return 1\n  else\n    echo ""Found exactly 1 instance of $value""\n  fi\n}\n# Check that exactly one tarball and one wheel are created\nexactly_one \'*.tar.gz\'\nexactly_one \'*.whl\'\n']"
""
"['pip install --user ruff', 'ruff --ignore=E401,E402,E701,E722,E731,E741,F401,F403,F405,F523,F524,F811,F841 --format=github --line-length=2793 --show-source --target-version=py37 .', 'python -m pip install --upgrade pip wheel setuptools\npip install "".[testing]""\n', 'flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics\n', 'pytest --version\n# make install test work...\nexport PYTHONPATH=$(python -m site --user-site) \npytest tests/ --ignore=tests/system/data/ --showlocals --verbose --show-capture=all --log-level=debug\n']"
"['python -m pip install --upgrade pip setuptools\npython -m pip install -r requirements-cython.txt\n# Make sure we install to have all c files to be shiped with bundle\npython -m pip install -vv -U .  # We set -vv to see compiler exceptions/warnings\n', 'python setup.py sdist', 'python -m pip install --upgrade pip setuptools\npip install cibuildwheel\ncibuildwheel --output-dir dist\n', 'python -m pip install --upgrade pip setuptools wheel\npython -m pip install -r requirements-win-test.txt\npython -m pip install ${{ matrix.aiokafka_whl }}\n', '# Remove source code to be sure we use wheel code\nrm -rf aiokafka\nmake ci-test-unit\n', 'brew install snappy\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements-ci.txt\npip install ${{ matrix.aiokafka_whl }}\n', '# Remove source code to be sure we use wheel code\nrm -rf aiokafka\nmake ci-test-unit\n', 'sudo apt-get update\nsudo apt-get install -y libsnappy-dev libzstd-dev libkrb5-dev\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements-ci.txt\npip install ${{ matrix.aiokafka_whl }}\n', '# Remove source code to be sure we use wheel code\nrm -rf aiokafka\nmake ci-test-unit\n', ""docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \\\n${{ env.img }} \\\nbash -exc '${{ env.py }} -m venv .env && \\\nsource .env/bin/activate && \\\nyum install -y epel-release && \\\nyum-config-manager --enable epel && \\\nyum install -y snappy-devel libzstd-devel krb5-devel && \\\npip install --upgrade pip setuptools wheel && \\\npip install -r requirements-ci.txt && \\\npip install ${{ matrix.aiokafka_whl }} && \\\nrm -rf aiokafka && \\\nmake ci-test-unit && \\\ndeactivate'\n"", 'sudo apt-get update\nsudo apt-get install -y libsnappy-dev libzstd-dev libkrb5-dev\n', 'python -m pip install -U ""pip>=20.1""\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements-ci.txt\npip install -vv -Ue .  # We set -vv to see compiler exceptions/warnings\n', 'make ci-test-unit\n', 'make flake\n', 'make check-readme\n', 'pip install -r requirements-docs.txt\nmake -C docs html\n', 'python -m pip install -U ""pip>=20.1""\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements-win-test.txt\npip install -vv -Ue .  # We set -vv to see compiler exceptions/warnings\n', 'make ci-test-unit\nmv coverage.xml coverage-ext.xml\n', 'make ci-test-unit\nmv coverage.xml coverage-py.xml\n', 'python -m pip install -U ""pip>=20.1""\necho ""::set-output name=dir::$(pip cache dir)""\n', 'brew install snappy\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements-ci.txt\npip install -vv -Ue .  # We set -vv to see compiler exceptions/warnings\n', 'make ci-test-unit\nmv coverage.xml coverage-ext.xml\n', 'make ci-test-unit\nmv coverage.xml coverage-py.xml\n', 'sudo apt-get update\nsudo apt-get install -y libsnappy-dev libzstd-dev libkrb5-dev krb5-user\n', 'python -m pip install -U ""pip>=20.1""\necho ""::set-output name=dir::$(pip cache dir)""\n', 'pip install --upgrade pip setuptools wheel\npip install -r requirements-ci.txt\npip install -vv -Ue .  # We set -vv to see compiler exceptions/warnings\n', 'make ci-test-all\nmv coverage.xml coverage-ext.xml\n', 'make ci-test-all\nmv coverage.xml coverage-py.xml\n']"
"['python dev.py -no-container\npip install -q Django==${{ matrix.django-version }}\npython -m pip install --upgrade pip\npython setup.py install\n', 'python runtests.py\n', 'python -m pip install --upgrade pip\npip install setuptools wheel twine\n', 'python setup.py sdist bdist_wheel\ntwine upload dist/*\n', 'python dev.py -no-container --with-docs\nmkdocs build\n', 'git clone https://github.com/neutronX/django-markdownx.git --branch gh-pages --single-branch gh-pages\nrm -rf gh-pages/docs\nmkdir gh-pages/docs\ncp -r docs/* gh-pages/docs/\ncd gh-pages\nVERSION=$(python setup.py -V)\necho ${VERSION} >> version\n> .nojekyll\ngit config --local user.email ""action@github.com""\ngit config --local user.name ""GitHub Action""\ngit add .\ngit commit -m ""Update documentations"" -a || true\necho ""Documentations for v${VERSION} has been deployed.""\n# The above command will fail if no changes were present, so we ignore\n# the return code.\n', 'python dev.py -no-container\npip install -q Django==${{ matrix.django-version }}\npython -m pip install --upgrade pip\npython setup.py install\n', 'python runtests.py\n']"
"['python -m pip install --upgrade pip build tox coverage codecov\n', 'python -m tox\n', 'python -m pip install --upgrade pip build tox\n', 'python -m tox\n', 'python -m pip install --upgrade pip build tox\nnpm install --legacy-peer-deps\n', 'sudo apt-get install aspell aspell-en\n', 'npm run build\npython -m tox\n', 'python -m pip install --upgrade pip build\npython -m pip install -r requirements/docs.txt\n', 'git config user.name ${{ secrets.GH_USER }}\ngit config user.email ""${{ secrets.GH_EMAIL }}""\ngit remote add gh-token ""https://${{ secrets.GH_TOKEN }}@github.com/facelessuser/pymdown-extensions.git""\ngit fetch gh-token && git fetch gh-token gh-pages:gh-pages\npython -m mkdocs gh-deploy -v --clean --remote-name gh-token\ngit push gh-token gh-pages\n', 'pip install --upgrade build\npython -m build -s -w\n']"
"['python -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'python tests/script/test_install.py\npython tests/script/test_docker.py\npython tests/script/test_features.py\n', 'python -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'python tests/script/test_deps.py\n', 'python -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'python tests/tutorials/test_tutorial_retinanet.py\n', 'python -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'python tests/tutorials/test_tutorial_tvm_pip.py\n', 'python -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'python tests/tutorials/test_tutorial_tvm.py\n', 'python -m pip install --upgrade pip\npython -m pip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\npython -m pip install --ignore-installed --verbose pip setuptools\ncd cm\npython setup.py install\npython -m cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\n', '# stop the build if there are Python syntax errors or undefined names\nflake8 cm/cmind --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 cm/cmind --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n', 'python tests/test_cm.py\n', 'python -m pip install --upgrade pip\npython -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\n', 'python tests/test_cm.py\n', 'python3 -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short --submitter=""cTuning"" --model=bert-99 --backend=${{ matrix.backend }} --device=cpu --scenario=Offline --test_query_count=5 --precision=${{ matrix.precision }} --quiet --env.CM_INDEX=on\n', 'python3 -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short --submitter=""cTuning"" --hw_name=default --model=resnet50 --implementation=${{ matrix.implementation }} --backend=${{ matrix.backend }} --device=cpu --scenario=Offline --test_query_count=500 --quiet \n', 'python3 -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short --submitter=""cTuning"" --hw_name=default --model=retinanet --implementation=${{ matrix.implementation }} --backend=${{ matrix.backend }} --device=cpu --scenario=Offline --test_query_count=5 --adr.compiler.tags=gcc --quiet  --env.CM_INDEX=on\n', 'python3 -m pip install cmind\ncm pull repo --url=${{ github.event.pull_request.head.repo.html_url }} --checkout=${{ github.event.pull_request.head.ref }}\ncm run script --quiet --tags=get,sys-utils-cm\n', 'cm run script --tags=run,mlperf,inference,generate-run-cmds --hw_name=default --model=resnet50 --backend=${{ matrix.backend }} --device=cpu --scenario=Offline --test_query_count=5 --quiet\n', 'echo ""files=$(git diff --name-only ${{ github.event.before }} | xargs)"" >> $GITHUB_OUTPUT\n', 'for file in ${{ steps.getfile.outputs.files }}; do\n  echo $file\ndone\npython3 -m pip install cmind\ncm pull repo --url=https://github.com/${{ github.repository }} --checkout=${{ github.ref_name }}\npython3 tests/script/process_dockerfile.py ${{ steps.getfile.outputs.files }}\n\nFOLDER=`cm find repo mlcommons@ck | cut -d\' \' -f3`\n\nUSER=cTuning\nEMAIL=admin@ctuning.org\n\ngit config --global user.name ""$USER""\ngit config --global user.email ""$EMAIL""\ngit remote set-url origin https://x-access-token:${{ secrets.ACCESS_TOKEN }}@github.com/${{ github.repository }}\ngit add *.Dockerfile\ngit diff-index --quiet HEAD || (git commit -am ""Updated dockerfile"" && git push)\n', 'echo ""files=$(git diff --name-only ${{ github.event.before }} | xargs)"" >> $GITHUB_OUTPUT\n', 'echo ${{ steps.getfile.outputs.files }}\nfor file in ${{ steps.getfile.outputs.files }}; do\n  echo $file\ndone\npython3 -m pip install cmind\ncm pull repo --url=https://github.com/${{ github.repository }} --checkout=${{ github.ref_name }}\npython3 tests/script/process_readme.py ${{ steps.getfile.outputs.files }}\n#REPO=${{ github.repository }}\n#CM_REPO=${REPO/\\//@}\n#FOLDER=`cm find repo ${CM_REPO} | cut -d\' \' -f3`\nFOLDER=`cm find repo mlcommons@ck | cut -d\' \' -f3`\ncd $FOLDER\necho ""Changed to $FOLDER""\n\nUSER=cTuning\nEMAIL=admin@ctuning.org\n\ngit config --global user.name ""$USER""\ngit config --global user.email ""$EMAIL""\ngit remote set-url origin https://x-access-token:${{ secrets.ACCESS_TOKEN }}@github.com/${{ github.repository }}\ngit add *.md\ngit diff-index --quiet HEAD || (git commit -am ""Updated docs"" && git push && echo ""Changes pushed"")\n']"
"['echo ""Name:      ${{ steps.buildx.outputs.name }}""\necho ""Endpoint:  ${{ steps.buildx.outputs.endpoint }}""\necho ""Status:    ${{ steps.buildx.outputs.status }}""\necho ""Flags:     ${{ steps.buildx.outputs.flags }}""\necho ""Platforms: ${{ steps.buildx.outputs.platforms }}""\n', 'echo ${{ steps.docker_build.outputs.digest }}', '(cd Resources/k8s/kustomize && yamllint .)', 'cp Resources/k8s/kustomize/base/secrets/postgres.env.sample Resources/k8s/kustomize/base/secrets/postgres.env\ncp Resources/k8s/kustomize/base/secrets/graphql.env.sample Resources/k8s/kustomize/base/secrets/graphql.env\ncp Resources/k8s/kustomize/base/config/pokeapi.env.sample Resources/k8s/kustomize/base/config/pokeapi.env\n', ""if [ ${GITHUB_REF#refs/heads/} = 'master' ]; then make kustomize-apply; else make kustomize-staging-apply; fi \nkubectl proxy &\nkubectl describe deployment\nbash Resources/scripts/wait.sh http://localhost:8001/api/v1/namespaces/pokeapi/services/pokeapi/proxy/api/v2/\n"", 'kubectl config set-context --current --namespace pokeapi\n', 'make k8s-migrate\nmake k8s-build-db\nbash Resources/scripts/wait.sh http://localhost:8001/api/v1/namespaces/pokeapi/services/pokeapi/proxy/api/v2/pal-park-area/5/\n', 'kubectl wait --timeout=120s --for=condition=complete job/load-graphql\nlast_command=$(kubectl get job -o jsonpath=\'{.status.succeeded}\' load-graphql)\ntest ""$last_command"" -eq 1\n']"
"['pip install --upgrade setuptools\npip install tox\n', 'tox', 'pip install tox\n', 'tox\n']"
"['python -m pip install --upgrade pip\npip install pytest\n', 'pytest\n']"
"['pip install -r requirements.txt', 'python runtests.py']"
"['python -m venv venv\n. venv/bin/activate\npip install -U pip\npip install -r requirements_dev.txt\npip install -e .\n', '. venv/bin/activate\n# stop the build if there are Python syntax errors or undefined names\nflake8 plexapi --count --select=E9,F63,F7,F82 --show-source --statistics\n# The GitHub editor is 127 chars wide\nflake8 plexapi --count --max-complexity=12 --max-line-length=127 --statistics\n', 'python -m venv venv\n. venv/bin/activate\npip install -U pip\npip install -r requirements_dev.txt\npip install -e .\n', 'mkdir -p ~/.cache/docker/${{ env.PLEX_CONTAINER }}\necho ""Image: ${{ env.PLEX_CONTAINER }}""\necho ""Tag: ${{ env.PLEX_CONTAINER_TAG }}""\ntoken=$(curl \\\n  --silent \\\n  ""https://auth.docker.io/token?scope=repository:${{ env.PLEX_CONTAINER }}:pull&service=registry.docker.io"" \\\n  | jq -r \'.token\')\ndigest=$(curl \\\n  --silent \\\n  --header ""Accept: application/vnd.docker.distribution.manifest.v2+json"" \\\n  --header ""Authorization: Bearer $token"" \\\n  ""https://registry-1.docker.io/v2/${{ env.PLEX_CONTAINER }}/manifests/${{ env.PLEX_CONTAINER_TAG }}"" \\\n  | jq -r \'.config.digest\')\necho ""Digest: $digest""\necho ""digest=$digest"" >> $GITHUB_OUTPUT\n', 'docker pull ${{ env.PLEX_CONTAINER }}:${{ env.PLEX_CONTAINER_TAG }}\ndocker save -o ~/.cache/docker/${{ env.PLEX_CONTAINER }}-${{ env.PLEX_CONTAINER_TAG }}.tar ${{ env.PLEX_CONTAINER }}:${{ env.PLEX_CONTAINER_TAG }}\necho ""Saved image: ${{ env.PLEX_CONTAINER }}:${{ env.PLEX_CONTAINER_TAG }}""\n', 'docker load -i ~/.cache/docker/${{ env.PLEX_CONTAINER }}-${{ env.PLEX_CONTAINER_TAG }}.tar\n', 'echo ""PLEXAPI_AUTH_SERVER_TOKEN=${{ secrets.PLEXAPI_AUTH_SERVER_TOKEN }}"" >> $GITHUB_ENV\n', '. venv/bin/activate\npython \\\n  -u tools/plex-bootstraptest.py \\\n  --destination plex \\\n  --advertise-ip 127.0.0.1 \\\n  --bootstrap-timeout 540 \\\n  --docker-tag ${{ env.PLEX_CONTAINER_TAG }} \\\n  --${{ matrix.plex }}\n', '. venv/bin/activate\npytest \\\n  -rxXs \\\n  --ignore=tests/test_sync.py \\\n  --tb=native \\\n  --verbose \\\n  --cov=plexapi \\\n  tests \n', '. venv/bin/activate\npython -u tools/plex-teardowntest.py\n', 'echo ""Failed to restore Python virtual environment from cache""\nexit 1\n', '. venv/bin/activate\ncoverage combine coverage-${{ matrix.plex }}*/.coverage*\ncoverage report --fail-under=50\ncoverage xml\n', 'pip install -U pip\npip install -r requirements.txt\npip install setuptools wheel\npython setup.py sdist bdist_wheel\n']"
""
"['conda config --prepend channels conda-forge\nconda config --set channel_priority strict\nconda create -n docs python=3.9 cython proj\nsource activate docs\npython -m pip install -e .\npython -m pip install -r requirements-docs.txt\nsphinx-build -b html docs/ docs/_build/\n', 'conda config --prepend channels conda-forge\nconda config --set channel_priority strict\nconda create -n sdist_env build twine cython proj=${{ env.PROJ_VERSION }}\nsource activate sdist_env\npython -m build --sdist\n', 'source activate sdist_env\ntwine check --strict dist/*\n', 'cd ""$VCPKG_INSTALLATION_ROOT""\ngit pull > nul\n./bootstrap-vcpkg.bat -disableMetrics\nvcpkg install --feature-flags=""versions,manifests"" --x-manifest-root=${GITHUB_WORKSPACE}/ci --x-install-root=$VCPKG_INSTALLATION_ROOT/installed\nmkdir -p ${GITHUB_WORKSPACE}/pyproj/proj_dir/share/proj\ncp ""$VCPKG_INSTALLATION_ROOT/installed/${{ matrix.triplet }}/share/proj/""* ${GITHUB_WORKSPACE}/pyproj/proj_dir/share/proj/\n', 'sudo apt-get update\nsudo apt-get install -qq sqlite3 libsqlite3-dev libtiff-dev libcurl4-openssl-dev cmake\nbash ci/proj-compile.sh git\n', 'python -V\npython -m pip install cython\npython -m pip install -e .\npython -m pip install -r requirements-test.txt\npyproj -v\n', 'python -m pytest --cov-report term-missing --cov=pyproj --cov-report xml\n', 'python -m pytest\n', 'python -m pytest\n', 'python -m pytest\n', '$PROJ_DIR/bin/projsync --quiet --bbox -175,0,-50,85\npython -m pytest\n', 'python -m pip install mypy types-certifi\n', 'mypy pyproj\n', 'apt-get update\napt-get -y install software-properties-common\nadd-apt-repository -y ppa:deadsnakes/ppa\napt-get update\n', 'apt-get install -y --no-install-recommends \\\n  python${{ matrix.python-version }} \\\n  python${{ matrix.python-version }}-dev \\\n  python${{ matrix.python-version }}-venv \\\n  python3-pip \\\n  g++\n', 'python${{ matrix.python-version }} -m venv testenv\n. testenv/bin/activate\npython -m pip install --upgrade pip\npython -m pip install -r requirements-dev.txt\npython -m pip install -e .\npython -m pip install -r requirements-test.txt\n', '. testenv/bin/activate\npython -m pytest --cov-report term-missing --cov=pyproj --cov-report xml\n', '. testenv/bin/activate\npython -m pytest\n', '. testenv/bin/activate\npython -m pytest\n', '. testenv/bin/activate\npython -m pytest\n', '. testenv/bin/activate\nprojsync --quiet --bbox -175,0,-50,85\npython -m pytest\n', 'conda config --prepend channels conda-forge\nconda config --set channel_priority strict\nexport INSTALL_DEPS=\'${{ matrix.python-implementation }}=${{ matrix.python-version }} cython proj=${{ matrix.proj-version }} numpy xarray pandas\'\nif [ ""${{ matrix.os }}"" = ""macos-latest"" -a ""${{ matrix.python-version }}"" = ""3.10"" ]; then\n  sed -i.bak \'/shapely/d\' requirements-test.txt;\nelse\n  export INSTALL_DEPS=""${INSTALL_DEPS} shapely"";\nfi;\nconda create -n test $INSTALL_DEPS\nsource activate test\npython -m pip install -e .\npython -m pip install -r requirements-test.txt\n', 'source activate test\npython -V\npyproj -v\nconda info\n', 'source activate test\npython -m pip install pylint\n', 'source activate test\npylint pyproj\n', 'source activate test\npython -m pytest --cov-report term-missing --cov=pyproj --cov-report xml\n', 'source activate test\npython -m pytest\n', 'source activate test\npython -m pytest\n', 'source activate test\npython -m pytest\n', 'source activate test\npython -m pytest\n', 'source activate test\nprojsync --quiet --bbox -175,0,-50,85\npython -m pytest\n', 'source activate test\npython -m pip install -r requirements-docs.txt\nsphinx-build -b html docs/ docs/_build/\nsphinx-build -b man docs/ docs/_build/\n']"
""
"['set -ex -o pipefail\nshopt -s lastpipe\ngit ls-remote https://github.com/GothenburgBitFactory/taskwarrior.git heads/""$TASK_VERSION"" | read -r TASK_DEVEL_SHA _ || :\n(\n  echo ALPINE_VERSION=""$ALPINE_VERSION""\n  echo PYTHON_VERSION=""$PYTHON_VERSION""\n  echo TASK_VERSION=""$TASK_VERSION"" ${TASK_DEVEL_SHA:+TASK_DEVEL_SHA=""$TASK_DEVEL_SHA""}\n  echo VIM_VERSION=""$VIM_VERSION""\n  echo VIMWIKI_VERSION=""$VIMWIKI_VERSION""\n  cat Dockerfile\n) | sha256sum | read -r tag _\ndocker login ""$DOCKER_REGISTRY"" -u ""$GITHUB_USER"" -p ""$GITHUB_TOKEN"" || :\ndocker pull ""$DOCKER_CACHE_IMAGE"":""$tag"" || :\nDOCKER_BUILDKIT=1 docker build \\\n  --build-arg BUILDKIT_INLINE_CACHE=1 \\\n  --cache-from ""$DOCKER_CACHE_IMAGE"":""$tag"" \\\n  ${ALPINE_VERSION:+--build-arg ALPINE_VERSION=""$ALPINE_VERSION""} \\\n  ${PYTHON_VERSION:+--build-arg PYTHON_VERSION=""$PYTHON_VERSION""} \\\n  ${TASK_VERSION:+--build-arg TASK_VERSION=""$TASK_VERSION""} \\\n  ${VIM_VERSION:+--build-arg VIM_VERSION=""$VIM_VERSION""} \\\n  ${VIMWIKI_VERSION:+--build-arg VIMWIKI_VERSION=""$VIMWIKI_VERSION""} \\\n  -t taskwiki_tests \\\n  .\ndocker tag taskwiki_tests ""$DOCKER_CACHE_IMAGE"":""$tag""\ndocker push ""$DOCKER_CACHE_IMAGE"":""$tag"" || :\n', 'make test PYTEST_FLAGS=""-n8""']"
"['python -m pip install --upgrade pip\npython -m pip install -r requirements/tests.txt\npython -m pip install -r requirements.txt\n', 'flake8', 'tox -r', 'python -m pip install -U pip\npython -m pip install -r requirements/build.txt\npython -m pip install -r requirements.txt\n', 'python -m build --sdist --wheel --outdir dist .\ntwine check dist/*\n']"
"['pip install -r docs/requirements.yml\npip install -e .\n', 'mkdocs build', 'mkdocs gh-deploy --force', 'conda info\nconda list\nconda config --show-sources\nconda config --show\n', 'python -m pip install . --no-deps\n', 'mypy opt_einsum\n', 'python -m pip install --upgrade pip\npython -m pip install black\n', 'set -e\nblack opt_einsum --check\n', 'conda info\nconda list\nconda config --show-sources\nconda config --show\n', 'python -m pip install . --no-deps\n', 'pytest -v --cov=opt_einsum opt_einsum/ --cov-report=xml\n', 'coverage report\n']"
"['make install', 'cp .env.example .env', 'echo $GOOGLE_CREDENTIALS_DATA > .google.json', 'make test-ci', 'LDFLAGS=`echo $(pg_config --ldflags)` make install', 'cp .env.example .env', 'make test', 'make install', 'cp .env.example .env', 'make test', ""echo '!**/*.html' >> .gitignore\nmake install\nlivemark build\n"", 'python -m pip install --upgrade pip\npip install setuptools wheel\n', 'python setup.py sdist bdist_wheel\n']"
"['python -m pip install --upgrade pip tomli\n', 'curl -sSL https://install.python-poetry.org | POETRY_HOME=""$HOME/.poetry"" python -\necho ""$HOME/.poetry/bin"" >> $GITHUB_PATH\n', 'export version_bump_rule=$(python .github/scripts/parse_bump_rule.py)\necho ""version_bump_rule: \'${version_bump_rule}\'""\npoetry version ""${version_bump_rule}""\nexport new_version=$(python .github/scripts/parse_version.py pyproject.toml)\necho ""new_version: \'${new_version}\'""\necho ""::set-output name=version::${new_version}""\n', 'export new_version=${{needs.get_new_version.outputs.version}}\necho ""Updating version to \'${new_version}\'""\npython .github/scripts/update_versions.py\n', 'python -m pip install --upgrade pip\npython -m pip install cibuildwheel\n', 'python -m cibuildwheel --output-dir wheelhouse\n', 'export new_version=${{needs.get_new_version.outputs.version}}\necho ""Updating version to \'${new_version}\'""\npython .github/scripts/update_versions.py\n', 'python -m pip install oldest-supported-numpy\npython setup.py sdist\n', 'export new_version=${{needs.get_new_version.outputs.version}}\necho ""Updating version to \'${new_version}\'""\npython .github/scripts/update_versions.py\n', 'export new_version=${{needs.get_new_version.outputs.version}}\ngit config user.name github-actions\ngit config user.email github-actions@github.com\ngit commit -m ""Update version for new release"" pyproject.toml setup.py src/quaternion/__init__.py\ngit tag -a ""v${new_version}"" -m ""Version ${new_version}""\ngit status\ngit push --follow-tags  # Will not trigger new workflow because it uses GITHUB_TOKEN\n']"
